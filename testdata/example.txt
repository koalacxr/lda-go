The increasing importance of non-coding RNA in biology and medicine has led to a growing interest in the problem of RNA 3-D structure prediction.
As is the case for proteins, RNA 3-D structure prediction methods require two key ingredients: an accurate energy function and a conformational sampling procedure.
Both are only partly solved problems.
Here, we focus on the problem of conformational sampling.
The current state of the art solution is based on fragment assembly methods, which construct plausible conformations by stringing together short fragments obtained from experimental structures.
However, the discrete nature of the fragments necessitates the use of carefully tuned, unphysical energy functions, and their non-probabilistic nature impairs unbiased sampling.
We offer a solution to the sampling problem that removes these important limitations: a probabilistic model of RNA structure that allows efficient sampling of RNA conformations in continuous space, and with associated probabilities.
We show that the model captures several key features of RNA structure, such as its rotameric nature and the distribution of the helix lengths.
Furthermore, the model readily generates native-like 3-D conformations for 9 out of 10 test structures, solely using coarse-grained base-pairing information.
In conclusion, the method provides a theoretical and practical solution for a major bottleneck on the way to routine prediction and simulation of RNA structure and dynamics in atomic detail.
Non-coding RNA is of crucial importance for the functioning of the living cell, where it plays key catalytic, regulatory and structural roles CITATION, CITATION.
Understanding the exact mechanisms behind these functions is therefore of great importance for both biology and medicine.
In many cases, this understanding requires knowledge of RNA structure in atomic detail.
However, determining the structure of an RNA molecule experimentally is typically a time consuming, expensive and difficult task CITATION.
Therefore, algorithms for RNA structure prediction have attracted much interest, initially with the main focus on predicting secondary structure.
Many noticeable advances have been made in the area of secondary structure prediction; most recently the introduction of statistical sampling had an important impact CITATION CITATION .
In the past years, an increasing number of relevant structures have become available, and much progress has been made in the understanding of the three dimensional structure of RNA.
The conformational space of RNA has been analyzed using methods inspired by the Ramachandran plot for proteins CITATION, CITATION, the RNA base pair interactions have been accurately classified CITATION, and the conformational space of the RNA backbone has been clustered into discrete recurring conformations CITATION, CITATION CITATION.
These new insights have led to several useful tools for modeling RNA 3-D structure CITATION, CITATION and significant advances in atomic resolution prediction have recently been reported CITATION, CITATION .
However, routine prediction of RNA 3-D structure still remains an important open problem, and with the growing gap between the number of known sequences and determined structures, the problem is becoming more and more pronounced.
The two key ingredients in algorithms for RNA 3-D structure prediction, namely an accurate energy function and a conformational sampling procedure CITATION, are both only partly solved problems.
Here, we focus on the latter problem.
The current state of the art in RNA conformational sampling is based on fragment assembly methods, which construct plausible conformations by stringing together short fragments obtained from experimental structures.
These methods have led to numerous important breakthroughs in the related fields of protein and RNA 3-D structure prediction in the last ten years CITATION CITATION.
Nonetheless, fragment assembly methods are not a panacea.
One of the problems associated with these methods is that they inherently discretize the continuous conformational space, and hence do not cover all relevant conformations CITATION.
This is problematic since the resolution of the conformational search procedure imposes limits on the energy function; the use of fine-grained energy terms requires continuous adjustments to the RNA's dihedral degrees of freedom, which fragment assembly methods cannot provide CITATION.
In other words, the shortcomings of the conformational sampling method need to be counteracted by tweaking the energy function.
Furthermore, full conformational detail is of great importance for a complete understanding of RNA catalysis, binding CITATION and dynamics CITATION .
Another fundamental problem with fragment assembly methods is their non-probabilistic nature, which makes their rigorous use in the framework of statistical physics problematic.
Particularly, it is currently impossible to ensure unbiased sampling in a Markov chain Monte Carlo framework using fragment assembly as a proposal function CITATION.
In other words, using a fragment library implies adding an inherently unknown additional term to the energy function CITATION.
This means that the unbiased simulation of the dynamics of an RNA molecule under the control of an all-atom empirical forcefield using fragment assembly methods is currently impossible.
For these reasons we have developed a new solution to the conformational sampling problem: a probabilistic model, called BARNACLE, that describes RNA structure in a natural, continuous space.
BARNACLE makes it possible to efficiently sample 3-D conformations that are RNA-like on a short length scale.
Such a model can be used purely as a proposal distribution, but also as an energy term enforcing realistic local conformations.
Imposing favorable long range interactions, such as hydrogen bonding between the bases, lies outside the scope of such a local model and is the task of a global energy function.
BARNACLE combines a dynamic Bayesian network CITATION, which suits the sequential nature of the RNA molecule, with directional statistics, a branch of statistics that is concerned with the representation of angular data.
The model is not only computationally attractive, but can also be rigorously interpreted in the language of statistical physics CITATION, CITATION, making it attractive from a theoretical viewpoint as well.
This approach is conceptually related to the probabilistic models of protein structure recently proposed by our group CITATION, CITATION.
However, the model presented here is clearly far from a trivial extension, as an RNA molecule has many more degrees of freedom than a protein; in the RNA backbone alone, there are 11 angles per residue CITATION, as opposed to two in proteins.
These many degrees of freedom combined with the limited number of experimentally determined RNA structures CITATION make this a particularly challenging statistical task for which a very different strategy was required.
In particular, the approach we used for proteins would in the case of RNA require the use of a probability density function on the 7-dimensional hypertorus, which poses a serious statistical and computational obstacle.
Below, we describe the probabilistic model in detail, and show that it captures the crucial aspects of local RNA structure.
We also demonstrate its usefulness in the context of RNA 3-D prediction, and end with an outlook on possible applications.
Knowledge of the Free Energy Landscape topology is the essential key to understanding many biochemical processes.
The determination of the conformers of a protein and their basins of attraction takes a central role for studying molecular isomerization reactions.
In this work, we present a novel framework to unveil the features of a Free Energy Landscape answering questions such as how many meta-stable conformers there are, what the hierarchical relationship among them is, or what the structure and kinetics of the transition paths are.
Exploring the landscape by molecular dynamics simulations, the microscopic data of the trajectory are encoded into a Conformational Markov Network.
The structure of this graph reveals the regions of the conformational space corresponding to the basins of attraction.
In addition, handling the Conformational Markov Network, relevant kinetic magnitudes as dwell times and rate constants, or hierarchical relationships among basins, completes the global picture of the landscape.
We show the power of the analysis studying a toy model of a funnel-like potential and computing efficiently the conformers of a short peptide, dialanine, paving the way to a systematic study of the Free Energy Landscape in large peptides.
Polymers and, more specifically, proteins, show complex behavior at the cellular system level, e.g. in protein-protein interaction networks CITATION, and also at the individual level, where proteins show a large degree of multistability: a single protein can fold in different conformational states CITATION CITATION.
As a complex system CITATION, CITATION, the dynamics of a protein cannot be understood by studying its parts in isolation, instead, the system must be analyzed as a whole.
Tools able to represent and handle the information of the entire picture of a complex system are thus necessary.
Complex network theory CITATION, CITATION has proved to be a powerful tool used in seemingly different biologically-related fields such as the study of metabolic reactions, ecological and food webs, genetic regulatory systems and the study of protein dynamics CITATION.
In this latter context, diverse studies have analyzed the conformational space of polymers and proteins making use of network representations CITATION CITATION, where nodes account of polymer conformations.
Additionally, some studies have tried to determine the common and general properties of these conformational networks CITATION, CITATION looking at magnitudes such as clustering coefficient, cyclomatic number, connectivity, etc. Recently, trying to decompose the network in modules corresponding to the free energy basins, the use of community algorithms over these conformational networks have been proposed CITATION.
Although this approach has opened a promising path for the analysis of Free Energy Landscapes, the community based description of the network leads to multiple characterizations of the FEL and thus it is difficult to establish a clear map from the communities found to the basins of the FEL.
A similar approach, commonly used to analyze the complex dynamics, is the construction of Markovian models.
Markovian state models let us treat the information of one or several trajectories of molecular dynamics as a set of conformations with certain transition probabilities among them CITATION, CITATION, CITATION.
Therefore, the time-continuous trajectory turns into a transition matrix, offering global observables as relaxation times and modes.
In CITATION CITATION the use of Markovian models is proposed with the aim of detecting FEL meta-stable states.
However, the above approaches to analyze FELs of peptides involves extremely large computational cost: either general community algorithms or large transition matrices.
Finally, other strategies to characterize the FEL that have successfully helped to understand the physics of biopolymers, are based on the study of the Potential Energy Surface CITATION, CITATION, CITATION CITATION.
The classical transition-state theory CITATION allows us to project the behavior of the system at certain temperature from the knowledge of the minima and transition states of the PES.
This approach entails some feasible approximations, such as harmonic approximation to the PES, limit of high damping, assumption of high barriers, etc. These approximations could be avoided working directly from the MD data.
In this article we make a novel study of the FEL capturing its mesoscopic structure and hence characterizing conformational states and the transitions between them.
Inspired by the approaches presented in CITATION, CITATION and CITATION, CITATION, we translate a dynamical trajectory obtained by MD simulations into a Conformational Markov Network.
We show how to efficiently handle the graph to obtain, through its topology, the main features of the landscape: conformers and their basins of attraction, dwell times, rate constants between the conformational states detected and a coarse-grained picture of the FEL.
The framework is shown and validated analyzing a synthetic funnel-like potential.
After this, the terminally blocked alanine peptide is studied unveiling the main characteristics of its FEL.
A current challenge is to develop computational approaches to infer gene network regulatory relationships based on multiple types of large-scale functional genomic data.
We find that single-layer feed-forward artificial neural network models can effectively discover gene network structure by integrating global in vivo protein:DNA interaction data with genome-wide microarray RNA data.
We test this on the yeast cell cycle transcription network, which is composed of several hundred genes with phase-specific RNA outputs.
These ANNs were robust to noise in data and to a variety of perturbations.
They reliably identified and ranked 10 of 12 known major cell cycle factors at the top of a set of 204, based on a sum-of-squared weights metric.
Comparative analysis of motif occurrences among multiple yeast species independently confirmed relationships inferred from ANN weights analysis.
ANN models can capitalize on properties of biological gene networks that other kinds of models do not.
ANNs naturally take advantage of patterns of absence, as well as presence, of factor binding associated with specific expression output; they are easily subjected to in silico mutation to uncover biological redundancies; and they can use the full range of factor binding values.
A prominent feature of cell cycle ANNs suggested an analogous property might exist in the biological network.
This postulated that network-local discrimination occurs when regulatory connections are explicitly disfavored in one network module, relative to others and to the class of genes outside the mitotic network.
If correct, this predicts that MBF motifs will be significantly depleted from the discriminated class and that the discrimination will persist through evolution.
Analysis of distantly related Schizosaccharomyces pombe confirmed this, suggesting that network-local discrimination is real and complements well-known enrichment of MBF sites in G1 class genes.
Hundreds of yeast RNAs are expressed in a cell cycle dependent, oscillating manner.
In both budding yeast and fission yeast, these RNAs cluster into four or five groups, each corresponding roughly to a phase of the cycle CITATION CITATION.
Large sets of phase-specific RNAs are also seen in animal and plant cells CITATION CITATION, arguing that an extensive cycling transcription network is a fundamental property of Eukaryotes.
The complete composition and connectivity of the cell cycle transcription network is not yet known for any eukaryote, and many components may vary over long evolutionary distances CITATION CITATION, CITATION, but some specific regulators are paneukaryotic, as are some of their direct target genes.
Coupled with experimental accessibility, this conservation of core components and connections make the yeast mitotic cycle an especially good test case for studies of network structure, function, and evolution.
To expose the underlying logic of this transcription network, a starting point is to decompose the cell cycle into its component phases and link the pertinent regulatory factors with their immediate regulatory output patterns, here in the form of phasic RNA expression.
One way to do this is to integrate multiple genome-wide data types that impinge on connection inference, including factor:DNA interaction data from chromatin IP studies, RNA expression patterns, and comparative genomic analysis.
This is appealing partly because these assays are genome-comprehensive and hypothesis-independent, so they can, in principle, reveal regulatory relationships not detected by classical genetics.
However, the scale and complexity of these datasets require new methods to discover and rank candidate connections, while also accommodating considerable experimental and biological noise.
Microarray RNA expression studies in budding yeast have identified 230 to 1,100 cycling genes, the upper number encompassing nearly a fifth of all yeast genes CITATION, CITATION, CITATION, CITATION.
Specifics of experimental design and methods of analysis contribute to the wide range in the number of genes designated as cycling, but there is agreement on a core set of nearly 200.
Yeast molecular genetic studies have established that transcriptional regulation is critical for controlling phase-specific RNA expression for some of these genes, though this does not exclude modulation and additional contributions from post-transcriptional mechanisms.
About a dozen Saccharomyces transcription factors have been causally associated with direct control of cell cycle expression patterns, including repressors, activators, co-regulators, and regulators that assume both repressing and activating roles, depending on context: Ace2, Fkh1, Fkh2, Mbp1, Mcm1, Ndd1, Stb1, Swi4, Swi5, Swi6, Yhp1, and Yox1.
These can serve as internal control true-positive connections.
Conversely, a majority of yeast genes have no cell cycle oscillatory expression, and true negatives can be drawn from this group.
A practical consideration is how well the behavior of a network is represented in critical datasets.
In this case, cells in all cell cycle phases are present in the mixed phase, exponentially growing yeast cultures used for the largest and most complete set of global protein:DNA interaction data so far assembled in functional genomics CITATION.
These data are further supported by three smaller studies of the same basic design CITATION CITATION.
This sets the cell cycle apart from many other transcription networks whose multiple states are either partly or entirely absent from the global ChIP data.
Equally important are RNA expression data that finely parse the kinetic trajectory for every gene across the cycle of budding yeast CITATION, CITATION and also in the distantly related fission yeast, S. pombe CITATION CITATION.
This combination of highly time-resolved RNA expression data and phase-mixed ChIP/array data can be used to assign protein:DNA interactions to explicit cell cycle phases, while evolutionary comparison with S. pombe highlight exceptionally conserved and presumably fundamental network properties.
Many prior efforts to infer yeast transcription network connections from genome-wide data CITATION CITATION, CITATION, CITATION were designed to address the global problem of finding connection patterns across the entire yeast transcriptome by using very large and diverse collections of yeast RNA, DNA, and/or chromatin immunoprecipitation data.
The present work focuses instead on a single cellular process and its underlying gene network, which represents a natural level of organization positioned between the single gene at one extreme and the entire interlocking community of networks that govern the entire cell.
To model regulatory factor:target gene behavior, we adapted neural networks to integrate global expression and protein:DNA interaction data.
Artificial neural networks are structural computational models with a long history in pattern recognition CITATION.
A general reason for thinking ANNs could be effective for this task is that they have some natural similarities with transcription networks, including the ability to create nonlinear sparse interactions between transcriptional regulators and target genes.
They have previously been applied to model relatively small gene circuits CITATION CITATION, though they have not, to our knowledge, been used for the problem of inferring network structure by integrating large-scale data.
We reasoned that a simple single-layer ANN would be well-suited to capture and leverage two additional known characteristics of eukaryotic gene networks.
First, factor binding in vivo varies over a continuum of values, as reflected in ChIP data, in vivo footprinting, binding site numbers and affinity ranges, and site mutation analyses.
These quantitative differences can have biological significance to transcription output by affecting cooperativity, background leaky expression or the lack of it, and the temporal sequencing of gene induction as factors become available or disappear.
This is quite different from a world in which binding is reduced to a simple two-state, present/absent call.
Neural networks are able to use the full range of binding probabilities in the dataset.
Second, ANNs can give weight and attention to structural features such as the persistent absence of specific factors from particular target groups of genes.
This negative image information is potentially important and not used by other methods applied to date CITATION, CITATION, CITATION, CITATION.
The inherent ability of ANNs to use these properties is a potential strength compared with algorithms that rest solely on positive evidence of factor:target binding or require discretization of binding measurements into a simplified bound/unbound call.
ANNs have been most famously used in machine learning as black boxes to perform classification tasks, in which the goal is to build a network based on a training dataset that will subsequently be used to perform similar classifications on new data of similar structure.
In these classical ANN applications, the weights within the network are of no particular interest, as long as the trained network performs the desired classification task successfully when extrapolating to new data.
ANNs are used here in a substantially different way, serving as structural models CITATION.
Specifically, we use simple feed-forward networks in which the results of interest are mainly in the weights and what they suggest about the importance of individual transcription factors or groups of factors for specifying particular expression outputs.
Here ANNs were trained to predict the RNA expression behavior of genes during a cdc28 synchronized cell cycle, based solely on transcription factor binding pattern, as measured by ChIP/array for 204 yeast factors determined in an exponentially growing culture CITATION.
The resulting ANN model is then interrogated to identify the most important regulator-to-target gene associations, as reflected by ANN weights.
Ten of the twelve major known transcriptional regulators of cell cycle phase-specific expression ranked at the very top of the 204-regulator list in the model.
The cell cycle ANNs were remarkably robust to a series of in silico mutations, in which binding data for a specific factor was eliminated and a new family of ANN models were generated.
Additional doubly and triply mutated networks correctly identified epistasis relationships and redundancies in the biological network.
This approach was also applied to two additional, independent cell cycle expression studies to illustrate generality across data platforms, and to probe how the networks might change under distinct modes of cell synchronization.
Analysis of the weights matrices from the resulting models shows that the neural nets take advantage of information about specifically disfavored or disallowed connections between factors and expression patterns, together with the expected positive connections for other factors, to assign genes to their correct expression outputs.
This led us to ask if there is a corresponding bias in the biological network against binding sites for specific factors in some expression families as suggested by the ANN.
We found that this is the case, in multiple sensu stricto yeast genomes relatively closely related to Saccharomyces cerevisiae, and also in the distantly related fission yeast S. pombe.
This appears to be a deeply conserved network architecture property, even though very few specific orthologous genes are involved.
A transcriptional regulatory network constitutes the collection of regulatory rules that link environmental cues to the transcription state of a cell's genome.
We recently proposed a matrix formalism that quantitatively represents a system of such rules and allows systemic characterization of TRS properties.
The matrix formalism not only allows the computation of the transcription state of the genome but also the fundamental characterization of the input-output mapping that it represents.
Furthermore, a key advantage of this pseudo-stoichiometric matrix formalism is its ability to easily integrate with existing stoichiometric matrix representations of signaling and metabolic networks.
Here we demonstrate for the first time how this matrix formalism is extendable to large-scale systems by applying it to the genome-scale Escherichia coli TRS.
We analyze the fundamental subspaces of the regulatory network matrix to describe intrinsic properties of the TRS.
We further use Monte Carlo sampling to evaluate the E. coli transcription state across a subset of all possible environments, comparing our results to published gene expression data as validation.
Finally, we present novel in silico findings for the E. coli TRS, including a gene expression correlation matrix delineating functional motifs; sets of gene ontologies for which regulatory rules governing gene transcription are poorly understood and which may direct further experimental characterization; and the appearance of a distributed TRN structure, which is in stark contrast to the more hierarchical organization of metabolic networks.
Complex regulatory networks control the transcription state of a genome and consequently the functional activity of a cell CITATION.
Even relatively simple unicellular organisms have evolved complicated networks of regulatory interactions, termed transcriptional regulatory networks, to respond to environmental stimuli CITATION, CITATION.
External signals known to impact transcription in microorganisms include carbon source, amino acid, and electron acceptor availability, pH level, and heat and cold stress CITATION CITATION.
Mapping the links between these environmental growth conditions through signaling networks and ultimately to the resulting transcriptional response is of primary interest in the study of cellular systems CITATION.
Consequently, reconstructions of the TRNs of model organisms are underway CITATION .
To effectively describe the interconnected functions of the regulated genes and associated regulatory proteins within a given TRN, we recently developed a formalism involving a regulatory network matrix called R CITATION.
The R matrix represents the components and reactions within a transcriptional regulatory system.
We illustrated how, by using the fundamental properties of linear algebra, this matrix formalism allows characterization of TRS properties and facilitates in silico prediction of the transcription state of the genome under any specified set of environmental conditions.
Importantly, as previously reported, the R matrix is distinct from existing approaches that use matrix formalisms and matrix algebra to analyze gene expression data, as it describes relationships governing gene transcription derived from experiments characterizing how specific inputs regulate the expression of individual genes.
In this way, the R matrix extends previous approaches for characterizing features of TRNs, including Boolean networks CITATION, CITATION CITATION, Bayesian networks CITATION, and stochastic equations CITATION.
By representing the regulatory rules in matrix form, we can characterize the fundamental subspaces of the matrix, which in turn uniquely represent properties of the TRS that the R matrix contains.
Furthermore, by using a pseudo-stoichiometric approach as discussed below, the R matrix representation of a TRN is consistent with, and thus easily integratable with, related approaches using stoichiometric matrices to computationally represent the reactions underlying metabolic and signaling networks CITATION CITATION .
To date, this approach for representing and analyzing TRSs has only been applied to relatively small systems, including the well-studied four-gene lac operon in Escherichia coli as well as a small 25-gene prototypic TRS CITATION.
Although these model systems have been useful for prototyping studies of the capabilities and behavior of the R matrix, a key unanswered question is how this approach scales to larger, more complex biological systems.
Here we present first steps toward this end by assembling the R matrix for the genome-scale E. coli TRN, for which regulatory relationships have been previously characterized CITATION and extensive experimental data are available CITATION, CITATION.
To our knowledge, the work that we present here represents the first R matrix-based model of a genome-scale TRS, and this work has enabled us to gain important insights into the behavior of the R matrix at a larger scale, challenges associated with the scale-up, as well as the underlying biology of E. coli transcriptional regulation.
Specifically, we derived R directly from a previously developed genome-scale model of E. coli in which transcriptional regulatory rules were overlaid on a constraint-based model of metabolism CITATION.
This integrated transcriptional regulatory-metabolic model is well-suited for these initial genome-scale R matrix efforts as Boolean regulatory relationships are already defined and the behavior of this model has been well-studied using constraint-based analyses CITATION, CITATION.
To validate our R matrix analysis, we compared the expression states that we predicted for various environmental growth conditions with available gene expression data.
We also explored the fundamental subspaces of a related matrix R representing the complete E. coli TRS to describe key systemic properties, including new hypotheses about network structure.
Ultimately, this work yields an understanding of how the E. coli transcriptional regulatory program functions as a whole and demonstrates the utility of the regulatory network matrix formalism in studying transcriptional regulatory systems at the genome scale moving forward.
The process of assigning a finite set of tags or labels to a collection of observations, subject to side conditions, is notable for its computational complexity.
This labeling paradigm is of theoretical and practical relevance to a wide range of biological applications, including the analysis of data from DNA microarrays, metabolomics experiments, and biomolecular nuclear magnetic resonance spectroscopy.
We present a novel algorithm, called Probabilistic Interaction Network of Evidence, that achieves robust, unsupervised probabilistic labeling of data.
The computational core of PINE uses estimates of evidence derived from empirical distributions of previously observed data, along with consistency measures, to drive a fictitious system M with Hamiltonian H to a quasi-stationary state that produces probabilistic label assignments for relevant subsets of the data.
We demonstrate the successful application of PINE to a key task in protein NMR spectroscopy: that of converting peak lists extracted from various NMR experiments into assignments associated with probabilities for their correctness.
This application, called PINE-NMR, is available from a freely accessible computer server.
The PINE-NMR server accepts as input the sequence of the protein plus user-specified combinations of data corresponding to an extensive list of NMR experiments; it provides as output a probabilistic assignment of NMR signals to sequence-specific backbone and aliphatic side chain atoms plus a probabilistic determination of the protein secondary structure.
PINE-NMR can accommodate prior information about assignments or stable isotope labeling schemes.
As part of the analysis, PINE-NMR identifies, verifies, and rectifies problems related to chemical shift referencing or erroneous input data.
PINE-NMR achieves robust and consistent results that have been shown to be effective in subsequent steps of NMR structure determination.
Labeling a set of fixed data with another representative set is the generic description for a large family of problems.
This family includes clustering and dimensionality reduction, an approach in which the original dataset is represented by a set of typically far lower dimension.
The representative set, often the parameter vector that signifies a set of data points, can be simply the cluster mean or may include additional parameters, such as the cluster diameter.
The labeling problem is important, because it is encountered in many applications involving data analysis, particularly where prior knowledge of the probability distributions is incomplete or lacking.
A challenging instance of the labeling problem arises naturally in nuclear magnetic resonance spectroscopy, which along with X-ray crystallography is one of the two major methods for determining protein structures.
Although NMR spectroscopy is not as highly automated as the more mature X-ray field, it has advantages over X-ray crystallography for structural studies of small proteins that are partially disordered, exist in multiple stable conformations in solution, exhibit weak interactions with ligands, or fail to crystallize readily CITATION, provided that the NMR signals can be assigned to specific atoms in the covalent structure of the protein.
The labeling problem known as the assignment problem, has been one of the major bottlenecks in protein NMR spectroscopy.
Protein NMR structure determination generally proceeds through a series of steps.
The usual approach is first to collect data used in determining backbone and aliphatic side chain assignments.
These assignments are then used to interpret data collected in order to determine interatomic or torsion angular constraints used in structure determination.
The front-end labeling process associates one or more NMR parameters with a physical entity ; the back-end labeling process associates NMR parameters with constraints that define or refine conformational states.
In reality, the distinction between front-end and back-end is artificial.
Strategies have been developed that use NOESY data for assignments CITATION, CITATION or for direct structure determination without assignments CITATION.
In addition, as demonstrated recently, structures of small proteins can be determined directly from assigned chemical shifts by a process that largely bypasses the back-end CITATION, CITATION.
Ideally, all available data should be used in a unified process that yields the best set of assignments and best structure consistent with experiment and with a probabilistic analysis that provides levels of confidence in the assignments and atomic coordinates.
The usual approach to the solution of the problem of assigning labels to subsets of peaks assembled from multiple sets of noisy spectra is to collect a number of multidimensional, multinuclear datasets.
After converting the time domain data to frequency domain spectra by Fourier transformation, peaks are picked from each spectrum for analysis.
Methods have been developed for automated peak picking or global analysis of spectra to yield models consisting of peaks with known intensity, frequency, phase, and decay rate or linewidth CITATION, CITATION.
In the ideal case, the resulting peak-lists identify combinatorial subsets of two or more covalently bonded nuclei by their respective frequencies.
These subsets must be assembled in a coherent way to best correspond to specific atoms in the amino acid sequence of the protein.
In practice, peak lists do not report on all nuclei, and noise peaks are commonplace.
In the examples analyzed here, the level of missing peaks varied between 9 percent and 38 percent, while the level of noise peaks varied between 10 percent and 135 percent.
The large number of false positives as well as false negatives typically present in the data result in an explosion of ambiguities during the assembly of subsets.
A common feature among prior approaches has been to divide the assignment of labels into a sequence of discrete steps and to apply varying methods at each step.
These steps typically include an assignment step CITATION CITATION, a secondary structure determination step CITATION CITATION, and a validation step CITATION.
The validation step, in which a discrete reliability measure indicates the possible presence of outliers, misassignments, or abnormal backbone chemical shift values, is sometimes omitted.
Other steps can be added, or steps can be split further into simpler tasks.
For example, backbone and side chain assignments frequently are carried out sequentially as separate processes.
Some approaches to sequence-specific assignment rely on a substantially reduced combinatorial set of input data by assuming a prior subset selection, e.g., prior spin system assembly CITATION, CITATION.
The specification of conformational states can be added as yet another labeling step.
For example, backbone dihedral angles can be specified on a grid as determined from chemical shifts CITATION, coupling constants and/or NOEs CITATION, or reduced dipolar couplings CITATION .
The NMR assignment problem has been highly researched, and is most naturally formulated as a combinatorial optimization problem, which can be subsequently solved using a variety of algorithms.
A 2004 review listed on the order of 100 algorithms and software packages CITATION, and additional approaches are given in a 2008 review CITATION.
Prior methods have included stochastic approaches, such as simulated annealing/Monte Carlo algorithms CITATION CITATION, genetic algorithms CITATION, exhaustive search algorithms CITATION, CITATION CITATION, heuristic comparison to predicted chemical shifts derived from homologous proteins CITATION, heuristic best-first algorithms CITATION CITATION, and constraint-based expert system that use heuristic best-first mapping algorithm CITATION.
Of these, the most established, as judged from BMRB entries that cite the assignment software packages used, are Autoassign CITATION and GARANT CITATION .
Similarly, a wide range of methods have been used to predict the protein secondary structural elements that play an important role in classifying proteins CITATION, CITATION.
Prior approaches to assigning a secondary structure label to each residue of the protein have included the method CITATION, the chemical shift index method CITATION, a database approach CITATION, an empirical probability-based method CITATION, a supervised machine learning approach CITATION, and a probabilistic approach that utilizes a local statistical potential to combine predictive potentials derived from the sequence and chemical shifts CITATION.
Recently, a fully automated approach to protein structure determination, FLYA, has been described that pipelines the standard steps from NMR spectra to structure and utilizes GARANT as the assignment engine CITATION.
The FLYA approach demonstrates the benefits of making use of information from each step in an iterative fashion to achieve a high number of backbone and side chain assignments.
Our goal is to implement a comprehensive approach that utilizes a network model rather than a pipeline model and relies on a probabilistic analysis for the results.
We reformulate the combinatorial optimization problem whereby each labeling configuration in the ensemble has an associated but unknown non-vanishing probability.
The PINE algorithm enables full integration of information from disparate steps to achieve a probabilistic analysis.
The use of probabilities provides the means for sharing and refining incomplete information among the current standard steps, or steps introduced by future developments.
In addition, probabilistic analysis deals directly with the multiple minima problem that arises in cases where the data does not support a single optimal and self-consistent state.
A common example is a protein that populates two stable conformational states.
The PINE-NMR package described here represents a first step in approaching the goal of a full probabilistic approach to protein NMR spectroscopy.
PINE-NMR accepts as input the sequence of the protein plus peak lists derived from one or more NMR experiments chosen by the user from an extensive list of possibilities.
PINE-NMR provides as output a probabilistic assignment of backbone and aliphatic side chain chemical shifts and the secondary structure of the protein.
At the same time, it identifies, verifies, and, if needed, rectifies, problems related to chemical shift referencing or the consistency of assignments with determined secondary structure.
PINE-NMR can make use of prior information derived independently by other means, such as selective labeling patterns or spin system assignments.
In principle, the networked model of PINE-NMR is extensible in both directions within the pipeline for protein structure determination : it can be combined with adaptive data collection at the front or with three-dimensional structure determination at the back end.
Such extensions should lead to a rapid and fully automated approach to NMR structure determination that would yield the structure most consistent with all available data and with confidence limits on atom positions explicitly represented.
In addition to its application to NMR spectroscopy, the PINE approach should be applicable to the unbiased classification of biological data in other domains of interest, such as systems biology, in which data of various types need to be integrated: genomics, proteomics, and metabolomics data collected as a function of time and environmental variables.
Trans/cis prolyl isomerisation is involved in several biological processes, including the development of numerous diseases.
In the HIV-1 capsid protein, such a process takes place in the uncoating and recruitment of the virion and is catalyzed by cyclophilin A. Here, we use metadynamics simulations to investigate the isomerization of CA's model substrate HAGPIA in water and in its target protein CypA.
Our results allow us to propose a novel mechanistic hypothesis, which is finally consistent with all of the available molecular biology data.
Proline trans/cis isomerization takes part in several fundamental biological processes, including protein folding CITATION CITATION, immune response CITATION, CITATION, ion channel gating CITATION and cellular signalling CITATION, CITATION, CITATION.
The process, which is also associated to the development of a variety of diseases including HIV-1 infection CITATION, CITATION, carcinogenesis CITATION and Alzheimer's CITATION, is catalyzed by prolyl isomerase enzymes CITATION, CITATION, CITATION .
The best characterized isomerization in vivo and in vitro occurs in the uncoating and recruitment of the human HIV-1 capsid protein in the virions CITATION, and it is catalyzed by cyclophilin A isomerase.
At the structural level, CypA features -helices flanking a beta-barrel, while CA is made of several -helices connected by loops.
In the X-ray structure of the complex CITATION, the targeted proline-containing backbone unit, is accommodated in a hydrophobic pocket of CypA.
Although the backbone unit is in trans conformation in the X-ray structure CITATION, NMR studies have shown that 45 percent of conformers are cis for CA-CypA R55A in aqueous solution CITATION.
The rather significant population of cis conformers could arise not only by the replacement of R55 with Ala, but also by crystal packing forces, different temperature conditions, along with different hydration and ionic strength in the two experiments.
Free energy calculations further support the hypothesis that CypA promotes a significant population of cis conformation CITATION, CITATION.
Thus, the cis population is likely to increase substantially from water where it is 10 percent CITATION to the complex in aqueous solution.
In vitro kinetic measurements show that CypA decreases the isomerization free energy barrier of modified substrate fragments in solution by 7 kcal/mol CITATION, CITATION.
Free energy calculations, based either on classical CITATION or quantum-mechanical/molecular mechanics CITATION, of the 6 aminoacids long substrate fragment point to a similar trend.
In addition, they suggest that H-bonding between the backbone unit of the targeted glycine-proline peptidyl bond and that of N102@CypA CITATION, CITATION, CITATION, as well as van der Waals interactions between substrate and the CypA hydrophobic pocket stabilize the transition state CITATION, CITATION, CITATION.
These hypotheses are consistent with the decrease of k cat/K m associated with the polymorphism of cyclophilins in 102 position and in the F60A, F113A and H126Q CypA mutants CITATION, CITATION.
NMR studies, along with computations, have further suggested that the motions of several CypA residues are linked to the enzymatic activity CITATION, CITATION CITATION.
Free energy calculations characterized a network of protein vibrations in CypA that are associated with its isomerase activity: flexible loops on the surface are connected to the active site by a network of hydrogen bonds CITATION, CITATION, CITATION .
In spite of the great progress in describing the catalytic process, key mechanistic issues need to be addressed.
Kinetic studies CITATION suggested that the overall process involves trans and cis forms in solution and in complex with the protein.
However, the studies so far consider mostly TS stabilization.
Most importantly, the current proposed mechanism cannot explain a plethora of molecular biology data.
These studies show that residues not involved in TS stabilization in the proposed mechanisms are important for the function, as their mutations cause a decrease of k cat/K m. Indeed, k cat/K m passes from 1.6 10 7 M 1 s 1 in the wild type to 1.6 10 4 M 1 s 1 by mutating the fully conserved R55 residue with Ala CITATION.
Although this mutation was proposed originally to destroy an H-bond stabilizing uniquely the TS CITATION, CITATION, such H-bond was subsequently ruled out in computational works CITATION, CITATION, CITATION and so far no functional role has been ascribed to R55.
In addition, the H54Q mutation, along with the I57V polymorphism, causes a decrease of k cat/K m, although these residues are not involved in TS stabilization .
Here we use molecular simulations to address these issues.
We calculate the free energy associated with the isomerisation of the 6 aminoacids long substrate fragment in water and in complex with CypA.
The free energy is calculated as a function of the two reaction coordinates and, which have been suggested to describe best the energetics of the process CITATION, as well as other pairs of different coordinates to cross-check our results.
We use here metadynamics CITATION, which has already been employed to predict the energetics of protein/peptide interactions CITATION.
The potential used is an effective one.
This choice allows a very efficient sampling because of its relatively small computational cost.
In spite of its limitations, CITATION this force field is expected to be relatively accurate to describe equilibrium conformations, which is a key aspect of our problem.
The accuracy of the force field for minima and transition states is assessed by comparing our results with first principle quantum chemistry calculations.
Based on this comparison, we find that the force-field biases on the energetics of the minima are negligible, whereas their influence on the barriers is more significant.
Therefore, here the free energy differences between minima are used to predict the relative populations of the equilibrium conformations, whilst the calculated barriers are only compared at the qualitative level to discriminate the most likely cis/trans isomerisation path.
The enzyme turns out to dramatically stabilize the populations of one specific cis conformer relative to the trans ones, which instead are by far the most stable in aqueous solution.
In addition, it lowers the free energy barrier of a specific, one way isomerization from trans to cis.
These findings allow us to propose a mechanistic hypothesis for the isomerization process that is consistent with all the available experimental data.
We perform a large-scale study of intrinsically disordered regions in proteins and protein complexes using a non-redundant set of hundreds of different protein complexes.
In accordance with the conventional view that folding and binding are coupled, in many of our cases the disorder-to-order transition occurs upon complex formation and can be localized to binding interfaces.
Moreover, analysis of disorder in protein complexes depicts a significant fraction of intrinsically disordered regions, with up to one third of all residues being disordered.
We find that the disorder in homodimers, especially in symmetrical homodimers, is significantly higher than in heterodimers and offer an explanation for this interesting phenomenon.
We argue that the mechanisms of regulation of binding specificity through disordered regions in complexes can be as common as for unbound monomeric proteins.
The fascinating diversity of roles of disordered regions in various biological processes and protein oligomeric forms shown in our study may be a subject of future endeavors in this area.
Many proteins and protein regions have been shown to be intrinsically disordered under native conditions; namely, they contain no or very little well-defined structure CITATION CITATION.
Intrinsically disordered proteins have been found in a wide scope of organisms and their disorder content was shown to increase with organism complexity CITATION CITATION.
Comparative analysis of the functional roles of disordered proteins suggest that they are predominantly located in the cell nucleus; are involved in transcription regulation and cell signaling; and also can be associated with the processes of cell cycle control, endocytosis, replication and biogenesis of cytoskeleton CITATION, CITATION .
IDPs have certain properties and functions that distinguish them from proteins with well-defined structures.
IDPs have no unique three-dimensional structure in an isolated state but can fold upon binding to their interaction partners CITATION, CITATION, CITATION CITATION.
Conformational changes upon binding in proteins with unstructured regions are much larger than those in structured proteins CITATION.
The conformations of disordered regions in a protein complex are determined not only by the amino acid sequences but also by the interacting partners CITATION, CITATION.
IDPs can have many different functions and can bind to many different partners using the same or different interfaces CITATION.
IDPs can accommodate larger interfaces on smaller scaffolds compared to proteins with well-defined structure CITATION, CITATION, CITATION.
IDPs typically have an amino acid composition of low aromatic content and high net charge as well as low sequence complexity and high flexibility CITATION, CITATION, CITATION.
Intrinsic disorder provides for a rapid degradation of unfolded proteins, thereby enabling a rapid response to changes in protein concentration CITATION.
Finally, intrinsic disorder offers an elegant mechanism of regulation through post-translational modifications for many cellular processes CITATION, CITATION .
Predictions of disorder in proteins take into account the characteristic features of unstructured proteins and have been shown to be rather successful, especially in the case of large regions.
According to the results of CASP7, the best prediction groups successfully identified 50 70 percent of the disordered residues with false positive rates from 3 percent to 16 percent CITATION.
Prediction methods aim to identify disordered regions through the analysis of amino acid sequences using mainly the physico-chemical properties of the amino acids CITATION, CITATION CITATION or evolutionary conservation CITATION, CITATION CITATION .
As protein interactions are crucial for protein function, the biological role of disordered proteins should also be studied in this context.
Indeed, folding of disordered proteins into ordered structures may occur upon binding to their specific partners CITATION, CITATION, CITATION CITATION which may allow disordered regions to structurally accommodate multiple interaction partners with high specificity and low affinity CITATION, CITATION CITATION.
Moreover, it has been shown that the binding mechanism, whether binding occurs between folded or unfolded chains, depends on the structural characteristics, interface properties, and degree of minimal frustration of monomers CITATION, CITATION.
Binding through unfolded or partially unfolded intermediates can provide a kinetic advantage through the fly-casting mechanism CITATION.
According to this mechanism a dimensionality reduction occurs when the folding of a disordered protein is coupled with binding, thereby speeding up the search for specific targets.
A database of continuous protein fragments has been compiled from the Protein Data Bank to include short protein chains bound to larger proteins CITATION, CITATION.
It has been argued that MORFs participate in the coupling of binding and folding, a hypothesis that was supported by the analysis of the composition and predicted disorder of MORF segments.
As a result of studying the subtle structural differences of the same proteins in different conditions and functional states, many so-called dual personality protein segments were found able to exist in both ordered and disordered states CITATION.
There is a continuous range between completely structured and completely disordered proteins in which intermediate cases are rather common CITATION : proteins that are disordered but compact, multi-domain proteins with disordered linkers, and ordered proteins with some local disorder.
Examples of proteins with intrinsically disordered regions which exhibit coupling between folding and binding have been described in the literature previously CITATION, CITATION, CITATION CITATION.
Nevertheless, the universality of this phenomenon and functional importance of many disordered regions remains unclear.
The question can be expanded further to how much intrinsic disorder do protein complexes contain and what is its functional importance?
To answer these questions we examine observed and predicted disorder in protein complexes and unbound proteins using a large-scale dataset of protein structures.
The atomic details of structures and the conserved binding mode analysis introduced earlier CITATION allow us to monitor changes happening on or near interaction interfaces and to infer their functional importance.
Hsp90 is a molecular chaperone essential for protein folding and activation in normal homeostasis and stress response.
ATP binding and hydrolysis facilitate Hsp90 conformational changes required for client activation.
Hsp90 plays an important role in disease states, particularly in cancer, where chaperoning of the mutated and overexpressed oncoproteins is important for function.
Recent studies have illuminated mechanisms related to the chaperone function.
However, an atomic resolution view of Hsp90 conformational dynamics, determined by the presence of different binding partners, is critical to define communication pathways between remote residues in different domains intimately affecting the chaperone cycle.
Here, we present a computational analysis of signal propagation and long-range communication pathways in Hsp90.
We carried out molecular dynamics simulations of the full-length Hsp90 dimer, combined with essential dynamics, correlation analysis, and a signal propagation model.
All-atom MD simulations with timescales of 70 ns have been performed for complexes with the natural substrates ATP and ADP and for the unliganded dimer.
We elucidate the mechanisms of signal propagation and determine hot spots involved in interdomain communication pathways from the nucleotide-binding site to the C-terminal domain interface.
A comprehensive computational analysis of the Hsp90 communication pathways and dynamics at atomic resolution has revealed the role of the nucleotide in effecting conformational changes, elucidating the mechanisms of signal propagation.
Functionally important residues and secondary structure elements emerge as effective mediators of communication between the nucleotide-binding site and the C-terminal interface.
Furthermore, we show that specific interdomain signal propagation pathways may be activated as a function of the ligand.
Our results support a conformational selection model of the Hsp90 mechanism, whereby the protein may exist in a dynamic equilibrium between different conformational states available on the energy landscape and binding of a specific partner can bias the equilibrium toward functionally relevant complexes.
Heat Shock Protein 90 is an essential ATPase directed molecular chaperone required for folding quality control, maturation and trafficking of client proteins CITATION CITATION.
Hsp90 represents a fundamental hub in protein interaction networks CITATION, CITATION, with key roles in many cellular functions.
Hsp90 oversees the correct maturation, activation and trafficking among specialized cellular compartments CITATION of a wide range of client proteins CITATION, CITATION, CITATION.
The functions of clients range from signal transduction to regulatory mechanisms and immune response CITATION.
Client proteins typically include numerous kinases, transcription factors and other proteins that serve as nodal points in integrating cellular responses to multiple signals CITATION.
Given its role at the intersection of fundamental cellular pathways, it is becoming increasingly clear that Hsp90 deregulation can be associated with many pathologies ranging from cancer to protein folding disorders and neurological diseases CITATION, CITATION.
Because of this role in disease development, pharmacological suppression of Hsp90 activity has become an area of very intense research, in molecular oncology in particular.
Targeted suppression of Hsp90 ATPase activity with a small molecule inhibitor, the benzoquinone ansamycin antibiotic 17-allylamino-17-demethoxygeldanamycin, and some of its derivatives CITATION, CITATION, has shown promising anticancer activity in preclinical models and has recently completed safety evaluation in humans CITATION.
Further clinical trials have also been initiated with other small molecules also used in drug combinations in various cancer types CITATION .
Hsp90 operates as a dimer in a complex cycle driven by ATP binding and hydrolysis and by ATP/ADP exchange.
Initial structural efforts concentrated on isolated, individual domains of human CITATION CITATION or yeast Hsp90 CITATION, CITATION, CITATION CITATION, the ER homologue Grp94 CITATION, CITATION or the Escherichia coli homologue, HtpG CITATION, CITATION.
The crystal structures of larger constructs have also been reported CITATION, CITATION.
The first X-ray crystal structures of full-length Hsp90 from yeast bound to the ATP mimic AMPPNP revealed a homodimeric structure in which the individual protomers have a twisted parallel arrangement CITATION.
Each protomer, in turn, is characterized by a modular architecture with three well-defined domains: an N-terminal regulatory Domain, responsible for ATP binding, a Middle Domain, which completes the ATPase site necessary for ATP hydrolysis and binds client proteins, and a C-terminal dimerization Domain which is required for dimerization CITATION.
The same global topology is shared by the ATP-bound states of the E.coli homolog HtpG CITATION and by the Endoplasmatic Reticulum paralog Grp94 CITATION.
Interestingly, crystal structures of the full-length constructs for Htpg or Grp94 in complex with either ADP or in the apo state showed substantially different conformations.
The HtpG apo state adopted an open structure in which each of the three domains exposed hydrophobic surface area, while in the ADP-bound form these hydrophobic surfaces clustered to form a more compact state CITATION .
Structural and biochemical studies of the solution state of Hsp90 and its complexes using small angle X-ray scattering CITATION have provided the first experimental evidence of a highly dynamic and stochastic nature of Hsp90, whereby the equilibrium between different conformational states of the molecular chaperone can be readily shifted to recruit a Hsp90 conformation that is suitable for efficient Cdc37 co-chaperone recognition.
More recent solution structure data obtained using SAXS, single particle cryo-electron microscopy and modeling approaches showed that the apo-Hsp90 dimer may be in equilibrium among different open, extended states, still preserving the constitutive dimerization provided by the CTDs, and that nucleotide binding may shift this equilibrium towards compact conformations CITATION CITATION.
In particular, SAXS data have revealed that the ADP-bound compact state of HtpG can be in equilibrium with an extended state, which could be significantly populated in the absence of crystal packing effects CITATION.
In contrast, crystal structures of AMPPNP and ADP-bound forms of the ER-paralog Grp94 showed that there is relatively little difference in conformation between the two nucleotide bound states in the crystal CITATION, representing extended structures.
Recent studies based on mutation analysis, cross-linking and electron microscopy CITATION, CITATION suggested that different, compact states can be accessed by Grp94 in the presence of ATP.
These studies have indicated that upon binding to a specific partner, functional states of Hsp90 can be recruited using the intrinsic conformational flexibility of Hsp90.
Although the exact mechanism of coupling between ATP-binding/hydrolysis and client protein folding is still unclear, the combination of X-ray structural observations and biochemical data supports a picture in which the chaperone undergoes conformational rearrangements bringing the two NTDs in close association in the ATP-bound state, but not in the ADP-bound or apo states.
This defines a conformational cycle that involves constitutive dimerization through the CTDs and transient, ATP-dependent dimerization of the NTDs in a molecular clamp mechanism.
In terms of intrinsic protein dynamics, the mechanism of conformational coupling to the ATPase cycle involves a tense, structurally rigid conformational state of Hsp90 upon ATP binding, whereas a subsequent hydrolysis to ADP leads to a more relaxed, structurally flexible state of Hsp90 CITATION, CITATION, CITATION.
Finally, in the nucleotide-free form, the dimer moves to an open state.
The crystal structures of the full-length dimer also highlight the remarkable flexibility of the ATP-lid, a segment composed of two helices and the intervening loop located immediately adjacent to the ATP binding site CITATION.
The lid is displaced from its position in the isolated Hsp90 NTD structure and folds over the nucleotide pocket to interact with the bound ATP yielding the closed conformation indicating its possible importance in the progression of the chaperone cycle.
These studies are reminiscent of the results from an H/D exchange mass spectrometry investigations on the human Hsp90 in solution, which showed that the co-chaperone and inhibitor binding to the NTD can induce conformational changes at the Hsp90 domain-domain interfaces CITATION.
Moreover, Frey and coworkers CITATION have shown that kinetic and equilibrium binding constants depend on the intrinsic conformational equilibrium of the Hsp90 obtained from different species, reflecting differential affinity and reactivity towards ATP.
The kinetic analysis of the ATPase cycle has suggested that during the ATPase cycle Grp94 may be predominantly in the open state.
In contrast in the yeast Hsp90 the open state is only populated to 20 percent and a closed structure is observed in the presence of nucleotides CITATION.
Hence, conformational transitions during the ATPase cycle are structurally similar for different Hsp90 proteins, while the energetic balance between individual steps may be species-dependent, which is manifested in differences in the binding kinetics CITATION.
Overall, the solution data have suggested that the molecular mechanism of the Hsp90 chaperone cycle can be more adequately described as a stochastic process, in which ATP binding can shift the intrinsic conformational equilibrium of Hsp90 between the open apo state, the ADP-bound compact and the ATP-bound, closed protein state seen in different crystal structures.
The most recent structural studies of the apo and nucleotide-bound conformations of the E. coli, yeast, and human Hsp90 homologs have further supported the existence of a universal three-state conformational cycle for Hsp90, consisting of open-apo, ATP-closed and ADP-compact nucleotide-stabilized states, whereby the intrinsic conformational equilibrium between these states can be highly-species dependent CITATION.
According to these results, the evolutionary pressure may act through thermodynamic stabilization of the functionally relevant Hsp90 conformations recruited from the conformational equilibrium, to ensure the adequate response to the presence of organism-specific co-chaperones and protein clients.
Importantly, ATP or ADP binding can shift the conformational equilibrium far away from the apo state for E. coli and yeast Hsp90, whereas the conformational equilibrium for human Hsp90 is largely dominated by the open form, even in the presence of the nucleotide binding.
Strikingly, this study has shown that nucleotide binding provides only small stabilization energy, thereby biasing, rather than determining, the occupancy of different conformational states existing in a dynamic equilibrium.
Overall, the intrinsic conformational flexibility of Hsp90 is critical to the molecular chaperon cycle, including structural adaptation to diversity of co-chaperones and client proteins CITATION.
Different steps in the cycle are accompanied by binding to different co-chaperone proteins with specific functions.
The Hop co-chaperone, for instance, arrests ATP hydrolysis and binds simultaneously to the Hsp70 molecular chaperone, coupling the two systems.
The Hop binding to Hsp90 involves interactions at both M-domains and CTD domains CITATION, CITATION stabilizing a conformation that is incompetent for ATP hydrolysis and N-terminal dimerization CITATION.
In contrast, the stress-regulated co-chaperone Aha1 substantially increases ATPase rates increasing Hsp90 chaperone activities CITATION.
In the case of binding to other co-chaperones, Cpr6 and Sba1, it was shown that ATP-binding and hydrolysis is required to ensure productive complex formation: interestingly, Sba1 binds to the NTD while Cpr6 binds to the CTD CITATION.
These observations suggest a role for the nucleotide in selecting and stabilizing different conformations of Hsp90, related to specific different functions in the chaperone cycle CITATION .
These crystallographic, cryo-EM, SAXS and three-dimensional single-particle reconstruction studies, applied to the isolated Hsp90 domains and full Hsp90 dimer in different species, have provided a wealth of novel insights into the molecular mechanism and function of Hsp90.
However, there are still a number of important unresolved problems concerning the atomic resolution understanding of the interplay between ligand binding and the global functional motions of the molecular chaperone.
We have recently performed computational studies of the Hsp90 conformational dynamics and analyzed at atomic resolution the effects of ligand binding on the energy landscape of the Hsp90 NTD by all-atom MD simulations.
MD simulations of Hsp90 NTD have been carried out for the apo protein and Hsp90 complexes with its natural ligands ATP, ADP, small molecule inhibitors, and peptides CITATION.
These simulations have clarified the role of ATP-lid dynamics, differences in local conformational changes and global flexibility, as well as the functional interplay between protein rigidity and entropy of collective motions depending on the interacting binding partners.
We have found that the energy landscape of the apo Hsp90 NTD may be populated by structurally different conformational states, featuring local conformational switching of the ATP-lid which is accessible on longer time scales.
The results of this study have suggested a plausible molecular model for understanding the mechanisms of modulation of molecular chaperone activities by binding partners.
According to this model, structural plasticity of the Hsp90 NTD can be exploited by the molecular chaperone machinery to modulate enhanced structural rigidity during ATP binding and increased protein flexibility as a consequence of the inhibitor binding.
CITATION .
The molecular basis of signal propagation mechanisms and inter-domain communication pathways in the Hsp90 as a function of binding ligands cannot be inferred directly from crystallographic studies.
As a result, computational approaches are instrumental in revealing the atomic details of inter-domain communication pathways between the nucleotide binding site and distant CTD, which may be involved in governing the chaperone equilibrium between major conformational states.
In this work, we have embarked on a comprehensive computational analysis of Hsp90 dynamics and binding which provides important insights into our understanding of the Hsp90 molecular mechanisms and function at atomic resolution.
We describe large-scale MD simulations to study the conformational motions and inter-domain communication pathways of the full-length yeast Hsp90 in three different complexes: with ATP, with ADP and in the apo form.
In support of the experimental hypotheses, our results provide atomic models of a cross-talk between N- and C-terminal binding sites that may induce an allosteric regulation of the complex molecular chaperone machinery.
These results of our study suggest that the low-resolution features of communication pathways in the Hsp90 complexes may be determined by the inherent topological architecture of the chaperone, yet specific signal communication pathways are likely to be selected and activated based on the nature of the binding partner.
A new method is presented for extraction of population firing-rate models for both thalamocortical and intracortical signal transfer based on stimulus-evoked data from simultaneous thalamic single-electrode and cortical recordings using linear multielectrodes in the rat barrel system.
Time-dependent population firing rates for granular, supragranular, and infragranular populations in a barrel column and the thalamic population in the homologous barreloid are extracted from the high-frequency portion of the recorded extracellular signals.
These extracted firing rates are in turn used to identify population firing-rate models formulated as integral equations with exponentially decaying coupling kernels, allowing for straightforward transformation to the more common firing-rate formulation in terms of differential equations.
Optimal model structures and model parameters are identified by minimizing the deviation between model firing rates and the experimentally extracted population firing rates.
For the thalamocortical transfer, the experimental data favor a model with fast feedforward excitation from thalamus to the layer-4 laminar population combined with a slower inhibitory process due to feedforward and/or recurrent connections and mixed linear-parabolic activation functions.
The extracted firing rates of the various cortical laminar populations are found to exhibit strong temporal correlations for the present experimental paradigm, and simple feedforward population firing-rate models combined with linear or mixed linear-parabolic activation function are found to provide excellent fits to the data.
The identified thalamocortical and intracortical network models are thus found to be qualitatively very different.
While the thalamocortical circuit is optimally stimulated by rapid changes in the thalamic firing rate, the intracortical circuits are low-pass and respond most strongly to slowly varying inputs from the cortical layer-4 population.
Following pioneering work in the 1970s by, e.g., Wilson and Cowan CITATION and Amari CITATION a substantial effort has been put into the investigation of neural network models, particularly in the form of firing-rate or neural field models CITATION.
Some firing-rate network models, in particular for the early visual system, have been developed to account for particular physiological data.
However, for strongly interconnected cortical networks, few mechanistic network models directly accounting for specific neurobiological data have been identified.
Instead most work has been done on generic network models and has focused on the investigation of generic features, such as the generation and stability of localized bumps, oscillatory patterns, traveling waves and pulses and other coherent structures, for reviews see Ermentrout CITATION or Coombes CITATION .
We here present a new method for identification of specific population firing-rate network models from extracellular recordings, apply the method to extract network models for thalamocortical and intracortical signal processing based on stimulus-evoked data from simultaneous single-electrode and multielectrode extracellular recordings in the rat somatosensory system, and analyze and interpret the identified firing-rate models using techniques from dynamical systems analysis.
Our study reveals large differences in the transfer function between thalamus and layer 4 of the barrel column, compared to that between cortical layers, and thus sheds direct light on how whisker stimuli is encoded in population firing-activity in the somatosensory system.
The derivation of biologically realistic, cortical neural-network models has generally been hampered by the lack of relevant experimental data to constrain and test the models.
Single electrodes can generally only measure the firing activity of individual neurons, not the joint activity of populations of cells typically predicted by population firing-rate models.
Kyriazi and Simons CITATION and Pinto et al. CITATION, CITATION thus developed models for the somatosensory thalamocortical signal transformation based on pooled data from single-unit recordings from numerous animals.
By contrast, multielectrode arrays provide a convenient and powerful technology for obtaining simultaneous recordings from all layers of the cerebral cortex, at one or more cortical locations CITATION.
The signal at each low-impedance electrode contact represents a weighted sum of the potential generated by synaptic currents and action potentials of neurons within a radius of a few hundred micrometers of the contact, where the weighting factors depend on the shape and position of the neurons, as well as the electrical properties of the conductive medium CITATION CITATION .
In the present paper we describe a new method for extraction of population firing-rate models for both thalamocortical and intracortical transfer on the basis of data from simultaneous thalamic single-electrode and cortical recordings using linear multielectrodes in the rat barrel system.
With so called laminar population analysis Einevoll et al. CITATION jointly modeled the low-frequency and high-frequency parts of such stimulus-evoked laminar electrode data to estimate the laminar organization of cortical populations in a barrel column, time-dependent population firing rates, and the LFP signatures following firing in a particular population.
These postfiring population LFP signatures were further used to estimate the synaptic connection patterns between the various populations using both current source density estimation techniques and a new LFP template-fitting technique CITATION .
Here we use the stimulus-evoked time-dependent firing rates for the cortical populations estimated using LPA, in combination with single-electrode recordings of the firing activity in the homologous barreloid in VPM, to identify population firing-rate models.
The models are formulated as nonlinear Volterra integral equations with exponentially decaying coupling kernels allowing for a mapping of the systems to sets of differential equations, the more common mathematical representation of firing-rate models CITATION, CITATION .
The population responses were found to increase monotonically both with increasing amplitude and velocity of the whisker flick CITATION, CITATION, CITATION.
A stimulus set varying both the whisker-flicking amplitude and the rise time was found to provide a rich variety of thalamic and cortical responses and thus to be well suited for distinguishing between candidate models.
The optimal model structure and corresponding model parameters are estimated by minimizing the mean-square deviation between the population firing rates predicted by the models and the experimentally extracted population firing rates.
A first focus is on the estimation of mathematical models for the signal transfer between thalamus and the layer-4 population, the population receiving the dominant thalamic input.
For this thalamocortical transfer our experimental data favors a model with fast feedforward excitation, a slower predominantly inhibitory process mediated by a combination of recurrent and feedforward interactions, and a mixed linear-parabolic activation function.
The identified thalamocortical circuits are seen to have a band-pass property, and in the frequency domain the largest responses for the layer-4 population is obtained for thalamic firing rates with frequencies around twenty Hz.
Very different population firing-rate models are identified for the intracortical circuits, i.e., the spread of population activity from layer 4 to supragranular and infragranular layers.
For the present experimental paradigm the extracted firing rates of the various cortical laminar populations are found to exhibit strong temporal correlations and simple feedforward models with linear or mixed linear-parabolic activation function are found to account excellently for the data.
The functional properties of the identified thalamocortical and intracortical network models are thus qualitatively very different: while the thalamocortical circuit is optimally stimulated by rapid changes in the thalamic firing rate, the intracortical circuits are low-pass and respond strongest to slowly varying inputs.
Preliminary results from this project were presented earlier in poster format CITATION .
Many important protein protein interactions are mediated by the binding of a short peptide stretch in one protein to a large globular segment in another.
Recent efforts have provided hundreds of examples of new peptides binding to proteins for which a three-dimensional structure is available but where no structure of the protein peptide complex is known.
To address this gap, we present an approach that can accurately predict peptide binding sites on protein surfaces.
For peptides known to bind a particular protein, the method predicts binding sites with great accuracy, and the specificity of the approach means that it can also be used to predict whether or not a putative or predicted peptide partner will bind.
We used known protein peptide complexes to derive preferences, in the form of spatial position specific scoring matrices, which describe the binding-site environment in globular proteins for each type of amino acid in bound peptides.
We then scan the surface of a putative binding protein for sites for each of the amino acids present in a peptide partner and search for combinations of high-scoring amino acid sites that satisfy constraints deduced from the peptide sequence.
The method performed well in a benchmark and largely agreed with experimental data mapping binding sites for several recently discovered interactions mediated by peptides, including RG-rich proteins with SMN domains, Epstein-Barr virus LMP1 with TRADD domains, DBC1 with Sir2, and the Ago hook with Argonaute PIWI domain.
The method, and associated statistics, is an excellent tool for predicting and studying binding sites for newly discovered peptides mediating critical events in biology.
Protein protein interactions are vital for all cellular processes, including signaling, DNA repair, trafficking, replication, gene-expression and metabolism.
These interactions can vary substantially in how they are mediated.
What perhaps most often comes to mind are interactions involving large interfaces, such as those inside the hemoglobin tetramer, however, many important protein interactions, particularly those that are transient, low-affinity or related to post-translational modification events like phosphorylation, are mediated by the binding of a globular domain in one protein to a short peptide stretch in another CITATION.
These stretches often reside in the non-globular and/or disordered parts of the proteome, including many of the disordered interaction hubs CITATION, CITATION, thus helping to explain many of the emerging functional roles for such regions.
Peptide regions binding to a common protein, or domain, often conform to a sequence pattern, or linear motif that captures the key features of binding CITATION.
For instance, SH3 domains bind PxxP motifs, WW domains bind PPxY or PPLP motifs, and SH2, 14-3-3 and PTB domains bind phosphorylated peptides CITATION.
Since they are generally held to be more chemically tractable than interactions involving larger interfaces, protein peptide interactions also represent an important new class of drug targets, and there are a growing number of small molecules that are designed to target them CITATION .
The discovery of new peptides and motifs mediating interactions has been of intense interest in recent years.
Several techniques have been developed to uncover new variants of peptides that bind to known partners.
For instance, phage display and peptide array technologies have been applied to uncover new peptide partners for many proteins or domains, including SH3 CITATION, WW CITATION and PDZ CITATION domains.
Several computational approaches have also been developed that use protein peptide complexes of known 3D structure to find additional peptides that are likely to bind, and recently, probabilistic interaction networks have been used to predict peptide regions corresponding to kinase substrate CITATION.
The common thread to all of these approaches is that they rely on prior knowledge of the type of peptide binding to a domain and often require further knowledge of the peptide binding site on the globular protein.
They are thus generally only effective for finding new variants of known peptides, and cannot directly uncover new protein peptide interaction types.
Protein protein docking is currently the only widely used technique that can be applied to this problem generally, however this approach has limited application for peptides longer than 4 residues largely owing to the high degree of flexibility that one must consider when docking a typical peptide of 5 10 residues or the need for a known peptide conformation which is only rarely available CITATION.
Moreover, docking methods are very sensitive to conformational changes and require very high-resolution structures to perform well.
Determining new protein peptide interaction types is problematic experimentally, mostly because it is difficult in advance to know the regions in larger proteins responsible for binding, necessitating painstaking experiments such as deletion mutagenesis coupled to binding assays.
To address this, several computational methods have been developed to discover new protein peptide-motif pairs using the principle of sequence over-representation in proteins with a common interacting partner CITATION CITATION.
These methods, together with much conventional work focused on understanding interactions, have identified or predicted hundreds of new peptide-motifs mediating interactions with particular protein domain families.
However, these discoveries rarely provide information about where the peptide binds the protein.
Knowing these details can suggest further experiments and help ultimately to design chemical modulators of the interaction.
Structures of protein peptide complexes for all newly discovered interactions will require substantial time to become available, though the rapid increase in structural data for single proteins means that very often 3D structures are available for at least part of a protein in isolation.
There is thus a widening gap between proteins of known structure that are known or predicted to bind to a particular peptide and available 3D complexes that would foster a deeper understanding of mechanism and afford the discovery of additional peptides.
Here we present a method that attempts to bridge this gap by predicting the binding site for peptides on protein surfaces.
We used a dataset of protein peptide complexes of known 3D structure extracted from the Protein Data Bank CITATION to define spatial position specific scoring matrices capturing preferences for how each amino acid binds to protein surfaces.
Three dimensional position specific scoring matrices have been used in the past to predict protein folding CITATION, to assess the quality of structural models CITATION or to predict the function of proteins based on the matches of these position specific scoring matrices to a new protein structure CITATION and to identify protein surface similarities CITATION.
However, to the best of our knowledge, they have not been used to predict interactions in this way.
For a new protein peptide pair, we identify candidate peptide binding sites by linking predicted sites for each residue on the protein surface according to peptide-deduced distance constraints.
We developed statistics to determine the confidence of a prediction to estimate whether or not a putative peptide binds.
When applied to a benchmark in a cross-validated fashion, we obtained excellent sensitivity and specificity, which allowed us to apply the approach to several new interactions, such as the interaction of the viral oncoprotein latent membrane protein 1 with the tumor necrosis factor receptor 1-associated death domain protein CITATION offering suggestions of binding sites for further investigation.
The determination of factors that influence protein conformational changes is very important for the identification of potentially amyloidogenic and disordered regions in polypeptide chains.
In our work we introduce a new parameter, mean packing density, to detect both amyloidogenic and disordered regions in a protein sequence.
It has been shown that regions with strong expected packing density are responsible for amyloid formation.
Our predictions are consistent with known disease-related amyloidogenic regions for eight of 12 amyloid-forming proteins and peptides in which the positions of amyloidogenic regions have been revealed experimentally.
Our findings support the concept that the mechanism of amyloid fibril formation is similar for different peptides and proteins.
Moreover, we have demonstrated that regions with weak expected packing density are responsible for the appearance of disordered regions.
Our method has been tested on datasets of globular proteins and long disordered protein segments, and it shows improved performance over other widely used methods.
Thus, we demonstrate that the expected packing density is a useful value with which one can predict both intrinsically disordered and amyloidogenic regions of a protein based on sequence alone.
Our results are important for understanding the structural characteristics of protein folding and misfolding.
Amyloid fibril formation is associated with an increase of structure content, which leads to fibrillar aggregation CITATION.
However, it should be noted that an increased level of the beta structure is a characteristic property of several different types of protein aggregates CITATION, CITATION.
In addition to proteins observed in amyloid diseases, recent studies have shown that diverse proteins not related to any amyloid disease can aggregate into fibrils under destabilizing conditions CITATION CITATION.
Normal proteins can become toxic when they undergo fibrillation CITATION.
There is no consensus about toxicity of the different states: small oligomers, large oligomers, protofilaments, protofibrils, filaments, mature fibrils, or amorphous aggregates.
Significant advancements in recent research have led to the discovery that the toxic species in the amyloid diseases may not be the fibrils themselves, but rather the pre-fibrillar aggregates CITATION.
A possible mechanism for toxicity of -synuclein protofibrils has been demonstrated CITATION.
It has been shown that protofibrils can form elliptical pores, like bacterial toxins, which can puncture cell membranes, resulting in cell death CITATION.
Therefore, the mechanism of amyloid formation is under intensive investigation.
Recognition of the factors that influence protein conformational changes and misfolding is one of the general fundamental problems, the solution to which will help us find effective treatments for amyloid illnesses.
The experimental observation that not all proteins are amyloidogenic and that specific continuous regions of amyloid-forming proteins are more amyloidogenic than others suggests that there is a sequence propensity for amyloid formation.
Moreover, the observation that some short peptides also can form amyloids implies that these segments, which usually are exposed to the environment, can nucleate the transition of native proteins into the amyloid state, and suggests that fibril formation is sequence-specific CITATION.
In the mechanism of amyloidogenesis for natively folded proteins such as 2-microglobulin and transthyretin, the partial unfolding observed is believed to be a prerequisite for the proteins' assembly into amyloid fibrils both in vitro and in vivo CITATION.
It has been suggested that residues with enhanced flexibility and solvent accessibility are important for the initiation of fibrillation CITATION.
This means that partial unfolding of the rigid native structure can provide a specific interface for the beginning of fibrillation.
Thus, to understand the molecular mechanism of amyloidosis, it is necessary to find factors that induce partial unfolding of proteins and subsequent amyloid fibril formation at or near physiological conditions.
Some intrinsically disordered proteins are involved in amyloid diseases.
This fact may indicate that disorder is a necessary condition for aggregation.
It has been shown that a very small change in the environment of such proteins often might cause their partial folding and aggregation CITATION.
Knowledge of characteristics that control the process of amyloid fibril formation is important for finding effective drugs for treatment of amyloid diseases.
Uversky and Fink in their review CITATION illustrate that protein fibrillogenesis requires a partially folded conformation .
The first high-resolution crystal of an amyloid fiber formed by a sequence-designed polypeptide has been obtained CITATION.
Recently, the atomic structure of the cross- spine CITATION for a seven-residue peptide segment from Sup35 was determined.
It is a double sheet, in which each sheet is formed from parallel segments stacked in register.
Side chains protruding from the two sheets form a dry, tightly self-complementing steric zipper that bonds the sheets.
Within each sheet, every segment is bound to two neighboring segments through stacks of both backbone and side-chain hydrogen bonds.
There are several computational methods for predicting a protein's propensity for amyloid fibril formation.
In the work of Fernandez et al. CITATION it was shown that a concentration of such defects as insufficient shielding of hydrogen bonds from water attack might yield an aggregation-induced nucleus.
But the analysis of these defects revealed that the extensive exposure of hydrogen bonds to water attack might be a necessary but not sufficient condition to imply a propensity for organized aggregation CITATION .
A computational algorithm has been suggested that detects the nonnative strand propensity of sequences by consideration of the relationships between protein local sequence and secondary structure in terms of tertiary contacts CITATION.
This algorithm detects sequences within the protein that are favorable for triggering amyloid fibril formation.
It is worthwhile to emphasize here that both algorithms for prediction of amyloidogenic properties of polypeptide chains that are considered above can be applied only to those proteins for which the three-dimensional structure is known.
Based on the physico chemical properties of aggregation sequences and a computational algorithm, a model was developed for predicting the aggregation rate for a broad range of polypeptide chains CITATION.
The model identifies aggregation sites within a protein and predicts the parallel or antiparallel organization of sheets in a fibril.
It should be noted, however, that the overpredictions of aggregation sites were not analyzed statistically.
On the other hand, there is a method for the prediction of amyloidogenic regions from amino acid sequence alone CITATION.
After the experimental investigation of the amyloidogenic properties of a model six-residue peptide and its mutants, the authors obtained a six-residue amyloidogenic pattern and used this pattern for the identification of amyloidogenic fragments in proteins CITATION.
This amyloidogenic pattern has been used to validate the premise that the amyloidogenicity of a protein is indeed localized in short protein stretches.
It has been demonstrated that the conversion of a soluble, non-amyloidogenic protein into an amyloidogenic-prone molecule can be triggered by a non-destabilizing six-residue amyloidogenic insertion in a particular structural environment.
Recently, a new method for identifying fibril-forming segments of proteins has been suggested CITATION.
This method is based on the threading of six-residue peptides through the known crystal structure of an amyloid fiber CITATION formed by the peptide from Sup35.
The putative prediction is accepted as a prediction if its energy evaluated with RosettaDesign is lower than the threshold energy.
It should be added that molecular dynamics can yield valuable information about the structural changes that arise at the atomic level upon the formation of amyloid fibrils CITATION CITATION, while such information is difficult to obtain experimentally.
Another interesting new method is based on sequence-specific interaction energies between pairs of protein fragments calculated from statistical analysis of the native folds of globular proteins CITATION.
This algorithm correctly predicts the positions of most aggregation-prone portions of some polypeptide chains.
The formation of a sufficient number of interactions is necessary to compensate for the loss of conformational entropy during the protein folding process.
Therefore, the structural uniqueness of native proteins is a result of the balance between the conformational entropy and the energy of residue interactions.
It seems that disordered regions in a protein chain do not have a sufficient number of interactions to compensate for the loss of conformational entropy that results from the formation of a globular state.
On the other hand, a large increase in the energy of interactions will lead to a loss of the unique structure because the strengthening of contact energy will speed up folding, but it is also likely to lead to erroneous folds .
It has been suggested that the lack of a rigid globular structure under physiological conditions might represent a considerable functional advantage for intrinsically disordered proteins, as their large plasticity allows them to interact efficiently with several different targets, as compared with a folded protein with limited conformational flexibility CITATION CITATION.
It has been shown that disordered regions are involved in DNA binding and other types of molecular recognition CITATION.
A large portion of the sequences of intrinsically disordered proteins contain segments of low complexity and high predicted flexibility CITATION CITATION.
It also has been indicated that a combination of low overall hydrophobicity and a large net charge represent a structural feature of intrinsically disordered proteins in comparison with small globular proteins CITATION, CITATION.
There are currently several widely used methods for prediction of disordered regions: GlobPlot CITATION, a simple propensity-based approach for evaluating the tendency of residues to be in a regular secondary structure; PONDR VL3H CITATION, which is able to distinguish experimentally verified disordered proteins from globular proteins by various machine learning approaches; DISOPRED CITATION, in which the definition of disorder is restrained to regions that are missing from X-ray structures but are specifically recognized by a support vector machine in the DISOPRED model; and IUPred CITATION, which assigns the order/disorder status to residues on the basis of their ability to form favorable pairwise contacts.
We were the first to our knowledge who used the number of contacts per residue as a parameter to distinguish folded and intrinsically disordered proteins CITATION.
We have extended our method to predict disordered regions and have made comparisons with the above-mentioned methods CITATION.
It has been demonstrated that our method is the best among widely used methods for the sets of proteins considered here.
Despite considerable efforts to understand the mechanism, it is still unclear what is responsible for amyloidogenic and disordered regions.
The goal of this work is to test our hypothesis about whether protein regions that possess expected strong packing density are responsible for the amyloidogenic properties of proteins, while regions with weak packing density simultaneously are responsible for the appearance of disordered regions.
We introduce a new parameter, namely mean packing density, which enables the prediction of both amyloidogenic and intrinsically disordered regions from protein sequence.
These findings support the concept that the occurrence of amyloidogenic and intrinsically disordered regions has a similar basis in different peptides and proteins.
Pitch is one of the most important features of natural sounds, underlying the perception of melody in music and prosody in speech.
However, the temporal dynamics of pitch processing are still poorly understood.
Previous studies suggest that the auditory system uses a wide range of time scales to integrate pitch-related information and that the effective integration time is both task- and stimulus-dependent.
None of the existing models of pitch processing can account for such task- and stimulus-dependent variations in processing time scales.
This study presents an idealized neurocomputational model, which provides a unified account of the multiple time scales observed in pitch perception.
The model is evaluated using a range of perceptual studies, which have not previously been accounted for by a single model, and new results from a neurophysiological experiment.
In contrast to other approaches, the current model contains a hierarchy of integration stages and uses feedback to adapt the effective time scales of processing at each stage in response to changes in the input stimulus.
The model has features in common with a hierarchical generative process and suggests a key role for efferent connections from central to sub-cortical areas in controlling the temporal dynamics of pitch processing.
Modelling the neural processing of pitch is essential for understanding the perceptual phenomenology of music and speech.
Pitch, one of the most important features of auditory perception, is usually associated with periodicities in sounds CITATION.
Hence, a number of models of pitch perception are based upon a temporal analysis of the neural activity evoked by the stimulus CITATION CITATION.
Most of these models compute a form of short-term autocorrelation of the simulated auditory nerve activity using an exponentially weighted integration time window CITATION CITATION.
Autocorrelation models have been able to predict the reported pitches of a wide range of complex stimuli.
However, choosing an appropriate integration time window has been problematic, and none of the previous models has been able to explain the wide range of time scales encountered in perceptual data in a unified fashion.
These data show that, in certain conditions, the auditory system is capable of integrating pitch-related information over time scales of several hundred milliseconds CITATION CITATION, while at the same time being able to follow changes in pitch or pitch strength with a resolution of only a few milliseconds CITATION, CITATION, CITATION CITATION.
Limits on the temporal resolution of pitch perception have also been explored by determining pitch detection and discrimination performance as a function of frequency modulation rate CITATION CITATION, the main conclusion being that the auditory system has a limited ability to process rapid variations in pitch.
The trade-off between temporal integration and resolution is not exclusive to pitch perception, but is a general characteristic of auditory temporal processing.
For instance, a long integration time of several hundred milliseconds is required to explain the way in which the detectability and perceived loudness of sounds increases with increasing sound duration CITATION, CITATION.
In contrast, much shorter integration times are necessary to explain the fact that the auditory system can resolve sound events separated by only a few milliseconds CITATION CITATION.
Therefore, it appears that the integration time of auditory processing varies with the stimulus and task.
Previously it was proposed that integration and resolution reflect processing in separate, parallel streams with different stimulus-independent integration times CITATION.
More recently, in order to reconcile perceptual data pertaining to temporal integration and resolution tasks, it was suggested that the auditory system makes its decisions based on multiple looks at the stimulus CITATION, using relatively short time windows.
However, to our knowledge no model has yet quantitatively explained the stimulus- and task-dependency of integration time constants.
Another major challenge for pitch modelling is to relate perceptual phenomena to neurophysiological data.
Functional brain-imaging studies strongly suggest that pitch is processed in a hierarchical manner CITATION, starting in sub-cortical structures CITATION and continuing up through Heschl's Gyrus on to the planum polare and planum temporale CITATION CITATION.
Within this processing hierarchy, there is an increasing dispersion in response latency, with lower pitches eliciting longer response latencies than higher pitches CITATION.
This suggests that the time window over which the auditory system integrates pitch-related information depends on the pitch itself.
However, no attempt has yet been made to explain this latency dispersion.
In this study, we present a unified account of the multiple time scales involved in pitch processing.
We suggest that top-down modulation within a hierarchical processing structure is important for explaining the stimulus-dependency of the effective integration time for extracting pitch information.
A highly idealized model, formulated in terms of interacting neural ensembles, is presented.
The model represents a natural extension of previous autocorrelation models of pitch in a form resembling a hierarchical generative process CITATION, CITATION, in which higher levels modulate the responses in lower levels via feedback connections.
Without modification, the model can account not only for a wide range of perceptual data, but also for novel neurophysiological data on pitch processing.
-Actinin is an actin crosslinking molecule that can serve as a scaffold and maintain dynamic actin filament networks.
As a crosslinker in the stressed cytoskeleton, -actinin can retain conformation, function, and strength.
-Actinin has an actin binding domain and a calmodulin homology domain separated by a long rod domain.
Using molecular dynamics and normal mode analysis, we suggest that the -actinin rod domain has flexible terminal regions which can twist and extend under mechanical stress, yet has a highly rigid interior region stabilized by aromatic packing within each spectrin repeat, by electrostatic interactions between the spectrin repeats, and by strong salt bridges between its two anti-parallel monomers.
By exploring the natural vibrations of the -actinin rod domain and by conducting bending molecular dynamics simulations we also predict that bending of the rod domain is possible with minimal force.
We introduce computational methods for analyzing the torsional strain of molecules using rotating constraints.
Molecular dynamics extension of the -actinin rod is also performed, demonstrating transduction of the unfolding forces across salt bridges to the associated monomer of the -actinin rod domain.
Cytoskeletal microfilament networks contribute to the mechanical stability of the cell by dynamically arranging and rearranging actin filaments for reinforcement.
The dynamic arrangement of actin filament requires actin filament crosslinking molecules such as -actinin.
-Actinin is a 200 kDa homodimer with three major structural motifs: the actin binding domain, the calmodulin homology domain, and the central rod domain CITATION.
Each monomer contains all three structural domains but the two monomers are arranged anti-parallel so that the two ABDs are at opposite ends of -actinin.
The arrangement of the two ABDs at opposite ends allows for -actinin to crosslink parallel actin filaments CITATION.
Actin filaments in the parallel arrangement are very dynamic; the actin filaments move laterally and horizontally in relationship to each other, and continuously bind and unbind -actinin crosslinking molecules CITATION.
Several cellular processes involving actin filament dynamic rearrangement and scaffolding by -actinin include: focal adhesion formation near membrane bound integrin molecules CITATION, cytokinesis and cytoplasmic dumping in the final stages of mitosis CITATION, CITATION, and z-disk formation and stabilization in muscle cells CITATION.
In order for -actinin to maintain its function as an actin filament scaffold in such a dynamic environment, the -actinin molecule must be partially flexible, meaning it must simultaneously be rigid and stable at some regions to resist external stress and be flexible at other regions to maintain binding in a dynamic environment CITATION CITATION .
Structure of the -actinin rod domain underlies the function of -actinin as a partially flexible actin filament crosslinker.
Each central rod domain monomer is 240 long and made up of 4 spectrin repeats connected by helical linkers CITATION, CITATION.
Other molecules with spectrin repeats include dystophin and utrophin.
The -actinin rod domain differs from the other spectrin family molecules by its shorter length, its more rigid helical linkers, and its dimerization CITATION.
The spectrin repeats structure of the rod domain contributes several vital characteristics to the -actinin rod domain: aromatic packing and hydrophobic residues within each repeat stabilize secondary structure CITATION ; acidic and basic surfaces on R1 and R4 confer strong dimerization interactions CITATION, Kd of 10 pM between monomers CITATION ; interaction of hydrophobic residues between R2 and R3 on both monomers and electrostatic interactions produce a coiled-coil homodimer conformation with a 12 degree bend and a 90 degree left handed twist CITATION.
Together these characteristics account for the rod domain maintaining both structural rigidity and flexibility.
The goal of this investigation is to understand the structural mechanisms of the partial flexibility of the -actinin rod domain.
The coiled-coil nature of the rod domain is an essential component of the rod domain structure.
Coiled-coils are the dominant conformation for fibrous proteins CITATION.
Most coiled-coils have a heptad conformation, with hydrophobic residues every seventh residue CITATION, CITATION.
The heptad conformation allows for hydrophobic insertion of one linker region into that of the other monomers by a knobs-into-holes mechanism CITATION.
The presence of heptad hydrophobic residues is common in coiled-coil structure but neither necessary nor sufficient CITATION, CITATION.
Coiled-coils with antiparallel dimers like the -actinin rod domain are stabilized mainly by electrostatic interactions between the monomers, and within the monomers CITATION.
In general the knobs-into-holes mechanism of coiled-coil conformation exists only when stabilized by electrostatic interactions CITATION.
The tendency of electrostatic interactions to play a key role in stabilizing coiled-coil dimers like -actinin is in contrast to globular proteins, where hydrophobic, VDW, and electrostatic interactions are equally significant to molecular stability CITATION.
The coiled-coil conformation of the rod domain is a significant structural feature, and the significance of electrostatic interactions to coiled-coil structure stability suggests a significant role of electrostatic interactions in mechanical properties of the -actinin rod domain.
Several studies have examined the mechanical properties of other molecules with rod-like coiled-coil conformations.
These studies on DNA CITATION CITATION, myosin CITATION CITATION, and keratin CITATION, CITATION together suggest the coiled-coil rod like structure contributes extensible rigidity and torsional and bending flexibility.
The tertiary structure of DNA is referred to as coiled-coil, and more commonly as a double-helix, because it consists of two intertwined -helices.
In contrast, -actinin and other fibrous proteins are referred to as coiled-coil due to intertwining in their quaternary structures.
The difference between the DNA coiled-coil conformation and the protein coiled-coil conformation is significant, but the mechanical properties can still be compared.
DNA is the most studied of the coiled-coil conformations and has been described as an elastic rod CITATION.
Its global mechanical behavior has been described as like a thin isotropic homogeneous rod, but its local mechanical behavior has been described as like an anisotropic heterogeneous rod with bending and torsional flexibility CITATION CITATION.
Myosin has an S2 region that functions as a lever arm in muscle sarcomeres.
Using a single molecule assay in a total internal reflection microscopy experiment CITATION, it has been shown that the S2 region has significant torsional flexibility underlying its lever arm function.
Keratin, the first coiled-coil structure to be discovered CITATION, is the major molecule in hair fibers, and investigation of its mechanical behavior with molecular dynamics has shown that it has strong stretching rigidity, over 1 nN of force is needed to stretch keratin 90 percent CITATION.
Removing the electrostatic interactions underlying the coiled-coil conformation of keratin significantly reduces its rigidity CITATION.
These studies suggest that the coiled-coil conformation in -actinin contributes extension rigidity but torsional and bending flexibility.
Studies of -actinin and other spectrin repeat molecules have similarly demonstrated extension rigidity of the coiled-coil rod domain CITATION CITATION.
Experimental investigation using atomic force microscopy of spectrin unfolding demonstrated that spectrin repeats unfold in a cooperative mechanism CITATION.
Several molecular dynamics investigations further characterize the extension rigidity of the -actinin rod domain as resulting from the strength of the helical linker between the spectrin repeats, and electrostatic and hydrogen bonding within each repeat CITATION CITATION.
There has been no investigation of the bending or torsional flexibility of the -actinin rod domain or other spectrin repeats, but investigation of -actinin structure using cryoelectron microscopy has shown that there must be some structural flexibility since -actinin molecules form stable actin filaments crosslinks in a range of crosslinking angles CITATION.
Is the flexibility of -actinin in crosslinking actin filaments due to torsional and bending flexibility of the rod domain?
What features of the coiled-coil structure of -actinin underlie its partial flexibility?
Using molecular dynamics and normal mode analysis this study investigates the mechanical partial flexibility of the -actinin rod domain.
Bending, torsion and extension simulations demonstrate that, as with other coiled-coil molecules, the -actinin rod domain has bending and torsional flexibility and extensional rigidity.
Normal mode analysis shows that the rod-like structure of -actinin contributes towards its bending and torsional flexibility.
Our simulations suggest that aromatic packing interactions determine the trajectory of torsion on the rod domain, and that electrostatic interaction between the monomers contributes extension rigidity to the rod domain.
The mature human brain is organized into a collection of specialized functional networks that flexibly interact to support various cognitive functions.
Studies of development often attempt to identify the organizing principles that guide the maturation of these functional networks.
In this report, we combine resting state functional connectivity MRI, graph analysis, community detection, and spring-embedding visualization techniques to analyze four separate networks defined in earlier studies.
As we have previously reported, we find, across development, a trend toward segregation between regions close in anatomical space and integration between selected regions distant in space.
The generalization of these earlier trends across multiple networks suggests that this is a general developmental principle for changes in functional connectivity that would extend to large-scale graph theoretic analyses of large-scale brain networks.
Communities in children are predominantly arranged by anatomical proximity, while communities in adults predominantly reflect functional relationships, as defined from adult fMRI studies.
In sum, over development, the organization of multiple functional networks shifts from a local anatomical emphasis in children to a more distributed architecture in young adults.
We argue that this local to distributed developmental characterization has important implications for understanding the development of neural systems underlying cognition.
Further, graph metrics are similar in child and adult graphs, with both showing small-world -like properties, while community detection by modularity optimization reveals stable communities within the graphs that are clearly different between young children and young adults.
These observations suggest that early school age children and adults both have relatively efficient systems that may solve similar information processing problems in divergent ways.
The mature human brain is both structurally and functionally specialized, such that discrete areas of the cerebral cortex perform distinct types of information processing.
These areas are organized into functional networks that flexibly interact to support various cognitive functions.
Studies of development often attempt to identify the organizing principles that guide the maturation of these functional networks.
CITATION CITATION .
A major portion of the work investigating the nature of functional human brain development is based on results from functional magnetic resonance imaging studies.
By examining the differences in the fMRI activation profile of a particular brain region between children, adolescents, and adults, the developmental trajectory of that region's involvement in a cognitive task can be outlined CITATION, CITATION, CITATION CITATION.
These experiments have been crucial to our current understanding of typical and atypical brain development.
In addition to fMRI activation studies, the relatively new and increasingly utilized method of resting state functional connectivity MRI allows for a complementary examination of the functional relationships between regions across development.
Resting state fcMRI is based on the discovery that spontaneous low-frequency blood oxygen level dependent signal fluctuations in sometimes distant, but functionally-related grey matter regions, show strong correlations at rest CITATION.
These low frequency BOLD fluctuations appear to relate to spontaneous neural activity CITATION CITATION.
In effect, rs-fcMRI evaluates regional interactions that occur when a subject is not performing an explicit task CITATION, CITATION, CITATION CITATION.
To date, rs-fcMRI has been used in several domains to examine systems-level organization of motor CITATION, memory CITATION, CITATION, attention CITATION, and task control systems CITATION, CITATION, CITATION .
In addition, because rs-fcMRI does not require active engagement in a behavioral task, it unburdens experimental design, subject compliance, and training demands.
Thus, rs-fcMRI is becoming a frequently used tool for examining changes in network structure in disease CITATION CITATION, in aging CITATION, CITATION, and across development CITATION, CITATION CITATION .
In previous work regarding task-level control in adults, we applied rs-fcMRI to a set of regions derived from an fMRI meta-analysis that included studies of control-demanding tasks.
This analysis revealed that brain regions exhibiting different combinations of control signals across many tasks are grouped into distinct fronto-parietal and cingulo-opercular functional networks CITATION, CITATION.
Based on functional activation profiles of these regions characterized in the previous fMRI study, the fronto-parietal network appears to act on a shorter timescale, initiating and adjusting top-down control.
In contrast, the cingulo-opercular network operates on a longer timescale providing set-initiation and stable set-maintenance for the duration of task blocks CITATION .
Along with these two task control networks CITATION, CITATION, a set of cerebellar regions showing error-related activity across tasks CITATION formed a separate cerebellar network.
In adults, the cerebellar network is functionally connected with both the fronto-parietal and cingulo-opercular networks CITATION, CITATION.
These functional connections may represent the pathways involved in task level control that provide feedback information to both control networks CITATION, CITATION .
Another functional network, and one of the most prominent sets of regions to be examined with rs-fcMRI, is the default mode network.
The default mode network was first characterized by a consistent decrease in activity during goal-directed tasks compared to baseline CITATION, CITATION.
Resting-state fcMRI analyses have repeatedly shown that these regions, along with associated medial temporal regions, are correlated at rest in adults CITATION, CITATION, CITATION, CITATION.
While the distinct function of the default mode network is often linked to internally directed mental activity CITATION, this notion continues to be debated CITATION, CITATION, CITATION CITATION .
Transcription factor regulation is often post-translational.
TF modifications such as reversible phosphorylation and missense mutations, which can act independent of TF expression level, are overlooked by differential expression analysis.
Using bovine Piedmontese myostatin mutants as proof-of-concept, we propose a new algorithm that correctly identifies the gene containing the causal mutation from microarray data alone.
The myostatin mutation releases the brakes on Piedmontese muscle growth by translating a dysfunctional protein.
Compared to a less muscular non-mutant breed we find that myostatin is not differentially expressed at any of ten developmental time points.
Despite this challenge, the algorithm identifies the myostatin smoking gun through a coordinated, simultaneous, weighted integration of three sources of microarray information: transcript abundance, differential expression, and differential wiring.
By asking the novel question which regulator is cumulatively most differentially wired to the abundant most differentially expressed genes?
it yields the correct answer, myostatin.
Our new approach identifies causal regulatory changes by globally contrasting co-expression network dynamics.
The entirely data-driven weighting procedure emphasises regulatory movement relative to the phenotypically relevant part of the network.
In contrast to other published methods that compare co-expression networks, significance testing is not used to eliminate connections.
Evolution, normal development, immune responses and aberrant processes such as diseases and cancer all involve at least some rewiring of regulatory circuits CITATION CITATION.
Indeed it is the subtle differences in circuit wiring that makes each individual unique.
The key nodes in regulatory circuits are frequently transcription factors CITATION.
Thus, there is a great deal of interest in developing methods for decoding TF changes.
Regulator-target interactions can be assessed by ChIP-on-chip but this requires large amounts of homogenous starting material and TF-specific reagents.
Furthermore, the recruitment of a TF to a promoter does not necessarily correlate with transcriptional status, so biological interpretation can be complex CITATION.
Likely sites of key regulatory mutations can be revealed by Whole Genome Scans but this approach requires large numbers of individuals and very dense SNP panels.
Even so, the exact causal gene may remain ambiguous if there are several genes near the marker.
In any case, little insight is gained into the underlying regulatory mechanisms.
In order to gain further insights into the regulatory apparatus, computational approaches are continuously being proposed.
To date, they all operate by integrating information from multiple levels of biological organisation particularly eQTL, protein-protein interaction and TF binding site data CITATION CITATION .
Identifying regulatory change solely through contrasts in gene expression data has been elusive because TF tend to be stably expressed at baseline levels CITATION close to the sensitivity of standard high-throughput expression profiling platforms.
Further, TF activation is often regulated post-translationally and thereby can act somewhat independently of expression level.
Biologically important common TF activation processes are poorly detected by conventional differential expression analysis.
We hypothesised that a system-wide network approach might have utility, on the grounds that while a differentially-regulated TF might not be DE between two systems, its new position in the network of the perturbed system might allow detection of the smoking gun.
To allow reliable evaluation of such a hypothesis a well-defined experimental model system is required.
Piedmontese cattle are double-muscled because they possess a genomic DNA mutation in the myostatin mRNA transcript CITATION.
The resulting dysfunctional myostatin protein is a transcriptional regulator that releases the brakes on muscle growth reflecting the importance of TGF- signalling pathways in the determination of final muscle mass and fibre composition CITATION, CITATION.
A preliminary analysis of the expression of myostatin in Piedmontese Hereford versus Wagyu Hereford animals found that DE of myostatin was not detectable using cDNA-based expression microarrays CITATION, CITATION .
Thus we have a system in which we know the identity of the gene containing the causal mutation, myostatin, but we cannot identify it by DE of the mRNA in muscle samples.
By contrasting the muscle transcriptomes of the Piedmontese and Wagyu crosses across 10 developmental time points, our aim was to establish the question to which myostatin is the answer.
In other words, what question do we need to ask of the gene expression data for it to reveal the identity of the transcriptional regulator containing the causal mutation?
We study the evolution of cooperation under indirect reciprocity, believed to constitute the biological basis of morality.
We employ an evolutionary game theoretical model of multilevel selection, and show that natural selection and mutation lead to the emergence of a robust and simple social norm, which we call stern-judging.
Under stern-judging, helping a good individual or refusing help to a bad individual leads to a good reputation, whereas refusing help to a good individual or helping a bad one leads to a bad reputation.
Similarly for tit-for-tat and win-stay-lose-shift, the simplest ubiquitous strategies in direct reciprocity, the lack of ambiguity of stern-judging, where implacable punishment is compensated by prompt forgiving, supports the idea that simplicity is often associated with evolutionary success.
Many biological systems employ cooperative interactions in their organization CITATION.
Humans, unlike other animal species, form large social groups in which cooperation among non-kin is widespread.
This contrasts with the general assumption that the strong and selfish individuals are the ones who benefit most from natural selection.
This being the case, how is it possible that unselfish behaviour has survived evolution?
Adopting the terminology resulting from the seminal work of Hamilton, Trivers, and Wilson CITATION CITATION, an act is altruistic if it confers a benefit b to another individual in spite of accruing a cost c to the altruist.
In this context, several mechanisms have been invoked to explain the evolution of altruism, but only recently an evolutionary model of indirect reciprocity has been developed by Nowak and Sigmund CITATION, which, with remarkable simplicity, addressed unique aspects of human sociality, such as trust, gossip, and reputation CITATION.
As a means of community enforcement, indirect reciprocity had been investigated earlier in the context of economics, notably by Sugden CITATION and Kandori CITATION.
More recently, many studies CITATION, CITATION, CITATION CITATION have been devoted to investigating how altruism can evolve under indirect reciprocity.
Indeed, according to Alexander CITATION, indirect reciprocity presumably provides the mechanism that distinguishes us humans from all other living species on Earth.
Moreover, as recently argued in CITATION, indirect reciprocity may have provided the selective challenge driving the cerebral expansion in human evolution.
In the indirect reciprocity game, any two players are supposed to interact at most once with each other, one in the role of a potential donor, with the other as a potential receiver of help.
Each player can experience many rounds, but never with the same partner twice, direct retaliation being unfeasible.
By helping another individual, a given player may increase her reputation, which may change the predisposition of others to help her in future interactions.
However, her new reputation depends on the social norm used by her peers to assess her action as a donor.
Previous studies of reputation-based models of cooperation reviewed recently CITATION indicate that cooperation outweighs defection whenever, among other factors, assessment of actions is based on norms that require considerable cognitive capacities CITATION, CITATION, CITATION, even when individuals are capable of making binary assessments only, in a world in black and white CITATION, as assumed in most recent studies.
Furthermore, stable cooperation hinges on the availability of reliable reputation information CITATION.
Such high cognitive capacity contrasts with technology-based interactions, such as e-trade, which also rely on reputation-based mechanisms of cooperation CITATION CITATION.
Indeed, anonymous one-shot interactions between individuals loosely connected and geographically dispersed usually dominate e-trade, raising issues of trust-building and moral hazard CITATION.
Reputation in e-trade is introduced via a feedback mechanism which announces the ratings of sellers.
Despite the success and high levels of cooperation observed in e-trade, it has been found CITATION that publicizing a detailed account of the seller's feedback history does not improve cooperation, as compared with publicizing only the seller's most recent rating.
In other words, practice shows that simple reputation-based mechanisms are capable of promoting high levels of cooperation.
In view of the previous discussion, it is hard to explain the success of e-trade on the basis of the results obtained so far for reputation-based cooperation in the context of indirect reciprocity.
Let us consider a world in black and white consisting of a set of tribes, such that each tribe lives under the influence of a single norm, common to all individuals.
Each individual engages once in the indirect reciprocity game with all other tribe inhabitants.
Her action as a donor will depend on her individual strategy, which dictates whether she will provide help or refuse to do it depending on her and the recipient's reputation.
Reputations are public: this means that the result of every interaction is made available to everyone through the indirect observation model introduced in CITATION.
This allows any individual to know the current status of the co-player without observing all of her past interactions.
On the other hand, this requires a way to spread the information to the entire population.
Consistently, language seems to be an important cooperation promoter CITATION, although recent mechanisms of reputation-spreading rely on electronic databases.
Since reputations are either GOOD or BAD, there are 2 4 16 possible strategies.
On the other hand, the number of possible norms depends on their associated order.
The simplest are the so-called first-order norms, in which all that matters is the action taken by the donor.
In second-order norms, the reputation of one of the players also contributes to decide the new reputation of the donor.
And so on, in increasing layers of complexity as shown in Figure 2, which illustrates the features of third-order norms such as those we shall employ here.
Any individual in the tribe shares the same norm, which in turn raises the question of how each inhabitant acquired it.
We do not address this issue here.
However, inasmuch as indirect reciprocity is associated with community enforcement CITATION, CITATION, one may assume, for simplicity, that norms are acquired through an educational process.
Moreover, it is likely that a common norm contributes to the overall cohesiveness and identity of a tribe.
It is noteworthy, however, that if norms were different for different individuals, the indirect observation model would not be valid, as it requires trust in judgments made by co-inhabitants.
For a norm of order n, there are possible norms, each associated with a binary string of length 2 n. We consider third-order norms : in assessing a donor's new reputation, the observer has to make a contextual judgment involving the donor's action, as well as her and the recipient's reputations scored in the previous action.
We introduce the following evolutionary dynamics in each tribe: during one generation all individuals interact once with each other via the indirect reciprocity game.
When individuals reproduce, they replace their strategy by that of another individual from the same tribe, chosen proportional to her accumulated payoff CITATION.
The most successful individuals in each tribe have a higher reproductive success.
Since different tribes are under the influence of different norms, the overall fitness of each tribe will vary from tribe to tribe, as well as the plethora of successful strategies that thrive in each tribe.
This describes individual selection in each tribe.
Tribes engage in pairwise conflicts with a small probability, associated with selection between tribes.
After each conflict, the norm of the defeated tribe will change toward the norm of the victor tribe, as detailed in the Methods section.
We consider different forms of conflict between tribes, which reflect different types of inter-tribe selection mechanisms: group selection CITATION, CITATION CITATION based on the average global payoff of each tribe as well as selection resulting from inter-tribe conflicts modeled in terms of games the display game of war of attrition, and an extended hawk dove game CITATION.
We perform extensive computer simulations of evolutionary dynamics of sets of 64 tribes, each with 64 inhabitants.
Once a stationary regime is reached, we collect information for subsequent statistical analysis.
We compute the frequency of occurrence of bits 1 and 0 in each of the 8-bit locations.
A bit is said to fixate if its frequency of occurrence exceeds or equals 98 percent.
Otherwise, no fixation occurs, which we denote by X, instead of by 1 or 0.
We analyze 500 simulations for the same value of b, subsequently computing the frequency of occurrence 1, 0, and X of the bits 1, 0, and X, respectively.
If 1 0 X, the final bit is 1; if 0 1 X, the final bit is 0; otherwise we assume it is indeterminate, and denote it by.
It is noteworthy that our bit-by-bit selection/transmission procedure, though artificial, provides a simple means of mimicking biological evolution, where genes are interconnected by complex networks and yet evolve independently.
Certainly, a co-evolutionary process would be more appropriate, and this will be explored in future work.
Angiogenesis plays a crucial role in a variety of physiological and pathological conditions including cancer, cardiovascular disease, and wound healing.
Vascular endothelial growth factor is a critical regulator of angiogenesis.
Multiple VEGF receptors are expressed on endothelial cells, including signaling receptor tyrosine kinases and the nonsignaling co-receptor Neuropilin-1.
Neuropilin-1 binds only the isoform of VEGF responsible for pathological angiogenesis, and is thus a potential target for inhibiting VEGF signaling.
Using the first molecularly detailed computational model of VEGF and its receptors, we have shown previously that the VEGFR Neuropilin interactions explain the observed differential effects of VEGF isoforms on VEGF signaling in vitro, and demonstrated potent VEGF inhibition by an antibody to Neuropilin-1 that does not block ligand binding but blocks subsequent receptor coupling.
In the present study, we extend that computational model to simulation of in vivo VEGF transport and binding, and predict the in vivo efficacy of several Neuropilin-targeted therapies in inhibiting VEGF signaling: blocking Neuropilin-1 expression; blocking VEGF binding to Neuropilin-1; blocking Neuropilin VEGFR coupling.
The model predicts that blockade of Neuropilin VEGFR coupling is significantly more effective than other approaches in decreasing VEGF VEGFR2 signaling.
In addition, tumor types with different receptor expression levels respond differently to each of these treatments.
In designing human therapeutics, the mechanism of attacking the target plays a significant role in the outcome: of the strategies tested here, drugs with similar properties to the Neuropilin-1 antibody are predicted to be most effective.
The tumor type and the microenvironment of the target tissue are also significant in determining therapeutic efficacy of each of the treatments studied.
Angiogenesis, the growth of new blood microvessels from preexisting microvasculature, is a critical physiological process for the growth of developing organs and during wound healing, ovulation, and pregnancy.
Coronary or peripheral ischemia may be relieved by inducing angiogenesis CITATION, CITATION, while diseases of hypervascularization, such as cancer or diabetic retinopathy, are targets of anti-angiogenic drugs CITATION, CITATION.
Neuronal expression of angiogenic receptors CITATION, CITATION suggests that this work may also be relevant to the development nervous system.
Our goal is to propose effective targeted therapies using anatomically accurate and molecularly detailed computational models of the growth factors and receptors involved in angiogenesis.
In this study, we predict that three methods of targeting the same molecule result in distinct therapeutic outcomes, and that one of these methods is more effective than the others.
Thus, identification of a therapeutic target must be followed by rational design of the targeting molecule to obtain characteristics that maximize the therapeutic potential.
In addition, the microenvironment in which the drug is to act for example, the expression level of receptors in the tissue is a critical factor in the impact of the therapy.
Vascular endothelial growth factor is a family of secreted glycoproteins and critical regulators of angiogenesis CITATION, CITATION.
In vitro, VEGF increases endothelial cell survival, proliferation, and migration.
In vivo, it increases vascular permeability, activates endothelial cells, and acts as a chemoattractant for nascent vessel sprouts.
Multiple splice isoforms of VEGF exist; the two most abundant in the human are VEGF 121 and VEGF 165.
Both isoforms bind to the VEGF receptor tyrosine kinases to induce signals.
VEGF 165 also interacts with nonsignaling Neuropilin co-receptors and with proteoglycans of the extracellular matrix CITATION, CITATION.
The binding sites on VEGF 165 for VEGFR2 and Neuropilin-1 are nonoverlapping, so VEGF 165 may bind both simultaneously CITATION.
There are thus two parallel pathways for VEGF 165 to bind its signaling receptor: binding directly to VEGFR2; and binding to Neuropilin-1, which presents VEGF to VEGFR2.
VEGF 121 can only form VEGFR2 complexes directly CITATION.
The VEGF 165 Neuropilin interaction is thus of particular value as a therapeutic target because VEGF 165 is the isoform of VEGF that has been identified as inducing pathological angiogenesis CITATION, CITATION : aberrant angiogenic signaling may be targeted while allowing the normal levels of physiological VEGF signaling to continue.
In previous work CITATION, CITATION, we developed computational models of VEGF interactions with endothelial cell receptors in vitro, and incorporated previously published experimental data to estimate the kinetic rate of VEGFR2-Neuropilin coupling by VEGF 165.
We showed that VEGFR2 Neuropilin coupling is sufficient to account for the observed differential effects of VEGF isoforms on multiple cell types and that our model reproduces the distinct VEGF binding and signaling effects on each of these cell types CITATION, CITATION CITATION.
In addition, we used the model to distinguish between alternate hypotheses of molecular mechanisms of action and demonstrated that the Neuropilin-1 antibody under investigation acts by blocking VEGFR Neuropilin coupling, not by blocking VEGF Neuropilin binding.
Here, we extend that validated model of the molecular interactions of the VEGF family and its receptors to predict the in vivo behavior of the system by including the ECM and basement membranes, as well as multiple cell types and geometrical parameters characteristic of the tissue.
Three methods for targeting the VEGF 165 Neuropilin interaction are modeled here.
First, a blockade of Neuropilin-1 expression may be induced by use of siRNA or other methods to prevent the synthesis of the protein in the cells.
Second, a protein that occupies the VEGF binding site on Neuropilin-1 can compete with VEGF 165 for binding to that receptor.
An example is a fragment of the placental growth factor isoform PlGF 2.
Full-length PlGF 2 binds Neuropilin-1 and VEGFR1 CITATION.
The fragment, denoted PlGF 2 here, contains only the Neuropilin-1 binding site and does not bind to VEGFR1.
This protein has been used to block VEGF binding to Neuropilin in vitro CITATION, CITATION.
An alternative would be a Neuropilin-binding fragment of VEGF 165 itself CITATION, CITATION.
Third, we may block the interaction between Neuropilins and the VEGFRs, preventing the presentation of VEGF 165 to the signaling receptor, but permitting Neuropilin to sequester that isoform.
This may be done using a Neuropilin-1 antibody CITATION CITATION that we have previously characterized as permitting VEGF binding to Neuropilin-1, but blocking the subsequent VEGFR coupling CITATION.
Each of these three strategies has been demonstrated to inhibit VEGF signaling in in vitro assays CITATION, CITATION, CITATION, CITATION ; here we predict their in vivo efficacy.
This is the first computational model to our knowledge to include the interactions of the VEGF family and their receptors explicitly and in biophysical detail.
The model includes the kinetics of all ligand receptor interactions, which allows us to examine both short-term and long-term behavior of the system.
All the parameters for the model have been obtained from previously published experimental data.
Analysis of characteristic parameters shows that the kinetics of VEGF interactions are slower than the diffusion process, so diffusion is assumed to be fast, and we construct a compartmental model with parenchymal cells secreting VEGF into the interstitial space and VEGF binding to receptors on the endothelial cell surface .
The geometrical parameters of the tissue under investigation here are also incorporated into the model: interstitial space, tumor cell volume and surface area, microvessel volume and surface area.
Changes to these parameters would result in changes to the kinetic parameters and concentrations in the model.
The results presented here are therefore tissue-specific, but the model may be applied to other tissues.
VEGFR2 is the primary signaling receptor for VEGF, and we first analyze the results of the model for a tissue in which the endothelial cells express VEGFR2 and Neuropilin-1, but not VEGFR1; the effect of VEGFR1 is considered later.
Initially, the system is in a steady state, as VEGF is secreted by the parenchymal cells and internalized by the endothelial cells, resulting in a flux through the interstitial space and the ECM.
One of three treatments is initiated at time zero and the time course of VEGF binding followed for 48 hours.
VEGF VEGFR2 and VEGF VEGFR1 binding are taken as a surrogate for VEGF signaling.
Biophysically detailed models of single cells are difficult to fit to real data.
Recent advances in imaging techniques allow simultaneous access to various intracellular variables, and these data can be used to significantly facilitate the modelling task.
These data, however, are noisy, and current approaches to building biophysically detailed models are not designed to deal with this.
We extend previous techniques to take the noisy nature of the measurements into account.
Sequential Monte Carlo methods, in combination with a detailed biophysical description of a cell, are used for principled, model-based smoothing of noisy recording data.
We also provide an alternative formulation of smoothing where the neural nonlinearities are estimated in a non-parametric manner.
Biophysically important parameters of detailed models are inferred automatically from noisy data via expectation-maximisation.
Overall, we find that model-based smoothing is a powerful, robust technique for smoothing of noisy biophysical data and for inference of biophysical parameters in the face of recording noise.
Recent advances in imaging techniques allow measurements of time-varying biophysical quantities of interest at high spatial and temporal resolution.
For example, voltage-sensitive dye imaging allows the observation of the backpropagation of individual action potentials up the dendritic tree CITATION CITATION.
Calcium imaging techniques similarly allow imaging of synaptic events in individual synapses.
Such data are very well-suited to constrain biophysically detailed models of single cells.
Both the dimensionality of the parameter space and the noisy and undersampled nature of the observed data renders the use of statistical techniques desirable.
Here, we here use sequential Monte Carlo methods CITATION, CITATION a standard machine-learning approach to hidden dynamical systems estimation to automatically smooth the noisy data.
In a first step, we will do this while inferring biophysically detailed models; in a second step, by inferring non-parametric models of the cellular nonlinearities.
Given the laborious nature of building biophysically detailed cellular models by hand CITATION CITATION, there has long been a strong emphasis on robust automatic methods CITATION CITATION.
Large-scale efforts have added to the need for such methods and yielded exciting advances.
The Neurofitter CITATION package, for example, provides tight integration with a number of standard simulation tools; implements a large number of search methods; and uses a combination of a wide variety of cost functions to measure the quality of a model's fit to the data.
These are, however, highly complex approaches that, while extremely flexible, arguably make optimal use neither of the richness of the structure present in the statistical problem nor of the richness of new data emerging from imaging techniques.
In the past, it has been shown by us and others CITATION, CITATION CITATION that knowledge of the true transmembrane voltage decouples a number of fundamental parameters, allowing simultaneous estimation of the spatial distribution of multiple kinetically differing conductances; of intercompartmental conductances; and of time-varying synaptic input.
Importantly, this inference problem has the form of a constrained linear regression with a single, global optimum for all these parameters given the data.
None of these approaches, however, at present take the various noise sources in recording situations explicitly into account.
Here, we extend the findings from CITATION, applying standard inference procedures to well-founded statistical descriptions of the recording situations in the hope that this more specifically tailored approach will provide computationally cheaper, more flexible, robust solutions, and that a probabilistic approach will allow noise to be addressed in a principled manner.
Specifically, we approach the issue of noisy observations and interpolation of undersampled data first in a model-based, and then in a model-free setting.
We start by exploring how an accurate description of a cell can be used for optimal de-noising and to infer unobserved variables, such as Ca 2 concentration from voltage.
We then proceed to show how an accurate model of a cell can be inferred from the noisy signals in the first place; this relies on using model-based smoothing as the first step of a standard, two-step, iterative machine learning algorithm known as Expectation-Maximisation CITATION, CITATION.
The Maximisation step here turns out to be a weighted version of our previous regression-based inference method, which assumed exact knowledge of the biophysical signals.
The aim of this paper is to fit biophysically detailed models to noisy electrophysiological or imaging data.
We first give an overview of the kinds of models we consider; which parameters in those models we seek to infer; how this inference is affected by the noise inherent in the measurements; and how standard machine learning techniques can be applied to this inference problem.
The overview will be couched in terms of voltage measurements, but we later also consider measurements of calcium concentrations.
Compartmental models are spatially discrete approximations to the cable equation CITATION, CITATION, CITATION and allow the temporal evolution of a compartment's voltage to be written asFORMULAwhere FORMULA is the voltage in compartment FORMULA, FORMULA is the specific membrane capacitance, and FORMULA is current evolution noise.
Note the important factor FORMULA which ensures that the noise variance grows linearly with time FORMULA.
The currents FORMULA we will consider here are of three types:
Axial currents along dendritesFORMULA
Transmembrane currents from active, passive, or other membrane conductancesFORMULA
Experimentally injected currentsFORMULAwhere FORMULA indicates one particular current type, FORMULA its reversal potential and FORMULA its maximal conductance in compartment FORMULA, FORMULA is the membrane resistivity and FORMULA is the current experimentally injected into that compartment.
The variable FORMULA represents the time-varying open fraction of the conductance, and is typically given by complex, highly nonlinear functions of time and voltage.
For example, for the Hodgkin and Huxley K -channel, the kinetics are given by FORMULA, withFORMULAand FORMULA themselves nonlinear functions of the voltage CITATION and we again have an additive noise term.
In practice, the gate noise is either drawn from a truncated Gaussian, or one can work with the transformed variable FORMULA.
Similar equations can be formulated for other variables such as the intracellular free Ca 2 concentration CITATION .
One of the most critical problems we face in the study of biological systems is building accurate statistical descriptions of them.
This problem has been particularly challenging because biological systems typically contain large numbers of interacting elements, which precludes the use of standard brute force approaches.
Recently, though, several groups have reported that there may be an alternate strategy.
The reports show that reliable statistical models can be built without knowledge of all the interactions in a system; instead, pairwise interactions can suffice.
These findings, however, are based on the analysis of small subsystems.
Here, we ask whether the observations will generalize to systems of realistic size, that is, whether pairwise models will provide reliable descriptions of true biological systems.
Our results show that, in most cases, they will not.
The reason is that there is a crossover in the predictive power of pairwise models: If the size of the subsystem is below the crossover point, then the results have no predictive power for large systems.
If the size is above the crossover point, then the results may have predictive power.
This work thus provides a general framework for determining the extent to which pairwise models can be used to predict the behavior of large biological systems.
Applied to neural data, the size of most systems studied so far is below the crossover point.
Many fundamental questions in biology are naturally treated in a probabilistic setting.
For instance, deciphering the neural code requires knowledge of the probability of observing patterns of activity in response to stimuli CITATION ; determining which features of a protein are important for correct folding requires knowledge of the probability that a particular sequence of amino acids folds naturally CITATION, CITATION ; and determining the patterns of foraging of animals and their social and individual behavior requires knowledge of the distribution of food and species over both space and time CITATION CITATION .
Constructing these probability distributions is, however, hard.
There are several reasons for this: biological systems are composed of large numbers of elements, and so can exhibit a huge number of configurations in fact, an exponentially large number, ii the elements typically interact with each other, making it impossible to view the system as a collection of independent entities, and iii because of technological considerations, the descriptions of biological systems have to be built from very little data.
For example, with current technology in neuroscience, we can record simultaneously from only about 100 neurons out of approximately 100 billion in the human brain.
So, not only are we faced with the problem of estimating probability distributions in high dimensional spaces, we must do this based on a small fraction of the neurons in the network.
Despite these apparent difficulties, recent work has suggested that the situation may be less bleak than it seems, and that an accurate statistical description of systems can be achieved without having to examine all possible configurations CITATION, CITATION, CITATION CITATION.
One merely has to measure the probability distribution over pairs of elements and use those to build the full distribution.
These pairwise models potentially offer a fundamental simplification, as the number of pairs is quadratic in the number of elements, not exponential.
However, support for the efficacy of pairwise models has, necessarily, come from relatively small subsystems small enough that the true probability distribution could be measured experimentally CITATION CITATION, CITATION.
While these studies have provided a key first step, a critical question remains: will the results from the analysis of these small subsystems extrapolate to large ones?
That is, if a pairwise model predicts the probability distribution for a subset of the elements in a system, will it also predict the probability distribution for the whole system?
Here we find that, for a biologically relevant class of systems, this question can be answered quantitatively and, importantly, generically independent of many of the details of the biological system under consideration.
And the answer is, generally, no.
In this paper, we explain, both analytically and with simulations, why this is the case.
Retroviral insertional mutagenesis screens, which identify genes involved in tumor development in mice, have yielded a substantial number of retroviral integration sites, and this number is expected to grow substantially due to the introduction of high-throughput screening techniques.
The data of various retroviral insertional mutagenesis screens are compiled in the publicly available Retroviral Tagged Cancer Gene Database.
Integrally analyzing these screens for the presence of common insertion sites requires an approach that corrects for the increased probability of finding false CISs as the amount of available data increases.
Moreover, significance estimates of CISs should be established taking into account both the noise, arising from the random nature of the insertion process, as well as the bias, stemming from preferential insertion sites present in the genome and the data retrieval methodology.
We introduce a framework, the kernel convolution framework, to find CISs in a noisy and biased environment using a predefined significance level while controlling the family-wise error.
Where previous methods use one, two, or three predetermined fixed scales, our method is capable of operating at any biologically relevant scale.
This creates the possibility to analyze the CISs in a scale space by varying the width of the CISs, providing new insights in the behavior of CISs across multiple scales.
Our method also features the possibility of including models for background bias.
Using simulated data, we evaluate the KC framework using three kernel functions, the Gaussian, triangular, and rectangular kernel function.
We applied the Gaussian KC to the data from the combined set of screens in the RTCGD and found that 53 percent of the CISs do not reach the significance threshold in this combined setting.
Still, with the FWE under control, application of our method resulted in the discovery of eight novel CISs, which each have a probability less than 5 percent of being false detections.
In retroviral insertional mutagenesis experiments, genes involved in the development of cancer are identified by determining the loci of viral insertions from tumors induced by retroviruses in mice CITATION, CITATION.
After infecting a host cell, the retrovirus inserts its own DNA into the host cell's genome, mutating the host cell's DNA in the process.
The mutation may alter the expression of genes in the vicinity of the insertion or, when inserted within a gene, alter the gene product.
When the affected gene is a cancer gene, activation of the proto-oncogene or inactivation of the tumor-suppressor gene can cause uncontrolled proliferation of cells.
Eventually this may give rise to tumors.
Throughout this text, these cancer-causing insertions are referred to as oncogenic insertions.
A tumor develops when an accumulation of oncogenic insertions causes uncontrolled proliferation of a cell.
As a result, the tumor tissue contains many copies of the cell bearing the oncogenic insertions that induced the proliferation, but only a few copies of cells carrying non-oncogenic insertions.
Consequently, when the DNA of the tumor is analyzed, one will encounter the insertion that induced proliferation in larger proportions than insertions that do not.
Regions in the genome that are found to carry insertions in multiple independent tumors are called common insertion sites.
As a result, the locations of the CISs are highly correlated with the location of genes involved in tumor development.
Cloning the flanking sequences of the inserted virus to determine the insertion loci, and analyzing these data to find significant CISs, therefore enable the discovery of new candidate cancer genes.
This is summarized in Figure S1.
The use of computational modeling and simulation has increased in many biological fields, but despite their potential these techniques are only marginally applied in nutritional sciences.
Nevertheless, recent applications of modeling have been instrumental in answering important nutritional questions from the cellular up to the physiological levels.
Capturing the complexity of today's important nutritional research questions poses a challenge for modeling to become truly integrative in the consideration and interpretation of experimental data at widely differing scales of space and time.
In this review, we discuss a selection of available modeling approaches and applications relevant for nutrition.
We then put these models into perspective by categorizing them according to their space and time domain.
Through this categorization process, we identified a dearth of models that consider processes occurring between the microscopic and macroscopic scale.
We propose a middle-out strategy to develop the required full-scale, multilevel computational models.
Exhaustive and accurate phenotyping, the use of the virtual patient concept, and the development of biomarkers from -omics signatures are identified as key elements of a successful systems biology modeling approach in nutrition research one that integrates physiological mechanisms and data at multiple space and time scales.
Nutritional science is presently undergoing a data explosion as an increasing number of studies are incorporating methods from genomics, transcriptomics, proteomics, and metabolomics.
However, it is presently unclear how these high-dimensional datasets can be related to the physiological characterization of phenotype using traditional nutritional research methods such as indirect calorimetry, nutrient balance, body composition assessment, and isotopic tracer methods.
Thus, a fundamental challenge for nutrition research is to connect these data that are collected at vastly different spatial, temporal, and dimensionality scales.
Although statistical analysis is still the method of choice to deal with the high dimensionality of -omics datasets, systems biology and computational modeling approaches begin to reveal quantitative mechanistic relationships between these various measurements.
A large variety of computational modeling approaches have been applied to wide-ranging levels of organization from molecules to humans.
The processes that are modeled include molecular interactions, signaling pathways, metabolic pathways, cellular growth, anatomical structures, and physiological processes.
Accordingly, computational approaches differ widely with application.
In this review, we discuss the relevance of current and future applications of computational modeling in nutrition research.
To this end, we first introduce important concepts in nutrition and the typical issues for modeling that arise in this field.
Then, we give a broader review of some representative modeling approaches that have successfully addressed key nutritional questions.
We then proceed to identify knowledge and technology gaps and suggest how the computational approaches may be integrated and extended to address these gaps and bring nutritional systems biology modeling an important step forward in the near future.
Nutrition research investigates the processes by which the living organism receives and utilizes the materials necessary for the maintenance of life and health CITATION.
Traditionally, nutritional research investigates these processes at the level of the whole organism.
From a thermodynamic viewpoint, all living organisms exist in a state that is far from equilibrium.
To maintain this state, it is of central importance to harvest sufficient energy from the surroundings.
This energy comes from the controlled combustion of the macronutrients carbohydrate, fat, and protein.
The overarching organizing principle expressed in the Dynamic Energy Budget theory CITATION, which considers that energy from food is extracted, stored in reserves, and distributed throughout the body to fuel the processes essential for life.
These processes include the generation of heat, maintenance of gradients across cell membranes, the production of gametes, the synthesis of structural mass, the establishment of maturity, somatic maintenance, and maturity maintenance.
This organization effectively decouples the organism's internal energy from the external world, facilitating homeostasis.
As such this principle has a clear relevance for nutritional physiology.
In contrast to the dietary macronutrient energy sources, dietary micronutrients, notably mineral elements and vitamins, also play a key role for the overall health of the organism.
Inadequate amounts of some dietary micronutrients have been demonstrated to cause classic deficiency diseases such as scurvy, beriberi, anemia, goiter, and cretinism.
As a third class, various essential nutrients exist that can be used for both energy harvesting, synthesis of structural mass, as well as precursors of specific bioactive compounds.
These nutrients include the essential amino acids and the essential omega-3 and omega-6 fatty acids.
Many health disorders are not necessarily caused by dietary deficiencies, but more generally from imbalances between intake and utilization of nutrients.
While there is general consensus that proper nutrition can prevent various chronic diseases, understanding the health effects of specific nutritional compounds is extraordinarily complicated.
First, delivery of a nutritional perturbation is difficult to control over long time periods and such perturbations often have relatively subtle effects over the time scales typically investigated.
Second, it is very difficult to unravel the distinctive bioactivity of a nutritional compound of interest when it is supplied in a background diet containing hundreds of other bioactive components.
Third, it can be difficult to assess the bioavailability of the nutrient of interest, especially at the level of specific target organs or cells.
The problem of bioavailability at the whole body level has had a long history of mathematical modeling, specifically for trace elements.
Computational kinetic methods were introduced in nutritional sciences along with the use of stable isotopes where the interpretation of the kinetic data required the development of appropriate mathematical models CITATION CITATION.
Typically, compartmental modeling approaches are used to describe the absorption, distribution, and elimination of a nutrient.
Common to most of these models is the high level of aggregation, where the body is adequately described by only a few compartments.
Together, these models aim to provide a rational basis for the determination of the nutritional requirements of the body, and for the understanding of differences in requirements for different micronutrients.
While such traditional modeling methods have been very useful, the real challenge for modeling in nutrition is to help understand and rationally manipulate the complex relationship between nutrition and health, which is determined by the integrated multiscale responses to nutrients, ranging from whole body to subcellular levels of organization and over time scales of minutes to years.
This difficulty is apparent from the problems that arise with current efforts to pinpoint the precise role of nutrition in the metabolic syndrome.
At the long time scale and whole body level of organization, a prolonged period of consuming more energy than is expended results in the gradual development of obesity and increases one's risk for developing insulin resistance a hallmark of the metabolic syndrome.
The study of insulin resistance has revealed that the function of this hormone at the level of organs and tissues occurs on the time scale of minutes to hours.
For example, insulin stimulation of skeletal muscle glucose uptake, inhibition of hepatic glucose output, inhibition of adipose tissue lipolysis, and a host of other physiological effects occur on this time scale.
Methods developed to unravel and quantify the molecular mechanisms underlying these effects have shown the involvement of complex intracellular signal transduction pathways, changes of gene expression, modification of enzyme kinetics, and intracellular molecular trafficking.
Furthermore, the production of insulin by pancreatic beta cells occurs in response to glucose and amino acids and can be modulated by fatty acids, all of which can clearly be influenced by diet and nutrition.
The unique electrophysiological properties of beta cells are influenced by the metabolism of glucose and fatty acids, while the electrical bursting and oscillatory behavior is coupled to insulin secretion on the time scales of seconds to minutes.
Thus, understanding how nutrition impacts the mechanisms underlying insulin resistance requires a quantitative analysis and description of a multiscale, highly coupled regulatory network that includes thousands of components, ranging over subcellular to whole body levels of organization and spanning time scales from seconds to years.
Although a conceptual perspective as outlined above can be derived from literature without too much effort, it is extremely difficult to develop an integrated quantitative understanding that spans the entire complexity of the mechanisms involved.
In principle, mathematical models offer this capability and therefore are required to more fully understand the physiological basis not only of the metabolic syndrome, but of the role of nutrition in health and disease in general.
Without such a quantitative and integrative approach, it is inevitable that one will get lost in the tangle of bubbles and arrows typical of conceptual models and find oneself unable to weigh the relative importance of each component or interaction in determining the overall physiological phenotype.
The field of mathematical modeling in nutrition is very diverse and presently no single mathematical formalism allows one to generate the required integrated quantitative understanding of nutrition as formulated above.
Therefore, in developing our vision of what is needed in the coming years, we now review several representative models that have successfully addressed key nutritional questions and together may help point the way to a more integrative modeling approach.
First, we review modeling approaches for processes at the cellular level describing the biochemical processes that operate to convert food ingredients into energy and building blocks for the cell as the fundamental unit of life.
Insight into these processes teaches us how metabolism is regulated at its most basic level.
Furthermore, modeling at the cellular level provides the entry point to considering the vast quantity and complexity of -omics data.
Second, we review the use of metabolic flux analysis as a framework for the quantitative analysis of material fluxes within the single cell as well as between different cell populations and organs, up to the whole body level.
Thus, MFA forms a natural bridge between different levels of organization and different time scales.
Thirdly, we review compartmental models of lipoprotein metabolism, because lipoproteins are the major mediators of lipid trafficking between organs, and many processes linked with lipids are associated with the metabolic syndrome, which includes cardiovascular diseases, obesity, and insulin resistance, modern plagues in industrialized societies.
Finally, we review mathematical models of body weight and composition regulation and the complex relationship with macronutrient metabolism at the whole body level.
Modeling at this whole body physiological level demonstrates the importance of considering long time scales that are characteristic of chronic diseases like obesity and metabolic syndrome.
Of course, we cannot cover all areas of modeling in the field of nutrition in this review.
For instance, we will not review models of gut-associated processes of nutrient absorption and bacterial conversion of nondigestible food components into such important compounds as short-chain fatty acids.
Another important area that we will not review is models of the neuro-hormonal regulation of food intake and metabolism.
Nevertheless, the collection of models that we chose to review ensures that we cover process occurring on a vast space time spectrum, from nanometer to meter and from microseconds to years.
Therefore, the four areas of modeling that we discuss provide a sufficiently broad basis for our goal, namely to review the available computational approaches that are key to answering central questions in nutrition and that can serve as a platform for the development of more integrative systems models.
Simultaneous spike-counts of neural populations are typically modeled by a Gaussian distribution.
On short time scales, however, this distribution is too restrictive to describe and analyze multivariate distributions of discrete spike-counts.
We present an alternative that is based on copulas and can account for arbitrary marginal distributions, including Poisson and negative binomial distributions as well as second and higher-order interactions.
We describe maximum likelihood-based procedures for fitting copula-based models to spike-count data, and we derive a so-called flashlight transformation which makes it possible to move the tail dependence of an arbitrary copula into an arbitrary orthant of the multivariate probability distribution.
Mixtures of copulas that combine different dependence structures and thereby model different driving processes simultaneously are also introduced.
First, we apply copula-based models to populations of integrate-and-fire neurons receiving partially correlated input and show that the best fitting copulas provide information about the functional connectivity of coupled neurons which can be extracted using the flashlight transformation.
We then apply the new method to data which were recorded from macaque prefrontal cortex using a multi-tetrode array.
We find that copula-based distributions with negative binomial marginals provide an appropriate stochastic model for the multivariate spike-count distributions rather than the multivariate Poisson latent variables distribution and the often used multivariate normal distribution.
The dependence structure of these distributions provides evidence for common inhibitory input to all recorded stimulus encoding neurons.
Finally, we show that copula-based models can be successfully used to evaluate neural codes, e.g., to characterize stimulus-dependent spike-count distributions with information measures.
This demonstrates that copula-based models are not only a versatile class of models for multivariate distributions of spike-counts, but that those models can be exploited to understand functional dependencies.
So far, it is still unknown which statistics are crucial for analysis in order to understand the neural code.
One approach is to analyze simultaneous spike-counts of neural populations.
Responses from populations of sensory neurons vary even when the same stimulus is presented repeatedly, and the variations between the simultaneous spike-counts are usually correlated at least for neighboring neurons.
These noise correlations have been subject of a substantial number of studies.
For computational reasons, however, these studies typically assume Gaussian noise.
Thus, correlated spike rates are generally modeled by multivariate normal distributions with a specific covariance matrix that describes all pairwise linear correlations.
For long time intervals or high firing rates, the average number of spikes is sufficiently large for the central limit theorem to apply and the normal distribution is a good approximation for the spike-count distributions.
Several experimental findings, however, suggest that processing of sensory information can take place on shorter time scales, involving only tens to hundreds of milliseconds CITATION, CITATION.
In this regime the normal distribution is no longer a valid approximation:
Its marginals are continuous with a symmetric shape, whereas empirical distributions of real spike-counts tend to have a positive skew .
The normal distribution has to be heuristically modified in order to avoid positive probabilities for negative values which are not meaningful for spike-counts.
This is a major issue for low rates for which the probability of negative values would be high.
The dependence structure of a multivariate normal distribution is always elliptical, whereas spike-count data often show a so-called tail-dependence with probability mass concentrated on one of the corners .
The multivariate normal distribution assumes second order correlations only.
Although it was shown that pairwise interactions are sufficient for describing the spike-count distributions of retinal ganglion cells and cortex cells in vitro CITATION, there is evidence for significant higher order interactions of spike-counts recorded from cortical areas in vivo CITATION .
Though not widespread for modeling spike-counts, alternative models have been proposed in previous studies that have Poisson distributed marginals and separate parameters for higher order correlations, e.g. the multiple interaction process model CITATION and the compound Poisson model CITATION.
Both models are point processes.
In terms of their induced spike-count distributions these models are special cases of the multivariate Poisson latent variables distribution first introduced by Kawamura CITATION and presented in a compact matrix notation by Karlis and Meligkotsidou CITATION.
Similar to the multivariate normal distribution this model has also a couple of shortcomings for spike-count modeling: Only Poisson-marginals can be modeled.
Negative correlations cannot be represented.
The dependence structure is inflexible: features like tail dependence cannot be modeled.
We use and extend a versatile class of models for multivariate discrete distributions that overcome the shortcomings of the afore-mentioned models CITATION, CITATION.
These models are based on the concept of copulas CITATION, which allow to combine arbitrary marginal distributions using a rich set of dependence structures.
In neuroscience they were also applied to model the distribution of continuous first-spike-latencies CITATION .
Figure 1 illustrates the copula concept using spike-count data from two real neurons.
Figure 1A shows the bivariate empirical distribution and its two marginals.
The distribution of the counts depends on the length of the time bin that is used to count the spikes, here FORMULA.
In the case considered, the correlation at low counts is higher than at high counts.
This is called lower tail dependence CITATION.
Figure 1B shows the discretized and rectified multivariate normal distribution.
On the other hand, the spike-count probabilities for a copula-based distribution correspond well to the empirical distribution in Figure 1A.
The paper is organized as follows.
The next Section Materials and Methods contains a description of methodological details regarding the multivariate normal distribution, the multivariate Poisson latent variables distribution, the copula approach for spike-counts and the model fitting procedures.
In this section we will also introduce a novel transformation for copula families.
The method is innovative and yields a novel result.
We will then describe the computational model used to generate synthetic data and the experimental recording and analysis procedures.
In the Section Results copula-based models will be applied to artificial data generated by integrate-and-fire models of coupled neural populations and to data recorded from macaque prefrontal cortex during a visual memory task.
The appropriateness of the models is also investigated.
The paper concludes with a discussion of the strengths and weaknesses of the copula approach for spike-counts.
Experimental work has shown that T cells of the immune system rapidly and specifically respond to antigenic molecules presented on the surface of antigen-presenting-cells and are able to discriminate between potential stimuli based on the kinetic parameters of the T cell receptor-antigen bond.
These antigenic molecules are presented among thousands of chemically similar endogenous peptides, raising the question of how T cells can reliably make a decision to respond to certain antigens but not others within minutes of encountering an antigen presenting cell.
In this theoretical study, we investigate the role of localized rebinding between a T cell receptor and an antigen.
We show that by allowing the signaling state of individual receptors to persist during brief unbinding events, T cells are able to discriminate antigens based on both their unbinding and rebinding rates.
We demonstrate that T cell receptor coreceptors, but not receptor clustering, are important in promoting localized rebinding, and show that requiring rebinding for productive signaling reduces signals from a high concentration of endogenous pMHC.
In developing our main results, we use a relatively simple model based on kinetic proofreading.
However, we additionally show that all our results are recapitulated when we use a detailed T cell receptor signaling model.
We discuss our results in the context of existing models and recent experimental work and propose new experiments to test our findings.
T cells of the adaptive immune system use their T cell receptors to scan the surfaces of antigen-presenting-cells for antigen in the form of specific peptides bound to major-histocompatibility complexes.
Scanning of APCs by T cells is rapid, with estimates suggesting that an individual T cell spends only 1 5 minutes interacting with a single APC if it lacks specific pMHC CITATION.
Experiments have demonstrated that T cells are extremely sensitive to specific pMHC, responding to as few as 1 10 pMHC in a sea of thousands of chemically similar self pMHC CITATION, CITATION, CITATION, CITATION.
It has also been demonstrated that a single amino acid substitution on a presented peptide can dramatically alter the T cell response CITATION.
Speed, sensitivity, and specificity have been dubbed the S 3 characteristics of antigen detection by T cells CITATION .
The observation that T cells transiently interact with APCs that do not express specific pMHC suggests that the decision to respond occurs within seconds of an encounter.
Rapid turnover of T cell-APC contacts in vivo accelerates the search for specific pMHC by allowing numerous unique T cell-APC interactions.
The decision to respond gives rise to a stop signal CITATION and is commonly followed by the formation of the immune synapse CITATION, a stable adhesion between the T cell and APC that persists for upwards of 30 minutes and facilitates a second, sustained phase of signaling.
Experiments and mathematical modeling have been extensively used to understand the efficiency of T cell activation.
During the sustained signaling phase, the serial binding of many TCR by a single pMHC has been postulated to increase T cell sensitivity CITATION.
Serial binding is expected because the bonds formed between TCR and agonist pMHC are transient, with half-lives in the range of 1 100 s CITATION, CITATION, CITATION.
On the other hand, T cell specificity has been addressed by the kinetic proofreading model CITATION, CITATION.
This model postulates that a series of TCR-proximal steps, such as the binding and subsequent phosphorylation of the TCR associated immunoreceptor tyrosine-based activation motifs by signaling molecules, occur upon pMHC binding, and that these signaling events require continued TCR engagement to proceed.
A productive signal is transduced only after several such transformations have taken place.
In this model, T cells are able to discriminate between different pMHC by imposing a threshold on the TCR-pMHC dissociation rate constant .
Combining serial binding and kinetic proofreading reveals that a balance between sensitivity and specificity gives rise to an optimal FORMULA for efficient T cell activation CITATION, CITATION, CITATION, an effect which has been experimentally observed CITATION, CITATION, CITATION.
The efficiency of T cell activation in these models, and others CITATION, CITATION, is usually quantified by the number of activated TCRs integrated over the whole cell, and after a relatively long period of interaction with an APC.
Additionally, several studies have reported correlations between the TCR/pMHC bond dissociation constant, but not FORMULA, and the efficiency of T cell activation as measured after FORMULA hour by cytoxicity and/or cytokine assays CITATION, CITATION, CITATION .
However, T cells have been observed to respond to stimulatory pMHC in less than a minute CITATION and, at least for cytotoxic T cells, a stable contact interface is not required for pMHC detection CITATION.
Serial binding/kinetic proofreading models do not predict specificity on these short time scales, in part because signals generated by high concentrations of weakly binding self pMHC are found to be comparable to signals generated by low concentrations of high affinity agonist pMHC CITATION, CITATION.
Moreover, the early T cell response is unlikely to be determined by an equilibrium parameter, such as FORMULA, as it is quite unlikely that the T cell-APC interface attains equilibrium at such short times.
It is more likely that FORMULA is an important determinant of the efficiency of T cell activation during the sustained phase of signaling, well after the initial decision to respond.
In this paper we investigate a putative mechanism for antigen discrimination during the early phase of TCR signaling.
Specifically, we examine the role of TCR/pMHC rebinding in allowing T cells to make rapid and reliable decisions to respond.
By explicitly accounting for TCR/pMHC rebinding within existing formulations of diffusion-limited membrane reactions, we find that rebinding has very little effect in canonical proofreading models.
A simple modification that accounts for signal persistence at the TCR allows individual TCR to integrate the duration of multiple rebinding events.
The consequence of this scheme is that discrimination in this sum-of-binding model is now sensitive to both the association and dissociation rate constants of the TCR-pMHC bond.
This enhanced sensitivity leads to the finding that a T cell can discriminate between a wider spectrum of antigens than would be predicted by a traditional serial binding/kinetic proofreading model.
We further show that coreceptors, but not TCR clustering, are important to achieve these rapid rebinding events.
In addition, we show that signal persistence at the TCR does not allow high concentrations of endogenous pMHC to generate spurious signals.
Finally, we show that our general conclusions are unchanged when our cartoon kinetic proofreading model is replaced by a detailed model of TCR-proximal signaling.
We propose that T cells discriminate antigen based on FORMULA and FORMULA via a threshold in the sum-of-binding which allows for rapid and reliable T cell responses to specific pMHC.
While many models of biological object recognition share a common set of broad-stroke properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored.
Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct parts have not been tuned correctly, assembled at sufficient scale, or provided with enough training.
Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware.
In analogy to high-throughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis.
We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature.
As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly accelerate progress in both artificial vision and our understanding of the computational underpinning of biological vision.
The study of biological vision and the creation of artificial vision systems are naturally intertwined exploration of the neuronal substrates of visual processing provides clues and inspiration for artificial systems, and artificial systems, in turn, serve as important generators of new ideas and working hypotheses.
The results of this synergy have been powerful: in addition to providing important theoretical frameworks for empirical investigations, biologically-inspired models are routinely among the highest-performing artificial vision systems in practical tests of object and face recognition CITATION CITATION .
However, while neuroscience has provided inspiration for some of the broad-stroke properties of the visual system, much is still unknown.
Even for those qualitative properties that most biologically-inspired models share, experimental data currently provide little constraint on their key parameters.
As a result, even the most faithfully biomimetic vision models necessarily represent just one of many possible realizations of a collection of computational ideas.
Truly evaluating the set of biologically-inspired computational ideas is difficult, since the performance of a model depends strongly on its particular instantiation the size of the pooling kernels, the number of units per layer, exponents in normalization operations, etc. Because the number of such parameters is typically large, and the computational cost of evaluating one particular model is high, it is difficult to adequately explore the space of possible model instantiations.
At the same time, there is no guarantee that even the correct set of principles will work when instantiated on a small scale.
Thus, when a model fails to approach the abilities of biological visual systems, we cannot tell if this is because the ideas are wrong, or they are simply not put together correctly or on a large enough scale.
As a result of these factors, the availability of computational resources plays a critical role in shaping what kinds of computational investigations are possible.
Traditionally, this bound has grown according to Moore's Law CITATION, however, recently, advances in highly-parallel graphics processing hardware have disrupted this status quo for some classes of computational problems.
In particular, this new class of modern graphics processing hardware has enabled over hundred-fold speed-ups in some of the key computations that most biologically-inspired visual models share in common.
As is already occurring in other scientific fields CITATION, CITATION, the large quantitative performance improvements offered by this new class of hardware hold the potential to effect qualitative changes in how science is done.
In the present work, we take advantage of these recent advances in graphics processing hardware CITATION, CITATION to more expansively explore the range of biologically-inspired models including models of larger, more realistic scale.
In analogy to high-throughput screening approaches in molecular biology and genetics, we generated and trained thousands of potential network architectures and parameter instantiations, and we screened the visual representations produced by these models using tasks that engage the core problem of object recognition tolerance to image variation CITATION CITATION, CITATION, CITATION.
From these candidate models, the most promising were selected for further analysis.
We show that this large-scale screening approach can yield significant, reproducible gains in performance in a variety of basic object recognitions tasks and that it holds the promise of offering insight into which computational ideas are most important for achieving this performance.
Critically, such insights can then be fed back into the design of candidate models, further guiding evolutionary progress.
As the scale of available computational power continues to expand, high-throughput exploration of ideas in computational vision holds great potential both for accelerating progress in artificial vision, and for generating new, experimentally-testable hypotheses for the study of biological vision.
The claim that genetic properties of neurons significantly influence their synaptic network structure is a common notion in neuroscience.
The nematode Caenorhabditis elegans provides an exciting opportunity to approach this question in a large-scale quantitative manner.
Its synaptic connectivity network has been identified, and, combined with cellular studies, we currently have characteristic connectivity and gene expression signatures for most of its neurons.
By using two complementary analysis assays we show that the expression signature of a neuron carries significant information about its synaptic connectivity signature, and identify a list of putative genes predicting neural connectivity.
The current study rigorously quantifies the relation between gene expression and synaptic connectivity signatures in the C. elegans nervous system and identifies subsets of neurons where this relation is highly marked.
The results presented and the genes identified provide a promising starting point for further, more detailed computational and experimental investigations.
It is an accepted common notion that genes play a major role in the formation of the nervous system; they specify neuronal cell types, help destine neurons into defined neural circuits, and provide important cues determining their communication CITATION, CITATION.
Many studies have identified specific genes in the nematode C. elegans that disrupt the development of neural circuits.
These genes are typically responsible for neuronal morphology, axon development, and synaptogenesis.
Such findings include, e.g., axon guidance genes CITATION, attractive and repulsive interactions CITATION CITATION, presynaptic input modulation CITATION, presynaptic differentiation CITATION, and synaptic specificity genes CITATION.
These findings have been based on specifically targeted studies, each designed to address a specific pathway, neuron type, receptor or transmitter.
Yet, it has been difficult to identify on a large scale mutations that determine the specific identity of synaptic connections, mainly because synaptic specification is one of the last steps in a complex process of neuronal differentiation and axonal migration CITATION.
Sieburth et al. CITATION presented the first large-scale screening for genes involved in the C. elegans neuromuscular junction.
The study identified more than 100 novel genes that have specific functions in the transmission of signals across this junction.
While the latter study was not aimed at identifying synaptic connectivity genes, it demonstrated the plausibility of addressing such questions in a large-scale manner.
In a recent and related study, Varadan et al. CITATION have applied an entropy minimization approach to the C. elegans' data to identify sets of synergistically interacting genes whose joint expression is common to most synapses and predicts neural connectivity, leaving aside the specific identity of the pre- and post-neurons.
Our study differs from theirs both in its goals, and in its methods.
It leads to the first quantitative characterization of the relation between the genetic properties of neurons and their synaptic connectivity, concomitantly addressing the majority of C. elegans neurons at large.
The existing C. elegans neural wiring diagram provides a connectivity signature for each neuron, specifying to which other neurons it is connected.
Each neuron has also an expression signature extracted from WormBase, specifying the genes directly associated with it.
Combined together, this data enables the investigation of the relation between expression and connectivity signatures across most of the C. elegans neurons.
We specifically address two attributes of this relation: the first asks whether it is possible to predict a neuron's connectivity signature based solely on its expression signature.
The second question is, to what extent do neurons with similar expression signatures have similar connectivity signatures?
We show that the expression signatures of neurons carry significant information about their connectivity signatures, and further identify specific genes that play a major role in determining this relation.
The gene sets we identify do not necessarily have a direct causal influence on synaptic connectivity and specificity; however, they provide putative gene targets for further experimental investigation.
Finally, we suggest a methodological way to study the relations between neurons' expression and connectivity signatures and their actual functional contribution to behavior.
Quantifying these relations allows addressing a classical question in neuroscience; what dominates the functionality of a neural circuit the local, genetic basis of the individual neurons, or the overall network structure determined by their connectivity?
The discovery of small molecules targeted to specific oncogenic pathways has revolutionized anti-cancer therapy.
However, such therapy often fails due to the evolution of acquired resistance.
One long-standing question in clinical cancer research is the identification of optimum therapeutic administration strategies so that the risk of resistance is minimized.
In this paper, we investigate optimal drug dosing schedules to prevent, or at least delay, the emergence of resistance.
We design and analyze a stochastic mathematical model describing the evolutionary dynamics of a tumor cell population during therapy.
We consider drug resistance emerging due to a single genetic alteration and calculate the probability of resistance arising during specific dosing strategies.
We then optimize treatment protocols such that the risk of resistance is minimal while considering drug toxicity and side effects as constraints.
Our methodology can be used to identify optimum drug administration schedules to avoid resistance conferred by one genetic alteration for any cancer and treatment type.
Alteration of the normal regulation of cell-cycle progression, division and death lies at the heart of the processes driving tumorigenesis.
A detailed molecular understanding of these processes provides an opportunity to design targeted anti-cancer agents.
The term targeted therapy refers to drugs with a focused mechanism that specifically act on well-defined protein targets or biological pathways that, when altered by therapy, impair the abnormal proliferation of cancer cells.
Examples of this type of therapy include hormonal-based therapies in breast and prostate cancer; small-molecule inhibitors of the EGFR pathway in lung, breast, and colorectal cancers such as erlotinib, gefitinib, and cetuximab ; inhibitors of the JAK2, FLT3 and BCR-ABL tyrosine kinases in leukemias such as imatinib, dasatinib, and nilotinib ; blockers of invasion and metastasis; anti-angiogenesis agents like bevacizumab ; proapoptotic drugs; and proteasome inhibitors such as bortezomib CITATION, CITATION.
The target-driven approach to drug development contrasts with the conventional, more empirical approach used to develop cytotoxic chemotherapeutics, and the successes of the past few years illustrate the power of this concept.
The absence of prolonged clinical responses in many cases, however, stresses the importance of continued basic studies into the mechanisms of targeted drugs and their failure in the clinic.
Acquired drug resistance is an important reason for the failure of targeted therapies.
Resistance emerges due to drug metabolism, drug export, and alteration of the drug target by mutation, deletion, or overexpression.
Depending on the cancer type and its stage, the therapy administered, and the genetic background of the patient, one or several genetic alterations may be necessary to confer drug resistance to cells.
In this paper, we investigate drug resistance emerging due to a single alteration.
For example, treatment of chronic myeloid leukemia with the targeted agent imatinib fails due to acquired point mutations in the BCR-ABL kinase domain CITATION.
To date, ninety different point mutations have been identified, each of which is sufficient to confer resistance to imatinib CITATION.
The second-generation BCR-ABL inhibitors, dasatinib and nilotinib, can circumvent most mutations that confer resistance to imatinib; the T315I mutation, however, causes resistance to all BCR-ABL kinase inhibitors developed so far.
Similarly, the T790M point mutation in the epidermal growth factor receptor confers resistance to the EGFR tyrosine kinase inhibitors gefitinib and erlotinib CITATION, which are used to treat non-small cell lung cancer.
Other mechanisms of resistance include gene amplification or overexpression of the P-glycoprotein family of membrane transporters which decreases intracellular drug accumulation, changes in cellular proteins involved in detoxification or activation of the drug, changes in molecules involved in DNA repair, activation of oncogenes such as Her-2/Neu, c-Myc, and Ras as well as inactivation of tumor suppressor genes like p53 CITATION CITATION .
The design of optimal drug administration schedules to minimize the risk of resistance represents an important issue in clinical cancer research.
Currently, many targeted drugs are administered continuously at sufficiently low doses so that no drug holidays are necessary to limit the side effects.
Alternatively, the drug may be administered at higher doses in short pulses followed by rest periods to allow for recovery from toxicity.
Clinical studies evaluating the advantages of different approaches have been ambivalent.
Some investigations found that a low-dose continuous strategy is more effective CITATION, while others have advocated more concentrated dosages CITATION.
The effectiveness of a low-dose continuous approach is often attributed to its targeted effect on tumor endothelial cells and the prevention of angiogenesis rather than low rates of resistance CITATION.
The continuous dosing strategy is often implemented as combination therapy, sometimes including a second drug administered at a higher dose in a pulsed fashion.
A significant amount of research effort has been devoted to developing mathematical models of tumor growth and response to chemotherapy.
In a seminal paper, Norton and Simon proposed a model of kinetic resistance to cell-cycle specific therapy in which tumor growth followed a Gompertzian law CITATION.
The authors used a differential equation model in which the rate of cell kill was proportional to the rate of growth for an unperturbed tumor of a given size.
They suggested that one way of combating the slowing rate of tumor regression was to increase the intensity of treatment as the tumor became smaller, thus increasing the chance of cure.
The authors also published a review of clinical trials employing dosing schedules related to their proposed dose-intensification strategy, and concluded that the concept of intensification was clinically feasible, and possibly efficacious CITATION.
Later, predictions of an extension of this model were validated in a clinical trial evaluating the effects of a dose-dense strategy and a conventional regimen for chemotherapy CITATION.
Their model and its predictions have become known as the Norton-Simon hypothesis and have generated substantial interest in mathematical modeling of chemotherapy and kinetic resistance.
For example, Dibrov and colleagues formulated a kinetic cell-cycle model to describe cell synchronization by cycle phase-specific blockers CITATION ; this model was then used for optimizing treatment schedules to increase the degree of synchronization and thus the effectiveness of a cycle-specific drug.
Agur introduced another model describing cell-cycle dynamics of tumor and host cells to investigate the effect of drug scheduling on responsiveness to chemotherapy CITATION ; this model was then used to optimize scheduling of chemotherapeutics to maximize efficacy while controlling host toxicity.
Other theoretical studies include a mathematical model of tumor recurrence and metastasis during periodically pulsed chemotherapy CITATION, a control theoretic approach to optimal dosing strategies CITATION, and an evaluation of chemotherapeutic strategies in light of their anti-angiogenic effects CITATION.
For a more comprehensive survey of kinetic models of tumor response to chemotherapy, we refer the reader to reviews CITATION CITATION and references therein.
There have also been substantial research efforts devoted to developing mathematical models of genetic resistance, i.e. resistance driven by genetic alterations in cancer cells.
Since mutations conferring resistance can arise as random events during the DNA replication phase of cell division, the dynamics of resistant populations are well-suited to description with stochastic mathematical models.
Coldman and co-authors pioneered this field by introducing stochastic models of resistance against chemotherapy to guide treatment schedules.
In 1986, Coldman and Goldie studied the emergence of resistance to one or two functionally equivalent chemotherapeutic drugs using a branching process model of tumor growth with a differentiation hierarchy CITATION.
In this model, the birth and death rates of cells were time-independent constants and each sensitive cell division gave rise to a resistant cell with a certain probability.
The effect of drug was modeled as an additional probabilistic cell kill law on the existing population, and the drug could be administered in a fixed dose at a series of fixed time points.
The goal of the model was to schedule the sequential administration of both drugs in order to maximize the probability of cure.
Later, the assumption of equivalence or symmetry between the two drugs was relaxed CITATION.
These models were also extended to include the toxic effects of chemotherapy on normal tissue, and an optimal control problem was formulated to maximize the probability of tumor cure without toxicity CITATION.
More recently, Iwasa and colleagues used a multi-type birth-death process model to study the probability of resistance emerging due to one or multiple mutations in populations under selection pressure CITATION.
The authors considered pre-existing resistance mutations and determined the optimum intervention strategy utilizing multiple drugs.
Multi-drug resistance was also investigated using a multi-type birth-death process model in work by Komarova and Wodarz CITATION, CITATION.
In their models, the resistance to each drug was conferred by genetic alterations within a mutational network.
The birth and death rates of each cell type were time-independent constants and cells had an additional drug-induced death rate if they were sensitive to one or more of the drugs.
The authors studied the evolution of resistant cells both before and after the start of treatment, and calculated the probability of treatment success under continuous treatment scenarios with a variable number of drugs.
Recently, the dynamics of resistance emerging due to one or two genetic alterations in a clonally expanding population of sensitive cells prior to the start of therapy were studied using a time-homogenous multi-type birth-death process CITATION, CITATION .
One common feature of these models of genetic resistance is that the treatment effect is formulated as an additional probabilistic cell death rate on sensitive cells, separate from the underlying birth and death process model with constant birth and death rates.
Under these model assumptions, the drug cannot alter the proliferation rate of either sensitive or resistant cells; however, a main effect of many targeted therapies is the inhibition of proliferation of cancer cells.
Inhibited proliferation in turn leads to a reduced probability of resistance since resistant cells are generated during sensitive cell divisions.
In this paper, we utilize a non-homogenous multi-type birth-death process model wherein the birth and death rates of both sensitive and resistant cells are dependent on a temporally varying drug concentration profile.
This study represents a significant departure from existing models of resistance since we incorporate the effect of inhibition of sensitive cell proliferation as well as drug-induced death, obtaining a more accurate description of the evolutionary dynamics of the system.
In addition, we generalize our model to incorporate partial resistance, so that the drug may also have an effect on the birth and death rates of resistant cells.
The goals of our analysis also differ from those of previous work.
Coldman and Murray were interested in finding the optimal administration strategy for multiple chemotherapeutic drugs in combination or sequential administration CITATION ; they aimed to maximize the probability of cure while limiting toxicity.
Komarova was interested in studying the effect of multiple drugs administered continuously on the probability of eventual cure CITATION.
In contrast, in this paper we derive estimates for the expected size of the resistant cell population as well as the probability of resistance during a full spectrum of continuous and pulsed treatment schedules with one targeted drug.
We then propose a methodology for selecting the optimal strategy from this spectrum to minimize the probability of resistance as well as to maximally delay the progression of disease by controlling the expected size of the resistant population, while incorporating toxicity constraints.
In many clinical scenarios, the probability of resistance is high regardless of dosing strategy, and thus the maximal delay of disease progression is a more realistic objective than tumor cure.
The methodology developed in this paper can be applied to study acquired resistance in any cancer and treatment type.
Atherosclerosis is the main cause of coronary heart disease and stroke, the two major causes of death in developed society.
There is emerging evidence of excess risk of cardiovascular disease at low radiation doses in various occupationally exposed groups receiving small daily radiation doses.
Assuming that they are causal, the mechanisms for effects of chronic fractionated radiation exposures on cardiovascular disease are unclear.
We outline a spatial reaction-diffusion model for atherosclerosis and perform stability analysis, based wherever possible on human data.
We show that a predicted consequence of multiple small radiation doses is to cause mean chemo-attractant concentration to increase linearly with cumulative dose.
The main driver for the increase in MCP-1 is monocyte death, and consequent reduction in MCP-1 degradation.
The radiation-induced risks predicted by the model are quantitatively consistent with those observed in a number of occupationally-exposed groups.
The changes in equilibrium MCP-1 concentrations with low density lipoprotein cholesterol concentration are also consistent with experimental and epidemiologic data.
This proposed mechanism would be experimentally testable.
If true, it also has substantive implications for radiological protection, which at present does not take cardiovascular disease into account.
The Japanese A-bomb survivor data implies that cardiovascular disease and cancer mortality contribute similarly to radiogenic risk.
The major uncertainty in assessing the low-dose risk of cardiovascular disease is the shape of the dose response relationship, which is unclear in the Japanese data.
The analysis of the present paper suggests that linear extrapolation would be appropriate for this endpoint.
Atherosclerosis is the main cause of coronary heart disease and stroke, the two major causes of death in developed society CITATION.
Though previously initiation of atherosclerosis was attributed mainly to lipid accumulation within the arterial walls, it is now widely accepted that inflammation plays a vital role in the initiation and progression of the disease CITATION CITATION .
For some time cardiovascular effects of high dose radiotherapy have been known CITATION, CITATION.
A variety of effects are observed, presumed to result from inactivation of large numbers of cells and associated functional impairment of the affected tissue.
Among such effects are direct damage to the structures of the heart including marked diffuse fibrotic damage, especially of the pericardium and myocardium, pericardial adhesions, microvascular damage and stenosis of the valves and to the coronary arteries; these sorts of damage occur both in patients receiving RT and in experimental animals CITATION.
There is emerging evidence of excess risk of cardiovascular disease at much lower radiation doses and occurring a long time after radiation exposure in the Japanese atomic bomb survivor Life Span Study cohort CITATION, CITATION and in various occupationally-exposed groups CITATION CITATION although not in all.
Assuming that they are causal, the likely mechanisms for such effects of low dose and/or chronic radiation exposures on cardiovascular disease are not clear CITATION, CITATION.
It is of interest that elevated levels of the pro-inflammatory cytokines IL-6, CRP, TNF- and INF-, but also increased levels of the anti-inflammatory cytokine IL-10, have been observed in the Japanese atomic bomb survivors CITATION, CITATION.
There was also dose-related elevation in erythrocyte sedimentation rate and in levels of IgG, IgA and total immunoglobulins in this cohort, all markers of systemic inflammation CITATION .
In this paper we outline a mathematical formulation of a model of cardiovascular disease that is largely based on the inflammatory hypothesis articulated by Ross CITATION, CITATION.
The motivation behind the mathematical modelling is to encompass various factors contributing to the inflammatory process and subsequently to atherosclerotic formation.
As atherosclerosis is not only a multifactorial, but also a multi-step disease, we concentrate on modelling chronic inflammation, primarily at early stages in the disease, but outlining a treatment for the later stages that lead to plaque rupture.
The model is to some extent based on a model of McKay et al. CITATION, although there are significant departures from and elaborations of this model.
In particular, features are borrowed from the generally rather simpler models of Cobbold et al. CITATION and Ibragimov et al. CITATION.
Stability analysis of a simplified version of the model will be performed.
We shall be particularly concerned with mechanisms for effects of cholesterol and fractionated low dose radiation exposure in this inflammation model, and outline a case for radiation-induced monocyte cell death as a candidate pathway.
The bacterial flagellar motor is a highly efficient rotary machine used by many bacteria to propel themselves.
It has recently been shown that at low speeds its rotation proceeds in steps.
Here we propose a simple physical model, based on the storage of energy in protein springs, that accounts for this stepping behavior as a random walk in a tilted corrugated potential that combines torque and contact forces.
We argue that the absolute angular position of the rotor is crucial for understanding step properties and show this hypothesis to be consistent with the available data, in particular the observation that backward steps are smaller on average than forward steps.
We also predict a sublinear speed versus torque relationship for fixed load at low torque, and a peak in rotor diffusion as a function of torque.
Our model provides a comprehensive framework for understanding and analyzing stepping behavior in the bacterial flagellar motor and proposes novel, testable predictions.
More broadly, the storage of energy in protein springs by the flagellar motor may provide useful general insights into the design of highly efficient molecular machines.
Bacteria swim by virtue of tiny rotary motors that drive rotation of helical flagella.
These motors are powered by a transmembrane proton flux which is converted into torque.
However, little is known about the detailed mechanisms of energy conversion, or torque generation.
Recently, a new result has provided direct insight into motor operation CITATION : at low speeds, the bacterial flagellar motor proceeds by steps.
This stepping is stochastic in nature, as manifested by the occurrence of occasional backward steps even for motors locked in one rotation direction.
What is the origin of motor steps and how can these steps be reconciled with the near perfect efficiency of the motor observed at low speeds CITATION ? We argue that steps, including backward steps, are an inevitable consequence of the physical structure of the motor a stator driving a bumpy rotor through a viscous medium.
In response to chemotactic signals, flagellar motors switch from counterclockwise to clockwise rotation causing cells to tumble or change directions.
In Escherichia coli, the basic mechanism of torque generation appears to be the same for both directions of motor rotation CITATION.
Torque is generated by the passage of FORMULA ions through the cytoplasmic membrane.
As shown schematically in Fig.
1A, torque is applied to the rotor, including the flagellum, by the stator, which is comprised of independent torque-generating units anchored to the peptidoglycan cell wall.
The exact number of torque-generating units can vary from motor to motor, with the maximum estimated to be at least 11 CITATION.
The rotor includes 26 circularly arrayed FliG proteins that contact the MotA/B complexes.
The torque-speed relation of the motor has been measured under a range of conditions CITATION CITATION.
The maximum torque in the high load, low speed regime tracks the electrochemical potential difference or proton motive force across the membrane, and the motor operates with nearly perfect efficiency CITATION.
Whereas torque and efficiency fall off at high speeds, proton flux and motor rotation are always strongly coupled with FORMULA protons passing through the membrane per MotA/B unit per rotation CITATION .
Recent experiments, where rotation was measured by attaching a polystyrene bead to a flagellar stump driven by a counterclockwise-locked FORMULA chimaeric motor at low speeds, revealed that the motor proceeds by steps CITATION.
The steps have average size FORMULA, which corresponds to 26 steps per rotation, exactly the number of copies of FliG around the rotor.
Occasional backward steps are observed and, interestingly, these are smaller on average than forward steps.
These observations, as well as the stepping mechanism itself, have so far remained unexplained.
It has been suggested that stepping is caused by the stochastic passage of ions.
However, as pointed out in CITATION, the energy provided by passage of a single ion can only move the rotor attached to a FORMULA polystyrene bead by FORMULA, much less than the typical observed step size.
Here we propose a simple physical model to explain stepping: the stator applies nearly constant torque to the rotor, but, at the same time, contact forces on the rotor produce a potential and therefore an additional torque with approximately the 26-fold periodicity of FliG.
Flagellar rotation is viewed as a circular random walk in a bumpy potential biased to favor rotation in a particular direction by the torque exerted by the stator elements.
Our model naturally accounts for the existence of backward steps, as well as the discrepancy between forward and backward step sizes, and also predicts that step statistics depend on the absolute position of the rotor around the circle.
Our predictions are found to be consistent with the available data, including angular diffusion of the motor CITATION, and suggest how steps could be used to study the physical structure of the motor.
A novel testable prediction is that the torque-speed relation will become sublinear at very low torques.
Our model for stepping relies on two main assumptions: constant or nearly constant torque between stator and rotor and an approximately 26-fold periodic contact potential.
All torque-generating units apply torque simultaneously and additively.
Following the model of Meister et al. CITATION, we assume that each MotA/B complex acts as a set of protein springs that reversibly store the energy available from FORMULA translocations.
The protein springs are attached to fixed sites of the rotor circumference.
When an FORMULA passes through the membrane, it causes a spring to detach from its attachment site, stretch, and reattach to the next site.
At stall, all springs are maximally stretched, such that the PMF matches the energy necessary to stretch a spring to its next site.
At low speeds, the rotor moves and springs relax, but these are quickly restretched by FORMULA passage, so that the system remains in quasi-equilibrium with the torque set by the PMF.
Spring stretching may vary slightly among units, but since there are several motor units, we assume that the instantaneous torque self-averages and is nearly constant in time.
Under this scenario, steps cannot be explained at the level of a single pair of MotA/B and FliG subunits, but must arise at the global level of the rotor-stator interaction.
There are contact forces between the stator and the rotor.
These forces may be caused by contact between the MotA/B stator units and FliG proteins, but also possibly by contact with FliF, FlgH or FlgI proteins each of which forms a circle of 26 copies.
There may be other periodicities to the contact forces as well, arising from the filament and the hook, which are 11-fold periodic, from FlgK and FlgL, FlgB, FlgC and FlgF and FliE.
We assume that a 26-fold periodicity is dominant, in agreement with experimental observations.
We therefore collect all contact forces in a potential FORMULA which we suppose to be approximately 26-fold periodic .
Since the motor operates at the molecular scale, its rotation is intrinsically stochastic as it is subject to random thermal fluctuations.
Another potential source of noise is fluctuations of the torque applied by the individual MotA/B stators, due to the discrete nature of the proton flux.
However, in presence of multiple independent stator units, this noise averages out and can be neglected.
Under the combined influence of the applied torque, the contact potential, and thermal fluctuations the rotor performs a circular and continuous random walk in a tilted, approximately periodic potential, which we model by the following Langevin equation:FORMULAwhere FORMULA is the drag coefficient, FORMULA the total torque exerted via protein springs by the stators, and where the potential FORMULA includes the torque, and the approximately 26-fold periodic contact potential:FORMULAThe term FORMULA represents Gaussian white noise and accounts for thermal fluctuations: FORMULA, where FORMULA is the rotor diffusion coefficient, related to the temperature and the drag coefficient via Einstein's relation: FORMULA.
In experiments, a load is attached to a flagellar stump and this load is largely responsible for the drag.
For simplicity, we assume that linkage between motor and load is instantaneous, as the relaxation is rapid compared to the typical stepping time .
ADP-glucose pyrophosphorylase, a key allosteric enzyme involved in higher plant starch biosynthesis, is composed of pairs of large and small subunits.
Current evidence indicates that the two subunit types play distinct roles in enzyme function.
Recently the heterotetrameric structure of potato AGPase has been modeled.
In the current study, we have applied the molecular mechanics generalized born surface area method and identified critical amino acids of the potato AGPase LS and SS subunits that interact with each other during the native heterotetrameric structure formation.
We have further shown the role of the LS amino acids in subunit-subunit interaction by yeast two-hybrid, bacterial complementation assay and native gel.
Comparison of the computational results with the experiments has indicated that the backbone energy contribution of the interface residues is more important in identifying critical residues.
We have found that lateral interaction of the LS-SS is much stronger than the longitudinal one, and it is mainly mediated by hydrophobic interactions.
This study will not only enhance our understanding of the interaction between the SS and the LS of AGPase, but will also enable us to engineer proteins to obtain better assembled variants of AGPase which can be used for the improvement of plant yield.
ADP-glucose pyrophosphorylase is a key regulatory allosteric enzyme involved in starch biosynthesis in higher plants.
It catalyzes the rate limiting reversible reaction and controls the carbon-flux in the -glucan pathway by converting Glucose-1-phosphate and ATP to ADP-glucose and pyrophosphate using Mg 2 as the cofactor CITATION CITATION.
Regulation of almost all AGPases depends on the 3-phosphoglyceric acid to inorganic phosphate ratio.
While 3-PGA functions as the main stimulator, Pi inhibits the activity of enzyme CITATION CITATION.
Plant AGPases consist of pairs of small and large subunits thereby constituting a heterotetrameric structure.
These two subunits are encoded by two distinct genes CITATION.
In potato tuber AGPase the sequence identity between the different subunits is 53 percent suggesting a common ancestral gene CITATION, CITATION.
The molecular weights of tetrameric AGPases range from 200 to 240 kDa depending on the tissue and plant species.
Specifically, molecular weights of LS and SS in potato tuber AGPase are 51 kDa and 50 kDa, respectively CITATION.
It was found that SS and LS have different roles in the enzyme functionality.
SS was shown to have both catalytic and regulatory functions whereas LS is mainly responsible for regulating the allosteric properties of SS CITATION CITATION.
These results were also supported by the studies that showed LS was incapable of assembling into a catalytically active oligomeric structure, whereas SS was able to form a homotetramer with catalytic properties CITATION, CITATION.
However, this SS homotetramer showed defective properties in terms of catalysis and regulation.
It required higher concentrations of 3-PGA for activation and was more sensitive to Pi inhibition.
These results suggested that LS was essential for the enzyme to function efficiently CITATION, CITATION, CITATION.
Alternatively, recent studies have indicated that the LS may bind to substrates glucose-1 phosphate and ATP.
The binding of the LS to substrates may allow the LS to interact cooperatively with the catalytic SS in binding substrates and effectors and, in turn, influence net catalysis CITATION, CITATION CITATION.
In addition, specific regions from both the LS and the SS were found to be important for subunit association and enzyme stability CITATION.
Also, using chimeric maize/potato small subunits, Cross et al. CITATION found a polymorphic motif in the SS which is critical for subunit interaction.
They have concluded that a 55-amino acid region between the residues 322 376 directly interacts with LS and significantly contributes to the overall enzyme stability.
Recently crystal structure of SS was found in a homotetrameric form by Jin et al. CITATION.
Neither the LS nor the heterotetrameric AGPase structure have been solved yet.
This is due to the difficulty of obtaining AGPase in stable form.
However, it is critical to elucidate the native heterotetrameric AGPase structure and identify the key residues taking place in subunit-subunit interactions to obtain a more detailed picture of the enzyme.
Understanding the structure and the hot spot residues in the subunit interface will enable us to manipulate the native enzyme to get a stable form which can be utilized for improving the yield of crops.
The feasibility of such an approach has been shown previously CITATION, CITATION.
We modeled the LS structure of potato tuber AGPase and proposed a model for the heterotetrameric AGPase CITATION.
In this study, we extended our previous work by examining our AGPase model to identify important residues mediating the interactions between the LS and the SS both by computational and experimental techniques.
Based on Molecular mechanics generalized born surface area method, two distinct LS domains are involved in LS-SS subunit interaction.
The residues of the potato AGPase LS Asn 97, Pro 327, Ile 330, Ile 335, Ile 339, Ile 340, and His 342 are involved in lateral interaction with the potato AGPase SS whereas residues Arg 45, Arg 88, Arg 92, and Trp 135 are involved in longitudinal interaction with the potato AGPase SS.
The effect of these mutations on the interactions of the LS and the SS of potato AGPase were further characterized in vivo using the bacterial complementation and the yeast two-hybrid methods.
Also, experimental results indicated that the backbone G binding energy of the interface amino acids is a decisive parameter for the subunit-subunit interaction rather than side chain G binding or total G binding energies.
This study will highlight the important structural aspects of AGPase structure and provide insights for further attempts to engineer a more functional form of the enzyme.
G protein coupled receptors, encoded by about 5 percent of human genes, comprise the largest family of integral membrane proteins and act as cell surface receptors responsible for the transduction of endogenous signal into a cellular response.
Although tertiary structural information is crucial for function annotation and drug design, there are few experimentally determined GPCR structures.
To address this issue, we employ the recently developed threading assembly refinement method to generate structure predictions for all 907 putative GPCRs in the human genome.
Unlike traditional homology modeling approaches, TASSER modeling does not require solved homologous template structures; moreover, it often refines the structures closer to native.
These features are essential for the comprehensive modeling of all human GPCRs when close homologous templates are absent.
Based on a benchmarked confidence score, approximately 820 predicted models should have the correct folds.
The majority of GPCR models share the characteristic seven-transmembrane helix topology, but 45 ORFs are predicted to have different structures.
This is due to GPCR fragments that are predominantly from extracellular or intracellular domains as well as database annotation errors.
Our preliminary validation includes the automated modeling of bovine rhodopsin, the only solved GPCR in the Protein Data Bank.
With homologous templates excluded, the final model built by TASSER has a global C root-mean-squared deviation from native of 4.6, with a root-mean-squared deviation in the transmembrane helix region of 2.1.
Models of several representative GPCRs are compared with mutagenesis and affinity labeling data, and consistent agreement is demonstrated.
Structure clustering of the predicted models shows that GPCRs with similar structures tend to belong to a similar functional class even when their sequences are diverse.
These results demonstrate the usefulness and robustness of the in silico models for GPCR functional analysis.
All predicted GPCR models are freely available for noncommercial users on our Web site .
G protein coupled receptors are integral membrane proteins embedded in the cell surface that transmit signals to cells in response to stimuli such as light, Ca 2, odorants, amino acids, nucleotides, peptides, or proteins and mediate many physiological functions through their interaction with heterotrimeric G proteins CITATION, CITATION.
Many diseases involve the malfunction of these receptors, making them important drug targets.
In human, the estimated number of GPCRs is approximately 948 CITATION, corresponding to about 5 percent of the total number of human genes CITATION.
However, more than 45 percent of all modern drugs target GPCRs; these represent around 25 percent of the 100 top-selling drugs worldwide CITATION, CITATION .
While knowledge of a protein's structure furnishes important information for understanding its function and for drug design CITATION, progress in solving GPCR structures has been slow CITATION.
Nuclear magnetic resonance spectroscopy and X-ray crystallography are the two major techniques used to determine protein structures.
NMR spectroscopy has the advantages that the protein does not need to be crystallized and dynamical information can be extracted.
However, high concentrations of dissolved proteins are needed; and as yet no complete GPCR structure has been solved by the method.
X-ray crystallography can provide very precise atomic information for globular proteins, but GPCRs are extremely difficult to crystallize.
In fact, only a single GPCR, bovine rhodopsin from the rod outer segment membrane, has been solved CITATION.
It is unlikely that a significant number of high-resolution GPCR structures will be experimentally solved in the very near future.
This situation limits the use of structure-based approaches for drug design and restricts research into the mechanisms that control ligand binding to GPCRs, activation and regulation of GPCRs, and signal transduction mediated by GPCRs CITATION .
Fortunately, as demonstrated by the recent CASP experiments CITATION, computer-based methods for deducing the three-dimensional structure of a protein from its amino acid sequence have been increasingly successful.
Among the three types of structure prediction algorithms homology modeling CITATION, CITATION, threading CITATION, CITATION, and ab initio folding CITATION CITATION CM, which builds models by aligning the target sequence to an evolutionarily related template structure, provides the most accurate models.
However, its success is largely dictated by the evolutionary relationship between target and template proteins.
For example, for proteins with greater than 50 percent sequence identity to their templates, CM models tend to be quite close to the native structure, with a 1- root-mean-squared-deviation from native for their backbone atoms, comparable to low-resolution X-ray and NMR experiments CITATION, CITATION.
When the sequence identity drops below 30 percent, termed the twilight zone, CM model accuracy sharply decreases because of the lack of a significant structure match and substantial alignment errors.
Here, the models provided by CM are often closer to the template on which the model is based rather than the native structure of the sequence of interest.
This has been a significant unsolved problem CITATION.
Among all registered human GPCRs, there are only four sequences that have a sequence identity to bovine RH greater than 30 percent.
Ninety-nine percent of human GPCRs, with an average sequence identity to bovine RH of 19.5 percent, lie outside the traditional comparative modeling regimen CITATION .
Recently CITATION, CITATION, CITATION, CITATION, we developed the threading assembly refinement methodology, which combines threading and ab initio algorithms to span the homologous to nonhomologous regimens.
In a large-scale, comprehensive benchmark test of 2,234 representative proteins from the Protein Data Bank CITATION, after excluding templates having greater than 30 percent sequence identity to the target, two thirds of single domain proteins can be folded to models with a C RMSD to native of less than 6.5 CITATION, CITATION.
As a significant advance over traditional homology modeling, many models are improved with respect to their threading templates .
In the absence of additional GPCR crystal structures, computer-based modeling may provide the best alternative to obtaining structural information CITATION CITATION.
In this work, we exploit TASSER to predict tertiary structures for all 907 GPCR sequences in the human genome that are less than 500 amino acids in length.
Only the sequence of the given GPCR is passed to TASSER and no other extrinsic knowledge is incorporated into our structure prediction approach.
Because the rearrangements of TM helices from RH may occur for nonhomologous GPCRs, the ability to refine templates is the most important advantage of using TASSER in comprehensive GPCR modeling.
Also, distinct from many other GPCR modeling methods that only attempt to model the TM helical regions CITATION, CITATION, CITATION, TASSER generates reasonable predictions for the loop regions.
In benchmark tests CITATION, for 39 percent of loops of four or more residues, TASSER models have a global RMSD less than 3 from native.
In contrast, using the widely used homology modeling tool, MODELLER CITATION, CITATION, the percentage of loops with this accuracy is 12 percent CITATION.
If one considers only the accuracy of the loop conformation itself, then 89 percent of the TASSER-generated loops have a local RMSD of less than 3, and the average RMSD for loops up to 50 residues is below 4.
This is especially important in GPCR modeling as the extracellular loops are often critical in determining ligand specificity CITATION CITATION.
Therefore, full-length TASSER models offer substantial advantages over traditional comparative modeling methods and are likely to be of greater aid in understanding the ligand and signaling interactions of GPCRs.
In a 1997 seminal paper, W. Maddison proposed minimizing deep coalescences, or MDC, as an optimization criterion for inferring the species tree from a set of incongruent gene trees, assuming the incongruence is exclusively due to lineage sorting.
In a subsequent paper, Maddison and Knowles provided and implemented a search heuristic for optimizing the MDC criterion, given a set of gene trees.
However, the heuristic is not guaranteed to compute optimal solutions, and its hill-climbing search makes it slow in practice.
In this paper, we provide two exact solutions to the problem of inferring the species tree from a set of gene trees under the MDC criterion.
In other words, our solutions are guaranteed to find the tree that minimizes the total number of deep coalescences from a set of gene trees.
One solution is based on a novel integer linear programming formulation, and another is based on a simple dynamic programming approach.
Powerful ILP solvers, such as CPLEX, make the first solution appealing, particularly for very large-scale instances of the problem, whereas the DP-based solution eliminates dependence on proprietary tools, and its simplicity makes it easy to integrate with other genomic events that may cause gene tree incongruence.
Using the exact solutions, we analyze a data set of 106 loci from eight yeast species, a data set of 268 loci from eight Apicomplexan species, and several simulated data sets.
We show that the MDC criterion provides very accurate estimates of the species tree topologies, and that our solutions are very fast, thus allowing for the accurate analysis of genome-scale data sets.
Further, the efficiency of the solutions allow for quick exploration of sub-optimal solutions, which is important for a parsimony-based criterion such as MDC, as we show.
We show that searching for the species tree in the compatibility graph of the clusters induced by the gene trees may be sufficient in practice, a finding that helps ameliorate the computational requirements of optimization solutions.
Further, we study the statistical consistency and convergence rate of the MDC criterion, as well as its optimality in inferring the species tree.
Finally, we show how our solutions can be used to identify potential horizontal gene transfer events that may have caused some of the incongruence in the data, thus augmenting Maddison's original framework.
We have implemented our solutions in the PhyloNet software package, which is freely available at: LINK.
Accurate species trees, which model the evolutionary histories of sets of species, play a central role in comparative genomics, conservation studies, and analyses of population divergence, among many other applications.
Traditionally, a species tree is inferred by sequencing a single locus in a group of species, its tree, known as the gene tree, is reconstructed using a method such as maximum likelihood, and this tree is declared to be the species tree.
The underlying assumption is, obviously, that the gene tree and the species tree are identical, and hence reconstructing the former amounts to learning the latter.
However, biologists have long recognized that this assumption is not necessarily always valid.
Nevertheless, due to limitations of sequencing technologies, this approach remained the standard method until very recently.
With the advent of whole-genome sequencing, complete genomes of various organisms are becoming increasingly available, and particularly important, data from multiple loci in organisms are becoming available.
The availability of such data has allowed for analyzing multiple loci in various groups of species.
These analyses have in many cases uncovered widespread incongruence among the gene trees of the same set of organisms.
Therefore, while reconstructing a gene tree requires considering the process of nucleotide substitution, reconstructing a species tree requires, in addition, considering the process that resulted in the incongruities among the gene trees, so that the species phylogeny is inferred by reconciling these incongruities.
In this paper, we address the problem of efficient inference of accurate species trees from multiple loci, when the gene trees are assumed to be correct, and their incongruence is assumed to be exclusively due to lineage sorting.
We also address the integration of horizontal gene transfer, as a potential cause of gene tree incongruence, into the framework.
Let us illustrate the process of lineage sorting and the way it causes gene tree incongruence.
From an evolutionary perspective, and barring any recombination, the evolutionary history of a set of genomes would be depicted by a tree that is the same tree that models the evolution of each gene in these genomes.
However, events such as recombination break linkage among the different parts of the genome, and those unlinked parts may take different paths through the phylogeny, which results in gene trees that differ from the species tree as well as from each other, due to lineage sorting.
Widespread gene tree incongruence due to lineage sorting has been shown recently in several groups of closely related organisms, including yeast CITATION, Drosophila CITATION, Staphylococcus aureus CITATION, and Apicomplexan CITATION.
In this case, gene trees need be reconciled within the branches of the species tree, as shown in Figure 1.
A few methods have been introduced recently for analyzing gene trees, reconciling their incongruities, and inferring species trees despite these incongruities.
Generally speaking, each of these methods follows one of two approaches: the combined analysis approach or the separate analysis approach; see Figure 2.
In the combined analysis aproach, the sequences from multiple loci are concatenated, and the resulting supergene data set is analyzed using traditional phylogenetic methods, such as maximum parsimony or maximum likelihood; e.g., CITATION.
In the separate analysis approach, the sequence data from each locus is first analyzed individually, and a reconciliation of the gene trees is then sought.
One way to reconcile the gene trees is by taking their majority consensus; e.g., CITATION.
Another is the democratic vote method, which entails taking the tree topology occurring with the highest frequency among all gene trees as the species tree.
Shortcomings of these methods based on the two approaches have been analyzed by various researchers CITATION, CITATION.
Recently, Bayesian methods following the separate analysis approach have been developed CITATION, CITATION.
While these methods have a firm statistical basis, they are very time consuming, taking hours and days even on moderate-size data sets, which limits their scalability .
In CITATION, Maddison proposed a parsimony-based approach for inferring species trees from gene trees by minimizing the number of extra lineages, or minimizing deep coalesces.
A heuristic for this approach was later described in CITATION.
In CITATION, Than et al. provided a two-stage heuristic for inferring the species tree under the MDC criterion.
However, no exact solutions for computing the MDC criterion exist.
In this paper, we provide a formal definition of the notion of extra lineages, first described in CITATION.
We then present exact solutions an integer linear programming algorithm and a dynamic programming algorithm for finding the optimal species tree topology from a set of gene tree topologies, under the MDC criterion.
Our solutions are based on two central observations: the species tree is a maximal clique in the compatibility graph of the set of species clusters, and quantifying the amount of incongruence between a set of gene trees and a species tree can be obtained by a simple counting of lineages within the branches of the species tree.
The accuracy and computational efficiency of these solutions, as we demonstrate, allow for analysis of genome-scale data sets and analysis of large numbers of data sets, such as those involved in simulation studies.
Given that MDC is a parsimonious explanation of the incongruence in the data, it is imperative that sub-optimal solutions are considered.
The computational efficiency of our solutions allow for a rapid exploration of sub-optimal solutions.
Last but not least, these exact solutions allow us to empirically study properties of MDC as an optimality criterion for inferring the species tree.
We have implemented both exact solutions in the PhyloNet software package CITATION .
We reanalyze the Apicomplexan data set of CITATION, the yeast data set of CITATION, and a large number of synthetic data sets of species/gene trees that we simulated using the Mesquite tool of CITATION.
For each data set, our method computed the species tree in at most a few seconds, and produced very accurate species trees, as we show.
In the case of the Apicomplexan data set, we provide a tree that is slightly different from the one proposed by the authors in CITATION, and discuss this tree.
For the yeast data set, we obtain a tree that is identical to the one proposed by the authors in CITATION, as well as other studies, such as CITATION.
In addition to the quality of the species trees and efficiency with which our method inferred them, one advantage of our method is that it can be used in an exploratory fashion, to screen multiple species tree candidates, and study the reconciliation scenarios within the branches of each of them.
We illustrate the utility of this capability on the yeast and Apicomplexan data sets.
Further, for the Apicomplexan data set, we illustrate how to screen for possible horizontal gene transfer events using the reconciliation scenarios computed by other methods.
Using the synthetic data sets, we study the statistical consistency, as well as convergence rate, of the MDC criterion.
We also show that it may be sufficient to consider only the set of clusters induced by the gene trees, which, in practice, may be much smaller than the set of all clusters of species, thus achieving further reduction in computation time.
Nonetheless, we present an example to illustrate that, in certain cases, focusing only on the gene tree clusters may result in a sub-optimal species tree under MDC.
The computational efficiency of our methods, coupled with the promising properties of the MDC criterion, makes our methods particularly applicable to large, genome-scale data sets.
Simian Virus 40 Large Tumor Antigen is an efficient helicase motor that unwinds and translocates DNA.
The DNA unwinding and translocation of LTag is powered by ATP binding and hydrolysis at the nucleotide pocket between two adjacent subunits of an LTag hexamer.
Based on the set of high-resolution hexameric structures of LTag helicase in different nucleotide binding states, we simulated a conformational transition pathway of the ATP binding process using the targeted molecular dynamics method and calculated the corresponding energy profile using the linear response approximation version of the semi-macroscopic Protein Dipoles Langevin Dipoles method.
The simulation results suggest a three-step process for the ATP binding from the initial interaction to the final tight binding at the nucleotide pocket, in which ATP is eventually locked by three pairs of charge-charge interactions across the pocket.
Such a cross-locking ATP binding process is similar to the binding zipper model reported for the F1-ATPase hexameric motor.
The simulation also shows a transition mechanism of Mg 2 coordination to form the Mg-ATP complex during ATP binding, which is accompanied by the large conformational changes of LTag.
This simulation study of the ATP binding process to an LTag and the accompanying conformational changes in the context of a hexamer leads to a refined cooperative iris model that has been proposed previously.
Helicases are a family of ATPase motors that couple the energy of ATP binding and hydrolysis to conformation changes, which in turn is coupled to the unwinding and translocation of DNA CITATION.
Simian Virus 40 large tumor antigen is an efficient hexameric helicase that belongs to the helicase superfamily III, as well as the AAA protein family.
The high resolution structures of LTag hexameric helicase in different nucleotide binding states have been previously reported CITATION, CITATION, including the Apo, the ATP-bound and the ADP-bound states.
These three structures reveal an iris-like motion of the hexamer helicase during the drastic conformational switches that are triggered by ATP binding and hydrolysis.
Accompanying the iris-motion of the LTag hexamer is the longitudinal movements of the six -hairpins along the central channel.
Despite the advancement in LTag helicase studies mentioned above, the detailed paths for these conformational switches and the corresponding energetics associated with the ATP binding process are unknown, which can be simulated by a computational approach using molecular dynamics and targeted molecular dynamics.
Molecular dynamics propagates the molecular system under the laws of classic mechanics CITATION, CITATION, and is suitable for studying conformational changes.
However, the current computational capability restricts the size of the studied system and the time scale of MD simulation.
For the studies of larger and more complex systems, targeted molecular dynamics has been used to accelerate the simulation, which adopts an additional holonomic constraint on the physical potential to reduce the root mean square deviation between the current structure and the final structure CITATION.
TMD is suitable to calculate the transition pathways between two known protein conformations.
The combination of MD and TMD methods have been widely applied to the dynamics studies of various systems, such as the Ras p21 in the signal transition pathway CITATION, F1-ATPase system CITATION, CITATION, the GroEL complex CITATION, and the human a-7 nAChR receptor CITATION.
Here we adopted TMD to calculate the whole transition pathway and used MD to simulate the accurate conformational change in certain key time slots.
In order to understand the energetic aspects of ATP triggered conformational changes of LTag hexameric helicase, we simulated the ATP binding process of LTag and the associated conformational changes.
We first built an Apo state with six ATPs placed 20 away from the binding pocket of the original Apo structure.
Then we used the TMD approach to calculate the transition pathway from the Apo state to the ATP bound state and examined the ATP binding process that powered this conformational transition.
The results suggest an ATP molecule goes through a three-step process before being locked inside the nucleotide pocket.
Meanwhile, the configurations of the binding pocket along the ATP binding pathway were evaluated by using the linear response approximation version of the semi-macroscopic protein dipoles langevin dipoles method, a method that is capable of evaluating binding free energies significantly faster than the microscopic methods with comparable accuracy CITATION.
In addition, the simulation results of the conformational transition reveal a refined pathway for the cooperative iris-like movement of LTag hexamer helicase previously observed in crystal structures.
Large-scale protein interaction networks have typically been discerned using affinity purification followed by mass spectrometry and yeast two-hybrid techniques.
It is generally recognized that Y2H screens detect direct binary interactions while the AP/MS method captures co-complex associations; however, the latter technique is known to yield prevalent false positives arising from a number of effects, including abundance.
We describe a novel approach to compute the propensity for two proteins to co-purify in an AP/MS data set, thereby allowing us to assess the detected level of interaction specificity by analyzing the corresponding distribution of interaction scores.
We find that two recent AP/MS data sets of yeast contain enrichments of specific, or high-scoring, associations as compared to commensurate random profiles, and that curated, direct physical interactions in two prominent data bases have consistently high scores.
Our scored interaction data sets are generally more comprehensive than those of previous studies when compared against four diverse, high-quality reference sets.
Furthermore, we find that our scored data sets are more enriched with curated, direct physical associations than Y2H sets.
A high-confidence protein interaction network derived from the AP/MS data is revealed to be highly modular, and we show that this topology is not the result of misrepresenting indirect associations as direct interactions.
In fact, we propose that the modularity in Y2H data sets may be underrepresented, as they contain indirect associations that are significantly enriched with false negatives.
The AP/MS PIN is also found to contain significant assortative mixing; however, in line with a previous study we confirm that Y2H interaction data show weak disassortativeness, thus revealing more clearly the distinctive natures of the interaction detection methods.
We expect that our scored yeast data sets are ideal for further biological discovery and that our scoring system will prove useful for other AP/MS data sets.
Insights into the architectures and mechanisms of cellular processes can be obtained by elucidation of genome-wide protein interaction networks that describe the physical associations between the component proteins.
Such maps, or interactomes, can be exploited to enhance many types of biological discovery including protein function prediction CITATION, inference of disease genes CITATION, and identification of condition-specific response modules CITATION.
The yeast Saccharomyces cerevisiae has been routinely employed as a model system for high-throughput studies and PINs have been determined using a number of platforms including yeast two-hybrid screens CITATION CITATION, affinity purification followed by mass spectrometry CITATION CITATION, and protein-fragment complementation assays CITATION.
Each approach perceives interactions in a distinct manner.
The Y2H and PCA techniques detect direct binary interactions, although the PCA approach does not rely upon expression of a reporter gene as required in Y2H screens, while the AP/MS techniques purify and identify protein complexes.
The reliability of each technique has been extensively debated in the literature and comprehensive analyses have resulted in contrasting conclusions CITATION, CITATION CITATION.
However, it is generally accepted that any measure of reliability is not absolute and largely dependent on the nature of a pre-defined gold standard reference set.
An additional complexity arises in the analysis, or interpretation, of an AP/MS data set because there is no standard, or well-defined, system to distinguish between the direct and indirect interactions present in a purified complex.
The only information available for an individual purification is its composition: a tagged bait protein and associated co-purified prey proteins.
Furthermore, the constituent proteins are identified by complex MS methods and different platforms often yield varying compositions for identical purifications CITATION, CITATION.
Another concern is that the compositions of the purifications are influenced by the protein abundances CITATION, CITATION, CITATION - proteins having a higher abundance are more likely to be detected in more purifications and, therefore, inferred to be involved in more interactions after tabulation of all bait-prey pairs CITATION.
To address these issues, a number of approaches for the analysis of AP/MS data sets have been employed CITATION, CITATION, CITATION, CITATION.
These techniques have the common goal of discerning protein pairs that are appreciably co-purified relative to some random background.
While each method determines scores representing the likelihood of observing two proteins together, the scores are computed using different procedures: Gavin et al. calculate log-ratios of observed co-occurrences relative to expected CITATION ; Krogan et al. utilize a combination of machine learning algorithms CITATION ; Collins et al. implement a supervised algorithm derived from Bayesian methods and optimized with empirically-derived parameters CITATION ; and Hart et al. determine interaction probabilities based on hypergeometric distributions CITATION.
The qualities of the generated PINs have been found to be superior to comparable data sets constructed by straightforward tabulations of bait-prey interactions CITATION, CITATION, CITATION.
These evaluations were generally deduced from direct comparisons against complexes manually curated by the Munich Information Center on Protein Sequences CITATION .
A recent study of high-throughput Y2H data sets explored the characteristic strengths and distributions of functional interactions and non-functional interactions in order to assess the extent to which the latter impedes the formation of functional protein complexes CITATION.
It was conjectured that the overall impact upon biochemical efficiencies had evolved to a tolerable limit.
Motivated by the use of randomization techniques as a tool to measure, or discover, enrichments of network motifs CITATION and connectivity correlations CITATION in complex networks, we developed a shuffling-based approach to assess the levels of interaction specificity detected in AP/MS data sets.
This system allows for the computation of pair-wise protein co-occurrence significance scores by comparing experimentally observed numbers with those from randomized realizations.
A CS score for two proteins provides a statistical measure of their propensity to co-purify, or interact, in an AP/MS data set.
The approach requires no training set or machine learning and is, therefore, applicable to any AP/MS data set for any species regardless of whether any curated information exists or not.
It is found that these AP/MS data sets contain significant enrichments of specific, or high-scoring, associations.
Additionally, we showed that high-quality direct physical interactions curated in two prominent data bases have significantly high CS scores.
Therefore, while the AP/MS data sets contain prevalent non-specific, or transient, associations, our scoring analysis reveals that there is an underlying preference for proteins to form selective, or discriminating, associations.
Our resultant scored interaction data sets were further assessed by comparisons against four diverse, high-quality reference data sets, each representing a unique manner of interaction detection, association mechanism, and/or curation.
For most references, we found that the accuracies of our scored interaction sets were manifestly higher than those of previous studies.
Additionally, our scored data sets are the only ones that typically outperformed experimental Y2H interaction sets CITATION CITATION.
A high-confidence PIN extracted from the AP/MS data of Gavin et al. CITATION was revealed to be free of abundance effects while those derived from the data of Krogan et al. CITATION contained weak abundance biases.
Therefore, it would appear that in high-quality AP/MS data sets, interaction specificity is not coupled with protein abundance.
We note that the converse has recently been found to be true of Y2H interaction data sets CITATION .
The high-confidence PIN derived from the data of Gavin et al. CITATION was shown to be highly modular, containing many localized densely-connected regions, and strikingly different to a commensurate random network.
We also demonstrated that the observed high modularity is not a result of misinterpreting indirect associations as direct interactions; rather, it is a result of direct physical associations.
Furthermore, we suggest that the modularity in Y2H interaction data sets may be underrepresented as indirect associations in these PINs are significantly enriched with manually-curated physical interactions, i.e., they are likely false negatives.
The high-confidence AP/MS PIN shows assortative mixing, meaning that proteins having similar numbers of total interactions prefer to interact with each other.
A consequence of assortativity is that high-degree proteins, or hubs, prefer to associate with each other rather than with proteins having very small numbers of total interactions.
In agreement with a previous study CITATION, we find that a consolidated Y2H PIN shows weak disassortative mixing while a manually-curated set of high-confidence physical binary interactions displays both, and in equal measure, assortative and disassortative mixing.
Therefore, high-quality AP/MS data appear assortative while Y2H interaction data appear disassortative.
We expect that our scored yeast data sets are ideal for further investigations involving biological discovery and that our procedure will prove useful for the analysis of current and future AP/MS data sets for a variety of species.
We have compared our high-quality AP/MS interaction data sets with those from Y2H screens and perceived a number of novel insights regarding their substances and network properties.
Certainly, their topologies are contrasting and must reflect their different methods of interaction detection.
Autoregulation of transcription factors and cross-antagonism between lineage-specific transcription factors are a recurrent theme in cell differentiation.
An equally prevalent event that is frequently overlooked in lineage commitment models is the upregulation of lineage-specific receptors, often through lineage-specific transcription factors.
Here, we use a minimal model that combines cell-extrinsic and cell-intrinsic elements of regulation in order to understand how both instructive and stochastic events can inform cell commitment decisions in hematopoiesis.
Our results suggest that cytokine-mediated positive receptor feedback can induce a switch-like response to external stimuli during multilineage differentiation by providing robustness to both bipotent and committed states while protecting progenitors from noise-induced differentiation or decommitment.
Our model provides support to both the instructive and stochastic theories of commitment: cell fates are ultimately driven by lineage-specific transcription factors, but cytokine signaling can strongly bias lineage commitment by regulating these inherently noisy cell-fate decisions with complex, pertinent behaviors such as ligand-mediated ultrasensitivity and robust multistability.
The simulations further suggest that the kinetics of differentiation to a mature cell state can depend on the starting progenitor state as well as on the route of commitment that is chosen.
Lastly, our model shows good agreement with lineage-specific receptor expression kinetics from microarray experiments and provides a computational framework that can integrate both classical and alternative commitment paths in hematopoiesis that have been observed experimentally.
Multipotent stem cells have the ability to both self-renew and differentiate, thus sustaining the stem cell pool and giving rise to mature, specialized cells, respectively.
The hematopoietic stem cell, located in the adult bone marrow, is well characterized and has served as a popular model system for understanding self-renewal, lineage commitment, and differentiation CITATION.
HSCs are responsible for producing the entire repertoire of blood cells through the process of hematopoiesis.
During hematopoiesis, HSCs lose the capacity to self-renew and differentiate into common myeloid progenitors and common lymphoid progenitors CITATION, CITATION.
Multipotent progenitors undergo further lineage-restricted differentiation to give rise to mature cells via bipotent progenitors.
In addition to this classical commitment paradigm in hematopoiesis, alternative pathways are emerging.
For example, it has also been observed that HSCs and multipotent progenitors can bypass canonical intermediate states during commitment CITATION, CITATION, CITATION.
The exact molecular events that direct lineage commitment at the stem cell stage or at the multipotent progenitor level remain elusive, but it is well appreciated that lineage-specific transcription factors and cytokine receptors play critical roles.
Lineage-specific transcription factors have been identified as master regulators of commitment and differentiation.
They drive the expression of pertinent lineage-specific genes, thereby initiating the phenotypic change in the progenitor cell down a specific differentiation path CITATION, CITATION.
Developmental potency of a multipotent progenitor is reflected by the co-expression of multiple lineage-specific transcription factors at low levels, a phenomenon known as transcriptional priming CITATION.
This promiscuous gene expression pattern in the progenitor cell necessitates that, during cell differentiation, a specific transcription factor is upregulated, chiefly by positive autoregulation CITATION, CITATION, and other lineage transcription factors are downregulated, primarily through cross-antagonism CITATION CITATION .
In addition to lineage-specific transcription factors, cell differentiation is also believed to be tightly regulated by cytokines.
Cytokines signal via their cognate receptors whose cytoplasmic domains activate various pathways involved in survival, proliferation, and differentiation CITATION CITATION.
It has been extensively debated whether cell fate during differentiation is a stochastic or an instructive process.
The stochastic theory claims that the differential expression of lineage-specific transcription factors due to intrinsic noise in progenitor cells dictates the commitment decision CITATION CITATION, whereas the instructive theory argues that the absolute dependence on lineage-specific cytokine receptor signals during differentiation shows that cell-fate decisions are regulated by extrinsic growth factor cues CITATION, CITATION, CITATION, CITATION.
An underlying question evoked by both of these theories is whether cytokines provide instructive cues or select lineage-committed progenitors by providing permissive survival and proliferation signals.
The instructive model does not account for the occurrence of certain mature cell types even when their lineage-specific receptors are knocked out CITATION, CITATION.
The predetermined distribution of the heterogeneous progenitor population into mature cells, as suggested by the stochastic model fails to explain how specific cell types can be enriched during stress or how homeostasis is restored after infections or therapy CITATION.
A recent landmark study utilizing bioimaging techniques at the single-cell level suggests that there is validity to both of these theories CITATION.
These authors showed that lineage-specific cytokines can strongly instruct lineage choice, although differentiation was still possible in the absence of lineage-specific cytokines.
A more comprehensive understanding of lineage commitment may emerge by analyzing the biochemical associations that coordinate cell-extrinsic and cell-intrinsic events.
The promiscuous gene expression pattern during differentiation is observed not only in lineage-specific transcription factors, but also in lineage-specific receptors.
A critical commitment signal during differentiation is the upregulation of the transcription factor, which aids in expressing the lineage-specific genes; however, the need to upregulate the lineage-specific receptor, an event also integral to commitment, is still unclear.
This is particularly confounding since the low number of lineage-specific receptors present in a progenitor cell is sufficient for providing permissive survival cues.
During lineage commitment, the expression of the cytokine receptor mirrors the expression of the transcription factor, often due to the presence of transcription factor binding domains in the promoter region of the receptor gene CITATION CITATION.
The advantage of regulating the lineage-specific receptor expression through the lineage-specific transcription factor is not apparent.
Recent biochemical evidence also suggests that cytokines can provide signals to functionally activate lineage-specific transcription factors through post-translational modifications CITATION and can also regulate the expression of transcription factors during cell differentiation CITATION .
Cell differentiation is believed to be an all-or-none switch-like event rather than a gradual transition of a precursor cell to a stable, mature cell.
Mathematical modeling and analysis have been successfully used to provide insights into the biological networks that give rise to such switch-like behaviors CITATION.
Typically, the networks involved in lineage specification seem to engender cellular memory through nonintuitive behaviors, such as bistable response profiles.
The components that generate bistability, the toggling of the system between two stable steady states, include nonlinear feedback loops CITATION, CITATION, external noise CITATION, and multi-site covalent modifications CITATION.
Previous lineage commitment models have suggested that transcriptionally primed multipotent progenitors are capable of exhibiting bistability purely via cell intrinsic events of autoregulation and cross-antagonism CITATION, CITATION, CITATION, but these models have assumed the existence of cooperative positive feedback loops to achieve bistability and do not consider the role of extracellular cues.
While cooperativity is a widely recognized biological mechanism that may play an important role in lineage commitment, alternative mechanisms can generate similar switch-like behavior in networks where cooperativity has not been observed.
For example, we have previously shown that cytokine-regulated, positive feedback of receptor can generate robust bistability to stimulus without cooperativity in a deterministic model for unilineage commitment CITATION.
Furthermore, even in networks with cooperativity, receptor-mediated feedback may provide additional robustness to the system behavior and, perhaps more importantly, offer an external mode of regulation of cell-fates.
Here, we present a minimal model that integrates the bidirectional regulation between lineage-specific cytokines and transcription factors with previously explored autofeedback loops and cross-antagonism to understand the interplay between cell-extrinsic and cell-intrinsic factors in fate decisions of hematopoietic progenitors.
Our model shows that the strength of cross-antagonism can be a critical determinant in achieving multistability.
The analyzed network exhibits a bilayer of memory with respect to external stimuli to provide robustness to both the bipotent and committed states.
The model suggests that noise in the network can enable stochastic switching between the stable states; however, the distribution of the uncommitted population among the various states during differentiation can still be strongly biased by external cues.
Furthermore, this modeling framework captures both classical and alternative modes of lineage commitment seen in hematopoiesis.
Although discrete cell fates are likely to represent high-dimensional attractors CITATION, CITATION, our minimal model may provide an initial step towards understanding how extrinsic factors integrate with intrinsic factors and may elucidate new mechanisms that underlie cell-fate decisions.
As a key factor in endemic and epidemic dynamics, the geographical distribution of viruses has been frequently interpreted in the light of their genetic histories.
Unfortunately, inference of historical dispersal or migration patterns of viruses has mainly been restricted to model-free heuristic approaches that provide little insight into the temporal setting of the spatial dynamics.
The introduction of probabilistic models of evolution, however, offers unique opportunities to engage in this statistical endeavor.
Here we introduce a Bayesian framework for inference, visualization and hypothesis testing of phylogeographic history.
By implementing character mapping in a Bayesian software that samples time-scaled phylogenies, we enable the reconstruction of timed viral dispersal patterns while accommodating phylogenetic uncertainty.
Standard Markov model inference is extended with a stochastic search variable selection procedure that identifies the parsimonious descriptions of the diffusion process.
In addition, we propose priors that can incorporate geographical sampling distributions or characterize alternative hypotheses about the spatial dynamics.
To visualize the spatial and temporal information, we summarize inferences using virtual globe software.
We describe how Bayesian phylogeography compares with previous parsimony analysis in the investigation of the influenza A H5N1 origin and H5N1 epidemiological linkage among sampling localities.
Analysis of rabies in West African dog populations reveals how virus diffusion may enable endemic maintenance through continuous epidemic cycles.
From these analyses, we conclude that our phylogeographic framework will make an important asset in molecular epidemiology that can be easily generalized to infer biogeogeography from genetic data for many organisms.
Phylogenetic inference from molecular sequences is becoming an increasingly popular tool to trace the patterns of pathogen dispersal.
The time-scale of epidemic spread usually provides ample time for rapidly evolving viruses to accumulate informative mutations in their genomes CITATION.
As a consequence, spatial diffusion among other processes can leave a measurable footprint in sampled gene sequences from these viruses CITATION.
Reconstructing both the evolutionary history and spatial process from these sequences provides fundamental understanding of the evolutionary dynamics underlying epidemics, e.g. CITATION, CITATION.
It is also hoped that these insights can be translated to effective intervention and prevention strategies CITATION and elucidating the key factors in viral transmission and gene flow over larger distances is central in formulating such strategies, e.g. CITATION .
Phylogeographic analyses are a common approach in molecular ecology, connecting historical processes in evolution with spatial distributions that traditionally scale over millions of years CITATION.
Many popular phylogeographic approaches CITATION, CITATION can be remiss in ignoring the interaction between evolutionary processes and spatial-temporal domains.
One first reconstructs a phylogeny omitting spatial information and then conditions the phylogeographic inferences on this reconstruction CITATION, CITATION, exploiting non-parametric tests to evaluate the significance of this conditional structure, e.g. CITATION, CITATION, CITATION.
To draw conclusions about the epidemic origin or epidemiological linkage between locations, however, we require a reconstruction of the dispersal patterns and process throughout the evolutionary history.
Considering locations as discrete states, this boils down to the well-known problem of ancestral state inference CITATION.
Parsimony is a popular heuristic approach to map characters onto a single phylogenetic tree CITATION.
Unfortunately, parsimony reconstructions ignore important sources of model uncertainty, including both uncertainty in the dispersal process as well as in the unknown phylogeny CITATION.
In addition, minimizing the number of state exchanges over a phylogeny is misleading when rates of evolution are rapid and when the state exchange probabilities are unequal CITATION .
Probabilistic methods draw on an explicit model of state evolution, permitting the ability to glimpse the complete state history over the entire phylogeny and conveniently draw statistical inferences CITATION CITATION.
These analyses typically employ continuous-time Markov chain models for discrete state evolution analogous to common nucleotide, codon or amino acid substitution models CITATION.
In contrast to parsimony, maximum likelihood-based reconstructions incorporate branch length differences in calculating the conditional probability of each ancestral state given the observed states at the phylogeny tips CITATION.
Bayesian reconstruction methods enable further generalization of this conditional probability analysis by removing the necessity to fix the Markov model parameters to obtain ancestral states and the necessity to specify a fixed tree topology with known branch lengths.
Bayesian inference integrates conclusions over all possible parameter values but to achieve this, however, requires prior probability distributions for all aspects of the model.
While probabilistic methods have been previously presented in a bio- or phylogeographic context, in particular Bayesian methods that integrate over phylogenetic uncertainty and Markov model parameter uncertainty CITATION, viral phylogeography studies have rarely made use of these developments.
This may be a consequence of low awareness of existing software implementations for arbitrary continuous-time Markov chain models CITATION, CITATION or a lack of appreciation for the uncertainty intrinsic in these reconstructions and the ease with which one can formally access epidemiological linkage through probabilistic approaches.
A recent phylogeographic study of influenza A H5N1 introduces a heuristic non-parametric test to evaluate whether parsimony-inferred migration events between two particular locations occur at significantly high frequency CITATION.
Null distributions for these frequencies arise from randomizing tip localities after false discovery rate correction to control for simultaneous testing issues.
Although this procedure addresses concerns about statistical inference on sparse frequency matrices, the multiple comparison correction still results in a conservative estimate of significant migration events.
Fully probabilistic approaches may further ease statistical inference, yet similar tests remain lacking for likelihood-based phylogeographic models.
Advances in evolutionary inference methodology have frequently demonstrated how novel approaches can be appended to a sequence of analyses, in many cases starting from alignment to parameter estimation conditional on tree reconstructions.
For example, demographic inference has involved genealogy reconstruction, estimating a time scale for the evolutionary history, and coalescent theory to quantify the demographic impact on this tree shape CITATION.
It is well acknowledged that such sequential procedures ignore important sources of uncertainty because they generally purge error associated with each intermediate estimate.
With the advent of novel computational techniques like Markov chain Monte Carlo sampling, it has become feasible to integrate many of the models involved and simultaneously estimate parameters of interest.
Demographic inference is a well-known example of genealogy-based population genetics that benefited from these advances CITATION, CITATION.
Bayesian MCMC methods also enable ancestral state reconstruction while simultaneously accounting for both phylogenetic and mapping uncertainty.
Although this adds much needed credibility to ancestral reconstruction CITATION, phylogeographic analysis would benefit even more from fully integrating spatial, temporal and demographic inference.
Here, we implement ancestral reconstruction of discrete states in a Bayesian statistical framework for evolutionary hypothesis testing that is geared towards rooted, time-measured phylogenies.
This allows character mapping in natural time scales, calibrated under a strict or relaxed molecular clock, in combination with several models of population size change.
We use this full probabilistic approach to study viral phylogeography and extend the Bayesian implementation to a mixture model in which exchange rates in the Markov model are allowed to be zero with some probability.
This Bayesian stochastic search variable selection enables us to construct a Bayes factor test that identifies the most parsimonious description of the phylogeographic diffusion process.
We also demonstrate how the geographical distribution of the sampling locations can be incorporated as prior specifications.
Through feature-rich visual summaries of the space-time process, we demonstrate how this approach can offer insights into the spatial epidemic history of Avian influenza A-H5N1 and rabies viruses in Africa.
The highly pathogenic avian influenza A-H5N1 viruses have been present for over a decade in Southern China and spread in multiple waves to different types of poultry in countries across Asia, Africa and Europe CITATION.
As a result, highly pathogenic A-H5N1 is now a panzootic disease and represents a continuous threat for human spill-over.
Strong surveillance has been in place since these viruses caused extensive outbreaks, but the source and early dissemination pathways have remained uncertain.
Because parsimony analysis has attempted to shed light on the latter CITATION, A-H5N1 provides an ideal example for comparison with Bayesian phylogeographic inference.
Rabies is endemic in Asia and Africa, where the primary reservoir and vector for rabies virus is the domestic dog.
Phylogenetic analysis has revealed several genotypes of lyssaviruses ; genotype 1 has been found responsible for classical rabies, a fatal disease in terrestrial mammals throughout the world CITATION, CITATION.
Here, we explore the phylogeographic history of RABV in domestic dogs in West Central Africa, using recently obtained sequence data, and evaluate the role of viral dispersal in maintaining RABV epidemic cycles.
The mechanism for cortical folding pattern formation is not fully understood.
Current models represent scenarios that describe pattern formation through local interactions, and one recent model is the intermediate progenitor model.
The intermediate progenitor model describes a local chemically driven scenario, where an increase in intermediate progenitor cells in the subventricular zone correlates to gyral formation.
Here we present a mathematical model that uses features of the IP model and further captures global characteristics of cortical pattern formation.
A prolate spheroidal surface is used to approximate the ventricular zone.
Prolate spheroidal harmonics are applied to a Turing reaction-diffusion system, providing a chemically based framework for cortical folding.
Our model reveals a direct correlation between pattern formation and the size and shape of the lateral ventricle.
Additionally, placement and directionality of sulci and the relationship between domain scaling and cortical pattern elaboration are explained.
The significance of this model is that it elucidates the consistency of cortical patterns among individuals within a species and addresses inter-species variability based on global characteristics and provides a critical piece to the puzzle of cortical pattern formation.
Cerebral cortical patterns have fascinated scientists for centuries with their beauty and complexity.
Numerous groups relate malformations in sulcal patterns to different diseases in humans, such as autism CITATION and attention deficit/hyperactivity disorder CITATION.
Though many advances have occurred in cortical development and sulcogenesis, the understanding of how sulci form and what factors determine the placement of sulci is still limited.
The cerebral cortex across species displays a variety of shapes and sizes and also wide array of sulcal patterning.
Studying the evolutionary development of sulcal patterns might provide clues about the cortical development taking place in humans.
A major advance in determining how these sulcal patterns form was the introduction of the axonal tension hypothesis CITATION.
This hypothesis describes a mechanically-based scenario where axonal tension, created by developing corticocortical connections in strongly interconnected regions, pulls together gyral walls and creates a folding pattern.
This hypothesis furthered the concept that variability between folding patterns among individuals is genetically driven, not just the consequence of random mechanical buckling from a confined cortex.
Other mechanochemical models have also been proposed to explain morphogenesis in the central nervous system CITATION .
Recently, it has been suggested that a cortical pattern can arise based on regional patterns of intermediate progenitor cells in the subventricular zone CITATION.
The intermediate progenitor model, which builds upon the intermediate progenitor cell hypothesis CITATION, states that during the development of the cortex certain radial glial cells in the ventricular zone are activated to create IP cells that travel to the SVZ.
These IP cells amplify the amount of neurons created in a given radial column.
Furthermore, a subset of IP cells creates a local amplification of neurons in upper cortical layers surrounded by areas of non-amplification, resulting in a wedge shape in the cortex.
This wedge shape is representative of a gyrus.
This new hypothesis is still being debated CITATION, CITATION and, if correct, could be a scenario for chemically-based pattern formation in the cortex.
Here, a relatively simple and, we believe, elegant chemically-driven mathematical model is proposed to explain how IP cell subsets are distributed spatially and temporally in the developing cortex.
Our model, which we call the Global Intermediate Progenitor model, uses a Turing reaction-diffusion system CITATION containing an activator and inhibitor on a prolate spheroidal surface to determine regional areas of activation of the production of IP cells.
The GIP model allows determination of the placement of the initial sulci underlying observed complex cortical patterns.
It also demonstrates that the initial folds of the arising sulcal pattern are governed by the global shape of the lateral ventricle.
The dependency on the global shape provides a critical piece to the puzzle of cortical development.
Many large-scale studies on intrinsically disordered proteins are implicitly based on the structural models deposited in the Protein Data Bank.
Yet, the static nature of deposited models supplies little insight into variation of protein structure and function under diverse cellular and environmental conditions.
While the computational predictability of disordered regions provides practical evidence that disorder is an intrinsic property of proteins, the robustness of disordered regions to changes in sequence or environmental conditions has not been systematically studied.
We analyzed intrinsically disordered regions in the same or similar proteins crystallized independently and studied their sensitivity to changes in protein sequence and parameters of crystallographic experiments.
The observed changes in the existence, position, and length of disordered regions indicate that their appearance in X-ray structures dramatically depends on changes in amino acid sequence and peculiarities of the crystallographic experiment.
Our study also raises general questions regarding protein evolution and the regulation of protein structure, dynamics, and function via variations in cellular and environmental conditions.
In the past decade, significant progress has been achieved in our understanding of the ubiquity and function of intrinsically disordered proteins CITATION CITATION.
What once seemed to be a set of exceptions to the traditional structure-to-function paradigm, where every protein was believed to have unique and stable 3D structure to carry out specific function, turned into a field where computational and experimental approaches were developed and combined to accurately characterize disordered proteins CITATION, understand their function CITATION, CITATION, CITATION or mechanisms of binding CITATION CITATION, and estimate their abundance in the protein universe CITATION CITATION.
Undoubtedly, bioinformatics analyses and methods played a significant role in this process, especially a set of predictors and statistical techniques CITATION, CITATION.
However, despite previous success, questions can be raised about the generality of our view of disordered proteins in terms of sequence-to-structure determinants and influence of environmental conditions.
Here, we attempt to address these questions by investigating the variability of observed disordered regions with changes in sequence and environmental conditions used for crystallization.
Recent studies document the effects of varying environmental conditions on regions of intrinsic disorder in similar proteins.
Zurdo et al. studied two yeast ribosomal stalk proteins, P1 and P2, which have different functional roles despite high sequence similarity and suggested that their functional differences stem from different structures CITATION.
Although neither protein is compact in solution and possesses folded structure under physiological pH and temperature, P1 was found to be mostly disordered with low helical content, whereas P2 had significant residual structure.
This residual structure disappeared at temperatures below 30 C, but was regained under low pH or in the presence of trifluoroethanol.
Palaninathan et al. reported that conformational changes were observed in the tertiary and quaternary structures in the crystals of the native human transthyretin CITATION.
At pH 4.0, TTR forms a tetramer and its crystal structure includes electron density for a functionally important EF helix-loop region.
At pH 3.5, this region is completely disordered.
Our search of the Protein Data Bank resulted in additional examples where slight changes in experimental conditions strongly correlated with the presence or absence of disordered regions.
One such case is cyclophilin 40, shown in Figure 1.
Cyp40 is one of the principal members of a family of large immunophilins found in mammals.
The exact biological function of large immunophilins is incompletely understood, though they are believed to be strongly associated with Hsp90 and play a crucial regulatory role in the upkeep of steroid receptor activity.
In PDB, Cyp40 is stored as 1IIP-A and 1IHG-A.
Both structures were obtained using the vapor diffusion, hanging drop method with recorded temperature of 277K, but 1IIP-A was crystallized at a pH of 8.0, whereas 1IHG-A was crystallized at pH of 6.1.
The two proteins are identical, yet a rmsd of 14.2 was obtained from their structural alignment.
Importantly, 1IHG-A contains an ordered region A299-Y365 that was absent from the structure of 1IIP-A.
Neither protein was solved in the presence of natural ligands.
In addition to experimental studies, computational analyses of redundant sets of experimentally determined structures for identical protein regions have provided evidence of the existence of numerous protein fragments observed in both ordered and disordered states CITATION.
The authors analyzed these dual-personality fragments and showed that they are characterized by amino acid compositions different than those for either ordered or disordered proteins and that their main functional roles are regulatory.
The examples discussed above demonstrate the strong influence experimental parameters can have on disordered residues in crystallized proteins.
However, a hypothesis that variation in experimental conditions could potentially trigger structural changes affecting the existence, position or length of intrinsically disordered regions has not been systematically tested and quantified.
In the following work, we provide evidence of significant variation of disordered regions, and protein structures in general, under the same or different experimental conditions that we believe can serve as a basic indicator of environmental regulation of protein structure and disordered regions in vivo.
We present a new approach to the handling and interrogating of large flow cytometry data where cell status and function can be described, at the population level, by global descriptors such as distribution mean or co-efficient of variation experimental data.
Here we link the real data to initialise a computer simulation of the cell cycle that mimics the evolution of individual cells within a larger population and simulates the associated changes in fluorescence intensity of functional reporters.
The model is based on stochastic formulations of cell cycle progression and cell division and uses evolutionary algorithms, allied to further experimental data sets, to optimise the system variables.
At the population level, the in-silico cells provide the same statistical distributions of fluorescence as their real counterparts; in addition the model maintains information at the single cell level.
The cell model is demonstrated in the analysis of cell cycle perturbation in human osteosarcoma tumour cells, using the topoisomerase II inhibitor, ICRF-193.
The simulation gives a continuous temporal description of the pharmacodynamics between discrete experimental analysis points with a 24 hour interval; providing quantitative assessment of inter-mitotic time variation, drug interaction time constants and sub-population fractions within normal and polyploid cell cycles.
Repeated simulations indicate a model accuracy of 5 percent.
The development of a simulated cell model, initialized and calibrated by reference to experimental data, provides an analysis tool in which biological knowledge can be obtained directly via interrogation of the in-silico cell population.
It is envisaged that this approach to the study of cell biology by simulating a virtual cell population pertinent to the data available can be applied to generic cell-based outputs including experimental data from imaging platforms.
Multiparameter flow cytometry is widely used to study the cell cycle and its perturbation in the context of both basic research and in routine clinical analysis CITATION CITATION.
Such analyses may use a wide range of fluorescent reporters that correlate to the expression of key molecular components of the cell cycle, such as cyclins and cyclin dependent kinases, CITATION or quantify DNA content CITATION.
Regardless of the particular fluorophores used the quantitative methodology and the ensuing synthesis of biological knowledge is based on statistical analyses of the experimental data sets.
For single variable distributions these may include calculations of moments of increasing orders to provide the mean, variance, skewness etc. or cumulative indices such as the Kolmogorov-Smirnov test CITATION CITATION.
More complex, multi-variate approaches may involve discriminant function, cluster or principal component analysis in an n-dimensional space CITATION CITATION.
In all of these approaches there is a common procedural thread: acquisition of data is followed by a statistical parameterisation of the measurement set to which biological form or function can be correlated.
In this work, we present an alternative, based on computational simulation of the experiment.
A stochastic simulation of the cell cycle dynamics within a large population is initialised with reference to a flow cytometry data set and then evolved, using evolutionary computer algorithms, with assessment of fitness measures derived from comparisons to subsequent data sets.
The cell-cycle information is then read directly from the in-silico populations.
The development of a simulated cell population approach has been driven by a requirement to track the evolution of large numbers of cells over multiple generations through the cell cycle and provide a means to track progression of both the whole cell population and distinct sub-groups CITATION, CITATION.
This is in the context of mapping the heterogeneity of cell cycle response to perturbation events e.g. effects on cell proliferation of anticancer therapeutics designed to block cell division.
In this report we present the conceptual basis of this simulated cell cytometry and detail of the methodology adopted.
To demonstrate the application of the technique and validate its potential we use it to quantify cell cycle perturbation in a tumour cell line by a topoismerase II inhibitor which causes endocycle routing in the late cell cycle.
The aim of the simulation is to predict the dynamic evolution of a large population of virtual cells through a life cycle corresponding to that prescribed by their real-life counterparts.
Furthermore, the model seeks to account for perturbations in the cell cycle progression of the virtual population.
The spatial position of the vcells within the cell cycle is initially determined from a real flow data set.
From this information, each vcell is assigned a temporal position within the mean inter-mitotic time, allowing cell cycle events such as DNA replication and cell division to be stochastically predicted.
After the vpopulation has evolved for a given period, they may be compared with a further experimental data set to enable important simulation parameters, governing their evolution, to be optimised and constrained so that correlations between the respective data sets are maximised.
Standard approaches to studying cell cycle involve statistical analysis of distributions either 1D involving nuclear content reporters CITATION or 2D when further cell cycle molecular reporters are also included CITATION, CITATION.
Thus these are inherently whole population measures and can only describe cell variability via global parameters such as the standard deviation from the mean.
Whilst automated analytical approaches have been developed in order to reduce user subjectivity CITATION CITATION the majority of flow analyses still involve user-defined gating of the as-measured data set to identify and segment a sub-population of cells.
Subsequent mapping of this population onto 2D dot plots of fluorescence provides temporal snapshots and further partitioning of cells to different compartments, G 1, S, G 2/M within normal and polypoid cycles.
These apparent quantitative assessments become inaccurate and, to varying extent, subjective, as they are based either on user identification of the various components in the dot plot by fitting of Gaussian distributions, representing the G 1, S, G 2/M fractions, to the DNA content histogram CITATION.
The challenge of the current investigation is to adopt a computational approach, where the analytical and interpretive steps are implemented at the simulated biology stage and not on the raw data outputs.
No new data is added in this approach and the computer simulations could be viewed as an elaborate form of data analysis.
However, the methodology does deliver new insight on process, delivering a continuous simulation of the dynamic evolution of the cellular system between fixed sampling points.
In this respect, it provides a physical validation when applying various hypotheses to interpret the experimental data.
It also goes some way to visualising the variation between individual cells that gives rise to biological heterogeneity as the stochastic simulation delivers a report on population dynamics in which each and every cell can be tracked.
Risk maps estimating the spatial distribution of infectious diseases are required to guide public health policy from local to global scales.
The advent of model-based geostatistics has allowed these maps to be generated in a formal statistical framework, providing robust metrics of map uncertainty that enhances their utility for decision-makers.
In many settings, decision-makers require spatially aggregated measures over large regions such as the mean prevalence within a country or administrative region, or national populations living under different levels of risk.
Existing MBG mapping approaches provide suitable metrics of local uncertainty the fidelity of predictions at each mapped pixel but have not been adapted for measuring uncertainty over large areas, due largely to a series of fundamental computational constraints.
Here the authors present a new efficient approximating algorithm that can generate for the first time the necessary joint simulation of prevalence values across the very large prediction spaces needed for global scale mapping.
This new approach is implemented in conjunction with an established model for P. falciparum allowing robust estimates of mean prevalence at any specified level of spatial aggregation.
The model is used to provide estimates of national populations at risk under three policy-relevant prevalence thresholds, along with accompanying model-based measures of uncertainty.
By overcoming previously unchallenged computational barriers, this study illustrates how MBG approaches, already at the forefront of infectious disease mapping, can be extended to provide large-scale aggregate measures appropriate for decision-makers.
Risk maps estimating the spatial distribution of infectious diseases in relation to underlying populations are required to support public health decision-making at local to global scales CITATION CITATION.
The advancement of theory, increasing availability of computation and growing recognition of the importance of robust handling of uncertainty have all contributed to the emergence in recent years of a new paradigm in the mapping of disease: the use of a special family of generalised linear models known as model-based geostatistics, generally implemented in a Bayesian framework CITATION, CITATION .
MBG models take point observations of disease prevalence from dispersed survey locations and generate continuous maps by interpolating prevalence at unsampled locations across raster grid surfaces.
The most striking advantage of MBG in disease mapping is its handling of uncertainty.
Interpolating sparse, often imperfectly sampled, survey data to predict disease prevalence across wide regions results in inherently uncertain risk maps, with the level of uncertainty varying spatially as a function of the density, quality, and sample size of available survey data, and moderated by the underlying spatial variability of the disease in question.
MBG approaches allow these sources of uncertainty to be propagated to the final mapped output, predicting a probability distribution for the prevalence at each location of interest.
Where predictions are made with small uncertainty, these distributions will be tightly concentrated around a central value; where uncertainty is large they will be more dispersed.
These techniques have been used to generate robust and informative risk maps for malaria CITATION CITATION, as well as a range of other infectious diseases CITATION CITATION, at scales varying from national to global.
Some studies have extended the handling of variation through space to also include the temporal dimension, allowing disease risk to be modelled and quantified over time as well as space CITATION, CITATION .
Implementation of MBG models over even relatively small areas is extremely computationally expensive.
Not only are the matrix algebra operations required to generate predictions at each individual pixel costly compared to simpler interpolation methods CITATION, CITATION, but this cost must be multiplied many times because prediction uncertainty is evaluated by generating many, equally probable, realisations of prevalence at each pixel.
Implementations of MBG disease models over large areas therefore tend to be via per-pixel computation whereby complete maps are built up by generating predictive realisations for each pixel independently.
This allows the computational task to be broken down into many small, more easily manageable, operations.
Such an approach yields appropriate measures of local uncertainty: the set of realisations for each pixel represents a posterior predictive distribution of prevalence from which summary statistics such as the mean, inter-quartile range or 95 percent credible intervals can be readily extracted, providing the user with valid uncertainty information for each individual location considered in isolation.
There is often a need to evaluate disease prevalence aggregated across spatial regions, temporal periods, or combinations of both CITATION, CITATION.
This may be to quantify and compare mean prevalence between countries or administrative units, for example, or to measure a shift in mean prevalence between the start and end of an intervention period or policy change.
In addition, MBG prevalence models can be used to estimate derived quantities such as population totals living in regions at different levels of risk, or the burden of disease cases expected within individual countries or continents as a function of underlying prevalence CITATION, quantities that by definition exist only over aggregated space-time units.
It is not possible, however, to construct posterior distributions for these aggregate quantities using a per-pixel approach.
To estimate the mean of a region made up of multiple pixels, and the uncertainty around this estimate, the correlation between all the pixels in the region must be known.
In a per-pixel approach, each pixel is modelled as independent of its neighbours, ignoring any spatial or temporal correlation.
Failing to account for correlation between pixels leads to gross underestimates of the uncertainty in the aggregated quantity, especially over large regions CITATION .
The solution to the problem outlined above is to replace per-pixel simulation of prevalence realisations with the simultaneous or joint simulation of all pixels to be aggregated, recreating appropriate spatial and temporal correlation between them CITATION.
Crucially, the set of pixel values can then be aggregated in any way, or used as input in derived aggregated quantities, and realisations of these aggregations will have the appropriate posterior predictive distributions.
Whilst conceptually simple, the extension from local to regional simulation induces a fundamental computational constraint in that the necessary calculations can no longer be disaggregated into separate tasks for each pixel.
This constraint has thus far prevented any use of MBG in disease mapping for the evaluation of aggregate quantities over very large areas, despite the profound public health importance of such measures.
Where examples of joint simulation in MBG disease mapping exist, they tend either to be over very small spatial regions CITATION or are achieved by simply breaking larger regions down manually into smaller more manageable tiles CITATION .
In this paper we use a new approximate algorithm for joint simulation to quantify, for the first time, aggregated uncertainty over space and time in a global scale MBG disease model for Plasmodium falciparum malaria prevalence CITATION.
We exemplify how this approach allows uncertainty in prevalence predictions to be enumerated at the continental, national, and sub-national scales at which public-health decisions are usually made.
We then extend the model architecture to estimate a second quantity of particular epidemiological interest: national populations at risk under different policy-relevant strata of P. falciparum transmission intensity.
PAR estimates form a fundamental metric for malaria decision-makers at national and international levels CITATION, CITATION and have also been used to assess equity in donor funding distributions CITATION, chart the changing exposure of human populations to the disease CITATION and provide baselines for predicted changes in exposure under climate change scenarios CITATION.
A range of techniques have been used to estimate PAR, including the use of MBG and other prevalence models to delineate risk strata in relation to underlying population distributions CITATION, CITATION, CITATION CITATION.
None of these studies have incorporated the inherent uncertainty in prevalence estimates, however, and the resulting PAR estimates are presented as point values with no uncertainty metrics.
Here we use the joint simulation framework to generate posterior predictive distributions of PAR living under conditions of low, medium, and high stable transmission within each malaria endemic country, allowing the uncertainty inherent in these estimates to be quantified in a formal statistical framework.
These PAR estimates are presented in full with this paper, making them available to any interested parties to support theoretical and applied epidemiological and public health applications.
In the remainder of this introductory section we outline the computational challenges of large scale joint simulation and review existing approaches to overcoming them.
In the methods section we present our algorithm for efficient joint simulation over very large grids, detail its implementation and testing with the global P. falciparum model, and its extension to estimating PAR.
The results section provides the outcome of the testing and validation procedures and examples of jointly simulated realisations of continental, national, and locally aggregated estimates of P. falciparum prevalence in 2007.
We present our national level estimates of PAR and exemplify how the accompanying uncertainty metrics can be communicated effectively to enhance their utility to decision-makers.
We conclude by discussing the strengths and weaknesses of our modelling architecture, the implications for the future of disease mapping, and useful directions for further research.
A general form for MBG models can be defined as follows:FORMULAsuch that in a disease survey of FORMULA individuals at a given location, the number observed to be infected FORMULA is modelled as binomially distributed with probability of infections given by FORMULA, the underlying prevalence of the disease in question, which is modelled as a transformation via an inverse link-function FORMULA of an unknown Gaussian process FORMULA CITATION, CITATION.
A Gaussian process in the context of disease mapping is a convenient probability distribution for 2-d surfaces, describing probabilities associated with different forms of the surface.
Using Bayesian inference, the Gaussian process can be updated to take account of the input data, providing a refined description of these probabilities.
Possible surfaces can then be drawn from this updated Gaussian process which, after passing through the inverse link-function, provide realisations of the target disease surface.
The Gaussian process can take a wide range of forms: the central tendency at any location is governed by the underlying mean function FORMULA, whilst textural properties are governed by the covariance function FORMULA.
The symbol FORMULA denotes a set of FORMULA parameters that define the form of the covariance and mean, which can include covariate coefficients.
In MBG, the aim is to estimate the joint posterior distribution of the model parameters FORMULA and the values of FORMULA evaluated at all locations and times for which a prediction is required - generally across the nodes of a regular raster grid.
Computationally, this task can be split into two distinct phases.
Firstly, Markov chain Monte Carlo can be used to generate realisations from the joint posterior of FORMULA and FORMULA at only the FORMULA space-time locations FORMULA where data exist, denoted FORMULA.
This is intuitive because it is only at these locations that the fit of the Gaussian process is evaluated, and this means the MCMC must only consider a multivariate normal distribution of dimension FORMULA, which is generally several orders of magnitude smaller than if all prediction locations across the raster grid were considered.
A realisation of FORMULA and FORMULA provides a skeleton from which the Gaussian process can be evaluated at all prediction locations across a raster grid in a second computational stage.
Conditional on these skeleton realisations, the value of FORMULA at each prediction location and time FORMULA can be sampled from its posterior predictive distribution:FORMULAwhere the posterior predictive mean and covariance parameters are given by the standard conditioning formulas for multivariate normal variables CITATION :FORMULAFORMULABy carrying out this two-step procedure over many realisations, samples are built up from the target posterior predictive distribution FORMULA.
In a per-pixel implementation, the predictive distributions FORMULA, FORMULA, FORMULA FORMULA at all FORMULA prediction locations in the output raster are realised independently to generate local models of uncertainty.
In this case, the largest single computational component is the population and factorisation of the data-to-data covariance matrix FORMULA which, in typical disease prevalence data sets where FORMULA is in the hundreds or thousands, is a relatively minor task that could generally be achieved on a standard desktop computer.
The subsequent sampling from the posterior predictive distribution is trivial: the posterior predictive mean and covariance refer to a single prediction location and sampling therefore amounts to drawing from a univariate normal distribution.
Total computation for each pixel is therefore modest, and the cost of generating the maps grows simply in proportion to the number of pixels involved, FORMULA.
Switching from a per-pixel implementation to a joint simulation over many prediction locations increases profoundly the computational challenge.
The efficiency of a per-pixel approach arises from the effective reduction of FORMULA to one, as each pixel is considered in isolation.
Joint simulation requires that FORMULA is preserved as the total number of prediction points, which can be many millions if large areas are considered at reasonably fine spatial resolution.
In addition to the FORMULA FORMULA data-to-data covariance matrix, the FORMULA FORMULA prediction-to-prediction and FORMULA FORMULA data-to-prediction covariance matrices must be populated.
More importantly, in the subsequent sampling from the posterior predictive multivariate normal distribution, the prediction-to-prediction covariance matrix must be factorised CITATION.
The computational cost of this operation is proportional to the cube of FORMULA.
To put this non-linear scaling in context, if a direct joint simulation of a 100 100 raster grid could be computed in one minute, a 1000 1000 grid would take approximately 6 10 7 seconds.
In practice these scaling factors along with those of memory and storage requirements mean direct joint simulation using the equations outlined above is generally limited to predictions at a maximum of around 10,000 points CITATION, CITATION, at least two orders of magnitude too few for global scale mapping at sub-10 km resolution, even at a single time period.
In bacterial genomes composed of more than one chromosome, one replicon is typically larger, harbors more essential genes than the others, and is considered primary.
The greater variability of secondary chromosomes among related taxa has led to the theory that they serve as an accessory genome for specific niches or conditions.
By this rationale, purifying selection should be weaker on genes on secondary chromosomes because of their reduced necessity or usage.
To test this hypothesis we selected bacterial genomes composed of multiple chromosomes from two genera, Burkholderia and Vibrio, and quantified the evolutionary rates of all orthologs within each genus.
Both evolutionary rate parameters were faster among orthologs found on secondary chromosomes than those on the primary chromosome.
Further, in every bacterial genome with multiple chromosomes that we studied, genes on secondary chromosomes exhibited significantly weaker codon usage bias than those on primary chromosomes.
Faster evolution and reduced codon bias could in turn result from global effects of chromosome position, as genes on secondary chromosomes experience reduced dosage and expression due to their delayed replication, or selection on specific gene attributes.
These alternatives were evaluated using orthologs common to genomes with multiple chromosomes and genomes with single chromosomes.
Analysis of these ortholog sets suggested that inherently fast-evolving genes tend to be sorted to secondary chromosomes when they arise; however, prolonged evolution on a secondary chromosome further accelerated substitution rates.
In summary, secondary chromosomes in bacteria are evolutionary test beds where genes are weakly preserved and evolve more rapidly, likely because they are used less frequently.
As the number of completely sequenced bacterial genomes has grown, the once surprising discovery of multiple chromosomes has become commonplace.
Setting aside the issue of nomenclature, why some bacterial genomes are divided into multiple, large replicons and others comprised of only a single DNA molecule is largely unknown CITATION.
Understanding the origin of secondary replicons helps frame the question.
Chromosomes may originate by three different mechanisms: by the split of a single chromosome, by chromosome duplication, or by acquisition of a large plasmid with essential genes, which ensures its prolonged maintenance.
Of these processes, the last has the greatest support because some secondary chromosomes have plasmid-like origins of replication CITATION.
However, it is the potential effects of genome subdivision that require further investigation and may explain variation in chromosome number and evolution in bacteria.
One advantage of a divided genome is the potential for faster replication and growth because of multiple origins of DNA replication.
For example, Vibrio spp.
with two chromosomes have among the fastest rates of cell division measured.
Yet in all bacteria, the single origin of replication per chromosome means that growth may occur faster than replication, a problem solved by the ability to initiate new cycles of replication before the completion of previous cycles.
As a result, daughter cells may be born with multiple partially replicated genomes that are enriched near the origin of replication CITATION .
Bacteria with multiple chromosomes face the additional challenge of maintaining synchronous replication; if chromosomes are of different sizes, either their timing or their rates of replication must vary.
In Vibrio, it has been demonstrated that the replication of the smaller, second chromosome is delayed during the cell cycle CITATION, CITATION, CITATION.
This delayed replication in effect reduces the dosage of genes on the second chromosome during periods of rapid growth CITATION, but does not alter the final heredity of each chromosome.
Each cell ultimately has one and only one copy of each chromosome, and no evidence yet suggests that this varies.
Therefore, variation in how bacterial chromosomes evolve is not, at least given current knowledge, an effect of variation in their effective numbers, as in the sex chromosomes of animals CITATION .
However, variation in gene dosage during the bacterial cell cycle can have profound effects on the expression of these genes as well as their evolutionary rates.
In bacteria with a single chromosome, genes distant from the origin of replication tend to be expressed less than those nearby, and thus distant genes evolve more rapidly CITATION .
In bacteria with multiple chromosomes, delayed replication of the smaller replicon could produce a similar effect on its expression and thus its evolution.
A recent report confirms this effect on expression in fast-growing cells: genes on the late replicating small chromosome of V. parahaemolyticus are expressed significantly less than those on the large chromosome, though expression varies more than would be expected from measured dosage effects CITATION.
In slow growing cells, overlapping replication cycles are unnecessary and hence no dosage and expression bias is found between chromosomes CITATION.
Replication bias within divided genomes could therefore accelerate evolution on secondary chromosomes.
This variation in expression caused by genome location, either relative to the origin of replication or on different chromosomes, can in principle exert selection for gene position.
Genes that must be expressed frequently should be near the origin of replication and on the primary chromosome CITATION, CITATION.
It therefore follows that in Vibrio, a significantly greater fraction of growth-essential and growth-contributing genes are found on the large, primary chromosome than on the small chromosome, ii near the origin relative to the terminus of the large chromosome, and even iii near the terminus of the large chromosome relative to the small chromosome CITATION.
When grown under optimal conditions, the dosage bias of these genes and hence their expression is exaggerated, but under more limiting conditions dosage bias and expression do not vary with gene position CITATION, CITATION.
Moreover, the growth rate of V. cholerae slows significantly when the replication rate of the second chromosome is genetically amplified CITATION, CITATION.
These findings imply that selection has shaped Vibrio genomes to contain genes whose functions benefit from higher dosage during rapid growth on the first chromosome and genes that should be expressed less on the second chromosome CITATION, CITATION .
Comparing related genomes with multiple chromosomes also suggests that their content has been segregated by priority and dispensability.
In general, the major chromosome tends to have significantly more conserved housekeeping genes, greater overall synteny, and greater conservation of content CITATION, CITATION, CITATION.
Together, these patterns support a general theory that secondary chromosomes are evolutionary test beds subject to reduced purifying selection and thus greater rates of change.
The key prediction of this theory is that genes found on secondary chromosomes should evolve faster and more variably than those on the primary chromosome.
Furthermore, if genes on secondary chromosomes have been less needed or used over long periods of time, then they should exhibit less bias towards the use of favored synonymous codons .
We tested this theory by studying the evolutionary rates of panorthologs, defined as orthologous genes present in single copy and, for a subset, obeying the consensus species phylogeny, among two sets of monophyletic, completely sequenced genomes with more than one chromosome.
We then compared the rates of ortholog families found on primary chromosomes with those on secondary chromosomes, calculated the codon bias of these genes, and evaluated their evolutionary patterns in the context of orthologs from sister taxa with only a single chromosome.
We found that orthologs on secondary chromosomes indeed evolved faster and displayed less skew towards purifying selection than those on primary chromosomes.
These increased rates of evolution appear to be a consequence of reduced selection for the use of specific codons and translational efficiency because of less frequent expression or necessity CITATION, CITATION, CITATION, CITATION.
Each prediction of the general theory that secondary chromosomes serve as evolutionary test beds for accessory genes was therefore met.
Apparent occupancy levels of proteins bound to DNA in vivo can now be routinely measured on a genomic scale.
A challenge in relating these occupancy levels to assembly mechanisms that are defined with biochemically isolated components lies in the veracity of assumptions made regarding the in vivo system.
Assumptions regarding behavior of molecules in vivo can neither be proven true nor false, and thus is necessarily subjective.
Nevertheless, within those confines, connecting in vivo protein-DNA interaction observations with defined biochemical mechanisms is an important step towards fully defining and understanding assembly/disassembly mechanisms in vivo.
To this end, we have developed a computational program PathCom that models in vivo protein-DNA occupancy data as biochemical mechanisms under the assumption that occupancy levels can be related to binding duration and explicitly defined assembly/disassembly reactions.
We exemplify the process with the assembly of the general transcription factors at the genes of the budding yeast Saccharomyces.
Within the assumption inherent in the system our modeling suggests that TBP occupancy at promoters is rather transient compared to other general factors, despite the importance of TBP in nucleating assembly of the preinitiation complex.
PathCom is suitable for modeling any assembly/disassembly pathway, given that all the proteins come together to form a complex.
Eukaryotic genes are thought to be regulated by hundreds of proteins that assemble into pre-initiation complexes at promoters using an ordered pathway.
One aspect of the PIC assembly pathway involves the recruitment of the general transcription factors, such as TBP and TFIIB, by sequence-specific activators.
TBP and TFIIB then contribute to the recruitment of RNA polymerase II and other GTF's, which eventually start transcription.
A fundamental question concerning our understanding of gene regulation is the extent to which each assembly and disassembly step is distinct at every gene in a genome.
Is the traditional biochemical view that TBP locks in or commits to a promoter, and in a recurring manner nucleates PIC formation valid in vivo?
And is the PIC disassembly process in vivo, simply the reverse of the assembly process?
Parts of the assembly/disassembly pathway have been rigorously defined in vitro with a few purified proteins and DNA, and this has provided us with our current parsimonious view of PIC regulation CITATION, CITATION, CITATION, CITATION.
In no case have assembly or disassembly reactions been reconstituted in a way that fully recapitulates the physiological setting at every gene, and so these questions remain open, in regards to the extent to which in vitro defined reactions mimic the in vivo events occurring throughout a genome.
The answer to this question is not readily addressed in vivo, since reactions are not as definable nor quantifiable as in vitro biochemical reactions with purified components.
Nonetheless, assays do exist for measuring relative levels of protein DNA complex formation in vivo, and so mechanistic inferences will be sought.
The goal here is to evaluate in vivo occupancy data in light of biochemical mechanisms that are intended to reflect the corresponding in vivo reaction.
The extent of biological insight is predicated on rather subjective assessments of the assumptions associated with interpretation of in vivo data.
Within the context of declared constraints and assumptions, we propose a means to model in vivo protein-DNA occupancy data, so as to better integrate and conceptualize massive genomic datasets.
This study is focused on the means of such modeling and the assumptions inherent in the data, using specific examples of PIC assembly.
Currently, perhaps the most widely used assay to measure the occupancy of proteins at genes in vivo is the chromatin immunoprecipitation assay.
In ChIP, proteins are crosslinked to DNA, the protein is then purified, and the bound DNA identified either through directed PCR or through genome-wide detection platforms.
In this way, for example, the relative occupancy level of TBP, TFIIB, pol II, and many other proteins at every promoter in the genome in a population of cells can be assayed.
Recent studies using differential ChIP and photobleaching experiments have provided compelling evidence for a dynamic state of PIC components in living cells CITATION, CITATION, CITATION.
Therefore, it is now within a conceptual framework to expect factors like RNA polymerase II, TBP, and other GTFs to undergo multiple assembly and disassembly cycles at promoters for each productive transcription event, rather than the traditional simple view that GTF's remain locked in place during multiple transcription cycles.
The existence and origins of distinct occupancy levels of PIC components on genes has not been systematically explored, and thus is the impetus for conducting the modeling studies described here.
Differential occupancy patterns for the GTFs have been described CITATION, and may be caused by gene-specific regulators that influence the recruitment or retention of specific general transcription factors, and thus assembly/disassembly mechanisms might differ from gene to gene.
Here, we attempt to utilize ChIP-chip binding information gleaned at every promoter in the yeast genome to either plausibly infer or exclude PIC assembly/disassembly mechanisms.
The major limitation in any such approach is that the number of permutations of possible assembly/disassembly mechanisms exceeds the amount of data available to constrain such mechanisms.
In other words, occupancy data, alone, is insufficient to uniquely specify an ordered PIC assembly and disassembly pathway.
Imposition of additional constraints, such as predefining either the assembly pathway, might however eliminate certain dissociation mechanisms as incompatible with the data, and thus serves the purpose of plausibly excluding mechanisms rather than uniquely identifying a mechanism.
Here, we develop a ChIP modeling program, termed PathCom, in the context of a fixed PIC assembly pathway to infer allowable dissociation mechanisms.
We validate the simulation using an existing chemical kinetics simulator COPASI CITATION.
Within the declared constraints, we discern the compatibility of different PIC disassembly mechanisms at nearly every transcriptionally-active gene in the yeast genome with existing ChIP-chip occupancy data.
One of the major goals of structural genomics projects is to determine the three-dimensional structure of representative members of as many different fold families as possible.
Comparative modeling is expected to fill the remaining gaps by providing structural models of homologs of the experimentally determined proteins.
However, for such an approach to be successful it is essential that the quality of the experimentally determined structures is adequate.
In an attempt to build a homology model for the protein dynein light chain 2A we found two potential templates, both experimentally determined nuclear magnetic resonance structures originating from structural genomics efforts.
Despite their high sequence identity, the folds of the two structures are markedly different.
This urged us to perform in-depth analyses of both structure ensembles and the deposited experimental data, the results of which clearly identify one of the two models as largely incorrect.
Next, we analyzed the quality of a large set of recent NMR-derived structure ensembles originating from both structural genomics projects and individual structure determination groups.
Unfortunately, a visual inspection of structures exhibiting lower quality scores than DLC2A reveals that the seriously flawed DLC2A structure is not an isolated incident.
Overall, our results illustrate that the quality of NMR structures cannot be reliably evaluated using only traditional experimental input data and overall quality indicators as a reference and clearly demonstrate the urgent need for a tight integration of more sophisticated structure validation tools in NMR structure determination projects.
In contrast to common methodologies where structures are typically evaluated as a whole, such tools should preferentially operate on a per-residue basis.
Experimentally determined three-dimensional structures of biomolecules form the foundation of structural bioinformatics, and any structural analysis would be impossible without them.
Two main techniques are available for biomolecular structure determination: x-ray crystallography and nuclear magnetic resonance spectroscopy.
It is important to realize that all resulting structure models are derived from their underlying experimental data.
Unfortunately, any experiment and thus any structure model will have errors associated with it.
Random errors depend on the precision of the experimental measurements and are propagated to the precision of the final models.
Systematic errors and mistakes often result from errors in the interpretation of the experimental data and relate directly to the accuracy of the final structure models.
For example, in NMR spectroscopy errors can be introduced by misassignment of the spectral signals; in x-ray crystallography errors are most likely made when the protein structure is positioned in the electron density CITATION, CITATION .
Several studies have shown that not all experimentally determined biomolecular structure models are of equally high quality CITATION CITATION.
Many different types of errors can be identified in protein structures, ranging from too tightly restrained bond lengths and angles, to molecules exhibiting a completely incorrect fold.
Where the former type of errors often does not have large consequences for the analysis of the structure and typically can be easily remedied by refinement in a proper force field CITATION, CITATION, the latter renders a structure model completely useless for all practical purposes.
Throughout the years several such errors have been uncovered in the Protein Data Bank CITATION, which often resulted in the replacement of the incorrect models with improved ones.
A typical example of an incorrectly folded structure model is the first crystal structure of photoactive yellow protein.
The structure was solved initially in 1989 CITATION and deposited under the now obsolete PDB entry 1PHY.
An updated model released 6 y later showed that in the original model the electron density had been misinterpreted CITATION.
Similar chain tracing problems led to an incorrect model for a DD-peptidase CITATION, which was corrected 10 y later when the structure was solved again but now at higher resolution CITATION .
Also, for structures determined using NMR spectroscopy, cases are known where reevaluation of the experimental data, often prompted by publication of a corresponding structure, has resulted in the replacement of structures in the PDB.
A well-known example is the original NMR structure of the oligomerization domain of p53 CITATION.
In this dimer of dimers, a difference in the orientation of the two dimers was observed between the NMR and crystal structure, the latter published shortly after the NMR structure CITATION.
Reexamination of the nuclear Overhauser enhancement data led to the identification of three misinterpreted peaks in the original p53 NOE assignments and the inclusion of several new NOEs, resulting in a revision of the original PDB entry CITATION.
A similar low number of misinterpreted NOE signals resulted in a largely incorrect fold for the anti factor AsiA CITATION.
In this case, it was not until a second solution structure of AsiA was published CITATION that the experimental data of the original AsiA structure were reexamined and the assignment errors were discovered CITATION .
In this paper, we describe a detailed analysis of two recently released NMR structures of the protein dynein light chain 2A, one from human and one from mouse.
Both structures originate from large structural genomics initiatives: the structure of human DLC2A was determined by the Northeast Structural Genomics Consortium, and the mouse variant was determined by the Center for Eukaryotic Structural Genomics.
Despite 96 percent sequence identity, large structural differences are observed between the two ensembles; an unexpected and extremely unlikely result.
Using the deposited experimental data we show that only the 1Y4O structure ensemble is correct.
Subsequently, we analyze both ensembles using various structure and data validation methods to show that the erroneous structure ensemble could have been identified prior to deposition.
Finally, we validate a large set of protein NMR structures that were released from the PDB in the period 2003 to 2005 and show that the DLC2A example does not stand on its own, but that more errors of this magnitude can be found.
We conclude with some suggestions on how, in the future, such large errors can be identified during the structure determination process using readily available validation software.
The threshold firing frequency of a neuron is a characterizing feature of its dynamical behaviour, in turn determining its role in the oscillatory activity of the brain.
Two main types of dynamics have been identified in brain neurons.
Type 1 dynamics shows a continuous relationship between frequency and stimulation current and, thus, an arbitrarily low frequency at threshold current; Type 2 shows a discontinuous f-I stim relationship and a minimum threshold frequency.
In a previous study of a hippocampal neuron model, we demonstrated that its dynamics could be of both Type 1 and Type 2, depending on ion channel density.
In the present study we analyse the effect of varying channel density on threshold firing frequency on two well-studied axon membranes, namely the frog myelinated axon and the squid giant axon.
Moreover, we analyse the hippocampal neuron model in more detail.
The models are all based on voltage-clamp studies, thus comprising experimentally measurable parameters.
The choice of analysing effects of channel density modifications is due to their physiological and pharmacological relevance.
We show, using bifurcation analysis, that both axon models display exclusively Type 2 dynamics, independently of ion channel density.
Nevertheless, both models have a region in the channel-density plane characterized by an N-shaped steady-state current-voltage relationship.
In summary, our results suggest that the hippocampal soma and the two axon membranes represent two distinct kinds of membranes; membranes with a channel-density dependent switching between Type 1 and 2 dynamics, and membranes with a channel-density independent dynamics.
The difference between the two membrane types suggests functional differences, compatible with a more flexible role of the soma membrane than that of the axon membrane.
It is now more than 60 years since Alan Hodgkin categorized the firing behaviour in his classical study of isolated axons from the crab Carcinus maenas CITATION.
In many respects his experiments still form the basis for analysis of firing patterns in nervous systems.
Using threshold dynamics and maximum frequency as parameters, he identified two major classes of repetitively firing axons : Class 1 axons start firing with very low frequency at threshold stimulation, yielding a continuous f-I stim relationship, whereas Class 2 axons start firing abruptly with a relatively high frequency at threshold, yielding a discontinuous f-I stim relationship.
On the basis of a similar categorization mammalian cortical neurons have also been separated into main classes CITATION, CITATION, one exhibiting Class 1 characteristics and another Class 2 characteristics.
The former class consists primarily of pyramidal neurons and the latter primarily of interneurons.
This differential classification of excitability has been shown to correlate with a differential bifurcation behaviour of corresponding dynamical models CITATION CITATION and successfully been used in analysing the coding properties of neurons CITATION CITATION.
To avoid confusion, and in accordance with the notation of Tateno and Robinson CITATION, we in the following use the terms Type 1 and Type 2 dynamics when referring to continuous and discontinuous f-I stim relationships, respectively.
This classification takes the threshold dynamics of the regular and fast spiking neurons, and that of the Class 1 and 2 axons, into account, but not all behavioural aspects of these classes CITATION .
The intricate interactions between the many factors involved in the dynamical regulation of neuronal firing are poorly understood CITATION.
The dominant idea is that different combinations of ion channel types explain the different patterns CITATION.
In a previous study we proposed a complementary explanation CITATION, CITATION.
We showed that both Type 1 and Type 2 behaviour can be simulated in a dynamical model of a hippocampal neuron CITATION by varying the membrane density of voltage-gated Na and K channels.
The model used was four-dimensional and based on a detailed experimental voltage-clamp study, thus comprising experimentally estimated parameters.
The choice of ion channel densities as bifurcation parameters was due to their physiological and pharmacological relevance.
Many drugs act by specifically blocking channels and thereby reducing ion channel density both at a somatic and at an axonal level.
Perhaps the most used local anaesthetic drug, lidocaine, acts by blocking sodium channels in axons and sensory nerve endings CITATION.
An increasing number of studies suggest a role for physiological regulation of channel densities, even at a relatively short time scale CITATION CITATION .
Each type of dynamics, i.e., Type 1 and 2, was found to be associated with distinct regions in the channel density plane or with corresponding surface areas of an oscillation volume in the FORMULA FORMULA I stim space.
In regions with high FORMULA and low FORMULA values the model exhibits Type 1 dynamics, whereas in regions with higher FORMULA values the model generates Type 2 dynamics.
A bifurcation analysis showed that the Type 1 dynamics of the model is due to saddle-node on invariant circle bifurcations CITATION, CITATION.
Figure 3A portrays such a bifurcation in a V-I stim plot, calculated for the model using region C1 values.
The Type 2 dynamics was found to be due to either local Andronov-Hopf bifurcations and/or global double limit cycle bifurcations CITATION, CITATION.
The dynamics of the model associated with region B values is due to double limit cycle and subcritical Andronov-Hopf bifurcations, while the dynamics associated with region A2 is exclusively due to double limit cycle bifurcations.
The double limit cycle bifurcation implies an unstable limit cycle, which is part of a separating structure which separates trajectories turning to a central stable point and those approaching a stable limit cycle.
However, preliminary calculations suggested that the bifurcation structure at the border between regions B and C1 is more complex than previously described.
When more bifurcation parameters are changed a more intricate loss of stability occurs CITATION .
Thus, to obtain a better understanding of the processes we reanalysed the hippocampal neuron model in more detail.
Furthermore, we extended the analysis to two other well-described excitable membranes, i.e., the myelinated axon of Xenopus laevis CITATION and the giant axon of Loligo forbesi CITATION.
We found that oscillations associated with a subregion of region C1 of the hippocampal model show Type 2 dynamics, and that the oscillations of both axon models exclusively show Type 2 dynamics.
We investigated the mathematical background to these findings, using techniques from bifurcation theory.
The results suggest that the hippocampal soma and the two studied axon membranes represent two distinct types of membrane with respect to the excitability pattern; membranes with a channel-density dependent switching between Type 1 and 2 dynamics, and membranes with a channel-density independent dynamics.
The difference between the two membrane types suggests functional differences, compatible with a more flexible role of the soma membrane than that of the axon membrane.
It has become clear that a large proportion of functional DNA in the human genome does not code for protein.
Identification of this non-coding functional sequence using comparative approaches is proving difficult and has previously been thought to require deep sequencing of multiple vertebrates.
Here we introduce a new model and comparative method that, instead of nucleotide substitutions, uses the evolutionary imprint of insertions and deletions to infer the past consequences of selection.
The model predicts the distribution of indels under neutrality, and shows an excellent fit to human mouse ancestral repeat data.
Across the genome, many unusually long ungapped regions are detected that are unaccounted for by the neutral model, and which we predict to be highly enriched in functional DNA that has been subject to purifying selection with respect to indels.
We use the model to determine the proportion under indel-purifying selection to be between 2.56 percent and 3.25 percent of human euchromatin.
Since annotated protein-coding genes comprise only 1.2 percent of euchromatin, these results lend further weight to the proposition that more than half the functional complement of the human genome is non-protein-coding.
The method is surprisingly powerful at identifying selected sequence using only two or three mammalian genomes.
Applying the method to the human, mouse, and dog genomes, we identify 90 Mb of human sequence under indel-purifying selection, at a predicted 10 percent false-discovery rate and 75 percent sensitivity.
As expected, most of the identified sequence represents unannotated material, while the recovered proportions of known protein-coding and microRNA genes closely match the predicted sensitivity of the method.
The method's high sensitivity to functional sequence such as microRNAs suggest that as yet unannotated microRNA genes are enriched among the sequences identified.
Futhermore, its independence of substitutions allowed us to identify sequence that has been subject to heterogeneous selection, that is, sequence subject to both positive selection with respect to substitutions and purifying selection with respect to indels.
The ability to identify elements under heterogeneous selection enables, for the first time, the genome-wide investigation of positive selection on functional elements other than protein-coding genes.
The human genome has been shaped by the evolutionary forces of mutation, genetic drift, and selection, with the latter acting, in the main, to purify functional regions of deleterious mutations.
By comparing the human and mouse genomes, previously it was estimated that about 5 percent of the human genome has undergone fewer point mutations than expected under a neutral substitution model CITATION, CITATION.
Accepting that this is most likely caused by the effects of purifying selection acting on deleterious mutations, the observation implies that at least 5 percent of the human genome is biologically functional.
Since the only known large class of functional genomic elements, protein-coding exons, is believed to constitute only 1.2 percent of our genome CITATION, this remains a surprising result.
To begin to understand the biological role of the remaining non-genic functional elements, the essential first step is their identification.
Recent studies have focussed on the most highly conserved of these elements, namely ultraconserved elements CITATION.
These elements exhibit a surprisingly high level of conservation that is rare even among protein-coding exons, and studies have begun to suggest intriguing roles of such elements in alternative splicing and development CITATION, CITATION.
However, the vast majority of non-genic elements are not perfectly conserved with respect to point mutations, and the reliable identification of these elements within a sea of neutrally evolving DNA has proved difficult.
Deep conservation among diverse phyla is a reliable sign of conserved biological function, but is less suitable for identifying recently evolved sequence.
Comparative methods for closely related species typically analyze substitution patterns to flag conserved regions CITATION.
These methods are well-developed, and they exploit phylogenetic information and correlations along the sequence to achieve high sensitivities.
Although extremely powerful, these methods can be hard to calibrate because of incompletely understood variations in neutral rates of substitution due to, for instance, methylation levels, chromatin state, transcriptional activity, and chromosomal location, and conservative calibration leads to a reduction of sensitivity.
Deep sequencing of mammalian genomes considerably improves the power of comparative methods CITATION, and while expensive, this will eventually represent the most satisfying solution to the sensitivity problem.
Of all mutation processes, point substitutions are the most prevalent, with insertions and deletions approximately 10-fold less frequent.
While nucleotide substitution models have been studied intensively CITATION, CITATION, with the exception of gene finding CITATION indels have largely been treated as evolutionary nuisance events, to be accounted for by alignment procedures, but otherwise uninformative.
Contrary to this view, we show that indels are highly informative evolutionary events.
We introduce a model describing the neutral distribution of indels over the genome, and show that this model fits a large proportion of human mouse alignment data remarkably well.
We then show that deviations from the model are, in the main, not caused by variations in the neutral indel rates, but are consistent with selection acting to purify the genome of deleterious indels that arise in functional regions.
We first applied this neutral indel model to derive upper and lower bounds on the proportion of genome under purifying selection with respect to indels.
Our observations can be explained by proposing that between 78.8 0.6 Mb and 100.0 0.8 Mb of the human genome has been under indel-purifying selection since the human mouse split.
Although still much higher than the 1.2 percent represented by coding exons, this represents a substantially lower estimate than the previous 5 percent estimate based on substitution-level conservation CITATION, CITATION, but is consistent with a more recent estimate CITATION.
Restricting ourselves to ancestral repeats, transposable elements inserted before the human mouse split, we found a near-exact fit between observations and the neutral model predictions.
Applying the same method as before, we predict that among the 1,263 Mb of TEs, only at most 1.2 Mb have been under sustained purifying selection.
This is the first time to our knowledge that a model of neutral evolution has quantified the proportion of TEs that have evolved neutrally.
As a second application of the neutral indel model, we identified a large proportion of sequence elements that have evolved under indel-purifying selection.
The model allowed us to calculate the predicted false-discovery rate for the entire set, as well as Bayesian posterior probabilities for individual elements to be under indel-purifying selection.
By correlating this set with various independent functional indicators, both positive and negative, it is shown to be highly enriched with functional DNA.
The key strength of the proposed method lies in its independence of selection with respect to point mutations.
Consequently, the method can provide independent confirmation of selection, thereby improving the specificity of methods based on substitutions alone.
Moreover, an exciting possibility is that the method allows identification of sequence elements that have been under heterogeneous selection, i.e., that have been subject to purifying selection with respect to indels, but subject to positive selection or relaxed constraints with respect to substitutions.
Examples of such elements would include spacers between regulatory elements whose relative distance is functionally constrained, such as those shown to exist in Drosophila CITATION.
Although functional, the nucleotide sequence of such spacers is probably immaterial, implying relaxed constraints with respect to substitutions.
An even more interesting class consists of elements whose sequence is under positive selection with respect to substitutions, while at the same time under purifying selection with respect to indels.
Since indels can be highly disruptive of function in protein-coding and RNA genes, as is evident from the 10-fold reduced indel rates in exons compared with neutrally evolving DNA, it is conceivable that such elements exist.
Without exploiting the evolutionary imprint of indel-purifying selection, it is difficult to see how to identify functional elements under positive selection with respect to substitutions in the absence of a comprehensive functional annotation, which only exists currently for protein-coding genes.
An analysis of percent sequence identity suggested that as much as 5 percent of DNA under indel-purifying selection, or roughly 3 5 Mb, may be under heterogeneous selection.
Among the indel-conserved elements identified, those that exhibit more than the expected number of substitutions for neutrally evolving DNA still showed correlations with the functional indicators mentioned above, thereby further confirming the existence of elements under heterogeneous selection.
Cerebellar Purkinje cells display complex intrinsic dynamics.
They fire spontaneously, exhibit bistability, and via mutual network interactions are involved in the generation of high frequency oscillations and travelling waves of activity.
To probe the dynamical properties of Purkinje cells we measured their phase response curves.
PRCs quantify the change in spike phase caused by a stimulus as a function of its temporal position within the interspike interval, and are widely used to predict neuronal responses to more complex stimulus patterns.
Significant variability in the interspike interval during spontaneous firing can lead to PRCs with a low signal-to-noise ratio, requiring averaging over thousands of trials.
We show using electrophysiological experiments and simulations that the PRC calculated in the traditional way by sampling the interspike interval with brief current pulses is biased.
We introduce a corrected approach for calculating PRCs which eliminates this bias.
Using our new approach, we show that Purkinje cell PRCs change qualitatively depending on the firing frequency of the cell.
At high firing rates, Purkinje cells exhibit single-peaked, or monophasic PRCs.
Surprisingly, at low firing rates, Purkinje cell PRCs are largely independent of phase, resembling PRCs of ideal non-leaky integrate-and-fire neurons.
These results indicate that Purkinje cells can act as perfect integrators at low firing rates, and that the integration mode of Purkinje cells depends on their firing rate.
Cerebellar Purkinje cells exhibit a wide range of dynamical phenomena.
They are intrinsic neural oscillators, firing spontaneously and highly rhythmically in the absence of synaptic input, at a rate of 10 180 Hz CITATION CITATION.
They also exhibit intrinsic bistability CITATION, CITATION, which influences their responses to sensory stimulation CITATION.
In addition, interactions between spontaneously firing Purkinje cells can result in waves of activity travelling down the cerebellar folia CITATION, or in high frequency oscillations CITATION, which may contribute to the generation of precise temporal patterns in the cerebellar network CITATION.
Hence, the firing of Purkinje cells is highly time- and state-dependent, and thus they represent excellent targets for dynamical systems analysis.
The phase response curve is a powerful tool to study neuronal dynamics at the cellular level.
The PRC describes the effect of a brief perturbation on the firing phase of a neuron, and can be used to predict the response of a neuron to more complex stimulation patterns CITATION CITATION.
The shape of the PRC is linked to the type of neuronal excitability CITATION, CITATION, to oscillatory stability CITATION and to network synchronization properties CITATION CITATION.
Studying Purkinje cell PRCs is therefore an essential step to explore their dynamic repertoire, probe their biophysical mechanisms, and to construct models of Purkinje cells to determine their role in information processing at the network level.
PRCs can be obtained by directly perturbing the membrane potential by short square current pulses CITATION CITATION or synaptic conductance pulses CITATION, CITATION CITATION, and via indirect methods CITATION CITATION.
Using the direct method, infinitesimal PRCs are obtained by repeatedly injecting brief current pulses while neurons are firing action potentials.
Phase and phase perturbation are measured by using the AP immediately preceding the current pulse as a reference, and we refer to these PRCs as traditional PRCs throughout this paper.
We show using electrophysiological experiments and in simulations that the interspike interval variability present in Purkinje cells introduces a systematic bias in this traditional calculation of the PRC.
The bias results from loss of causality caused by the jitter of the APs surrounding the current pulse, and gives rise to an empty triangular region in the PRC plot, which we call the Bermuda Triangle.
We introduce a method for calculating the PRC which corrects for this bias by using all spikes in the spike train as a reference, one at a time.
We refer to PRCs obtained by this method as corrected PRCs.
Note that in our study both traditional and corrected PRCs are calculated using the same experimental data: perturbation of the firing of Purkinje cells with brief square current pulses.
Using the corrected method we show that the shape of the Purkinje cell PRC changes fundamentally depending on the firing rate of the neuron.
N-Acetyl-L-Glutamate Kinase is the structural paradigm for examining the catalytic mechanisms and dynamics of amino acid kinase family members.
Given that the slow conformational dynamics of the NAGK may be rate-limiting, it is of importance to assess the mechanisms of the most cooperative modes of motion intrinsically accessible to this enzyme.
Here, we present the results from normal mode analysis using an elastic network model representation, which shows that the conformational mechanisms for substrate binding by NAGK strongly correlate with the intrinsic dynamics of the enzyme in the unbound form.
We further analyzed the potential mechanisms of allosteric signalling within NAGK using a Markov model for network communication.
Comparative analysis of the dynamics of family members strongly suggests that the low-frequency modes of motion and the associated intramolecular couplings that establish signal transduction are highly conserved among family members, in support of the paradigm sequence structure dynamics function.
Many recent studies, both experimental and computational, point to the inherent ability of proteins to undergo, under native state conditions, large-amplitude conformational changes that are usually linked to their biological function.
Proteins have access, via such equilibrium fluctuations, to an ensemble of conformers encoded by their 3-dimensional structure; and ligand binding essentially shifts the population of these pre-existing conformers in favour of the ligand-bound form CITATION CITATION.
With the accessibility of multiple structures resolved for a given protein in different forms, it is now possible to identify the principal changes in structure assumed by a given protein upon binding different ligands, which are observed to conform to those intrinsically accessible to the protein prior to ligand binding CITATION CITATION.
The observations suggest the dominance of proteins' intrinsic dynamics in defining the modes of interactions with the ligands.
This is in contrast to the induced-fit model CITATION where the ligand induces the change in conformation.
Instead, the Monod-Wyman-Changeux CITATION model of allostery where a selection from amongst those conformers already accessible is triggered upon ligand binding.
Yet, the choice between intrinsic vs induced dynamics, and the correlations between dynamics and function, are still to be established, and presumably depend on the particular systems of study CITATION.
NMR relaxation experiments provide evidence, for example, for the existence of correlations between the time scales of large-amplitude conformational motions and catalytic turnover CITATION, CITATION ; and collective motions in the low frequency regime appear to be potentially limiting reaction rates.
On the other hand, other studies point to the different time scales and events that control catalysis and binding events CITATION, CITATION.
Furthermore, while the intrinsic dynamics in the unbound form is observed to be the dominant mechanism that facilitates protein-protein or protein-ligand complexation, the ligand may also promote structural rearrangements on a local scale at the binding site CITATION, CITATION, CITATION.
Given that proteins' collective dynamics, and thereby potential functional motions, are encoded by the structure, proteins grouped in families on the basis of their fold similarities would be expected to share relevant dynamical features CITATION CITATION.
It is of paramount importance, in this respect, to have a clear understanding of collective motions and their relationship to binding or catalytic activities, if any, toward gaining deeper insights into functional mechanisms shared by members of protein families.
Protein dynamics can be explored by means of all-atom force fields and simulations, or by coarse-grained models and methods.
All-atom simulations such as Molecular Dynamics describe the conformational fluctuations of the system over a broad range of timescales.
Except for small proteins, the main limitation of MD is that the timescales computationally attainable do not allow for accurate sampling of slow and large-amplitude motions that are usually of biological interest.
CG approaches, on the other hand, lack atomic details but provide insights into global movements.
Among them, Elastic Network Models have found wide use in conjunction with normal mode analyses in the last decade CITATION.
ENMs describe the protein as a network, the nodes of which are usually identified by the spatial positions of C -atoms.
Elastic springs of uniform force constant connect the nodes in the simplest ENM, referred to as the anisotropic network model CITATION CITATION.
Despite the oversimplified description of the protein conveyed by the ENMs, a surge of studies have shown that the predicted low-frequency modes describe well experimentally observed conformational changes and provide insights into potential mechanisms of function and allostery CITATION CITATION, CITATION CITATION, in accord with NMAs performed CITATION, CITATION with more detailed models and force fields.
Additionally, recent studies by Orozco and co-workers CITATION, and Liu et al CITATION point to the similarities of the conformational space described by the low-frequency modes obtained from MD and that from CG NMA, provided that MD runs are long enough to accurately sample the collective motions.
The present study focuses on the amino acid kinase family.
This family comprises the following enzymes on the basis of sequence identity and structural similarities: N-acetyl-L-glutamate kinase, carbamate kinase, glutamate-5-kinase, UMP kinase, aspartokinase and the fosfomycin resistance kinase.
Rubio and co-workers CITATION have exhaustively studied this family and proposed that the shared fold among the members is likely to give rise to a similar mechanism of substrate binding and catalysis.
NAGK is the most widely studied member of this family taking into account the large amount of structural information gathered CITATION, CITATION.
This enzyme indeed serves as a structural paradigm for the AAK family, such that studying its structure-encoded dynamics can shed light on the mechanisms shared by family members to perform their function CITATION .
NAGK catalyzes the phosphorylation of NAG, which is the controlling step in arginine biosynthesis.
The hallmark of this biosynthetic route in bacteria is that it proceeds through N-acetylated intermediates, as opposed to mammals that produce non-acetylated intermediates.
Consequently, NAGK activity may be selectively inhibited and, taking into account that it is the controlling enzyme of arginine biosynthesis, it is a potential target for antibacterial drugs.
In many organisms, NAGK phosphorylation is the controlling step in arginine biosynthesis.
In these cases, NAGK is feedback inhibited by the end product arginine, and recent studies shed light on this mechanism of inhibition CITATION, CITATION.
NAGK from Escherichia Coli, on the other hand, is arginine-insensitive.
Its mechanism of phosphoryl transfer has been the most thoroughly characterized among the enzymes that catalyze the synthesis of acylphosphates.
In particular, crystallographic studies by Rubio and coworkers CITATION, CITATION have provided insights into its mechanisms of binding and catalysis.
EcNAGK is a homodimer of 258 residues, each monomer being folded into an sandwich.
The N-domain of each subunit/monomer makes intersubunit contacts and hosts the NAG binding site, whereas the C-domain binds the ATP.
The phosphoryl transfer reaction takes place at the interface between the two domains within each subunit.
Kinetic studies show no evidence of cooperativity between subunits CITATION, suggesting that the dimeric structure provides thermodynamic stability, only, to the monomeric fold that has been evolutionary selected to perform the catalytic function.
The diverse crystallographic structures solved for the bound state of this enzyme indicate two types of functional motions CITATION : X-ray structures of EcNAGK complexed with either ADP or with the inert ATP analogue AMPPNP have a too narrow active site to let the substrates bind directly; whereas the unbound structure has a more open active site.
This suggests that the enzyme undergoes a conformational closure that is likely to be triggered upon nucleotide binding, since all these complexes display a closed structure whether NAG is bound or not.
The ternary complex with ADP and NAG displays the ability to exchange NAG with a sulphate ion in solution without opening the active site.
The NAG lid therefore must be able to open and close independently of other structural elements.
The aim of the present study is two-fold.
Firstly, given the interest in acquiring deeper knowledge on the enzymatic mechanism of EcNAGK and the potential role of slow dynamics in the pre-disposition of the enzymatic function, we analyze here the low-frequency modes of motion of EcNAGK.
Secondly, using EcNAGK as the paradigm of AAK family, we assess to what extent the slow modes of motion are shared by other members of the AAK family.
G-protein coupled receptors, the largest family of proteins in the human genome, are involved in many complex signal transduction pathways, typically activated by orthosteric ligand binding and subject to allosteric modulation.
Dopaminergic receptors, belonging to the class A family of G-protein coupled receptors, are known to be modulated by sodium ions from an allosteric binding site, although the details of sodium effects on the receptor have not yet been described.
In an effort to understand these effects, we performed microsecond scale all-atom molecular dynamics simulations on the dopaminergic D 2 receptor, finding that sodium ions enter the receptor from the extracellular side and bind at a deep allosteric site.
Remarkably, the presence of a sodium ion at this allosteric site induces a conformational change of the rotamer toggle switch Trp6.48 which locks in a conformation identical to the one found in the partially inactive state of the crystallized human 2 adrenergic receptor.
This study provides detailed quantitative information about binding of sodium ions in the D 2 receptor and reports a possibly important sodium-induced conformational change for modulation of D 2 receptor function.
G-protein coupled receptors are highly sophisticated signal transduction machines able to respond to extracellular stimulus by activating diverse intracellular signaling pathways.
Recent research in this field is unveiling the complexity of the mechanisms involved, which are far from being understood in detail.
Among these mechanisms, allosteric modulation plays a central role in the fine-tuning of signaling.
Unfortunately, the experimental study of allosteric regulatory processes in GPCRs is difficult because contemporary techniques are unable to provide structural information of sufficient spatial resolution and time scales for describing specific atomic-level aspects of allosteric interactions.
In this scenario, computational methods like molecular dynamics simulation can be used to provide unique insight into some elusive aspects of the problem, by meeting the aforementioned requirements in terms of both spatial resolution and time scale needed.
The present work describes the application of long MD simulations for exploring the effect of sodium ions in class A GPCRs and the mechanism involved in their role as allosteric modulators.
Dopaminergic receptors are GPCRs, belonging to the class A family, which have been used as drug targets for the treatment of diverse central nervous system disorders.
Allosteric modulation of dopaminergic GPCR by sodium ions has been extensively studied experimentally CITATION CITATION and confirmed for the receptor subtypes in D 2 and D 4.
It has been proposed that sodium ions bind in an allosteric site of the transmembrane region located below the orthosteric binding site.
The high conservation of Asp2.50 among GPCRs suggests its structural importance for the GPCR function.
Presumably, the negatively charged carboxylic group of Asp2.50 interacts via electrostatic interaction with the positively charged sodium ion.
Indeed, the introduction of uncharged residues in position 2.50 produces sodium insensitivity in D 2 and D 4 CITATION, CITATION, demonstrating that the presence of a negative charge in the allosteric site is essential for maintaining sodium sensitivity.
Neve et al. CITATION proposed that the sodium binding site is a pyramidal hydrogen-bonding network formed by Asp2.50, Ser3.39, Asn7.45, and Ser7.46, with a sodium ion occupying the centre.
However, and in spite of the experimental information available, the molecular mechanism of the allosterically sodium-induced modulation on GPCR activation remains unknown.
In an effort to elucidate the mechanism of the sodium-induced effect, we have computationally investigated the mobility of sodium ions in the sodium-sensitive D 2 receptor.
All-atom molecular dynamics simulations of the D 2 receptor were performed to analyze more than 6 s of simulation data.
This analysis comprises a single 1.1 s long simulation, one hundred 50 ns simulations and biased metadynamics simulations to compute accurate two-dimensional free energy profiles CITATION, CITATION .
The simulation of the sodium ion's trajectory into the D 2 receptor reveals a sodium ion entrance from the extracellular side of the receptor, which is followed by its threading into a pyramidal hydrogen-bonding network CITATION near the allosteric binding site deep inside the receptor.
Most remarkably, the analysis of all these trajectories showed a correlation between the position of the sodium and the conformation of the Trp6.48 side chain.
This residue is part of the so called rotamer switch, a set of residues which are known to undergo major conformational change upon GPCR activation CITATION, CITATION, CITATION.
Kobilka and co-workers propose based on experimental data that the conformational state of the rotamer switch Trp6.48 is affected by ligands in the orthosteric binding site and is linked to a particular receptor response CITATION, CITATION.
Our study shows that, for dopaminergic receptors, the conformational state of the rotamer switch Trp6.48 can also be modulated from an allosteric site by sodium ions locking Trp6.48 in a conformational state identical to the partially inactive structure of the experimentally derived 2 adrenergic receptor CITATION .
Extracting network-based functional relationships within genomic datasets is an important challenge in the computational analysis of large-scale data.
Although many methods, both public and commercial, have been developed, the problem of identifying networks of interactions that are most relevant to the given input data still remains an open issue.
Here, we have leveraged the method of random walks on graphs as a powerful platform for scoring network components based on simultaneous assessment of the experimental data as well as local network connectivity.
Using this method, NetWalk, we can calculate distribution of Edge Flux values associated with each interaction in the network, which reflects the relevance of interactions based on the experimental data.
We show that network-based analyses of genomic data are simpler and more accurate using NetWalk than with some of the currently employed methods.
We also present NetWalk analysis of microarray gene expression data from MCF7 cells exposed to different doses of doxorubicin, which reveals a switch-like pattern in the p53 regulated network in cell cycle arrest and apoptosis.
Our analyses demonstrate the use of NetWalk as a valuable tool in generating high-confidence hypotheses from high-content genomic data.
An important challenge in the analyses of high throughput datasets is integration of the data with prior knowledge interactions of the measured molecules for the retrieval of most relevant biomolecular networks CITATION CITATION.
This approach facilitates interpretation of the data within the context of known functional interactions between biological molecules and subsequently leads to high-confidence hypothesis generation.
Typically, this procedure would entail identification of genes with highest or lowest data values, which is then followed by identification of associated networks.
However, retrieval of most relevant biological networks/pathways associated with the upper or lower end of the data distribution is not a trivial task, mainly because members of a biological pathway do not usually have similar data values, which necessitates the use of various computational algorithms for finding such networks of genes CITATION, CITATION, CITATION, CITATION, CITATION CITATION.
One class of methods for finding relevant networks utilize optimization procedures for finding highest-scoring subnetworks/pathways of genes based on the data values of genes CITATION, CITATION.
Although this approach is likely to result in highly relevant networks, it is computationally expensive and inefficient, and is therefore not suitable for routine analyses of functional genomics data in the lab.
The most popular of the existing methods of extraction of relevant networks from genomic data, however, usually involve a network building strategy using a pre-defined focus gene set, which is typically a set of genes with most significant data values CITATION, CITATION.
The network is built by filling in other nodes from the network either based on the enrichment of interactions for the focus set CITATION, or based on the analysis of shortest paths between the focus genes CITATION, CITATION.
Both methods aim at identifying genes in the network that are most central to connecting the focus genes to each other.
Problems associated with these methods have been outlined previously CITATION.
However perhaps most importantly, the central genes identified by these methods may have incoherent data values with the focus genes, as data values of nodes are not accounted for during the network construction process using the seed gene list.
This may result in uninformative networks that are not representative of the networks most significantly represented in the genomic data.
In addition, these methods do not account for genes with more subtle data values that collectively may be more important than those with more obvious data values CITATION.
Although powerful data analysis methods for finding sets of genes with significant, albeit subtle, expression changes have been developed, such an approach has not been incorporated into methods for extracting interaction networks that are most highlighted by the data.
In order to overcome these problems, we have employed the method of random walks in graphs for scoring the relevance of interactions in the network to the data.
The method of random walks has been well-established for structural analyses of networks, as it can fully account for local as well as global topological structure within the network CITATION, CITATION and it is very useful for identifying most important/central nodes CITATION CITATION.
Here, instead of working with a pre-defined set of focus genes, we overlay the entire data distribution onto the network, and bias the random walk probabilities based on the data values associated with nodes.
This method, NetWalk, generates a distribution of Edge Flux values for each interaction in the network, which then can be used for dynamical network building or further statistical analyses.
Here, we describe the concept of NetWalk, demonstrate its usefulness in extracting relevant networks compared to Ingenuity Pathway Analysis, and show the use of NetWalk results in comparative analyses of highlighted networks between different conditions.
We tested NetWalk on experimentally derived genomic data from breast cancer cells treated with different concentrations of doxorubicin, a clinically used chemotherapeutic agent.
Using NetWalk, we identify several previously unreported network processes involved in doxorubicin-induced cell death.
From these studies we propose that NetWalk is a valuable network based analysis tool that integrates biological high throughput data with prior knowledge networks to define sub-networks of genes that are modulated in a biologically meaningful way.
Use of NetWalk will greatly facilitate analysis of genomic data.
Prior experiences can influence future actions.
These experiences can not only drive adaptive changes in motor output, but they can also modulate the rate at which these adaptive changes occur.
Here we studied anterograde interference in motor adaptation the ability of a previously learned motor task to reduce the rate of subsequently learning a different motor task.
We examined the formation of the motor system's capacity for anterograde interference in the adaptive control of human reaching-arm movements by determining the amount of interference after varying durations of exposure to Task A. We found that the amount of anterograde interference observed in the learning of Task B increased with the duration of Task A. However, this increase did not continue indefinitely; instead, the interference reached asymptote after 15 40 trials of Task A. Interestingly, we found that a recently proposed multi-rate model of motor adaptation, composed of two distinct but interacting adaptive processes, predicts several key features of the interference patterns we observed.
Specifically, this computational model predicts the initial growth and leveling off of anterograde interference that we describe, as well as the asymptotic amount of interference that we observe experimentally.
Understanding the mechanisms underlying anterograde interference in motor adaptation may enable the development of improved training and rehabilitation paradigms that mitigate unwanted interference.
The history of prior action in the human motor system is known to influence not only future performance through memory, but also the capacity for future learning.
Interference and savings are two oppositely-directed phenomena that produce this effect.
Interference describes the ability of one task to impair the learning of another, while savings describes the ability of previous learning to enhance future learning.
For example, previous work has shown that after initial learning and subsequent washout of a visuomotor rotation task, relearning is faster than the initial learning, even if the performance levels of the learner at the onset of learning and relearning are identical CITATION CITATION.
Similarly, in a saccadic gain adaptation task, after learning and subsequent opposite-learning such that the motor output returns to pre-learning levels, relearning is also observed to be consistently faster than initial learning CITATION .
Other studies have demonstrated that previous learning can hinder or interfere with future learning CITATION CITATION.
An experimental paradigm commonly used to study interference is the A 1BA 2 paradigm, where a subject is instructed to serially learn Task A, Task B, and then Task A again - often with various time delays inserted between tasks.
In this paradigm, Task B is usually made to be the opposite of Task A. Two types of interference can be studied with this paradigm retrograde interference: how Task B interferes with the memory of Task A 1, and anterograde interference: how the memory of Task A 1 interferes with the subsequent learning of Task B. Note that both retrograde and anterograde interference can affect performance in Task A 2.
Although anterograde interference can often have quite substantial effects CITATION CITATION, it has not received as much attention as retrograde interference in the motor adaptation literature.
This is surprising because retrograde interference tends to have a relatively small effect on performance in the studies where it is reported CITATION, CITATION CITATION, whereas anterograde interference often has substantially larger effects CITATION CITATION.
In fact, several interference studies have been specifically designed to minimize the effects of anterograde interference because they recognized the potential it has for masking retrograde interference CITATION, CITATION.
Acquiring a better understanding of the mechanisms underlying anterograde interference is important not merely to provide greater insight into retrograde interference effects, but because the learning phenomenon is significant in and of itself as the primary cause of interference during motor adaptation.
Anterograde interference has been observed in force-field adaptation studies CITATION CITATION, CITATION and visuomotor rotation studies CITATION, and has been shown to weaken as the time between tasks is increased CITATION.
A recently-proposed computational model for motor adaptation has suggested a possible mechanism for anterograde interference CITATION.
In this model, one internal adaptive process responds quickly to motor error, but rapidly forgets, while another adaptive process learns slowly from motor error, but has good retention.
The contributions of these two processes are combined to generate the net motor output.
In the transition from Task A to Task B, the fast process will quickly learn the new task, while the slow process will be reluctant to follow because of its good retention of the previous task.
The multi-rate model predicts that the residual contribution of the slow process would hinder adaptation to Task B, resulting in anterograde interference.
The model also predicts that as training in Task A is extended, the amount of interference will also increase, but then level off beyond 15 40 training trials in Task A. Here, using a simple AB paradigm to avoid retrograde interference effects, we examine for the first time how the duration of exposure to Task A influences the amount of anterograde interference observed in Task B in order to determine how the capacity for interference is built up with practice.
We then use the predictions of the multi-rate model to determine whether anterograde interference stems from interactions between the different timescales of motor learning.
Male Rocky Mountain elk produce loud and high fundamental frequency bugles during the mating season, in contrast to the male European Red Deer who produces loud and low fundamental frequency roaring calls.
A critical step in understanding vocal communication is to relate sound complexity to anatomy and physiology in a causal manner.
Experimentation at the sound source, often difficult in vivo in mammals, is simulated here by a finite element model of the larynx and a wave propagation model of the vocal tract, both based on the morphology and biomechanics of the elk.
The model can produce a wide range of fundamental frequencies.
Low fundamental frequencies require low vocal fold strain, but large lung pressure and large glottal flow if sound intensity level is to exceed 70 dB at 10 m distance.
A high-frequency bugle requires both large muscular effort and high lung pressure, but at least 10 dB more intensity level can be achieved.
Glottal efficiency, the ration of radiated sound power to aerodynamic power at the glottis, is higher in elk, suggesting an advantage of high-pitched signaling.
This advantage is based on two aspects; first, the lower airflow required for aerodynamic power and, second, an acoustic radiation advantage at higher frequencies.
Both signal types are used by the respective males during the mating season and probably serve as honest signals.
The two signal types relate differently to physical qualities of the sender.
The low-frequency sound relates to overall body size via a strong relationship between acoustic parameters and the size of vocal organs and body size.
The high-frequency bugle may signal muscular strength and endurance, via a vocalizing at the edge mechanism, for which efficiency is critical.
Contrary to expectation based on body size, some large male mammals use high-pitched vocalization for display.
The dichotomy between low frequency and high frequency calls for vocal signaling of male characteristics is rarely so dramatic as in two closely related cervid species: European red deer and Rocky Mountain elk.
During the mating season, one species is recognizable by a low frequency roar, while the other is well-known for its high frequency bugle CITATION, CITATION.
Acoustic signals in the vocal communication of mammals are generally very complex because various selective pressures shape them CITATION.
The complexity can be related to natural and sexual selection.
For example, a signal is considered honest if reliable information about the sender can be extracted, such as body size or physical strength.
An animal's body size or physical strength has important implications for its physiology, ecology, fecundity, or its aggressive interactions and mating success CITATION.
The male red deer mating call was selected for low vocal tract resonance characteristics that provide reliable information about body size due to interconnected size-dependent factors involved in sound production CITATION.
In contrast, it is difficult to make the case that body size is signaled by the high fundamental frequency whistle-like bugle of the elk.
Elk calls sometimes contain low frequency components, but not consistently.
The signature is the bugle.
What provoked the evolution of such calls that would generally be associated with much smaller animals?
Here we investigate the physiological tradeoffs related with the production of high and low frequency sounds.
We have simulated red deer and elk calls with a finite-element model of oscillating vocal folds positioned within a laryngeal cartilaginous framework, applying intrinsic laryngeal muscle activations CITATION and a wave propagation model of the vocal tract CITATION with the goal to better understand the physiology of this intriguing system.
The larynx finite element model was based on the anatomy and biomechanics of Rocky Mountain elk and red deer larynges CITATION, CITATION .
The On-Off direction-selective ganglion cell in mammalian retinas responds most strongly to a stimulus moving in a specific direction.
The DSGC initiates spikes in its dendritic tree, which are thought to propagate to the soma with high probability.
Both dendritic and somatic spikes in the DSGC display strong directional tuning, whereas somatic PSPs are only weakly directional, indicating that spike generation includes marked enhancement of the directional signal.
We used a realistic computational model based on anatomical and physiological measurements to determine the source of the enhancement.
Our results indicate that the DSGC dendritic tree is partitioned into separate electrotonic regions, each summing its local excitatory and inhibitory synaptic inputs to initiate spikes.
Within each local region the local spike threshold nonlinearly amplifies the preferred response over the null response on the basis of PSP amplitude.
Using inhibitory conductances previously measured in DSGCs, the simulation results showed that inhibition is only sufficient to prevent spike initiation and cannot affect spike propagation.
Therefore, inhibition will only act locally within the dendritic arbor.
We identified the role of three mechanisms that generate directional selectivity in the local dendritic regions.
First, a mechanism for DS intrinsic to the dendritic structure of the DSGC enhances DS on the null side of the cell's dendritic tree and weakens it on the preferred side.
Second, spatially offset postsynaptic inhibition generates robust DS in the isolated dendritic tips but weak DS near the soma.
Third, presynaptic DS is apparently necessary because it is more robust across the dendritic tree.
The pre- and postsynaptic mechanisms together can overcome the local intrinsic DS.
These local dendritic mechanisms can perform independent nonlinear computations to make a decision, and there could be analogous mechanisms within cortical circuitry.
The On-Off direction-selective ganglion cell of the mammalian retina spikes vigorously to moving stimuli, but only weakly to stationary light spots.
It responds most strongly over a limited range of stimulus directions, and the direction producing the maximal response is called the preferred direction, while a stimulus moving in the opposite direction, called the null direction, produces little or no response CITATION.
We refer to such directionally-tuned spike responses as direction-selective.
On-Off DSGCs are sharply bistratified neurons that respond with a transient depolarization and burst of spikes at both the onset and termination of a bright stimulus within the receptive field.
Similarly the leading edge of a bright bar crossing the receptive field will produce a transient On-response, and, if the bar is wide relative to the dendritic extent and the speed low enough, the trailing-edge will produce a distinct, temporally separate Off-response.
In their original description of the DSGC, Barlow and Levick CITATION noted that direction-selective spike output was produced for stimuli that covered only a small fraction of the dendritic arbor.
They proposed that the synaptic mechanism comprised subunits that were repeated in an array across the receptive field.
In contrast to most ganglion cells, which initiate spikes in the axon initial segment, the DSGC initiates spikes in the dendritic tree CITATION.
The dendritic spikes are thought to propagate to the soma and initiate a somatic spike, similar to neurons in other regions of the brain where dendritic spiking is important for signal processing CITATION.
These observations suggest that some type of local dendritic processing could provide the basis for the proposed subunits.
Evidence for dendritic spiking in the DSGC was observed in low amplitude spikelets, which appear when somatic spiking is suppressed by local application of tetrodotoxin to the soma, or by hyperpolarizing the soma CITATION.
Dendritic spikes are hypothesized to initiate somatic spikes with high probability because they are rarely seen under normal conditions.
Further, both somatic and dendritic spiking responses are strongly tuned to preferred-direction stimuli, whereas the somatic graded potential shows relatively weak directional tuning CITATION CITATION.
This implies that the DSGC does not employ the mechanism used by most other ganglion cells for synaptic integration, where spikes initiated at the soma reflect the summation of synaptic inputs over the dendritic tree CITATION.
Instead it suggests that DSGC dendrites sum synaptic inputs and generate local spikes which then propagate to the soma, in the process amplifying the responses' directional selectivity.
In addition to dendritic spiking in the DSGC, other mechanisms are also important for generating its direction-selective response.
GABAergic inhibition is essential, and presynaptic mechanisms render both excitatory and inhibitory synaptic inputs to the DSGC directionally-tuned CITATION, CITATION.
Both excitatory and inhibitory inputs vary in amplitude and relative timing as a function of direction.
Further, postsynaptic integration of excitatory and inhibitory inputs has been hypothesized to contribute to DS signals CITATION CITATION.
Postsynaptic inhibition resulting from null direction movement could produce DS signals in two ways: it could block the propagation of dendritic spikes or it could block their initiation CITATION CITATION, CITATION .
However, the relative contributions of presynaptic and postsynaptic mechanisms to the DS spiking of the DSGC remains unclear.
Initial theoretical studies suggested that postsynaptic mechanisms might suffice CITATION and this received some experimental support CITATION.
However, more recently, presynaptic mechanisms have appeared to be the most significant CITATION, CITATION, CITATION, CITATION.
We wanted to revisit this issue to delineate the relative contributions of presynaptic and postsynaptic mechanisms in a calibrated model.
To investigate how dendritic processing of synaptic PSPs could amplify DS, we constructed multi-compartment biophysical models of DSGCs, digitized from tracer-injected morphologies calibrated to physiological data obtained prior to tracer injection.
We stimulated the models with moving light bars that activated synaptic inputs.
The goal was to explore how morphology, voltage-gated channels, and synaptic inhibition affect the initiation and propagation of dendritic spikes, and to compare these with the known physiological properties.
Our simulations show that sub-threshold PSPs from the distal dendritic regions of the On-Off DSGC are heavily attenuated by propagation to the soma, but that spikes initiated within local dendritic regions can propagate with high probability to the soma and back-propagate to the remainder of the dendritic tree.
Therefore active amplification of DS appears to take place during spike initiation in the dendrites.
Sumoylation, the covalent attachment of SUMO to proteins, differs from other Ubl pathways.
In sumoylation, E2 ligase Ubc9 can function without E3 enzymes, albeit with lower reaction efficiency.
Here, we study the mechanism through which E3 ligase RanBP2 triggers target recognition and catalysis by E2 Ubc9.
Two mechanisms were proposed for sumoylation.
While in both the first step involves Ubc9 conjugation to SUMO, the subsequent sequence of events differs: in the first E2-SUMO forms a complex with the target and E3, followed by SUMO transfer to the target.
In the second, Ubc9-SUMO binds to the target and facilitates SUMO transfer without E3.
Using dynamic correlations obtained from explicit solvent molecular dynamic simulations we illustrate the key roles played by allostery in both mechanisms.
Pre-existence of conformational states explains the experimental observations that sumoylation can occur without E3, even though at a reduced rate.
Furthermore, we propose a mechanism for enhancement of sumoylation by E3.
Analysis of the conformational ensembles of the complex of E2 conjugated to SUMO illustrates that the E2 enzyme is already largely pre-organized for target binding and catalysis; E3 binding shifts the equilibrium and enhances these pre-existing populations.
We further observe that E3 binding regulates allosterically the key residues in E2, Ubc9 Asp100/Lys101 E2, for the target recognition.
Protein function is regulated by numerous mechanisms, one of which is post-translational modification.
Covalent binding of ubiquitin and ubiquitin-like modifiers to target proteins constitute a key step in cellular processes including differentiation, apoptosis, cell cycle, and stress response CITATION CITATION.
Here, we focus on one member of the Ubl super-family, SUMO, with the aim of figuring out the mechanism through which SUMO is conjugated to its target proteins.
SUMO-1, -2, -3 and -4 exist in mammals CITATION CITATION.
Sumoylation can change the proteins' intracellular localization, interaction patterns with other proteins and modifications by other post-translational events.
It is important in development CITATION and is related to cancer drug resistance CITATION, CITATION.
For simplicity, below, SUMO refers to SUMO-1.
At least 100 different proteins have been reported as targets for sumoylation CITATION CITATION.
Analogous to conjugation mechanisms of Ub/Ubls, SUMO is attached to target proteins following sequential activation by E1, E2 and in most cases, E3 enzymes CITATION.
Following activation of the SUMO precursor CITATION, the E1 enzyme Aos1/Uba2 and SUMO form a thioester bond.
The SUMO thioester is next transferred to the active cysteine of Ubc9, the single known E2 enzyme of the sumoylation pathway CITATION, CITATION, CITATION.
Then SUMO is transferred from E2 to a target protein lysine residue.
E3 enzymes that ensure target specificity and increase reaction efficiency usually mediate this step.
Among the sumoylation targets, RanGAP1, p53 and I B are modified without an E3 ligase in vitro, although the reaction rates are slower compared to E3-mediated conjugation CITATION.
E2 ligase Ubc9 is essential CITATION, CITATION and conserved CITATION.
It recognizes a consensus sumoylation motif, -K-x-D/E, where represents a hydrophobic residue, K is the SUMO acceptor lysine, x is any amino acid and D/E is an acidic residue CITATION.
The E2 ligase also interacts with E3 enzymes during the transfer of SUMO to targets CITATION.
In addition to the consensus sumoylation motif, sumoylation target RanGAP1 has a second contact surface with the E2 ligase Ubc9, which is thought to be responsible for the higher efficiency of modification compared to other substrates CITATION.
A fragment of the E3 enzyme RanBP2, consisting of the IR1-M-IR2 domains is sufficient for E3 activity in vivo and in vitro CITATION.
Moreover, IR1-M and M-IR2 constructs are also functional with IR1-M being the catalytic core domain CITATION CITATION.
The activity of the E3 fragment indicates that E3 exerts its catalytic effect by altering the structural properties of the E2-SUMO complex, increasing the affinity of the complex for specific protein targets, rather than by forming direct target interactions CITATION.
The crystal structure of the SUMO-RanGAP1-Ubc9-RanBP2 complex supports this idea CITATION.
Recent work also shows that E3 ligase RanBP2 prevents dissociation of SUMO from its target RanGAP1, leading to an increase in the sumoylated RanGAP1 levels CITATION.
Due to the strong interactions between RanGAP1 and E2, it has been a debated question whether RanBP2 exerts its E3 activity for RanGAP1 or whether it only maintains the complex at the nuclear pore complex CITATION, CITATION .
Our aim is to understand the mechanism through which the E3 ligase RanBP2 triggers target recognition and catalysis by E2 in sumoylation.
We carried out explicit solvent molecular dynamic simulations for the E2 ligase Ubc9, SUMO, and the E2-SUMO complex with and without the E3 enzyme RanBP2.
We modeled the conjugated E2-SUMO complex, in RanBP2 bound and unbound forms, based on the SUMO-RanGAP1-Ubc9-RanBP2 crystal structure.
Our results indicate that E3 binding induces a higher population of target binding and catalysis-ready E2, restricting the conformational space of the E2-SUMO complex.
We observe that RanBP2 binding enhances the correlations between the fluctuations of E2 residues involved in catalytic activity and target recognition, which implies that RanBP2 is indeed an E3 ligase for the sumoylation of the target protein RanGAP1.
Our results further lead us to propose that the mechanism through which E3 ligase RanBP2 triggers E2 target recognition and catalysis in sumoylation is allostery: RanBP2 is an allosteric effector of E2 ligase Ubc9.
Below, we refer to the specific proteins simulated rather than the protein functional class to which they belong.
These were the ones crystallized by Reverter and Lima CITATION .
Various characteristics of complex gene regulatory networks have been discovered during the last decade, e.g., redundancy, exponential indegree distributions, scale-free outdegree distributions, mutational robustness, and evolvability.
Although progress has been made in this field, it is not well understood whether these characteristics are the direct products of selection or those of other evolutionary forces such as mutational biases and biophysical constraints.
To elucidate the causal factors that promoted the evolution of complex GRNs, we examined the effect of fluctuating environmental selection and some intrinsic constraining factors on GRN evolution by using an individual-based model.
We found that the evolution of complex GRNs is remarkably promoted by fixation of beneficial gene duplications under unpredictably fluctuating environmental conditions and that some internal factors inherent in organisms, such as mutational bias, gene expression costs, and constraints on expression dynamics, are also important for the evolution of GRNs.
The results indicate that various biological properties observed in GRNs could evolve as a result of not only adaptation to unpredictable environmental changes but also non-adaptive processes owing to the properties of the organisms themselves.
Our study emphasizes that evolutionary models considering such intrinsic constraining factors should be used as null models to analyze the effect of selection on GRN evolution.
The genetic basis of organismal evolution is one of the fundamental problems in biology CITATION CITATION.
The modes of selection for phenotypes would influence the fixation probabilities of the mutations that affect the phenotypes CITATION, and the profile of the mutations fixed during the course of evolution would determine the architecture of the genomes and the genetic systems underlying the phenotypes CITATION.
However, because genetic systems would modify the phenotypic effects of the mutations, the properties of the genetic system would influence the rates and directions of phenotypic evolution as well as the mutational robustness and evolvability CITATION CITATION.
Therefore, both phenotypes and genetic systems have evolved by mutually influencing each other.
Gene regulatory networks constitute important parts of such genetic systems and are involved in various biological processes such as environmental responses in unicellular organisms and cell differentiation in multicellular organisms CITATION, CITATION, CITATION.
Recent theoretical and experimental studies have revealed that complex GRNs have evolved by successive gene duplication, changes in regulatory interactions, and particularly in prokaryotes, horizontal gene transfer CITATION CITATION.
In addition, recent studies have addressed the structural features of complex GRNs such as redundancy, scale-free outdegree distributions and exponential indegree distributions CITATION, CITATION CITATION and the contribution of these features to genetic characteristics such as mutational robustness and evolvability CITATION CITATION .
One important question with regard to the evolution of complex GRNs is the evolutionary origin of these structural and mutational properties.
Various evolutionary processes simultaneously influence GRN evolution and these properties are interrelated.
It is thus difficult to identify the factors that have promoted the evolution of these properties, which could evolve as a result of being directly influenced by selection and also incidentally as a result of other factors CITATION CITATION.
Thus, to identify the factors responsible for the evolution of the properties of complex GRNs, it is necessary to consider not only selection but also various mutational processes and constraining processes.
Selection for phenotype is one of the most important driving forces of organismal evolution.
However, the impact of phenotypic selection on the evolution of GRNs is unclear.
The mode of selection strongly influences the fate of mutations and the profile of mutations fixed during the course of evolution ultimately determines the architecture of GRNs.
Thus, it is important to examine how different modes of phenotypic selection would affect the evolution of GRNs.
However, there are significant limitations to our general understanding of the processes of adaptation in evolutionary biology.
Many previous studies on the evolution of mutational robustness with respect to GRNs have focused on the fixation of phenotypically neutral mutations under stabilizing selection with a constant optimal environment CITATION, CITATION.
On the other hand, the fixation of beneficial mutations for phenotypic adaptation under changing environments is limited CITATION .
Many studies have suggested that some examples of GRN architectures are related to mutational robustness and evolvability CITATION, CITATION, CITATION, CITATION.
Theoretical studies have proposed that these genetic properties appear to be evolvable traits CITATION, CITATION CITATION and that these genetic properties could play a significant role in organismal evolution CITATION.
However, it is unclear how mutational robustness and evolvability influences the process of GRN evolution.
Certain properties of GRN might have evolved through non-adaptive processes such as mutations and biophysical constraints on gene regulation CITATION, CITATION, CITATION CITATION.
Mutations in particular is the ultimate source of genetic variation.
Thus, the biased properties of mutations can potentially influence the tendency of an organism to evolve.
For example, the probability of a transcription factor binding site formation as a result of mutations could vary by several orders of magnitude mainly owing to the extensive variation in the size of potential cis-regulatory regions among organisms CITATION, CITATION, and the rate of gene deletion could be several times higher than the rate of gene duplication in certain organisms CITATION, CITATION.
Moreover, it has been suggested that the horizontal transfer of regulatory genes is observed to a lesser extent than that of phenotypic genes CITATION.
Several studies have suggested that certain characteristic features of complex GRNs, such as redundancy and scale-free degree distributions could evolve as an inevitable outcome of mutations CITATION, CITATION.
However, these previous studies have not considered certain essential evolutionary processes such as selection and gene duplication.
It is therefore unclear whether such characteristic features of complex GRNs evolved as a result of selection or as a result of the inherent properties of the mutations.
The purpose of this study was to identify the evolutionary causes of various structural and mutational properties of complex GRNs, such as redundancy, indegree and outdegree distributions, mutational robustness, and evolvability.
For this purpose, we constructed an individual-based model of GRN that dynamically controls gene expression levels and allows populations to evolve under various fluctuating conditions of selection with various kinds of mutations such as gene duplication and deletion, cis-, trans-regulatory mutation and horizontal gene transfer.
In this study, to explore selective conditions that promote the evolution of complex GRNs, we first examine the evolution of GRNs under various conditions of fluctuating selection.
Second, for showing the adaptive mechanisms for the evolution of complex GRNs, we examine the fitness effect of all the mutations that arose during the evolution.
Third, to explore whether internal factors of organisms promote or inhibit the evolution of GRNs, we examined the impact of gene expression cost, constraints on expression dynamics, and several types of mutational biases such as the relative rates of gene duplication and deletion, the possibility of formation of new transcription factor binding sites and horizontal gene transfers.
Finally, on the basis of the results of the above analyses, we discuss the major evolutionary causes of various properties of complex GRNs, i.e., redundancy, scale-free out-degree distributions, exponential in-degree distributions, mutational robustness, and evolvability.
Understanding the principles governing axonal and dendritic branching is essential for unravelling the functionality of single neurons and the way in which they connect.
Nevertheless, no formalism has yet been described which can capture the general features of neuronal branching.
Here we propose such a formalism, which is derived from the expression of dendritic arborizations as locally optimized graphs.
Inspired by Ram n y Cajal's laws of conservation of cytoplasm and conduction time in neural circuitry, we show that this graphical representation can be used to optimize these variables.
This approach allows us to generate synthetic branching geometries which replicate morphological features of any tested neuron.
The essential structure of a neuronal tree is thereby captured by the density profile of its spanning field and by a single parameter, a balancing factor weighing the costs for material and conduction time.
This balancing factor determines a neuron's electrotonic compartmentalization.
Additions to this rule, when required in the construction process, can be directly attributed to developmental processes or a neuron's computational role within its neural circuit.
The simulations presented here are implemented in an open-source software package, the TREES toolbox, which provides a general set of tools for analyzing, manipulating, and generating dendritic structure, including a tool to create synthetic members of any particular cell group and an approach for a model-based supervised automatic morphological reconstruction from fluorescent image stacks.
These approaches provide new insights into the constraints governing dendritic architectures.
They also provide a novel framework for modelling and analyzing neuronal branching structures and for constructing realistic synthetic neural networks.
Neuronal circuits are composed of a large variety of branched structures axons and dendrites forming a highly entangled web, reminiscent of a stochastic fractal CITATION.
Despite this apparent chaos, more than a century ago Ram n y Cajal was able to extract order from this neuroanatomical complexity, formulating fundamental anatomical principles of nerve cell organization CITATION.
Cajal described three biological laws of neuronal architecture : optimization principles for conservation of space, cytoplasm and conduction time in the neural circuitry.
These principles helped him to classify his observations and allowed him to postulate a wide variety of theories of functionality and directionality of signal flow in various brain areas.
In the meantime, many of these ideas have been substantiated by applying more rigorous statistical analysis: circuitry and connectivity considerations as well as simple wire-packing constraints have been shown to determine the statistics of dendritic morphology CITATION CITATION.
It has also been shown mathematically that the specific organization and architecture of many parts of the brain reflect the selection pressure to reduce wiring costs for the circuitry CITATION CITATION .
In parallel, the development of compartmental modelling techniques based on the theories of Wilfrid Rall have highlighted the importance of a neuron's precise branching morphology for its electrophysiological properties CITATION, and have shown that dendrites can play an important role in the computations performed on the inputs to the cell CITATION, CITATION.
In fact, requirements for highly selective connectivity CITATION, CITATION, coherent topographic mapping, sophisticated computation or signal compartmentalization at the level of the single cell CITATION and the network could all contribute to this observed intricacy of brain wiring.
These two lines of investigation raise the question as to whether computation plays the determining role in shaping the morphological appearance of neuronal branching structures.
Alternatively, the simple laws of material and conduction time preservation of Ram n y Cajal could have more influence.
Using computational techniques it has become possible to construct synthetic neuronal structures in silico governed by the simulation of physical and biological constraints CITATION, CITATION CITATION.
In two recent papers CITATION, CITATION, we derived a growth algorithm for building dendritic arborisations following closely the constraints previously described by Ram n y Cajal.
The algorithm builds tree structures which minimize the total amount of wiring and the path from the root to all points on the tree, corresponding to material and conduction time conservation respectively.
Synthetic insect dendrite morphologies were faithfully reproduced in terms of their visual appearance and their branching parameters in this way.
Here we explore the algorithm's general applicability and its potential to describe any type of dendritic branching.
If the algorithm is sufficient to accurately mimic the essential structure of neuronal circuitry we can resolve the relative importance of computation and wiring constraints in shaping neuronal morphology.
We can then claim that Ram n y Cajal's laws are sufficient for shaping neuronal morphology.
Specific computation will then only play a subordinate role in determining a neuron's branching pattern.
We show here that while Cajal's laws do represent a strict constraint on neuronal branching, a neuronal morphology has a certain freedom to operate within these constraints.
Firstly, by adjusting the balance between the two wiring costs, a dendrite can efficiently set its electrotonic compartmentalization, a quantity attributable to computation.
Secondly, the density profile of the spanning field in which a dendrite grows determines its shape dramatically.
Thirdly, a few weaker constraints such as the suppression of multifurcations, the addition of spatial jitter or the sequential growth of sub-regions of a dendrite are helpful for reproducing the dendritic branching patterns of particular preparations.
These additional constraints might shed light on further functional, computational, developmental or network determinants for certain dendritic structures, and more of these will follow when applying our method to many more preparations.
Moreover, the simple principles presented in this study can be used to efficiently edit, visualize, and analyze neuronal trees.
Finally, these approaches allow one to generate highly realistic synthetic branched structures, and to automatically reconstruct neuronal branching from microscopy image stacks.
Many alternative splicing events are regulated by pentameric and hexameric intronic sequences that serve as binding sites for splicing regulatory factors.
We hypothesized that intronic elements that regulate alternative splicing are under selective pressure for evolutionary conservation.
Using a Wobble Aware Bulk Aligner genomic alignment of Caenorhabditis elegans and Caenorhabditis briggsae, we identified 147 alternatively spliced cassette exons that exhibit short regions of high nucleotide conservation in the introns flanking the alternative exon.
In vivo experiments on the alternatively spliced let-2 gene confirm that these conserved regions can be important for alternative splicing regulation.
Conserved intronic element sequences were collected into a dataset and the occurrence of each pentamer and hexamer motif was counted.
We compared the frequency of pentamers and hexamers in the conserved intronic elements to a dataset of all C. elegans intron sequences in order to identify short intronic motifs that are more likely to be associated with alternative splicing.
High-scoring motifs were examined for upstream or downstream preferences in introns surrounding alternative exons.
Many of the high- scoring nematode pentamer and hexamer motifs correspond to known mammalian splicing regulatory sequences, such as GCATG, indicating that the mechanism of alternative splicing regulation is well conserved in metazoans.
A comparison of the analysis of the conserved intronic elements, and analysis of the entire introns flanking these same exons, reveals that focusing on intronic conservation can increase the sensitivity of detecting putative splicing regulatory motifs.
This approach also identified novel sequences whose role in splicing is under investigation and has allowed us to take a step forward in defining a catalog of splicing regulatory elements for an organism.
In vivo experiments confirm that one novel high-scoring sequence from our analysis, CTATC, is important for alternative splicing regulation of the unc-52 gene.
One of the interesting lessons learned from the analysis of the human genome is that we may possess fewer than 25,000 genes CITATION.
One mechanism to dramatically increase the complexity of the human proteome from this lower-than-expected number of genes is to allow some genes to encode multiple proteins.
This process can be accomplished by alternative precursor messenger RNA splicing.
Studies that use expressed sequence tag alignments to identify alternatively spliced genes have led researchers to predict that up to 60 percent of human genes are alternatively spliced CITATION CITATION.
Alternative splicing events can be regulated in tissue-specific, developmental, and hormone-responsive manners, providing additional mechanisms for the regulation of gene expression CITATION, CITATION.
Understanding alternative splicing and its regulation is a key component to understanding metazoan genomes.
The current models for alternative splicing regulation are based on the interactions of intronic or exonic RNA sequences, known as cis elements, with splicing regulatory proteins known as trans-acting splicing factors CITATION.
The binding of splicing factors to the pre-mRNA regulates the ability of the spliceosomal machinery to recognize and promote alternative splicing.
The role of intronic elements in regulating splicing is well established and has been shown to work in a combinatorial fashion based on the trans-acting factors that are present.
For example, the inclusion of the 18-nucleotide-long, neural-specific N1 exon of the human c-SRC gene is regulated by the downstream control sequence found in the intron downstream of the N1 exon.
This sequence serves as a recruitment site for both constitutive and neuronal cell-specific splicing factors such as nPTB, FOX-1, and FOX-2 CITATION CITATION.
The vertebrate RNA-binding protein FOX-1 can also regulate muscle-specific alternative splicing through interactions with the RNA sequence GCAUG CITATION, and repeats of this sequence have been shown to be important for alternative splicing regulation of the fibronectin exon EIIIB and the rat calcitonin/CGRP exon 4 CITATION, CITATION.
Many other examples of complex and combinatorial regulation of alternative splicing through intronic cis elements have been demonstrated, and combinatorial interactions between proteins such as Nova-1, polypyrimidine tract binding protein, and ETR-3, with specific cis sequences, are important for alternative splicing regulation CITATION CITATION .
Intronic sequences are non-coding, and therefore they should have less evolutionary selective pressure to maintain their sequence.
An exception to this should be intronic sequences that regulate alternative splicing.
In an analysis of alternatively spliced human cassette exons, it was found that on average, approximately 100 nucleotides of intron sequence, flanking either side of the exon, tend to be highly conserved between the mouse and human genomes, with 88 percent identity in the upstream sequences and 80 percent identity in the downstream sequences CITATION.
Some clues to potential splicing regulatory motifs arise from these studies.
For example, Sorek and Ast found that the sequence TGCATG was the second most common hexamer in the first 100 nucleotides downstream of alternatively spliced exons, appearing in 18 percent of these intronic regions CITATION.
Another study of aligned mouse/human alternative exons found that GCATG is the most overrepresented pentamer in the proximal conserved region of the intron downstream of alternative exons CITATION.
A third study found that TGCATG is frequently located in introns flanking brain-enriched alternative exons, and its presence and spacing are highly conserved in these genes from fish to man CITATION .
Using the nematode Caenorhabditis elegans as a model system, we have been working to take advantage of comparative genomics to identify cis-acting regulators of alternative splicing.
The C. elegans gene structure, splicing machinery, and gene expression regulation is similar to that of other higher eukaryotes, with the exception that the average intron size is smaller.
Our lab has previously developed methods for identification of alternatively spliced genes in C. elegans by aligning the genome sequence with ESTs and mRNA sequence CITATION.
We developed an algorithm, the Wobble Aware Bulk Aligner, for creating interspecies genome alignments between C. elegans and the related roundworm, Caenorhabditis briggsae CITATION.
WABA employs a hidden Markov model to identify aligned regions as coding, high homology, low homology, and no homology.
It also factors the AT richness of C. elegans introns into its calculations when it defines an intronic region as high homology CITATION.
C. briggsae and C. elegans diverged approximately 100 million years ago, yet are indistinguishable by eye CITATION.
Alignment of these two genomes revealed that exonic sequences are highly conserved between these species, but intronic and intergenic sequences are rarely conserved CITATION.
We found that these rare, high homology sequences in introns are more likely to occur in the introns flanking alternatively spliced exons than in total introns CITATION.
We hypothesized that these conserved intronic regions were cis-acting regulatory elements for alternative splicing.
This nematode alignment, with relatively limited regions of high homology, provides the possibility for more specific pinpointing of intronic splicing regulatory elements than the much longer 100-nucleotide-long conserved regions flanking alternative exons in mouse/human alignments CITATION .
In this paper, we present the analysis of conserved regions of introns flanking alternatively spliced exons in C. elegans and correlate these conserved regions with alternative splicing regulation.
We collected these conserved sequence regions into a database and searched for overrepresented pentamers and hexamers relative to a total intron database, similar to the method used by Burge's group to identify exonic splicing enhancers CITATION.
This allowed us to create a table of potential intronic alternative splicing cis-regulatory motifs.
Since many RNA recognition motif containing splicing factors recognize specific sequences on the order of 4 6 nucleotides in length CITATION, CITATION, CITATION, CITATION CITATION, the high-scoring motifs in this catalog may represent specific binding sites for particular splicing factors.
Several of our highest scoring motifs in this analysis correlate with known vertebrate splicing regulatory elements, for example, GCATG CITATION, but several have not been previously identified.
A number of candidates identified by this method were tested in an in vivo splicing reporter system in C. elegans.
We have used this analysis to identify and confirm a new, highly conserved, alternative splicing regulatory element, CTATC.
We show that this sequence works in coordination with GCATG to regulate the alternative splicing of the unc-52 gene.
Computational methods attempting to identify instances of cis-regulatory modules in the genome face a challenging problem of searching for potentially interacting transcription factor binding sites while knowledge of the specific interactions involved remains limited.
Without a comprehensive comparison of their performance, the reliability and accuracy of these tools remains unclear.
Faced with a large number of different tools that address this problem, we summarized and categorized them based on search strategy and input data requirements.
Twelve representative methods were chosen and applied to predict CRMs from the Drosophila CRM database REDfly, and across the human ENCODE regions.
Our results show that the optimal choice of method varies depending on species and composition of the sequences in question.
When discriminating CRMs from non-coding regions, those methods considering evolutionary conservation have a stronger predictive power than methods designed to be run on a single genome.
Different CRM representations and search strategies rely on different CRM properties, and different methods can complement one another.
For example, some favour homotypical clusters of binding sites, while others perform best on short CRMs.
Furthermore, most methods appear to be sensitive to the composition and structure of the genome to which they are applied.
We analyze the principal features that distinguish the methods that performed well, identify weaknesses leading to poor performance, and provide a guide for users.
We also propose key considerations for the development and evaluation of future CRM-prediction methods.
Cis-acting transcriptional regulation involves the sequence-specific binding of transcription factors to DNA CITATION, CITATION.
In most cases, multiple transcription factors control transcription from a single transcription start site cooperatively.
A limited repertoire of transcription factors performs this complex regulatory step through various spatial and temporal interactions between themselves and their binding sites.
On a genome-wide scale, these transcription factor binding interactions are clustered as distinct modules rather than distributed evenly.
These modules are called cis-regulatory modules.
On DNA sequences, promoters, enhancers, silencers and others, are examples of these modules.
The appropriate transcription factors compete and bind to these elements, and act as activators or repressors.
Generally a CRM ranges from a few hundred basepairs long to a few thousand basepairs long; several transcription factors bind to it, and each of these transcription factors can have multiple binding sites .
Berman et al. CITATION demonstrated the feasibility of identifying CRMs by sequence analysis.
They scanned the Drosophila genome for clusters of potential binding sites for five gap gene transcription factors that are known to, together regulate the early Drosophila embryo.
They found more than a third of the dense clusters of these binding sites correspond to be CRMs regulating early embryo gene expressions, characteristic of genes regulated by maternal and gap transcription factors.
Similarly, exploiting the property that many CRMs contain clusters of similar transcription factor binding sites, Schroeder et al. CITATION computationally predicted CRMs over the genomic regions of genes of interest with gap or mixed maternal-gap transcription factors, and identified both known and novel CRMs within the segmentation gene network.
Recent study has confirmed the importance of CRM functions, and revealed how subtle changes to the original arrangement of module elements can affect its function.
Gompel et al. CITATION found that modifications to cis-regulatory elements of a pigmentation gene Yellow can cause a wing pigmentation spot to appear on Drosophila biarmipes similar to that seen in Drosophila melanogaster, thus showing that mutations in CRMs can generate novelty between species.
In a later study CITATION they showed the creation and destruction of distinct regulatory elements of same gene can lead to a same morphological change.
Williams et al. CITATION investigated the genetic switch whereby the Hox protein ABD-B controls bab expression in a sexually dimorphic trait in Drosophila.
They discovered the functional difference of this case lies not only in the creation and destruction of the binding sites, but also in their orientations and spacings.
There is also evidence showing that disruption of cooperations within a specific CRM can lead to malformation and disease.
One example is given by Kleinjan et al. CITATION.
The deletion of any distal regulatory elements of PAX6 changes its expression level and causes congenital eye malformation, aniridia, and brain defects in human.
Knowledge of social contact patterns still represents the most critical step for understanding the spread of directly transmitted infections.
Data on social contact patterns are, however, expensive to obtain.
A major issue is then whether the simulation of synthetic societies might be helpful to reliably reconstruct such data.
In this paper, we compute a variety of synthetic age-specific contact matrices through simulation of a simple individual-based model.
The model is informed by Italian Time Use data and routine socio-demographic data.
The model is named Little Italy because each artificial agent is a clone of a real person.
In other words, each agent's daily diary is the one observed in a corresponding real individual sampled in the Italian Time Use Survey.
We also generated contact matrices from the socio-demographic model underlying the Italian IBM for pandemic prediction.
These synthetic matrices are then validated against recently collected Italian serological data for Varicella and ParvoVirus.
Their performance in fitting sero-profiles are compared with other matrices available for Italy, such as the Polymod matrix.
Synthetic matrices show the same qualitative features of the ones estimated from sample surveys: for example, strong assortativeness and the presence of super- and sub-diagonal stripes related to contacts between parents and children.
Once validated against serological data, Little Italy matrices fit worse than the Polymod one for VZV, but better than concurrent matrices for B19.
This is the first occasion where synthetic contact matrices are systematically compared with real ones, and validated against epidemiological data.
The results suggest that simple, carefully designed, synthetic matrices can provide a fruitful complementary approach to questionnaire-based matrices.
The paper also supports the idea that, depending on the transmissibility level of the infection, either the number of different contacts, or repeated exposure, may be the key factor for transmission.
A century after the first contributions giving birth to mathematical epidemiology, and after 20 years of fast growth since the first public health oriented contributions CITATION CITATION, infectious diseases modeling has recently received a further dramatic impulse from pandemics threats.
The Bio-terrorism and SARS first, the fear of a potentially devastating pandemic of avian flu then, and finally the recent pandemic of A/H1N1 influenza, have all fostered the development of more and more detailed predictive tools.
These range from traditional models to network analysis, to highly detailed, large scale, individual-based models CITATION CITATION.
IBM are highly flexible tools for policy makers as they allow to define intervention measures at the finest possible levels.
For the first time, a pandemic model on a continental scale has been proposed CITATION .
A critical aspect common to all such models, is the parameterization of social contact patterns, i.e. how people socially mix with each other CITATION.
Social contact patterns are the key factors underlying the transmission dynamics of directly transmitted close-contacts infectious diseases CITATION.
Different models, independently of their level of complexity or geographical scale, are sensitive to the parameterization of social contact patterns.
In a relatively simple case, where individuals are stratified by age only, contact patterns are represented in the form of contact matrices whose entries represent the average number of contacts that individuals in age group i have with individuals in age group j, per unit of time.
Until recently, contact patterns were estimated indirectly by calibrating suitably restricted contact matrices using observed epidemiological data, such as serological or case notifications data.
The two major examples of this indirect approach are the Who-Acquires-Infection-From-Whom matrix CITATION, and the proportionate/preferred mixing approach CITATION.
Such approaches have important restrictions: in a population divided in n age groups, a contact matrix contains nxn n 2 unknown entries.
Therefore, in order to estimate the n 2 parameters from the n data points some simplifying assumptions about the structure of the matrix are needed.
In addition, indirect approaches can only estimate adequate contacts or transmission rates, i.e. composite parameters given by the product between a contact rate and the corresponding risk of infection per contact.
Recently, important progress has been made in this area through direct collection of contact data by means of sample surveys CITATION CITATION.
The direct approach is based on appropriate definitions of an at risk event.
Survey respondents are then asked to record in a diary relevant characteristics of all the individuals they had contact with during a randomly assigned day, or other factors such as the location where the contact occurred.
Standardized international survey data on social contact patterns in 8 European countries are currently available CITATION.
In addition, contact matrices, and time in contact matrices, have been estimated from secondary data sources such as transportation surveys CITATION or time use data CITATION, which are increasingly available.
In the case of time use data, the underlying hypothesis is that the amount of time people spend doing the same activity in the same place is relevant for the transmission of the disease.
A drawback of time use data is that they usually do not give direct information about the number of social contacts of respondents, or the time they spent in contacts.
They only give marginal information on the time individuals allocated to the various daily activities CITATION.
Therefore, these data need to be augmented with other data and/or assumptions to produce reliable estimates of contact matrices CITATION.
A way to supplement time use data relies on socio-demographic sources which provide information on the size and distribution of the arenas where contacts take place.
For example, for school contacts we often know the average class size and the average pupils-teacher ratio for all compulsory grades.
As for contacts within the household, we have information on household size and composition.
For most other activities, however, there is little information.
Assumptions, e.g. independency, are therefore necessary to give some coarse ideas of contact patterns CITATION.
However, this approach ignores the structure of the social networks where contacts are formed.
A promising approach is then to reconstruct such networks by the simulation of appropriate artificial social networks.
A first example is the social network generated by the Portland synthetic population CITATION.
In that case, contact and time in contact matrices by age are by-products of the social dynamics of the Portland model.
These matrices have the standard expected features: population contacts cluster around children and adult, children interact most frequently with other children close to their own age, etc. However, such matrices were neither compared with other contact matrices, nor validated against empirical epidemiological data.
Thus, no actual evaluation of their goodness in explaining transmission of infections is available.
In this paper, we follow the same line and aim to reconstruct contact and time-in-contacts matrices by simulating a suitable minimalistic socio-demographic individual-based model for Italy.
The model is parameterized by integrating time use data from the Italian time use survey CITATION and other official socio-demographic data CITATION CITATION.
In the model, each artificial agent is a clone of a real individual, i.e. there is a one-to-one correspondence between the diary of each artificial agent and the one of a corresponding real survey participant.
Since the sample is representative of the Italian population, but the size of the model population is comparable to that of a small Italian city, we named the model Little Italy.
From this point of view, our model resembles the Portland model CITATION, and the Eemnes model CITATION.
In the Little Italy world, agents physically displace during the day in order to attend their various daily activities in the corresponding location.
In these locations, agents contact other agents.
We defined a contact as having shared the same physical environment during a given time slot.
With our approach we generate three different types of contact matrices, possibly informative of distinct aspects of the biology of transmission: a matrix describing the time spent in contact CITATION, a matrix counting the number of repetition of contact episodes, and a matrix counting contacts as the average number of different persons contacted, i.e. the number of different social partnerships, as in CITATION .
In addition, we extracted an adequate CITATION contact matrix from the socio-demographic model underlying the Italian IBM for pandemic prediction and mitigation CITATION, that we named Big-Italy.
The synthetic contact matrices computed by simulation of Little and Big-Italy are tested against recently collected Italian serological data on Varicella and ParvoVirus.
Their performances are compared with other contact matrices available for Italy, i.e. the Polymod and time use matrices.
Computational efforts to identify functional elements within genomes leverage comparative sequence information by looking for regions that exhibit evidence of selective constraint.
One way of detecting constrained elements is to follow a bottom-up approach by computing constraint scores for individual positions of a multiple alignment and then defining constrained elements as segments of contiguous, highly scoring nucleotide positions.
Here we present GERP, a new tool that uses maximum likelihood evolutionary rate estimation for position-specific scoring and, in contrast to previous bottom-up methods, a novel dynamic programming approach to subsequently define constrained elements.
GERP evaluates a richer set of candidate element breakpoints and ranks them based on statistical significance, eliminating the need for biased heuristic extension techniques.
Using GERP we identify over 1.3 million constrained elements spanning over 7 percent of the human genome.
We predict a higher fraction than earlier estimates largely due to the annotation of longer constrained elements, which improves one to one correspondence between predicted elements with known functional sequences.
GERP is an efficient and effective tool to provide both nucleotide- and element-level constraint scores within deep multiple sequence alignments.
The identification and annotation of all functional elements in the human genome is one of the main goals of contemporary genetics in general, and the ENCODE project in particular CITATION, CITATION, CITATION.
Comparative sequence analysis, enabled by multiple sequence alignments of the human genome to dozens of mammalian species, has become a powerful tool in the pursuit of this goal, as sequence conservation due to negative selection is often a strong signal of biological function.
After constructing a multiple sequence alignment, one can quantify evolutionary rates at the level of individual positions and identify segments of the alignment that show significantly elevated levels of conservation.
Several computational methods for constrained element detection have been developed, with most falling into one of two broad categories: generative model-based approaches, which attempt to explicitly model the quantity and distribution of constraint within an alignment, and bottom-up approaches, which first estimate constraint at individual positions and then look for clusters of highly constrained positions.
A widely used generative approach, phastCons CITATION, uses a phylo-Hidden Markov Model to find the most likely parse of the alignment into constrained and neutral hidden states.
While HMMs are widely used in modeling biological sequences, they have known drawbacks: transition probabilities imply a specific geometric state duration distribution, which in the context of phastCons means predicted constrained and neutral segment length.
This may bias the resulting estimates of element length and total genomic fraction under constraint.
One of the leading bottom-up approaches is GERP CITATION, which quantifies position-specific constraint in terms of rejected substitutions, the difference between the neutral rate of substitution and the observed rate as estimated by maximum likelihood, and heuristically extends contiguous segments of constrained positions in a BLAST-like CITATION manner.
However, GERP is computationally slow because its maximum likelihood computation uses the Expectation Maximization algorithm CITATION to estimate a new set of branch lengths for each position of the alignment; this step is also undesirable methodologically because it involves estimating k real-valued parameters from k nucleotides of data.
Furthermore, the extension heuristic used by GERP may induce biases in the length of predicted CEs.
In this work we present GERP, a novel bottom-up method for constrained element detection that like GERP uses rejected substitutions as a metric of constraint.
GERP uses a significantly faster and more statistically robust maximum likelihood estimation procedure to compute expected rates of evolution that results in a more than 100-fold reduction in computation time.
In addition, we introduce a novel criterion of grouping constrained positions into constrained elements using statistical significance as a guide and assigning p-values to our predictions.
We apply a dynamic programming approach to globally predict a set of constrained elements ranked by their p-values and a concomitant false positive rate estimate.
Using GERP we analyzed an alignment of the human genome and 33 other mammalian species, identifying over 1.3 million constrained elements spanning over 7 percent of the human genome with high confidence.
Compared to previous methods, we predict a larger fraction of the human genome to be contained in constrained elements due to the annotation of many fewer but longer elements, with a very low false positive rate.
Chelt, a cholera-like toxin from Vibrio cholerae, and Certhrax, an anthrax-like toxin from Bacillus cereus, are among six new bacterial protein toxins we identified and characterized using in silico and cell-based techniques.
We also uncovered medically relevant toxins from Mycobacterium avium and Enterococcus faecalis.
We found agriculturally relevant toxins in Photorhabdus luminescens and Vibrio splendidus.
These toxins belong to the ADP-ribosyltransferase family that has conserved structure despite low sequence identity.
Therefore, our search for new toxins combined fold recognition with rules for filtering sequences including a primary sequence pattern to reduce reliance on sequence identity and identify toxins using structure.
We used computers to build models and analyzed each new toxin to understand features including: structure, secretion, cell entry, activation, NAD substrate binding, intracellular target binding and the reaction mechanism.
We confirmed activity using a yeast growth test.
In this era where an expanding protein structure library complements abundant protein sequence data and we need high-throughput validation our approach provides insight into the newest toxin ADP-ribosyltransferases.
Sequence data from over 6,500 genome projects is available through the Genomes OnLine Database CITATION and more than 60,000 protein structures are in the Protein Data Bank.
While these sequences represent large diversity, a limited number of possible folds estimated at 1,700 CITATION helps researchers organize the sequences by structure.
A single fold performs a limited number of functions, between 1.2 and 1.8 on average CITATION.
Therefore, structure knowledge helps pinpoint function.
Researchers are combining sequence and structure data to expand protein families such as the mono-ADP-ribosyltransferase protein toxins that participate in human diseases including diphtheria, cholera and whooping cough CITATION .
ADP-ribosylation is a post-translational modification that plays a role in many settings CITATION.
ADP-ribosyltransferases bind NAD and covalently transfer a single or poly ADP-ribose to a macromolecule target, usually protein, changing its activity.
Many prokaryotic ADPRT toxins damage host cells by mono-ADP-ribosylating intracellular targets.
G-proteins are common targets including: eukaryotic elongation factor 2, elongation factor thermo unstable, Ras, Rho and Gs-.
Other targets include actin CITATION kinase regulators CITATION and RNA-recognition motifs CITATION .
Researchers use ADPRT toxins to develop vaccines CITATION, as drug targets, to kill cancer cells CITATION, as stent coatings to prevent restenosis after angioplasty CITATION, as insecticides, to deliver foreign proteins into cells using toxin receptor-binding and membrane translocation domains, to study cell biology CITATION, CITATION, to understand the ADP-ribosylation reaction and to identify biosecurity risks.
ADPRTs occur in viruses, prokaryotes, archaea and eukaryotes.
Genomes acquire them through horizontal gene transfer CITATION CITATION.
Several authors have reviewed the prokaryotic ADPRT family CITATION, CITATION, CITATION.
Examples include Pseudomonas aeruginosa exoenzyme S, Vibrio cholerae cholera toxin, Bordetella pertussis pertussis toxin and Corynebacterium diphtheriae diphtheria toxin.
Toxic ADPRTs are divided into the CT and DT groups to better organize the family.
We focus on the CT group, which we divide into the ExoS-like, C2-like, C3-like and CT-PT-like toxins.
CT group primary sequences are related through a specific structure-linked pattern CITATION.
The ADPRT pattern, updated from previous reports CITATION, CITATION and written as a regular expression is:FORMULA
The toxin catalytic domain consists of several regions.
We describe them here going from the N- to C-terminus using previously introduced nomenclature CITATION, CITATION.
Region A is sometimes present and recognizes substrate, when ExoT recognizes Crk, for example.
Its recognition of ExoT targets is an exception rather than a general rule for ADPRTs.
Except for the CT-PT-like subgroup, region B an active site loop flanked by two helices appears early in the toxin sequence.
It stabilizes the catalytic Glu, binds the nicotinamide ribose and the adenine phosphate.
It also stabilizes the target substrate and helps specific bonds rotate during the ADPRT reaction, in turn, helping to bring the nucleophile and electrophile together for reaction.
Region 1 is at the end of a -sheet, with sequence pattern YFL RX.
It is important for binding A-phosphate, nicotinamide phosphate, nicotinamide, adenine ribose and the target substrate.
Region F follows region 1 and sometimes recognizes substrate.
The region 2 follows on a -sheet with sequence pattern YF -X-S-T- SQT.
It binds adenine, positions the catalytic Glu, orients the ADP-ribosyl-turn-turn loop and maintains active site integrity.
The phosphate-nicotinamide loop is immediately after the STS motif.
It interacts with the target and binds N-phosphate.
Menetrey et al. suggested the PN loop is flexible and implicated it in locking the nicotinamide in place during the reaction CITATION.
Region 3 consists of the ARTT loop leading into the -sheet with pattern QE -X-E.
It recognizes and stabilizes the target and binds the N-ribose to create a strained NAD conformation.
The ARTT loop is plastic, having both in and out forms that might aid substrate recognition CITATION.
The FAS region mediates activator binding when present CITATION, CITATION, CITATION, CITATION .
Researchers have long debated the ADPRT reaction details.
Some suggest an S N2 mechanism CITATION, CITATION, but many now favor the S N1 mechanism CITATION CITATION.
Tsuge et al. recently devised a specific version of this mechanism for iota toxin, which we follow closely in this work CITATION, CITATION.
The reaction follows three steps: the toxin cleaves nicotinamide to form an oxacarbenium ion, the oxacarbenium O 5D-P N bond rotates to relieve strain and forms a second ionic intermediate.
Finally, the target makes a nucleophilic attack on the second ionic intermediate.
The S N1mechansim believed widely applicable to CT group toxins is a template for new toxins given the historical structure similarity and consistent NAD conformation in the active site as shown in Figures 1 and 2.
Quaternary structure for the toxins is wide-ranging.
Several combinations exist for toxin domains and receptor binding or membrane translocation domains.
The B domains have diverse structures and functions and exist as fusions or separate polypeptides.
Various formats include: A-only, two-domain AB, three-domain AB and AB 5.
C3-like toxins are A-only.
ExoS-like toxins have toxic A-domains and are often paired with Rho GTPase activating protein, which are not true B domains.
C2-like toxins are AB toxins that contain B domains that are structural duplicates of the A domain.
These B domains are not toxins they bind proteins that are similar to anthrax protective antigen including Vip1, C2-II and Iota Ib CITATION, CITATION.
DT group toxins are three-domain, single polypeptide AB toxins where the B domain contains both a receptor-binding and a membrane-translocation domain.
The CT-PT-like toxins are AB 5 and have B domains that form a receptor-binding pentamer CITATION .
Low overall sequence identity hampers conventional sequence-based homology searches CITATION, CITATION, CITATION CITATION.
One challenge key to filling gaps in the toxin family is to link new sequences and known toxins.
Depending only on amino acid sequence alignment techniques to discover new toxins is imprudent.
Instead the trend is to use more structure information in the search because many primary sequences produce the same fold CITATION.
Researchers can then link these sequences through fold recognition CITATION .
Otto et al. used PSI-BLAST to identify new ADPRT toxins, including SpvB from Salmonella enterica CITATION.
More recently a similar strategy yielded 20 potential new toxins CITATION.
This led to interesting examples later characterized including: CARDS toxin from Mycoplasma pneumonia CITATION, SpyA from Streptococcus pyogenes CITATION and HopU1 from Pseudomonas syringae CITATION .
PSI-BLAST is a classic way to expand protein families, but it has limits.
For example, unrelated sequences often capture the search.
Also, nearly a decade has passed since Pallen et al. released the last detailed data mining results for the toxin family CITATION.
The sequence and structure databases and remote homolog detection tools have advanced during this time.
Masignani et al. proposed that a match between the conserved ADPRT pattern with corresponding secondary structure is one way to reduce dependence on sequence identity.
The pattern helps ensure function and reduces the total sequence set to a smaller subset for screening secondary structure prediction ensures that key active site parts are present CITATION .
Our contribution is to expand ADPRT toxin family using a new approach.
The difference is that we use fold-recognition searches extensively rather than relying on PSI-BLAST or secondary structure prediction.
Our genomic data mining combines pattern- and structure-based searches.
A bioinformatics toolset allows us to discover new toxins, classify and rank them and assess their structure and function.
Often, data mining studies simply present a table of hits with aligned sequences, but do not interpret or analyze those hits in detail.
Our aim rather than to explicitly confirm the roles of the six proteins, 15 domains, 18 loops and 120 residues discussed is to develop a theoretical framework for understanding new toxins, based on 100s 1000s of jobs per sequence.
We intend our in silico approach to guide and complement rather than replace follow-up in vitro and in vivo studies.
Here, we extract features and patterns from known ADPRT toxins and explain how they fit new toxins.
We use in silico methods to probe structure, secretion, cell entry, activation, NAD substrate binding, intracellular target binding and reaction mechanism.
A computer approach is fitting for several reasons.
Such an environment is a safe way to study new toxins.
Challenges in cloning, expressing, purifying and crystallizing often prevent in vitro characterization.
Also, ADPRTs are abundant within bacterial genomes and researchers make the sequences available faster than we can conduct biochemical studies.
New toxins might play a role in current outbreaks and are also excellent drug targets against antibiotic resistance.
Our new study design expands the family by 15 percent .
Cell-based validation complements our in silico approach.
We use Saccharomyces cerevisiae as a model host to study toxin effects.
Increasingly, researchers are turning to yeast to study bacterial toxins.
Yeast are easy to grow, have well-characterized genetics and are conserved with mammals in cellular processes including: DNA and RNA metabolism, signalling, cytoskeletal dynamics, vesicle trafficking, cell cycle control and programmed cell death CITATION CITATION.
We place the toxin genes under the control of a copper-inducible promoter to test putative toxins for ADP-ribosyltransferase activity in live cells CITATION.
A growth-defective phenotype clearly shows toxicity.
Substitutions to catalytic signature residues confirms ADP-ribosyltransferase activity causes the toxicity.
Indeed, pairing in silico and cell-based studies helps identify and characterize new ADPRT toxins.
Understanding the computations performed by neuronal circuits requires characterizing the strength and dynamics of the connections between individual neurons.
This characterization is typically achieved by measuring the correlation in the activity of two neurons.
We have developed a new measure for studying connectivity in neuronal circuits based on information theory, the incremental mutual information.
By conditioning out the temporal dependencies in the responses of individual neurons before measuring the dependency between them, IMI improves on standard correlation-based measures in several important ways: it has the potential to disambiguate statistical dependencies that reflect the connection between neurons from those caused by other sources provided that the dependencies have appropriate timescales, for the study of early sensory systems, it does not require responses to repeated trials of identical stimulation, and it does not assume that the connection between neurons is linear.
We describe the theory and implementation of IMI in detail and demonstrate its utility on experimental recordings from the primate visual system.
To understand the function of neuronal circuits and systems, it is essential to characterize the connections between individual neurons.
The major connections between and within many brain areas have been mapped through anatomical studies, but these maps specify only the existence of connections, not their strength or dynamics.
Measuring the strength and dynamics of the connection between two neurons requires physiological experiments in which the activity of both neurons is measured.
The most direct of these experiments involves intracellular recordings, which allow the connection between the two neurons to be directly investigated.
However, intracellular recordings are difficult to perform in vivo and impossible to obtain from more than a few cells at a time.
Instead, most physiological studies of connectivity rely on extracellular recordings from multi-electrode arrays.
In these experiments, it is not usually possible to explicitly verify anatomical connectivity, nor to directly characterize the connections.
Instead, the strength and dynamics of functional connectivity must be inferred through statistical methods.
The traditional method for characterizing the strength and dynamics of the connection between two neurons is the cross correlation function, which measures the linear correlation between two signals over a range of specified delays CITATION.
While C XY and its variants have been used successfully in a number of studies, it has limitations that must be considered when studying the connection between neurons CITATION CITATION.
The limitations of C XY arise from the fact that it is a measure of the total dependency between two signals and, thus, implicitly assumes that all dependencies between them are due to their connection.
In the case of neurons, there are in fact many potential sources of dependency shared external stimuli, intrinsic cellular and network properties, etc. and C XY cannot disambiguate these dependencies from those due to the actual connection.
Several modified versions of C XY have been proposed to address these drawbacks.
For example, if neuronal activity in response to repeated trials of the same external stimulus is available for analysis, as is often the case in early sensory systems, the shift-predictor can be used to remove some of the correlations due to the stimulus CITATION.
Further modifications to C XY have also been proposed to remove the correlations due to stimulus-driven covariations in activity CITATION and background activity CITATION.
While these modified approaches have certainly improved upon the standard C XY, the confound of dependencies due to the connection and those arising from other sources remains a general problem.
In addition to correlation-based methods, there are several other approaches to characterizing the dependency between two signals that can be used to study the connection between two neurons.
These methods can be generally divided into two classes: model-based and model-free.
The most common model-based approach to characterizing dependency is Granger causality CITATION.
With GC, one signal is predicted in two different ways: using an autoregressive model based on its own past and using a multivariate autoregressive model based on its own past and the past of the second signal.
The strength of the dependency is given by the difference in the predictive power of the two models and the dynamics of the dependency are reflected in the regression parameters that correspond to the influence of the second signal.
The power of model-based approaches such as GC is dependent on the validity of the underlying model; if the dependency between the two signals is approximately linear, then the characterization provided by GC will be accurate, but in situations where the properties of the dependency are complex or unknown, as is often the case with neurons, a model-free approach may be more appropriate.
The most common model-free approach to characterizing dependency is transfer entropy, the information-theoretic analog of GC CITATION.
TE measures the reduction in the entropy of one signal that is achieved by conditioning on its own past and the past of the second signal relative to the reduction in entropy achieved by conditioning on its own past alone.
TE is a powerful tool for measuring the overall strength of a dependency, but is not suitable for characterizing its dynamics.
In this paper, we detail a new model-free approach for characterizing both the strength and dynamics of a dependency by conditioning out the temporal correlations in both signals before assessing the strength of the dependency at different delays.
This approach can overcome some of the confounds that are common in studies of neuronal connectivity CITATION CITATION, as it has the potential to disambiguate statistical dependencies that reflect the connection between neurons from those caused by other sources provided that the dependencies have appropriate timescales.
In the following sections, we outline the theory behind our measure, which we call incremental mutual information, illustrate its usage on simulated neuronal activity and experimental recordings from the primate visual system, and consider its relationship to other common measures of dependence.
Matlab code for measuring incremental mutual information is available for download at LINK
Yeast successfully adapts to an environmental stress by altering physiology and fine-tuning metabolism.
This fine-tuning is achieved through regulation of both gene expression and protein activity, and it is shaped by various physiological requirements.
Such requirements impose a sustained evolutionary pressure that ultimately selects a specific gene expression profile, generating a suitable adaptive response to each environmental change.
Although some of the requirements are stress specific, it is likely that others are common to various situations.
We hypothesize that an evolutionary pressure for minimizing biosynthetic costs might have left signatures in the physicochemical properties of proteins whose gene expression is fine-tuned during adaptive responses.
To test this hypothesis we analyze existing yeast transcriptomic data for such responses and investigate how several properties of proteins correlate to changes in gene expression.
Our results reveal signatures that are consistent with a selective pressure for economy in protein synthesis during adaptive response of yeast to various types of stress.
These signatures differentiate two groups of adaptive responses with respect to how cells manage expenditure in protein biosynthesis.
In one group, significant trends towards downregulation of large proteins and upregulation of small ones are observed.
In the other group we find no such trends.
These results are consistent with resource limitation being important in the evolution of the first group of stress responses.
Unicellular organisms are sensitive to environmental challenges.
Their internal milieu acts as a buffer against such changes by mounting an adaptive response involving modifications at different cellular levels.
Appropriate adaptive responses require intracellular signaling, changes in the conformation and activity of proteins, changes in transcription and translation of genes, etc. CITATION.
Many of the cellular modifications that characterize any adaptive response are due to the need for acquiring new protein functionalities while shutting down other protein functionalities that are not required in the new conditions.
These changes ultimately fine tune the mechanisms and processes that allow the cell to function appropriately and survive under changing environments.
Such fine tuning is shaped by various functional requirements and physiological constraints.
The functional requirements are a result of the specific demands that are imposed on cell survival by the environment.
On the other hand, the physiological constraints are defined by the limits within which the cell is physically capable of changing the activity of its component parts to meet the functional requirements.
From a global point of view, adaptive responses can be seen as a multi-optimization problem because cells evolved appropriate responses to cope with different types of stress, while optimizing different parts of its metabolism for each of those responses CITATION, CITATION.
For example, cells simultaneously have to increase the concentration of specific metabolites and proteins, while decreasing the concentration of other components to prevent an increase in the concentration of unneeded metabolites.
Such an increase could strain cell solubility capacity or increase spurious reactivity to dangerous levels.
These and other functional constraints are likely to provide sustained evolutionary pressures that ultimately select a specific gene expression profile that leads to suitable adaptive responses.
With these arguments in mind, it is thus important to identify the functional requirements and quantitative physiological constraints that may significantly shape adaptive responses.
Among others, minimization of energetic expenditure plays an important role in cells growing exponentially in a rich medium.
Several signatures that are consistent with minimization of metabolic cost have already been identified in the properties of the set of proteins that is expressed when cells are growing in rich media .
For example, genes coding for proteins that are highly abundant under basal conditions have a pattern of synonymous codon usage that is well adapted to the relative abundance of synonymous tRNAs in the yeast S. cerevisiae and in Escherichia coli CITATION, CITATION .
Another signature that is found in genes that are highly expressed under basal conditions is a sequence bias that minimizes transcriptional and translational costs CITATION.
This minimization of metabolic cost is further observed in the relative amino acid composition of abundant proteins under the same conditions.
These proteins are enriched with metabolically cheaper amino acids CITATION .
A final example of a general signature is the codon bias of long genes.
This bias is such that the probability of missense errors is reduced during translation CITATION, CITATION, CITATION.
These biases suggest that reducing overall costs in metabolism, whenever possible, may significantly increase cellular fitness.
This view is consistent with the observation that small changes in gene expression affecting the levels of protein synthesis influence the fitness of specific E. coli strains CITATION .
This body of results strongly supports the notion that metabolic cost acts as a selective pressure in shaping the properties of cells growing in a rich medium, in absence of environmental stresses.
Thus, one might ask if minimization of metabolic cost is also an important factor in the evolution of adaptive responses to stress conditions.
It is predictable that this evolutionary pressure might leave stronger signatures in adaptive responses that require the use of higher ATP amounts by the cell, such as adaptation to heat, weak organic acids, or NaCl.
In these three cases, it has been reported that ATP concentrations decrease due to a high energy demand CITATION .
Given that protein synthesis is one of the costliest biosynthetic efforts for the cell CITATION, the minimization of metabolic cost might have biased the properties of proteins whose expression change during adaptation.
Therefore, here we ask the following questions.
Is there a signature that is consistent with a selective pressure for minimizing metabolic cost in proteins synthesis during adaptive responses to stress?
Can one find general signatures in the physicochemical properties proteins and in the expression patterns of genes that are involved in the adaptive response to different environmental challenges?
If so, what physiological constraints are consistent with those signatures?
We address these questions by investigating how is the value of several properties of proteins related to changes in gene expression levels during various environmental changes.
We find that genes whose expression is upregulated during different types of adaptive responses tend to code for proteins that are small, while genes whose expression is downregulated during the same responses tend to code for proteins that are large.
This is a signature that is consistent with a selective pressure for minimizing metabolic cost in proteins synthesis.
It is more significant in adaptive responses where changes in gene expression levels affect a large fraction of the genome.
To our knowledge, this is the first general and global signature that has been identified for the properties of proteins involved in adaptive responses to stress.
Synchronization of 30 80 Hz oscillatory activity of the principle neurons in the olfactory bulb is believed to be important for odor discrimination.
Previous theoretical studies of these fast rhythms in other brain areas have proposed that principle neuron synchrony can be mediated by short-latency, rapidly decaying inhibition.
This phasic inhibition provides a narrow time window for the principle neurons to fire, thus promoting synchrony.
However, in the olfactory bulb, the inhibitory granule cells produce long lasting, small amplitude, asynchronous and aperiodic inhibitory input and thus the narrow time window that is required to synchronize spiking does not exist.
Instead, it has been suggested that correlated output of the granule cells could serve to synchronize uncoupled mitral cells through a mechanism called stochastic synchronization, wherein the synchronization arises through correlation of inputs to two neural oscillators.
Almost all work on synchrony due to correlations presumes that the correlation is imposed and fixed.
Building on theory and experiments that we and others have developed, we show that increased synchrony in the mitral cells could produce an increase in granule cell activity for those granule cells that share a synchronous group of mitral cells.
Common granule cell input increases the input correlation to the mitral cells and hence their synchrony by providing a positive feedback loop in correlation.
Thus we demonstrate the emergence and temporal evolution of input correlation in recurrent networks with feedback.
We explore several theoretical models of this idea, ranging from spiking models to an analytically tractable model.
Synchronization of neural activity has been suggested to facilitate coding CITATION CITATION and propagation of activity CITATION CITATION.
Synchronous stimulus-induced oscillatory activity has long been known to occur in the olfactory system of mammals CITATION CITATION.
Synchronous, rhythmic activity has been proposed to play a role in odor discrimination tasks CITATION.
In insects, disruption of synchronous oscillations can impair discrimination of chemically similar odorants CITATION.
In mice, enhancement of synchronous oscillations in the olfactory bulb using genetic modifications improves performance in fine discrimination tasks CITATION.
In the mammalian olfactory system, mitral cell synchrony contributes to the generation of the gamma oscillations in the local field potential; for example, in the cat olfactory system, increases in the synchrony between mitral cells are accompanied by a concomitant increase in the power of the gamma band in the local field potential CITATION.
Mitral cells have been shown to undergo synchronization during odor-evoked responses CITATION or during olfactory nerve stimulation CITATION.
Although, previous experimental and modeling studies have highlighted the role of granule cells CITATION and lateral inhibition CITATION in the production of gamma oscillations in the olfactory bulb, the exact mechanism by which such mitral cell synchronization occurs in the mitral-granule cell network connected by reciprocal recurrent and lateral connections remains largely unknown.
A possible mechanism of synchronization of mitral cells in the olfactory bulb is suggested by recent experimental evidence.
In paired recordings from mitral cells, activation of a mitral cell elicits fast unitary inhibitory post-synaptic potentials in a second mitral cell CITATION, CITATION, CITATION.
These IPSC's are due to the synaptic activation of the shared granule cells via the mitral-granule cell dendrodendritic synapses.
Although the individual IPSC's are fast, they arrive randomly, i.e. the output of the granule cells is not time locked to the stimulus.
The temporally prolonged barrage of these unitary IPSC's produced in response to the spiking in the first mitral cell results in a slow rising and long lasting hyperpolarization in the second mitral cell CITATION CITATION.
There is a variable delay between the evoked IPSC's in the second mitral cell and the spike times in the first mitral cell CITATION.
Thus, the evoked IPSC's occur asynchronously CITATION CITATION, aperiodically CITATION and the kinetics of hyperpolarization in an ensemble average of the evoked IPSC's show a slow rise time and a slow decay constant CITATION CITATION.
In addition, the peak amplitudes of the ensemble average are small, CITATION.
The prolonged, asynchronous barrages of IPSC's have been shown to be a result of long latency, asynchronous and long lasting mitral cell recruitment of granule cells CITATION.
Furthermore, recent experimental studies into the origin of synchrony between mitral cells suggests that recovery from shared IPSC inputs from common granule cells is the primary driving mechanism for mitral cell synchrony CITATION, CITATION.
These physiologically measured properties of mitral-granule cell interactions suggest a novel mechanism of synchronization of mitral cells in the olfactory bulb.
Previous studies have proposed that noise can synchronize oscillators CITATION.
For neurons to undergo such noise-induced synchronization they should be periodically firing and should have some shared fast fluctuations in their inputs.
Recent studies on the mechanism of generation of synchronized oscillatory activity by long lasting asynchronous, aperiodic inhibition in the olfactory bulb have revealed exactly such a novel role for noise CITATION.
It was shown that two mitral cells firing in the gamma frequency range can undergo synchronization upon receiving common inhibitory input from granule cells.
The degree of synchronization was shown to depend on the degree of correlation in the noisy input shared by the two neurons.
Although spiking was synchronized, the shared noise itself was aperiodic.
In all of the experimental and theoretical studies of stochastic synchronization to date, the degree of correlation is imposed and held fixed.
In our study the degree of input correlation emerges intrinsically from within the network and is amplified over time due to the dynamics of the network.
In addition, our study utilizes theoretically derived probability distribution of phase difference for uncoupled oscillators receiving shared noise to investigate the conditions necessary for the existence of bistability in the magnitude of input correlation.
Here we consider the case in which correlated fluctuations from granule cells arise naturally from granule cells that connect to many mitral cells.
The input correlation to any pair of mitral cells could increase if the shared pool of presynaptic granule cells increased their stochastic firing rate thus providing a greater amount of common noise.
In the olfactory bulb, synapses between mitral and granule cells are dendrodendritic, and almost always reciprocal CITATION.
Thus, if a granule cell synapses on a pair of mitral cells, those mitral cells also synapse on that granule cell.
We hypothesize that, since a pair of mitral cells with correlated input is more likely to fire synchronously, this pair is also more likely to provide correlated input to their common granule cell.
In turn the common granule cell could then increases its release of transmitter increasing the correlation to the mitral cells.
The result of this is that the feedback provides an amplification of correlation.
The goal of this paper is to use computational and analytic techniques to show that such feedback will increase correlation and as a consequence, synchrony between oscillating mitral cells.
We describe three models for feedback induced correlation and stochastic sychronization.
We first study one pair of mitral cells and one common granule cell.
The mitral cells are modeled as simple phase oscillators which are perturbed through their phase-resetting curves.
The granule cell is modeled as a noisy leaky integrate and fire neuron receiving synaptic input from the mitral cell oscillators.
The second model replaces each phase oscillator with the conductance-based Morris-Lecar oscillator.
Finally, to allow for analytic approaches, we reduce the first two models to a discrete time map which we study using an averaging technique.
T-Cell antigen Receptor repertoire is generated through rearrangements of V and J genes encoding and chains.
The quantification and frequency for every V-J combination during ontogeny and development of the immune system remain to be precisely established.
We have addressed this issue by building a model able to account for V -J gene rearrangements during thymus development of mice.
So we developed a numerical model on the whole TRA/TRD locus, based on experimental data, to estimate how V and J genes become accessible to rearrangements.
The progressive opening of the locus to V-J gene recombinations is modeled through windows of accessibility of different sizes and with different speeds of progression.
Furthermore, the possibility of successive secondary V-J rearrangements was included in the modelling.
The model points out some unbalanced V-J associations resulting from a preferential access to gene rearrangements and from a non-uniform partition of the accessibility of the J genes, depending on their location in the locus.
The model shows that 3 to 4 successive rearrangements are sufficient to explain the use of all the V and J genes of the locus.
Finally, the model provides information on both the kinetics of rearrangements and frequencies of each V-J associations.
The model accounts for the essential features of the observed rearrangements on the TRA/TRD locus and may provide a reference for the repertoire of the V-J combinatorial diversity.
Functional antigen receptors expressed by T lymphocytes are generated during ontogeny by somatic recombination of gene segments coding for the variable, the joining, and the constant segments CITATION.
The recombination mechanism is largely dependent on both the accessibility of the loci and the RAG enzymatic complex CITATION CITATION.
The murine TRA/TRD locus is composite, encoding TR and chains and encompassed of more than 100 functional V genes CITATION.
In theory, each of the V genes may target one of the 49 functional J genes.
The use of V and J genes during the process of recombination has been widely debated, and the studies support the consensus that V-J combinations are not random, with a use of J segments starting at the 5 end and proceeding to the 3 end CITATION CITATION.
The accessibility of the J region is controlled by the TR enhancer, located at the 3 end of the C gene CITATION and by two promoters: T early, located at the 5 end of the J region and ii J49 located 15 Kb downstream of TEA.
Both of the promoters are activated by E CITATION, CITATION, CITATION.
E controls all the V to J associations whereas the two promoters are required for the rearrangements of the J genes situated at the 5 end of the J region.
However, the analyses of TEA-deleted alleles and those of blockade of TEA transcription showed significant alterations in J use and support the hypothesis that the TEA promoter can regulate both positively the promoters located in the first 12 Kb of J genes and negatively the downstream promoters CITATION, CITATION CITATION .
A particularity of the TRA locus is an absence of allelic exclusion CITATION and its ability to undergo multiple cycles of secondary rearrangements CITATION, CITATION.
The process of successive rearrangements is stopped by either positive selection, which downregulates recombinase expression CITATION or by cell death.
Therefore, the impact of secondary rearrangements on the TR gene assembly regulation remains to be defined.
Regarding the V and J gene use, it is suggested that the first V-J association targets the secondary one into a set of J segments located near the J segment involved in the primary rearrangement CITATION, CITATION.
The rules governing the use of the V genes have not been clearly elucidated.
Nevertheless, observations converge to a consensus: the use of V segments would progress from proximal V genes, located near the J region, towards the V genes located in the distal region CITATION, CITATION.
At this point in time, the mechanism involved in the control of accessibility of V genes remains to debate CITATION .
The current state of the technology permits the analysis of some V-J combinations, essentially those at the extremities of the locus but still fails to establish a complete estimation of the V-J combinations.
The main obstacle comes from the fact that some V genes are duplicated in similar copies in the V region central part, making problematic their unambiguous identification by molecular methods CITATION .
Consequently, numerical modelling of the V-J recombination process may offer valuable support to overcome the difficulty for accessing to a global view of TRA repertoire.
For instance, if the J genes are chosen in a sequential way in the model, their use results unimodal, whereas it is known from experimental data that TRA/TRD locus displays two Hot Spots of recombination CITATION CITATION.
This discrepancy led us to build a mathematical model, parameterized from experimental data, on all V and J genes, including those in distal, proximal, and central positions.
Confrontation between the data obtained from experiments and from modelling makes possible an estimation of dynamical parameters, such as the accessibility to rearrangements and the frequencies of the V-J associations, giving a more accurate estimation of the TRA combinatorial diversity.
Metagenomics is a discipline that enables the genomic study of uncultured microorganisms.
Faster, cheaper sequencing technologies and the ability to sequence uncultured microbes sampled directly from their habitats are expanding and transforming our view of the microbial world.
Distilling meaningful information from the millions of new genomic sequences presents a serious challenge to bioinformaticians.
In cultured microbes, the genomic data come from a single clone, making sequence assembly and annotation tractable.
In metagenomics, the data come from heterogeneous microbial communities, sometimes containing more than 10,000 species, with the sequence data being noisy and partial.
From sampling, to assembly, to gene calling and function prediction, bioinformatics faces new demands in interpreting voluminous, noisy, and often partial sequence data.
Although metagenomics is a relative newcomer to science, the past few years have seen an explosion in computational methods applied to metagenomic-based research.
It is therefore not within the scope of this article to provide an exhaustive review.
Rather, we provide here a concise yet comprehensive introduction to the current computational requirements presented by metagenomics, and review the recent progress made.
We also note whether there is software that implements any of the methods presented here, and briefly review its utility.
Nevertheless, it would be useful if readers of this article would avail themselves of the comment section provided by this journal, and relate their own experiences.
Finally, the last section of this article provides a few representative studies illustrating different facets of recent scientific discoveries made using metagenomics.
For most of its history, life on Earth consisted solely of microscopic life forms, and microbial life still dominates Earth in many aspects.
The estimated 5 10 30 prokaryotic cells inhabiting our planet sequester some 350 550 Petagrams of carbon, 85 130 Pg of nitrogen, and 9 14 Pg of phosphorous making them the largest reservoir of those nutrients on Earth CITATION.
Bacteria and archaea live in all environments capable of sustaining other life and in many cases are the sole inhabitants of extreme environments: from deep sea vents with temperatures of 340 C to rocks found in boreholes 6 km beneath the Earth's surface.
Bacteria, archea, and microeukaryotes dominate Earth's habitats, compound recycling, nutrient sequestration, and, according to some estimates, biomass.
Microbes are not only ubiquitous, they are essential to all life, as they are the primary source for nutrients, and the primary recyclers of dead matter back to available organic form.
Along with all other animals and plants, the human condition is profoundly affected by microbes, from the scourges of human, farm animal, and crop pandemics, to the benefits in agriculture, food industry, and medicine to name a few.
We humans have more bacterial cells inhabiting our body than our own cells CITATION, CITATION.
It has been stated that the key to understanding the human condition lies in understanding the human genome CITATION, CITATION.
But given our intimate relationship with microbes CITATION, researching the human genome is now understood to be a necessary though insufficient condition: sequencing the genomes of our own microbes would be necessary too.
Also, to better understand the role of microbes in the biosphere, it would be necessary to undertake a genomic study of them as well.
The study of microbial genomes started in the late 1970s, with the sequencing of the genomes of bacteriophages MS2 CITATION and -X174 CITATION.
In 1995 microbiology took a major step with the sequencing of the first bacterial genome Haemophilus influenza CITATION.
The genomes of 916 bacterial, 1,987 viral, and 67 archaeal species are deposited in GenBank release 2.2.6.
Having on hand such a large number of microbial genomes has changed the nature of microbiology and of microbial evolution studies.
By providing the ability to examine the relationship of genome structure and function across many different species, these data have also opened up the fields of comparative genomics and of systems biology.
Nevertheless, single organism genome studies have limits.
First, technology limitations mean that an organism must first be clonally cultured to sequence its entire genome.
However, only a small percentage of the microbes in nature can be cultured, which means that extant genomic data are highly biased and do not represent a true picture of the genomes of microbial species CITATION CITATION.
Second, very rarely do microbes live in single species communities: species interact both with each other and with their habitats, which may also include host organisms.
Therefore, a clonal culture also fails to represent the true state of affairs in nature with respect to organism interaction, and the resulting population genomic variance and biological functions.
New sequencing technologies and the drastic reduction in the cost of sequencing are helping us overcome these limits.
We now have the ability to obtain genomic information directly from microbial communities in their natural habitats.
Suddenly, instead of looking at a few species individually, we are able to study tens of thousands all together.
Sequence data taken directly from the environment were dubbed the metagenome CITATION, and the study of sequence data directly from the environment metagenomics CITATION .
However, environmental sequencing comes with its own information-restricting price tag.
In single organism genomics practically all of the microbe's genome is sequenced, providing a complete picture of the genome.
We know from which species the DNA or RNA originated.
After assembly, the location of genes, operons, and transcriptional units can be computationally inferred.
Control elements and other cues can be identified to infer transcriptional and translational units.
Consequently, we achieve a nearly complete and well-ordered picture of all the genomic elements in the sequenced organism.
We may not recognize all the elements for what they are, and some errors may creep in, but we can gauge the breadth of our knowledge and properly annotate those areas of the genome we manage to decipher.
In contrast, the sequences obtained from environmental genomic studies are fragmented.
Each fragment was obviously sequenced from a specific species, but there can be many different species in a single sample, for most of which a full genome is not available.
In many cases it is impossible to determine the true species of origin.
The length of each fragment can be anywhere between 20 base pairs and 700 bp, depending on the sequencing method used.
Short sequence reads that are dissociated from their original species can be assembled to lengths usually not exceeding 5,000 bp; consequently, the reconstruction of a whole genome is generally not possible.
Even the reconstruction of an entire transcriptional unit can be problematic.
In addition to being fragmented and incomplete, the volume of sequence data acquired by environmental sequencing is several orders of magnitude larger than that acquired in single organism genomics.
For these reasons, computational biologists have been developing new algorithms to analyze metagenomic data.
These computational challenges are new and very exciting.
We are entering an era akin to that of the first genomic revolution almost two decades ago.
Whole organism genomics allows us to examine the evolution not only of single genes, but of whole transcriptional units, chromosomes, and cellular networks.
But more recently, metagenomics gave us the ability to study, on the most fundamental genomic level, the relationship between microbes and the communities and habitats in which they live.
How does the adaptation of microbes to different environments, including host animals and other microbes, manifest itself in their genomes?
For us humans, this question can strike very close to home, when those habitats are our own bodies and the microbes are associated with our own well-being and illnesses: almost every aspect of human life, as well as the life of every other living being on the planet, is affected by microbes.
We now have the experimental technology to understand microbial communities and how they affect us, but the sheer volume and fragmentary nature of the data challenge computational biologists to distill all these data into useful information.
In this article we shall briefly outline some experimental, technological, and computational achievements and challenges associated with metagenomic data, from sequence generation and assembly through the various levels of metagenomic annotation.
We will also discuss computational issues that are unique to environmental genomics, such as estimating the metagenome size and the handling of associated metadata.
Finally, we will review some studies highlighting the advantages of metagenomic-based research, and some of the insights it has enabled.
Productive cell migration requires the spatiotemporal coordination of cell adhesion, membrane protrusion, and actomyosin-mediated contraction.
Integrins, engaged by the extracellular matrix, nucleate the formation of adhesive contacts at the cell's leading edge, and maturation of nascent adhesions to form stable focal adhesions constitutes a functional switch between protrusive and contractile activities.
To shed additional light on the coupling between integrin-mediated adhesion and membrane protrusion, we have formulated a quantitative model of leading edge dynamics combining mechanistic and phenomenological elements and studied its features through classical bifurcation analysis and stochastic simulation.
The model describes in mathematical terms the feedback loops driving, on the one hand, Rac-mediated membrane protrusion and rapid turnover of nascent adhesions, and on the other, myosin-dependent maturation of adhesions that inhibit protrusion at high ECM density.
Our results show that the qualitative behavior of the model is most sensitive to parameters characterizing the influence of stable adhesions and myosin.
The major predictions of the model, which we subsequently confirmed, are that persistent leading edge protrusion is optimal at an intermediate ECM density, whereas depletion of myosin IIA relieves the repression of protrusion at higher ECM density.
In multicellular organisms, cell migration is of paramount importance for physiological processes such as tissue homeostasis and repair, immune surveillance and response, and developmental patterning.
In culture, the crawling of mammalian cells on a surface coated with extracellular matrix protein such as fibronectin is classically described as a cycle of distinct subprocesses: membrane protrusion and formation of new adhesive bonds with the underlying substratum at the cell's leading edge, followed by contraction of the cell body forwards, and finally detachment of adhesions at the cell's rear CITATION.
The primary molecular hubs for the integration of these subprocesses are integrins, adhesion receptors that recognize specific ECM proteins.
Upon ligation, integrins cluster to form adhesive contacts that orchestrate the activation of a host of signal transduction pathways and the anchorage of actin filaments inside the cell CITATION, CITATION.
Thus, they provide not only physical linkages between the ECM and actin cytoskeleton, through which myosin II motors generate contractile force, but also platforms for localizing biochemical signals that govern leading edge protrusion CITATION, CITATION.
Of particular importance in that regard is the integrin-mediated activation of Rac.
Its isoforms are small GTPases of the Rho family that, among other cellular functions, promote cell spreading and formation of broad, flat membrane structures called lamellipodia CITATION.
Despite these molecular insights, the bases for the dynamics of cell migration subprocesses, seemingly stochastic on the one hand, yet spatiotemporally coordinated on the other, are only beginning to be clarified CITATION .
One of the most mechanistically telling aspects of cell migration is its dependence on ECM density.
The general observation is that overall migration speed, determined from the movement of the cell centroid, is optimal at an intermediate ECM density CITATION, CITATION.
The physical interpretation of this finding was that the optimal ECM density corresponds to a density of integrin-ECM bonds that allows for both productive motility at the cell front and detachment of older adhesions at the rear of the cell, through myosin-dependent contractility.
More recently, this conceptual model has been refined based on detailed measurements of F-actin dynamics and myosin II recruitment in PtK 1 cells, revealing an optimal myosin II/F-actin density ratio at intermediate ECM density CITATION .
Further insight came through the implication that not all adhesions actively contribute to membrane protrusion signaling; apparently, only newer adhesions formed at the cell's leading edges do CITATION.
It seems that maturation of a nascent adhesion to form a stable, focal adhesion, marked by actomyosin-dependent growth of the complex perpendicular to the leading edge CITATION, is accompanied by loss of its ability to mediate protrusion signaling.
In Chinese hamster ovary .K1 cells expressing paxillin-enhanced green fluorescent protein, total internal reflection fluorescence microscopy has revealed that, during steady protrusion, the small nascent adhesions are rapidly formed and turned over CITATION, in proportion to the protrusion velocity CITATION.
This phenotype is mediated by signaling through Rac, which can be activated in a variety of ways, one of them involving the Rac effector, p21-activated kinase.
Among its various functions, active PAK phosphorylates the focal adhesion protein paxillin on Ser 273, providing a binding site for the recruitment of the scaffold protein GIT1; GIT1 binds both PIX, a guanine-nucleotide exchange factor that activates Rac, and PAK, which is activated in turn by Rac.
Thus, the pathway constitutes a positive feedback circuit.
Disrupting the circuit, for example through expression of paxillin with Ser 273 Ala mutation or kinase-dead PAK, abrogates protrusion and nascent adhesion formation, whereas expression of paxillin with phosphorylation-mimicked Ser 273 Asp mutation or constitutively active PAK enhances these responses CITATION.
Myosin II opposes the influence of Rac/PAK signaling in this context, promotes adhesion maturation, and strongly inhibits the protrusion phenotype CITATION, CITATION ; this effect is expected to be more prominent at higher ECM density CITATION .
Here, through computational modeling and stochastic simulations, we develop new ideas about mechanisms that might give rise to the dynamical interplay between cell protrusion and adhesion at the cell's leading edge CITATION CITATION.
Analysis of the model suggests that protrusion signaling mediated by nascent adhesions is inherently sensitive because of positive feedback but also susceptible to regulation by other feedback loops involving stable adhesions and myosin II.
These regulatory mechanisms shape the dependence of the protrusion/adhesion phenotypic balance on ECM density, which we compare to experimentally observed dynamics in CHO.K1 cells.
Kinetically stable proteins, those whose stability is derived from their slow unfolding kinetics and not thermodynamics, are examples of evolution's best attempts at suppressing unfolding.
Especially in highly proteolytic environments, both partially and fully unfolded proteins face potential inactivation through degradation and/or aggregation, hence, slowing unfolding can greatly extend a protein's functional lifetime.
The prokaryotic serine protease -lytic protease has done just that, as its unfolding is both very slow and so cooperative that partial unfolding is negligible, providing a functional advantage over its thermodynamically stable homologs, such as trypsin.
Previous studies have identified regions of the domain interface as critical to LP unfolding, though a complete description of the unfolding pathway is missing.
In order to identify the LP unfolding pathway and the mechanism for its extreme cooperativity, we performed high temperature molecular dynamics unfolding simulations of both LP and trypsin.
The simulated LP unfolding pathway produces a robust transition state ensemble consistent with prior biochemical experiments and clearly shows that unfolding proceeds through a preferential disruption of the domain interface.
Through a novel method of calculating unfolding cooperativity, we show that LP unfolds extremely cooperatively while trypsin unfolds gradually.
Finally, by examining the behavior of both domain interfaces, we propose a model for the differential unfolding cooperativity of LP and trypsin involving three key regions that differ between the kinetically stable and thermodynamically stable classes of serine proteases.
-lytic protease, a prokaryotic serine protease of the chymotrypsin family, has evolved an unusual energetic landscape, providing it a functional advantage over its metazoan homologs.
Unlike most proteins, LP's active state is not stabilized by thermodynamics, but by a large kinetic barrier to unfolding, with an unfolding t 1/2 of 1 year.
CITATION While thermodynamically stable homologs like trypsin have similar unfolding rates, they are degraded at rates up to 100x faster than LP under highly proteolytic conditions.
CITATION, CITATION In addition, the rates of LP unfolding and degradation are nearly identical, indicating that partial unfolding leading to proteolysis is negligible.
Therefore, LP's functional advantage is derived from not only its very slow unfolding, which it shares with trypsin, but also its suppression of local unfolding events that would render it protease-accessible.
Thus, it appears that the evolution of LP has generated such extreme cooperativity in unfolding in order to maximize its functional lifetime under harsh conditions.
The cost of maximizing resistance to unfolding comes in the form of extremely slow folding and the consequent loss of thermodynamic stability of the active state relative to the unfolded state.
CITATION, CITATION However, LP also evolved a large Pro-region folding catalyst, which speeds folding by nine orders of magnitude and is then degraded by the mature protease, decoupling the folding and unfolding landscapes so that unfolding resistance can be maximized.
CITATION, CITATION, CITATION
Given LP's unusual energetic landscape and its reliance on kinetic stability, much effort has focused on elucidating its unfolding mechanism in detail.
Native-state hydrogen-deuterium exchange showed over half of its 194 backbone amides are well-protected from exchange, and 31 have protection factors greater than 10 9.
CITATION This extreme rigidity is spread throughout both domains and is indicative of LP's high unfolding cooperativity.
Thermodynamic decomposition of the unfolding energetics into entropic and enthalpic contributions suggested a prominent role for the extensive domain interface in unfolding, with the critical step involving solvation of the domain interface while the individual domains remain relatively intact.
CITATION Mutational studies on LP inspired by the acid-resistant homolog NAPase were consistent with this hypothesis.
The distribution of salt-bridges in NAPase and LP differ markedly; replacement of a salt-bridge at LP's domain interface with an intra-domain salt-bridge resulted in significant increases in LP's resistance to low pH unfolding.
CITATION A major component of the domain interface, the Domain Bridge, is the only covalent linkage between the two domains.
This structure exists only in prokaryotic proteases and varies considerably among LP and its homologs.
The area buried by the domain bridge is inversely correlated with the high-temperature unfolding rate for four kinetically stable proteases, indicating both its relevance and that it is weakened early in unfolding.
CITATION Another domain interface component is a -hairpin in the C-terminal domain, unique to kinetically stable proteases, that forms part of the active site.
Substitution of a more stable -turn was consistent with an unfolding pathway where C H loses its domain interface contacts early in unfolding.
CITATION Despite much progress, we still lack a global picture of LP unfolding, especially at high resolution.
For higher-resolution views of protein folding/unfolding, researchers have often turned to -value analysis.
CITATION CITATION These studies involve large-scale protein engineering experiments which investigate the molecule's folding and unfolding kinetics after making perturbing mutations, normally hydrophobic deletions.
By analyzing sufficiently large numbers of perturbations, structure in the transition state ensemble can be inferred and a folding/unfolding mechanism can be proposed.
Unfortunately, the extremely slow folding and unfolding rates for LP make large-scale -value analysis on LP impractical.
As an alternative, we decided to investigate the LP unfolding pathway computationally in order to explain previous experiments and guide new ones.
High-temperature molecular dynamics unfolding simulations offer the highest structural and temporal resolution for studying protein unfolding, but their results must be validated experimentally.
Since unfolding rates for proteins are typically very slow under physiological conditions, very high temperatures are required to accelerate the unfolding into the ns range required for computational analysis.
As a consequence, initially there was significant concern as to the relevance of the high temperature TSEs to real proteins under physiological conditions.
Daggett and co-workers have been pioneers in this field, using Chymotrypsin Inhibitor 2 as a model system and have shown that the simulated unfolding calculations agree remarkably well with experimental -values and were even able to predict faster folding mutants.
CITATION CITATION Further work on other proteins by multiple groups has established MD unfolding simulations as a useful tool in examining protein unfolding at atomic resolution while correlating well with experiments.
CITATION CITATION
A critical step in analyzing unfolding simulations is accurately pinpointing the TSE from the multitude of conformations generated.
Because the TSE is experimentally accessible through a molecule's folding and unfolding kinetics, its identification computationally can be used for both explanatory and predictive purposes.
Various methods for identifying the TSE have been used in the past, breaking down into conformational clustering and landscape methods.
CITATION, CITATION, CITATION, CITATION, CITATION CITATION Conformational clustering relies on all-versus-all comparisons of conformations, often by C RMSD, while landscapes separating native from unfolded structures can be generated using properties of the conformations, such as the fraction of native contacts or secondary structure.
Here, we report the results of multiple MD simulations carried out at high temperature in order to probe the mechanism of LP's extremely cooperative unfolding.
Due to the robustness and cooperativity of LP unfolding, the same TSE is obtained using either conformational clustering or landscape methods.
The simulated unfolding pathway for LP matches well with previously described experiments and provides atomic resolution to previous models for LP unfolding which highlight the role of the domain interface.
In addition, we have performed similar simulations on trypsin with the goal of understanding the observed experimental differences in unfolding cooperativity.
Through a novel method for calculating cooperativity in MD simulations, we show LP unfolds significantly more cooperatively than trypsin, mirroring the experimental results.
Finally, by analyzing the domain interfaces of both proteins during unfolding, we propose a mechanism for how this differential cooperativity is achieved.
Human disease is heterogeneous, with similar disease phenotypes resulting from distinct combinations of genetic and environmental factors.
Small-molecule profiling can address disease heterogeneity by evaluating the underlying biologic state of individuals through non-invasive interrogation of plasma metabolite levels.
We analyzed metabolite profiles from an oral glucose tolerance test in 50 individuals, 25 with normal and 25 with impaired glucose tolerance.
Our focus was to elucidate underlying biologic processes.
Although we initially found little overlap between changed metabolites and preconceived definitions of metabolic pathways, the use of unbiased network approaches identified significant concerted changes.
Specifically, we derived a metabolic network with edges drawn between reactant and product nodes in individual reactions and between all substrates of individual enzymes and transporters.
We searched for active modules regions of the metabolic network enriched for changes in metabolite levels.
Active modules identified relationships among changed metabolites and highlighted the importance of specific solute carriers in metabolite profiles.
Furthermore, hierarchical clustering and principal component analysis demonstrated that changed metabolites in OGTT naturally grouped according to the activities of the System A and L amino acid transporters, the osmolyte carrier SLC6A12, and the mitochondrial aspartate-glutamate transporter SLC25A13.
Comparison between NGT and IGT groups supported blunted glucose- and/or insulin-stimulated activities in the IGT group.
Using unbiased pathway models, we offer evidence supporting the important role of solute carriers in the physiologic response to glucose challenge and conclude that carrier activities are reflected in individual metabolite profiles of perturbation experiments.
Given the involvement of transporters in human disease, metabolite profiling may contribute to improved disease classification via the interrogation of specific transporter activities.
Disease heterogeneity has challenged the practice of medicine.
Individuals with the same apparent disease at our current diagnostic resolution often show remarkable variation in prognosis and treatment responsiveness, presumably because a superficially similar disease state can arise from diverse combinations of genetic and environmental factors CITATION.
Efforts to resolve the heterogeneity have focused on collecting increasing amounts of quantitative patient information, including genotypic CITATION and mRNA CITATION and protein expression data CITATION with the hope of establishing better clinical classifiers based on aberrant activities of specific, targetable biological pathways.
Using tumor biopsy samples, oncologists are now exploring the incorporation of genomewide expression profiling into therapy CITATION, CITATION.
However, for complex human diseases that span multiple organ systems, metabolomics the analysis of a broad array of metabolite levels from biologic fluid samples such as blood or urine represents a minimally-invasive way to obtain quantitative biologic information from patients to uncover disease pathophysiology and aid diagnostic and prognostic classification CITATION .
Metabolomics data analysis may be facilitated by techniques applied to other high-throughput omic data types.
For microarray data, the integration of network information from protein-protein interaction data or predefined biologic pathways has greatly assisted elucidation of underlying processes and led to the development of increasingly robust and accurate gene-based classifiers for disease CITATION, CITATION.
We hypothesize that the characterization of human disease by metabolomic profiling should similarly benefit from interpreting metabolite changes in the context of known metabolic reactions.
We use data derived from oral glucose tolerance tests in 25 individuals with normal and 25 with impaired glucose tolerance CITATION.
We first sought significant overlaps between observed metabolite changes and preconceived definitions of metabolic pathways.
Next we applied an unbiased pathway analysis by mapping the metabolite changes to a recent reconstruction of the human metabolic network CITATION and use a recently developed variant CITATION of previous approaches CITATION derived for mRNA expression analysis to find active metabolic modules connected subnetworks of highly changed metabolites.
While the biased approach yielded little, the resulting unbiased pathway models highlight the interconnectedness between changed metabolites and propose a role for solute carriers in OGTT metabolite profiles.
Hierarchical clustering and principal component analysis confirmed the importance of specific transporters by demonstrating that metabolites cluster naturally according to activities of the System A and L amino acid and SLC6A12 osmolyte transporters.
Furthermore, they suggest an important role for the SLC25A13 mitochondrial aspartate-glutamate transporter in interindividual metabolite profile variability.
Comparison of NGT and IGT active modules suggest blunted glucose- and/or insulin-stimulated enzyme and transporter activities in the IGT group.
Given that transporters are implicated in multiple human diseases, the interrogation of transporter activities by perturbation-based metabolic profiling may ultimately contribute to improved disease classification and resolution of disease heterogeneity.
The relationship between Apolipoprotein E and the aggregation processes of the amyloid peptide has been shown to be crucial for Alzheimer's disease.
The presence of the ApoE4 isoform is considered to be a contributing risk factor for AD.
However, the detailed molecular properties of ApoE4 interacting with the A peptide are unknown, although various mechanisms have been proposed to explain the physiological and pathological role of this relationship.
Here, computer simulations have been used to investigate the process of A interaction with the N-terminal domain of the human ApoE isoforms.
Molecular docking combined with molecular dynamics simulations have been undertaken to determine the A peptide binding sites and the relative stability of binding to each of the ApoE isoforms.
Our results show that from the several ApoE isoforms investigated, only ApoE4 presents a misfolded intermediate when bound to A. Moreover, the initial -helix used as the A peptide model structure also becomes unstructured due to the interaction with ApoE4.
These structural changes appear to be related to a rearrangement of the salt bridge network in ApoE4, for which we propose a model.
It seems plausible that ApoE4 in its partially unfolded state is incapable of performing the clearance of A, thereby promoting amyloid forming processes.
Hence, the proposed model can be used to identify potential drug binding sites in the ApoE4-A complex, where the interaction between the two molecules can be inhibited.
Alzheimer's disease is one of the most common neurodegenerative diseases at the present time.
The disease is characterized by the formation of neurofibrillary tangles and plaques in the brain, leading to neuronal dysfunction, neuronal loss and finally death.
The main component of the plaques is the amyloid- peptide, a 39 43 amino acids long hydrophobic peptide generated by the cleavage of the amyloid precursor, which accumulates in the form of soluble and non-soluble aggregates.
The connection between Apolipoprotein E and AD is well established CITATION, CITATION.
Structurally, ApoE is a 299 residues protein with an N-terminal domain involved in binding to heparin, low density lipoprotein receptors and LDLR-related proteins CITATION, CITATION.
The C-terminal domain has been related to heparin and lipid binding CITATION, CITATION.
Three main isoforms have been described for human ApoE, i.e. ApoE2, ApoE3 and ApoE4.
The standard variant is ApoE3, while ApoE2 is defective for receptor binding, causing APOE 2/ 2 homozygotic individuals to have a higher predisposition to diseases related to high amounts of cholesterol and triglycerides CITATION, CITATION.
For ApoE4, the receptor binding affinity remains unaffected, but APOE 4/ 4 homozygotic individuals have higher risk for coronary heart disease and a significantly greater risk for developing AD.
CITATION, CITATION Around 80 percent of all AD cases are related to the genetic variance at the ApoE locus CITATION, CITATION .
The only difference between the ApoE isoforms is found in residues 112 and 158, where Cys112/Cys158 corresponds to ApoE2, Cys112/Arg158 to ApoE3, and Arg112/Arg158 to ApoE4.
The presence of cysteines at these positions confers oligomerization properties to ApoE.
Indeed, ApoE2 and ApoE3 are able to form disulfide-linked homo- and hetero-oligomers due to the presence of respectively two and one Cys residue.
ApoE4 lacks the possibility of strong disulfide linking; however, it is unclear whether weaker interactions could promote the oligomerization of ApoE4.
The Cys/Arg substitution in ApoE4 also has molecular impact in terms of intra-protein polar contacts: the orientation of Arg61 is different in ApoE4 compared to ApoE3; the orientation of Arg61 towards the C-terminal domain facilitates a salt bridge between Arg61 and Glu255.
The electrostatic interaction between Arg61 and Glu255 promotes an N- and C-domain interaction that packs the structure tighter, which seems crucial for the interaction of ApoE4 with triglyceride-rich lipoproteins.
The interaction between Arg61 and Glu255 is absent in ApoE3 leading to a more open structure and a preferential binding of phospholipid-rich high-density lipoproteins CITATION, CITATION.
Chemical and thermal denaturation experiments have shown that the most unstable structure belongs to ApoE4, which displays a partially unfolded intermediate containing some structure that may be related to the fact that ApoE4 enhances the deposition of A CITATION, CITATION .
Although different mechanisms have been proposed to explain the physiological and pathological relationship between ApoE and the A peptide, the details of the interaction between ApoE and A at a molecular level are unknown.
Such detailed knowledge is however important for the understanding of the pathological mechanisms of AD, and may also help to identify potential therapeutic target sites where the interaction between ApoE4 and A can be blocked.
In the present study we are using molecular docking simulations based on global minimum energy to investigate the interaction process of A with the N-terminal domain of the different ApoE isoforms in order to determine potential A peptide binding sites in ApoE.
In the next step, molecular dynamics calculations are undertaken to explore the conformational dynamics of ApoE under A interaction and evaluate the stability of each of the ApoE-A complexes.
From the analysis and the statistics of the electrostatic interactions of the three ApoE isoforms, we present a model explaining the role of the A -ApoE interaction and its relevance for AD.
Genes with common functions often exhibit correlated expression levels, which can be used to identify sets of interacting genes from microarray data.
Microarrays typically measure expression across genomic space, creating a massive matrix of co-expression that must be mined to extract only the most relevant gene interactions.
We describe a graph theoretical approach to extracting co-expressed sets of genes, based on the computation of cliques.
Unlike the results of traditional clustering algorithms, cliques are not disjoint and allow genes to be assigned to multiple sets of interacting partners, consistent with biological reality.
A graph is created by thresholding the correlation matrix to include only the correlations most likely to signify functional relationships.
Cliques computed from the graph correspond to sets of genes for which significant edges are present between all members of the set, representing potential members of common or interacting pathways.
Clique membership can be used to infer function about poorly annotated genes, based on the known functions of better-annotated genes with which they share clique membership.
We illustrate our method by applying it to microarray data collected from the spleens of mice exposed to low-dose ionizing radiation.
Differential analysis is used to identify sets of genes whose interactions are impacted by radiation exposure.
The correlation graph is also queried independently of clique to extract edges that are impacted by radiation.
We present several examples of multiple gene interactions that are altered by radiation exposure and thus represent potential molecular pathways that mediate the radiation response.
Guilt-by-association, the assumption that genes with similar expression patterns participate in common cellular functions, drives a growing body of effort to extract cellular pathways from microarray data CITATION CITATION.
The general tenet is that genes encoding proteins participating in a common pathway will display correlated expression levels when analyzed at sufficient scale, and that the identities and known functions of these genes can be used to highlight existing and assimilate new functional pathways.
A number of recent studies validate the concept of guilt-by-association, demonstrating that genes co-expressed across multiple conditions are more likely to represent common functions than would be expected by chance alone CITATION, CITATION.
To date the computational methods to extract such patterns lag far behind the general agreement about their utility.
The majority of methods to extract pathways of co-regulation from microarray data begin with a measure of similarity e.g., Euclidean distance, Pearson's correlation coefficient that describes the degree to which expression levels between pairs of genes are correlated across multiple conditions CITATION.
The matrix of correlations across the microarray, typically representing the pairwise similarity of the expression patterns of thousands of genes, is the starting point from which to organize genes into clusters.
Clustering includes a wide variety of algorithms for organizing multivariate data into groups with approximately similar expression patterns, and a wealth of clustering approaches has been proposed CITATION.
However, there are several important limitations to the vast majority of clustering algorithms that are in contrast to the reality of biology.
The first is that they are disjoint, requiring that a gene be assigned to only one cluster.
While this simplifies the amount of data to be evaluated, it places an artificial limitation on the biology under study in that many genes play important roles in multiple but distinct pathways.
The other main problem is that most measures of similarity used in clustering algorithms do not permit the recognition of negative correlations, which are also common and equally meaningful.
As an alternative to assigning genes to clusters, the correlation matrix can be thresholded to create a graph comprised only of edges whose weights exceed a predefined value.
Allocco and colleagues originally described such graphs as relevance networks CITATION.
In a relevance network, both positive and negative correlations exceeding a specified threshold are retained and displayed graphically, allowing visual recognition of highly connected subsets of genes.
Recent studies have mined relevance networks to extract co-expressed genes in cancer cells CITATION, CITATION and myopathic muscle biopsies CITATION.
While those efforts provided gene subsets of biological relevance to the respective conditions, they were limited to pairwise relationships that could be extracted manually from the graphs.
Relevance networks contain many dense sub-graphs of tightly interconnected gene sets that intuitively represent the greatest potential for identifying members of common pathways.
Without a systematic means to extract the aggregate relationships between multiple genes, however, many of the most interesting relationships remain embedded within the web of correlations.
We have developed a computational approach that exploits graph theoretical algorithms to identify comprehensively the tightly connected subsets of genes present in relevance networks.
In the most extreme case, in which a sub-graph contains all possible edges between vertices in the sub-graph, this structure is called a clique.
In terms of gene expression, clique represents the most trusted potential for identifying a set of interacting genes.
Solving clique, however, is a nondeterministic polynomial-complete problem, and a classic graph-theoretic problem in its own right CITATION.
We have previously developed novel graph algorithms that employ vertex cover and allow clique to be solved in polynomial time CITATION CITATION.
Recently we applied these algorithms to identify cliques of co-expressed genes as part of an effort to annotate quantitative trait loci associated with neural function CITATION.
Here we extend these algorithms to identify differential gene relationships, i.e., gene-gene interactions that are induced or repressed by a specific treatment.
We illustrate our approach using a set of microarray data that was generated from spleen of mice exposed in vivo to low-dose ionizing radiation.
Radiation is a well known agent of DNA damage at relatively high but sub-lethal doses CITATION.
The response to lower doses, however, such as those received from medical imaging, radiotherapy and occupational exposures, is poorly defined and largely dependent upon genetic background CITATION.
The data used herein were derived from a study that explored the role of genetic susceptibility in the response to IR.
Six strains of inbred laboratory mice were exposed to 10 cGy X-rays in vivo, after which gene expression changes in spleen were profiled using microarrays.
We describe our graph theoretical-based toolchain for identifying overlapping subsets of genes with tightly correlated expression levels and demonstrate the biological insight that this method provides.
When introduced into a novel environment, mammals establish in it a preferred place marked by the highest number of visits and highest cumulative time spent in it.
Examination of exploratory behavior in reference to this home base highlights important features of its organization.
It might therefore be fruitful to search for other types of marked places in mouse exploratory behavior and examine their influence on overall behavior.
Examination of path curvatures of mice exploring a large empty arena revealed the presence of circumscribed locales marked by the performance of tortuous paths full of twists and turns.
We term these places knots, and the behavior performed in them knot-scribbling.
There is typically no more than one knot per session; it has distinct boundaries and it is maintained both within and across sessions.
Knots are mostly situated in the place of introduction into the arena, here away from walls.
Knots are not characterized by the features of a home base, except for a high speed during inbound and a low speed during outbound paths.
The establishment of knots is enhanced by injecting the mouse with saline and placing it in an exposed portion of the arena, suggesting that stress and the arousal associated with it consolidate a long-term contingency between a particular locale and knot-scribbling.
In an environment devoid of proximal cues mice mark a locale associated with arousal by twisting and turning in it.
This creates a self-generated, often centrally located landmark.
The tortuosity of the path traced during the behavior implies almost concurrent multiple views of the environment.
Knot-scribbling could therefore function as a way to obtain an overview of the entire environment, allowing re-calibration of the mouse's locale map and compass directions.
The rich vestibular input generated by scribbling could improve the interpretation of the visual scene.
When introduced into a novel environment rats establish in it a preferred place characterized by the highest frequency of visits, by the highest cumulative dwell time, by an upper bound on the number of stops per roundtrip performed from it, by low outbound trajectory speed and high inbound trajectory speed, with the speed relationship reversed in later stages of the session CITATION CITATION.
The existence of highly organized behavior across the whole arena in reference to this so-called home base CITATION illustrates the influence a preferred place might have on the overall organization of exploratory behavior and prompts the search for other types of preferred places that may have a similar influence on the organization of exploratory behavior.
In contrast to the situation with rats, there is an ambiguity regarding the establishment of home bases in mice: while some studies mention home bases, their establishment is mostly of a low occurrence in the forced exploration setup.
Even though mice readily establish home bases near physical objects CITATION CITATION and near nesting material CITATION, they are reported to fail to establish distinct home bases during forced exploration of a relatively featureless environment CITATION CITATION .
When mice used as a control group in another study were injected with saline and placed in the exposed portion of a large open field arena, they established in it preferred places, typically not more than one per session, which they visited sporadically, tracing in them a tortuous path full of twists, turns, and bends that looked like a knot.
We termed these places knots, and the behavior performed in them - knot-scribbling.
This knot phenomenon, which appeared in its full blown form in the saline-injected mice, was subsequently uncovered, albeit in a less striking form, also in intact mice.
In the present study we describe the full-blown knot phenomenon in saline-injected mice and only then verify the existence of knots in intact mice.
To uncover knots we utilize a basic feature of the locomotor path its curvature.
Depending on the size of the window that is used for measuring path curvature this measure discloses various features of path texture, from overall large-scale curvature to fine-grained tortuosity CITATION.
In the present study we use a window size that makes the fine-grained tortuosity of the path as conspicuous as possible.
Curvature has been measured previously only as a cumulative measure of paths without coupling a particular degree of curvature to particular topographical locations CITATION CITATION.
To uncover knots we first calculate fine-grained path curvature for each data point on the path traced by the mouse.
Then we partition the arena into small 5 5cm unit areas and summate, for each unit area, the curvature of the path included within that unit area.
The magnitude of path curvature per unit area is then color coded in a visualization of the paths, thus highlighting the unit areas that are marked by the highest curvature.
Having uncovered a high curvature locale we then design and use algorithms that define its boundaries and quantify some of its features.
Finally, we ask whether the knots we discover are also endowed with the classical home base features, and consider their function.
Acute effects of sex steroid hormones likely contribute to the observation that post-pubescent males have shorter QT intervals than females.
However, the specific role for hormones in modulating cardiac electrophysiological parameters and arrhythmia vulnerability is unclear.
Here we use a computational modeling approach to incorporate experimentally measured effects of physiological concentrations of testosterone, estrogen and progesterone on cardiac ion channel targets.
We then study the hormone effects on ventricular cell and tissue dynamics comprised of Faber-Rudy computational models.
The female model predicts changes in action potential duration at different stages of the menstrual cycle that are consistent with clinically observed QT interval fluctuations.
The male model predicts shortening of APD and QT interval at physiological testosterone concentrations.
The model suggests increased susceptibility to drug-induced arrhythmia when estradiol levels are high, while testosterone and progesterone are apparently protective.
Simulations predict the effects of sex steroid hormones on clinically observed QT intervals and reveal mechanisms of estrogen-mediated susceptibility to prolongation of QT interval.
The simulations also indicate that acute effects of estrogen are not alone sufficient to cause arrhythmia triggers and explain the increased risk of females to Torsades de Pointes.
Our results suggest that acute effects of sex steroid hormones on cardiac ion channels are sufficient to account for some aspects of gender specific susceptibility to long-QT linked arrhythmias.
In the past decade, studies have suggested that female gender is an independent risk factor for long-QT dependent cardiac arrhythmias CITATION CITATION.
Since the differences in QT intervals in males and females appear from the time of puberty CITATION, CITATION, sex steroid hormone effects on cardiac repolarization have been implicated.
Clinical studies have found no difference in QT interval in male and female children, but shorter QT intervals in men versus women under age 50 CITATION.
The international Long QT syndrome registry 1998 reported that females had higher risk of a first cardiac event between 15 and 40 years CITATION.
Moreover, clinical findings observed that more than 68 percent of drug-induced torsade de pointes occur in women CITATION CITATION .
It is known that one way that sex steroid hormones cause functional physiological changes is via transcriptional regulation.
Sex hormones may bind to sex hormone receptors and then translocate into the nucleus.
In the nucleus, a ligand-bound sex hormone receptor acts a transcription factor by binding to the promoter region of genes containing a hormone responsive element, leading to regulation of gene expression.
For example, in the heart, lipocalin-type prostaglandli D synthase has been found to be transcriptionally upregulated by estradiol and estrogen receptor CITATION.
This genomic action requires several hours before the effects can be observed.
In addition to the genomic pathway, sex steroid hormones may induce a rapid activation of mitogen-activated protein kinase leading to transcription factor activation CITATION, CITATION as well as activation of membrane bound endothelial nitric oxide synthase CITATION, CITATION .
Interestingly, recent studies have demonstrated that sex steroid hormones may also act acutely and rapidly modulate cardiac ion channel activity directly via a PI3K/Akt/eNOS pathway CITATION CITATION.
Testosterone induced phosphorylation of the Ser/Thr kinase Akt and eNOS leads to NO synthase 3 activation and production of nitric oxide CITATION.
NO leads to s-nitrosylation of cysteine residues on the channel underlying the slow delayed rectifier K current CITATION.
L-type Ca 2 current is conversely suppressed by NO via a cGMP dependent pathway.
Regulation of I Ks and I Ca,L by testosterone is dose-dependent CITATION and leads to shortening of action potential duration CITATION and QT intervals CITATION CITATION.
In adult men, the serum testosterone level is reported to be 10 to 35 nM CITATION, however circulating levels of testosterone begin to decline in men as young as 40 CITATION.
QT intervals are shorter in adult men than in adult women until around the age of 50 CITATION, suggesting a likely role for circulating testosterone.
In females, progesterone fluctuates through the menstrual cycle.
The reported serum progesterone level is 2.5 nM in the follicular phase and 40.6 nM in the luteal phase CITATION.
It was recently shown by Nakamura et al. that progesterone increases I Ks current through the NO production pathways and prevents cAMP-enhancement of I Ca,L CITATION .
The apparent result of acute effects of progesterone and testosterone is to shorten ventricular repolarization and diminish incidence of arrhythmias CITATION, CITATION, CITATION, CITATION.
Recently, experiments have suggested protective effects of testosterone against arrhythmia.
In vivo experiments show that orchiectomized male rabbits treated with dihydrotestosterone had shorter QT interval and APD 90 compared to non-DHT treated rabbits CITATION, CITATION.
Also, experiments in testosterone treated female animals have shown that DHT reduces drug-induced arrhythmia by dofetilide CITATION .
The acute effects of estradiol result in suppression of human ether-a-go-go-related gene underlying the rapid delayed rectifier current by directly binding to the channel, altering channel kinetics and reducing current CITATION.
Kurokawa and co-workers showed that 17 -estradiol increases the channel rate of closure and lessens repolarizing current.
They also showed that in the presence of E2, hERG is more sensitive to block by drugs.
The group proposed that aromatic centroid of E2 may be responsible for increasing the sensitivity of hERG block by E4031 via interaction with the aromatic side chain of Phe 656 and aromatic rings of the hERG blocker.
Because the concentration of E2 is not constant through the menstrual cycle, but rather fluctuates from the peak follicular phase serum E2 level of 1 nM to 0.7 nM in the luteal phase, and E2 has dramatic effects on sensitivity to hERG block within this range, it stands to reason that susceptibility to drug-induced arrhythmia by hERG block may vary through the menstrual cycle.
Although studies have shown that female hormones estradiol and progesterone have opposite effects on cardiac repolarization: E2 prolongs QT intervals, and progesterone reduces QT interval CITATION, CITATION, CITATION, the question of whether normal hormonal fluctuations are sufficient to account for variability in QT during the menstrual cycle in not known.
Neither are the effects of physiological concentrations of hormones on arrhythmia susceptibility well understood.
Some studies do report that dynamic fluctuations in QT intervals during the menstrual cycle are related to changes in susceptibility to TdP risk CITATION, CITATION.
Other studies in postmenopausal women also suggest the importance of female hormones as estrogen hormone replacement therapy prolongs QT intervals and increases arrhythmia risk CITATION, CITATION, CITATION.
Other data have not found marked fluctuation in QT interval during specific phases of the menstrual cycle CITATION, CITATION, CITATION.
Burke et al., found that the corrected QT interval does not significantly change through menstrual cycle in pre-menopausal women; however, QT c is reduced in the luteal phase after autonomic blockade CITATION.
A study of drug-induced QT prolongation during the menstrual cycle observed that QT c did not vary during the menstrual cycle, but QT c shortening was more pronounced in the luteal phase with ibutilide application CITATION.
Nonetheless, both the clinical and experimental data suggest that women have both longer QT intervals than men and are more likely to develop long-QT dependent arrhythmias and TdP arrhythmias CITATION, CITATION.
Women are especially susceptible to increased arrhythmia risk in response to QT-prolongation drugs CITATION, CITATION, CITATION, CITATION .
It is a major challenge to specifically determine the relationship between sex steroid hormones and arrhythmia susceptibility in males and females since the cardiac system is extraordinarily complex.
In order to attribute risk to a particular parameter, in this case physiologically relevant concentrations of sex steroid hormones, the specific effect must be studied in isolation without other perturbations to the system.
This is the strength of the computational approach that we employ.
In the present study, we focus on acute effects of sex steroid hormones on cardiac ion channel targets.
We use guinea pig models that incorporate the effects of hormones measured experimentally from guinea pig, and then can test these changes specifically within the complex cellular and tissue milieu.
Importantly, we use the model to make predictions about the effects of physiological concentrations of sex steroid hormones on gender specific cardiac physiology parameters and arrhythmia susceptibility.
Some recent experimental studies investigating functional effects of sex hormones on cardiac function have utilized hormone concentrations in the micromolar range that is orders of magnitude higher than the nanomolar physiological circulating concentration of E2 CITATION.
This is a critical consideration because micromolar concentrations of E2 are apparently cardioprotective via effects on L-type Ca 2 current.
Although high hormone concentrations may be relevant during phases such as pregnancy, a recent study showed that E2 at 1 nM did not have significant effects on I Ks or I Ca,L CITATION.
Our model simulations reproduce observed fluctuations of QT through the menstrual cycle in females in both cell and tissue-level.
Simulations also predict that effects of testosterone and progesterone on ion channels hasten repolarization and protect from drug-induced arrhythmias.
It is widely believed that the modular organization of cellular function is reflected in a modular structure of molecular networks.
A common view is that a module in a network is a cohesively linked group of nodes, densely connected internally and sparsely interacting with the rest of the network.
Many algorithms try to identify functional modules in protein-interaction networks by searching for such cohesive groups of proteins.
Here, we present an alternative approach independent of any prior definition of what actually constitutes a module.
In a self-consistent manner, proteins are grouped into functional roles if they interact in similar ways with other proteins according to their functional roles.
Such grouping may well result in cohesive modules again, but only if the network structure actually supports this.
We applied our method to the PIN from the Human Protein Reference Database and found that a representation of the network in terms of cohesive modules, at least on a global scale, does not optimally represent the network's structure because it focuses on finding independent groups of proteins.
In contrast, a decomposition into functional roles is able to depict the structure much better as it also takes into account the interdependencies between roles and even allows groupings based on the absence of interactions between proteins in the same functional role.
This, for example, is the case for transmembrane proteins, which could never be recognized as a cohesive group of nodes in a PIN.
When mapping experimental methods onto the groups, we identified profound differences in the coverage suggesting that our method is able to capture experimental bias in the data, too.
For example yeast-two-hybrid data were highly overrepresented in one particular group.
Thus, there is more structure in protein-interaction networks than cohesive modules alone and we believe this finding can significantly improve automated function prediction algorithms.
Biological function is believed to be organized in a modular and hierarchical fashion CITATION.
Genes make proteins, proteins form cells, cells form organs, organs form organisms, organisms form populations and populations form ecosystems.
While the higher levels of this hierarchy are well understood, and the genetic code has been deciphered, the unraveling of the inner workings of the proteome poses one of the greatest challenges in the post-genomic era CITATION.
The development of high-throughput experimental techniques for the delineation of protein-protein interactions as well as modern data warehousing technologies to make data available and searchable are key steps towards understanding the architecture and eventually function of the cellular network.
These data now allow for searching for functional modules within these networks by computational approaches and for putatively assigning protein function.
A recent review by Sharan et al. CITATION surveys the current methods of network based prediction methods for protein function.
Proteins must interact to function.
Hence, we can expect protein function to be encoded in a protein interaction network.
The basic underlying assumption of all methods of automated functional annotation is that pairwise interaction is a strong indication for common function.
Sharan et al. differentiate two basic approaches of network based function prediction: direct methods, which can be seen as local methods applying a guilt-by-association principle CITATION to immediate or second neighbors in the network, and module assisted methods which first cluster the network into modules according to some definition and then annotate proteins inside a module based on known annotations of other proteins in the module.
So instead of guilt-by-association, one could speak of kin-liability.
The latter approach to function prediction necessitates a concept of what is to be considered a module in a network.
Most researchers consider cohesive sets of proteins which are highly connected internally, but only sparsely with the rest of the network CITATION CITATION.
Such methods have yielded considerable success at the level of very small scale modules and in particular protein complexes.
Is the concept of a module as a group of cohesively interacting proteins also useful on larger scales?
Some researchers have argued that modularity in this sense is a universal principle such that small cohesive modules combine to form larger cohesive entities in a nested hierarchy CITATION, CITATION.
But is this view really adequate to describe the architecture of protein interactions?
Recently, Wang and Zhang CITATION questioned whether cohesive clusters in protein interaction networks carry biological information at all and suggested a simple network growth model based on gene duplication which would produce the observed structural cohesiveness as an evolutionary byproduct without biological significance.
We will not go as far as questioning the content of biological information in the network structure but rather argue against the model of a cohesively linked group of nodes in a network as an adequate proxy for a functional module on all scales of the network.
Consider, as first example, protein complexes.
Indeed, they consist of proteins working together and experimentally isolated together.
Only the large scale analysis of protein complexes CITATION, CITATION revealed that they are more dynamic than previously assumed.
Many proteins can not only be found in a single, but in a multitude of complexes.
The information about proteins connecting complexes will be lost when searching only for cohesively interacting groups of proteins.
As a second example, consider transmembrane proteins, like receptors in signal transduction cascades.
They tend to interact with many different cytoplasmic proteins as well as with their extra-cellular ligands.
Still, only rarely do different transmembrane receptors interact with each other.
Thus, the functional class of transmembrane receptors will not be identified when looking for cohesive modules.
Here, we ask whether such features, which are not discovered by algorithms searching for cohesive modules, are also present in the overall structure of the cellular network.
If this is the case, methods searching only for cohesive modules would not be able to identify them.
We group proteins self-consistently into functional roles if they interact in similar ways with other proteins according to their functional roles.
Such a role may well be a cohesive module, meaning that proteins in this class predominantly interact with other proteins of this class, but it does not have to.
In other words, we do not impose a structure of cohesive modules on the network in our analysis but rather find the structural representation that is best supported by the data.
Using the abstraction of a functional role, we generate an image graph of the original network which depicts only the predominant interactions among classes of proteins, thus allowing a bird's-eye view of the network.
In the case of a protein interaction network studied here, we found sound evidence that cohesive modules on a global scale do not adequately represent the network's global structure.
We found cohesive groups of proteins acting as intermediates and specifically connecting other groups of proteins.
Furthermore, we even identified groups of proteins which are only sparsely connected within themselves, but with similar patterns of interaction to other proteins.
Thus, approaches searching only for cohesive modules which are sparsely connected to the rest of the network might not be sufficient to represent all characteristics of cellular networks.
Our findings suggest that hierarchical modularity as nested, cohesively interacting groups of proteins has to be reconsidered as a universal organizing principle.
In which cases does a clustering of a network into cohesive modules not reflect its original architecture?
Consider the toy network in figure 1 A. There are four known types of proteins in this network.
Type FORMULA may represents some biological process involving five proteins connected to four proteins of type FORMULA.
These are linked to another biological process FORMULA which involves five further proteins which finally are linked to four proteins of type FORMULA.
Not all nodes of the same type necessarily share the same set of neighbors.
Some nodes of the same type do not have any neighbors in common with nodes of their type or have more neighbors in common with nodes of a different type.
This shows that in this hypothetical example, direct methods of functional annotations may be limited in their accuracy.
Clustering the network into cohesive modules cannot capture the full structure of the network.
The nodes of type B will never be recognized as a proper cluster, because they are not connected internally at all.
The structure of the example network can, however, be perfectly captured by a simple image graph with 4 nodes.
The nodes in an image graph correspond to the types of nodes in the network.
Nodes of type FORMULA are connected to other nodes of type FORMULA and to nodes of type FORMULA.
Nodes of type FORMULA have connections to nodes of types FORMULA and FORMULA and so forth.
The concept of defining types of nodes by their relation to other types of nodes is known as regular equivalence in the social sciences CITATION, CITATION.
Structure recognition in networks can then be seen as finding the best fitting image graph for a network.
In this context, clustering into functional modules means representing the network by an image graph consisting of isolated, self-linking nodes.
Once an assignment of nodes into classes is obtained, the rows and columns of the incidence matrix can be reordered such that rows and columns corresponding to nodes in the same class are adjacent.
The ordering of rows and columns representing nodes in the same class is random.
This leads to a characteristic structure with dense blocks in the adjacency matrix corresponding to the links in the image graph and sparse or zero blocks corresponding to the links absent in the image graph.
Structure recognition in networks is therefore also called block modeling and together with the concepts of structural and regular equivalence has a long history in the social sciences CITATION, CITATION.
In our further discussion, we will denote image graphs that consist only of isolated, self-linked nodes as in figure 1 B, diagonal image graphs due to the block structure along the diagonal in the adjacency matrix that they induce.
Accordingly, we will call all other image graphs non-diagonal image graphs .
The evolutionary dynamics of HIV during the chronic phase of infection is driven by the host immune response and by selective pressures exerted through drug treatment.
To understand and model the evolution of HIV quantitatively, the parameters governing genetic diversification and the strength of selection need to be known.
While mutation rates can be measured in single replication cycles, the relevant effective recombination rate depends on the probability of coinfection of a cell with more than one virus and can only be inferred from population data.
However, most population genetic estimators for recombination rates assume absence of selection and are hence of limited applicability to HIV, since positive and purifying selection are important in HIV evolution.
Yet, little is known about the distribution of selection differentials between individual viruses and the impact of single polymorphisms on viral fitness.
Here, we estimate the rate of recombination and the distribution of selection coefficients from time series sequence data tracking the evolution of HIV within single patients.
By examining temporal changes in the genetic composition of the population, we estimate the effective recombination to be 1.4 0.6 10 5 recombinations per site and generation.
Furthermore, we provide evidence that the selection coefficients of at least 15 percent of the observed non-synonymous polymorphisms exceed 0.8 percent per generation.
These results provide a basis for a more detailed understanding of the evolution of HIV.
A particularly interesting case is evolution in response to drug treatment, where recombination can facilitate the rapid acquisition of multiple resistance mutations.
With the methods developed here, more precise and more detailed studies will be possible as soon as data with higher time resolution and greater sample sizes are available.
The human immunodeficiency virus ranks among the most rapidly evolving entities known CITATION, enabling the virus to continually escape the immune system.
After infection with HIV, patients typically enter an asymptomatic period lasting several years during which the virus is present at low to medium levels, typically at a viral load of FORMULA to FORMULA copies per ml plasma.
Nevertheless, the number of virions produced and removed is estimated to be around FORMULA per day with a generation time slightly less than two days CITATION.
Due to this rapid turnover and the high mutation rate of FORMULA per site and generation, the sequence diversity of HIV within a single patient can rise to FORMULA percent within a few years and the divergence from the founder strain increases by FORMULA percent per year CITATION, although this rate is not constant CITATION.
The genotypic diversity is subject to positive selection for novel variants that are not recognized by the host immune system or that reduce the sensitivity to anti-retroviral drugs CITATION CITATION, as well as to purifying selection by functional constraints CITATION.
In addition to high substitution rates and strong selection, genomes of different HIV particles within the same host frequently exchange genetic information.
This form of viral recombination works as follows: Whenever a cell is coinfected by two or more viruses, the daughter virions can contain two RNA strands from different viruses CITATION, CITATION.
In the next round of infection, recombinant genomes are generated by template switching of the reverse transcriptase while producing cDNA.
It has been shown that recombination in HIV contributes significantly to the genetic diversity within a patient CITATION CITATION.
In cases of super-infection with several HIV-1 subtypes, recombination can give rise to novel forms that become part of the global epidemic CITATION .
The observation of recombinant viruses after a change in anti-retroviral drug therapy CITATION suggests that recombination might play an important role in the evolution of drug resistance, as predicted by theoretical models CITATION.
The amount by which recombination speeds up the evolution of drug resistance depends on the parameters governing the population dynamics CITATION, many of which are not known to sufficient accuracy.
In vitro estimates of the recombination rate have shown that the reverse transcriptase switches templates about FORMULA times while transcribing the entire genome, resulting in a recombination rate of FORMULA per site and generation CITATION, CITATION.
However, the bare template switching rate is only of secondary importance, since recombination can generate diversity only if the virion contains two RNA strands that originate from different viruses, which requires coinfection of host cells CITATION.
The effective in vivo recombination rate is therefore a compound quantity, to which the template switching rate and the probability of coinfection of a single host cell contribute.
This effective recombination rate has been estimated with coalescent based methods developed in population genetics CITATION, CITATION.
These methods use a single sample of sequences obtained from the diverse population and estimate the recombination rate from topological incongruencies in the phylogenetic tree of the sequence sample.
Together with an estimate of the mutation rate, this allows to estimate the recombination rate in real time units.
Shriner et al. CITATION report an estimate of FORMULA per site and generation, implying almost ubiquitous coinfection of host cells.
Here, we present a different method to estimate recombination rates from longitudinal sequence data, which has been obtained from 11 patients at approximately 6 month intervals CITATION, CITATION.
By comparing sequence samples from successive time points, we can estimate recombination rates from the distance and time dependence of the probability of cross-over between pairs of polymorphic sites.
We find that the effective rate of recombination is FORMULA per site and generation.
Furthermore, we estimate the strength of selection on nonsynonymous polymorphisms by measuring the rate at which allele frequencies change.
We find that a fraction of about 15 percent of the observed nonsynonymous polymorphisms are selected stronger than FORMULA percent per generation.
Biological function of proteins is frequently associated with the formation of complexes with small-molecule ligands.
Experimental structure determination of such complexes at atomic resolution, however, can be time-consuming and costly.
Computational methods for structure prediction of protein/ligand complexes, particularly docking, are as yet restricted by their limited consideration of receptor flexibility, rendering them not applicable for predicting protein/ligand complexes if large conformational changes of the receptor upon ligand binding are involved.
Accurate receptor models in the ligand-bound state, however, are a prerequisite for successful structure-based drug design.
Hence, if only an unbound structure is available distinct from the ligand-bound conformation, structure-based drug design is severely limited.
We present a method to predict the structure of protein/ligand complexes based solely on the apo structure, the ligand and the radius of gyration of the holo structure.
The method is applied to ten cases in which proteins undergo structural rearrangements of up to 7.1 backbone RMSD upon ligand binding.
In all cases, receptor models within 1.6 backbone RMSD to the target were predicted and close-to-native ligand binding poses were obtained for 8 of 10 cases in the top-ranked complex models.
A protocol is presented that is expected to enable structure modeling of protein/ligand complexes and structure-based drug design for cases where crystal structures of ligand-bound conformations are not available.
Interactions between proteins and small molecules are involved in many biochemical phenomena.
Insight into these processes relies on detailed knowledge about the structure of protein/ligand complexes, e.g. how enzymes stabilize substrates and cofactors in close proximity.
Moreover, almost all drugs are small-molecule ligands that interact with enzymes, receptors or channels.
Accordingly, ligand-bound receptor complex structures are a critical prerequisite for understanding biological function and for structure based drug design.
However, structure determination of protein/ligand-complexes can be difficult, time-consuming and expensive.
Crystal structures of protein/ligand complexes are usually obtained either by co-crystallization or soaking and it is a common problem that even when conditions for crystallizing the apo-protein are well established these might not be transferable to the protein/ligand complex CITATION CITATION.
Particularly, conformational transitions of the receptor associated with ligand binding pose a severe challenge to the structure elucidation of holo complexes CITATION CITATION .
When structures of ligand-bound protein conformations are not available, structure-based drug design becomes highly challenging.
Several studies showed that virtual screening to an apo-structure usually results in a poor enrichment factor compared to the holo-structure even when the structural difference between both is comparably small CITATION CITATION.
Therefore, the development of docking programs aims at allowing a certain degree of receptor flexibility either by using an ensemble of structures instead of a single receptor conformation CITATION CITATION or by explicitely modeling flexibility such as sidechain variations, predefined flexibility of certain parts of the structure and also small variations of the backbone.
Incorporating receptor flexibility in molecular docking is a substantial progress and has been shown to enhance both enrichment factors and the ability to predict correct binding poses, particularly in cases when docking a compound to a receptor structure that has been crystallized with a different ligand which is usually the case when searching for novel drugs.
However, the degree of flexibility thus far is limited to either sidechain motions or small variations of the backbone and thus, the availability of a holo-structure or an apo-structure that is highly similar to the holo conformation is currently a prerequisite for a successful docking, severely limiting structure-based drug design.
Particularly, receptors that undergo a substantial conformational transition upon ligand binding are currently precluded from structure based drug design.
Although protein-ligand crystals suitable for diffraction might not be accessible, several experimental techniques exist to detect conformational changes.
In many cases where proteins undergo domain reorientations upon ligand binding they adopt a different shape in the ligand bound state, corresponding to a change in the radius of gyration that can be studied either by NMR, where a more compact shape causes a descrease in the rotational correlation time CITATION CITATION or by small-angle scattering of x-rays or neutrons CITATION CITATION.
These shape descriptions provide invaluable information for modeling of structures CITATION CITATION and macromolecular assemblies CITATION, CITATION as well as insight into protein dynamics.
Here, we present a method to predict the structure of protein/ligand complexes for proteins that undergo a large conformational change upon ligand binding.
The protocol solely requires the apo-structure, a known ligand and experimental data on the shape of the holo-structure.
Here, we apply the radius of gyration as shape information, a quantity that can frequently be readily assessed more easily than an x-ray structure.
We developed a simulation protocol that combines biased conformational sampling, docking and molecular dynamics simulations and applied it to ten ligand-binding proteins.
We chose cases where both, the unbound conformation and the bound conformation are known from x-ray crystallography in order to be able to a-posteriori validate the predicted receptor conformations and docking poses.
The conformational changes involved range from 2.1 to 7.1 backbone RMSD and the binding site geometries differ substantially between the apo and the holo conformations.
In nine of the ten cases we predict holo receptor conformations close to the native ligand-bound conformation and in eight cases we predict ligand binding poses close to the native state, rendering our method suitable for blind predictions of protein/ligand complexes involving large conformational transitions.
The proteome of the radiation- and desiccation-resistant bacterium D. radiodurans features a group of proteins that contain significant intrinsically disordered regions that are not present in non-extremophile homologues.
Interestingly, this group includes a number of housekeeping and repair proteins such as DNA polymerase III, nudix hydrolase and rotamase.
Here, we focus on a member of the nudix hydrolase family from D. radiodurans possessing low-complexity N- and C-terminal tails, which exhibit sequence signatures of intrinsic disorder and have unknown function.
The enzyme catalyzes the hydrolysis of oxidatively damaged and mutagenic nucleotides, and it is thought to play an important role in D. radiodurans during the recovery phase after exposure to ionizing radiation or desiccation.
We use molecular dynamics simulations to study the dynamics of the protein, and study its hydration free energy using the GB/SA formalism.
We show that the presence of disordered tails significantly decreases the hydration free energy of the whole protein.
We hypothesize that the tails increase the chances of the protein to be located in the remaining water patches in the desiccated cell, where it is protected from the desiccation effects and can function normally.
We extrapolate this to other intrinsically disordered regions in proteins, and propose a novel function for them: intrinsically disordered regions increase the surface-properties of the folded domains they are attached to, making them on the whole more hydrophilic and potentially influencing, in this way, their localization and cellular activity.
The dominant paradigm for describing the functioning of proteins is that of well-defined, structured molecular machines undergoing concerted, conformational changes while carrying out their function CITATION CITATION.
However, over the past few years it has become clear that the reality is much more complex, and that there are proteins that simply do not have a defined tertiary structure, and yet still carry out multitudes of different important functions.
These intrinsically disordered proteins and protein segments, also called natively unfolded or natively disordered, are by definition difficult to study using classical methods of structural biology, but they have in recent years received significant attention, largely due to two facts CITATION CITATION.
First, it has become clear that these proteins are extremely abundant.
Using mostly bioinformatics approaches, it has been shown that in eukaryotes about 30 percent of all proteins are largely intrinsically disordered and that about 50 percent have long ID stretches CITATION.
Among signaling proteins, in particular, about 70 percent have long disordered segments CITATION.
These numbers are large: even if some of the predicted disordered regions actually prove to be structured, it is likely that IDPs in eukaryotes by far exceed, for example, the entire population of membrane proteins.
The second motivating factor is the fact that IDPs are involved in a host of extremely important cellular functions such as molecular recognition, assembly, protein modification and entropic chain functions CITATION CITATION, CITATION, CITATION, CITATION.
For example, in complex cellular activities such as cell signaling or regulation, it is often required that actions of many key molecular players be tightly controlled and coordinated through interaction and recognition based on unique identifying features CITATION, CITATION, which, in the case of signaling proteins, are often located in ID regions.
When recognition of multiple binding partners and high-specificity/low-affinity binding is required, the choice on the molecular level often involves IDPs or ID regions CITATION, CITATION, CITATION.
Finally, large numbers of IDPs are known to be involved in human diseases such as cancer, neurodegenerative diseases, diabetes, cardiovascular diseases and amyloidoses CITATION .
Thus far, some of the biggest advances in the study of IDPs have been accomplished through bioinformatics and data mining approaches CITATION, CITATION, CITATION CITATION.
It has been shown that amino-acid sequences of IDPs tend to exhibit high hydrophilicity, low sequence entropy, and lack of the so-called order-promoting amino acids and bulky hydrophobic residues.
Based on such features, several bioinformatics tools have been developed that can reliably predict the level of intrinsic disorder in a given sequence CITATION, CITATION.
Moreover, using mostly NMR, small-angle X-ray scattering, and different spectroscopic methods CITATION, CITATION, CITATION, basic structural features of IDPs have been elucidated, including their molecular size, level of structural heterogeneity, role of transient structure in coupled binding-and-folding events, aggregation tendencies and presence of persistent structure CITATION, CITATION, CITATION.
In particular, a combination of computer simulation and experimentally-determined restraints has provided some of the first ensemble-level pictures of IDPs CITATION, CITATION, CITATION.
However, compared with our knowledge of the structure and mechanism of ordered proteins, our understanding of IDPs is still extremely rudimentary and incomplete, primarily because of their structural and dynamic complexity.
In the present study, we use molecular dynamics simulations and free energy calculations to focus on the role of IDPs in an extremophile bacterium: Deinococcus radiodurans.
D.
radiodurans is a non-motile, non-spore-forming bacterium that belongs to the Deinococcaceae family CITATION, CITATION.
It is characterized by an extreme ability to withstand high doses of desiccation and ionizing radiation.
For example, this bacterium can survive a dose of 5000 Gy of ionizing radiation, inducing more than 200 DNA double-strand breaks, with no effect on its viability CITATION, CITATION.
However, the molecular mechanisms underlying the high radiation resistance of D. radiodurans are thought to have evolved primarily as a side effect of mechanisms to counter extreme desiccation, as the bacterium thrives in dry, arid environments CITATION.
Over the years, evidence has accumulated suggesting that there exists no single, dominant mechanism responsible for the extremophilic nature of D. radiodurans, but that rather a combination of different mechanisms is at play CITATION, CITATION.
These range from passive structural contributions, such as the increased genome copy number CITATION, compact nucleotide organization CITATION, high intracellular concentration of the ROS-scavenger manganese CITATION, to active enzymatic repair mechanisms, including nucleotide and base excision repair and DNA double-strand break repair CITATION, CITATION, CITATION .
However important these mechanisms may be, it is also possible that the bacterium's proteome has undergone major structural adaptations in order to cope with environmental stresses.
One potential strategy to address this possibility is to compare the proteome of D. radiodurans with those of its non-extremophile relatives and look for conspicuous differences.
Recently, one of us has taken exactly this approach and focused on the presence and the putative biological role of ID regions in the proteome of D. radiodurans.
A subset of proteins in D. radiodurans was identified that contain highly hydrophilic stretches with low sequence complexity, indicative of intrinsic disorder, that are absent in non-extremophile homologues.
Interestingly, this list includes a preponderance of housekeeping and rescue-and-repair proteins, including DNA polymerase III, a nudix hydrolase, rotamase, ABC transporters, adenine deaminase and LEA proteins.
To further probe the significance of this finding, we here focus on a variant of nudix hydrolase, occuring naturally as a dimer, and analyze the properties of its ID regions.
Nudix hydrolases CITATION are a large class of enzymes present in all organisms that hydrolyze a wide range of pyrophosphates, including nucleotide di- and triphosphates, dinucleotides and nucleotide sugars.
Importantly, some members of the family degrade oxidized nucleotides and in this way prevent potentially mutagenic effects these would have if incorporated into nucleic acids.
Other family members regulate the concentration of metabolic intermediates and signaling molecules CITATION.
Interestingly, D. radiodurans exhibits 26 different types of nudix hyrolase, which is about three times more than what would be expected given the size of its genome, and is the highest number per Mbp of any bacterial genome known.
In particular, it has been reported that the majority of D. radiodurans nudix genes are strongly induced during the stationary phase, which has been implicated in metabolic reprogramming.
IDPs, such as the tail regions of nudix hydrolase, are extremely dynamic and are very difficult to study by X-ray crystallography or nuclear magnetic resonance.
However, atomistic computer simulations and molecular dynamics techniques are ideally suited to provide a complete, atomistic picture of the diverse, dynamic ensembles characterizing the IDPs.
Here, using sequence analysis methods and molecular dynamics simulations in conjunction with hydration free energy calculations, we show that the ID regions in this protein significantly alter its solvation properties, making the whole protein significantly more hydrophilic.
We hypothesize that this, in fact, is the principal functional role of these ID regions, and that they have evolved to keep the key housekeeping and repair enzymes solvated, and consequently functional, under extreme desiccation conditions.
Many protein functions can be directly linked to conformational changes.
Inside cells, the equilibria and transition rates between different conformations may be affected by macromolecular crowding.
We have recently developed a new approach for modeling crowding effects, which enables an atomistic representation of test proteins.
Here this approach is applied to study how crowding affects the equilibria and transition rates between open and closed conformations of seven proteins: yeast protein disulfide isomerase, adenylate kinase, orotidine phosphate decarboxylase, Trp repressor, hemoglobin, DNA -glucosyltransferase, and Ap 4A hydrolase.
For each protein, molecular dynamics simulations of the open and closed states are separately run.
Representative open and closed conformations are then used to calculate the crowding-induced changes in chemical potential for the two states.
The difference in chemical-potential change between the two states finally predicts the effects of crowding on the population ratio of the two states.
Crowding is found to reduce the open population to various extents.
In the presence of crowders with a 15 radius and occupying 35 percent of volume, the open-to-closed population ratios of yPDI, AdK, ODCase and TrpR are reduced by 79 percent, 78 percent, 62 percent and 55 percent, respectively.
The reductions for the remaining three proteins are 20 44 percent.
As expected, the four proteins experiencing the stronger crowding effects are those with larger conformational changes between open and closed states.
Larger proteins also tend to experience stronger crowding effects than smaller ones e.g., comparing yPDI and TrpR.
The potentials of mean force along the open-closed reaction coordinate of apo and ligand-bound ODCase are altered by crowding, suggesting that transition rates are also affected.
These quantitative results and qualitative trends will serve as valuable guides for expected crowding effects on protein conformation changes inside cells.
It is increasingly recognized that protein dynamics serves the critical link between structure and function CITATION CITATION.
An important manifestation of protein dynamics is the sampling of alternative conformations.
These conformational changes can be triggered by substrate binding CITATION and post-translational modifications such as phosphorylation CITATION.
Increasingly, structures of the same proteins at different functional states are becoming available.
These structures provide atomistic details of conformational changes.
For example, adenylate kinase, an enzyme that catalyzes the phosphoryl transfer from ATP to AMP, undergoes a significant conformational transition, from an open conformation in the apo form and to a closed conformation in the ligand-bound form CITATION CITATION.
The main differences between these two conformations occur in the ATP- and AMP-binding domains, with the CORE domain relatively rigid.
Other examples with well-characterized conformational transitions include proteins responsible for signal transduction across cell membranes CITATION CITATION and ion channels CITATION .
Biophysical characterizations of protein conformational changes have mostly been carried out under dilute conditions.
However, the environments where proteins perform their biological functions, i.e., extracellular space, cell membrane, and cytoplasm, are crowded with macromolecules.
For example, the cytoplasm of Escherichia coli contains about 300 400 g/l of macromolecules CITATION, which are estimated to occupy over 30 percent of the total volume.
In cell membranes, membrane proteins occupy a similar level of the total surface area CITATION.
How the crowded cellular environments affect the equilibria and transition rates between different conformations of proteins is still poorly understood.
Qualitatively, one expects that macromolecular crowding will significantly modify the energy landscapes of conformational changes, favoring more compact structures over more open ones CITATION.
Such effects of crowding have been scrutinized experimentally.
Molecular dynamics simulations have also been carried out to investigate the energy landscapes of a number of proteins under crowding, in the context of either conformational change CITATION or folding-unfolding transition CITATION CITATION.
To speed up conformational sampling, the proteins in these studies were represented at a coarse-grained level.
Recently we have developed an approach CITATION, CITATION, referred to as postprocessing, which opens the door to atomistic modeling of proteins under crowding.
In this approach, the motions of a test protein are simulated in the absence of crowders.
Conformations from this simulation are then used to calculate the change in chemical potential if they are transferred to a crowded solution.
The dependence of the change in chemical potential on reaction coordinates then captures the influence of crowding on the energy landscape.
The postprocessing approach has been applied to study effects of crowding on protein folding and binding stability CITATION, CITATION, CITATION and on the open-closed equilibrium of the HIV-1 protease dimer CITATION.
In the latter application it has been shown that the postprocessing approach yields results identical to those obtained from direct simulations of the protein in the presence of crowders CITATION, CITATION .
Here we apply the postprocessing approach to investigate the impact of macromolecular crowding on the open-closed equilibria of seven proteins: AdK CITATION, CITATION, yeast protein disulfide isomerase CITATION, CITATION, orotidine phosphate decarboxylase CITATION, Trp repressor CITATION, hemoglobin CITATION, DNA -glucosyltransferase CITATION, and Ap 4A hydrolase CITATION, CITATION.
The biological functions and subcellular locations of these proteins are listed in Table 1.
We find crowding to reduce the open-to-closed population ratios to various extents; the potentials of mean force along the open-closed reaction coordinate of apo and ligand-bound ODCase, and hence the transition rates, are similarly affected.
The biological implications of these results are discussed below.
G-quadruplex DNA is a four-stranded DNA structure formed by non-Watson-Crick base pairing between stacked sets of four guanines.
Many possible functions have been proposed for this structure, but its in vivo role in the cell is still largely unresolved.
We carried out a genome-wide survey of the evolutionary conservation of regions with the potential to form G-quadruplex DNA structures across seven yeast species.
We found that G4 DNA motifs were significantly more conserved than expected by chance, and the nucleotide-level conservation patterns suggested that the motif conservation was the result of the formation of G4 DNA structures.
We characterized the association of conserved and non-conserved G4 DNA motifs in Saccharomyces cerevisiae with more than 40 known genome features and gene classes.
Our comprehensive, integrated evolutionary and functional analysis confirmed the previously observed associations of G4 DNA motifs with promoter regions and the rDNA, and it identified several previously unrecognized associations of G4 DNA motifs with genomic features, such as mitotic and meiotic double-strand break sites.
Conserved G4 DNA motifs maintained strong associations with promoters and the rDNA, but not with DSBs.
We also performed the first analysis of G4 DNA motifs in the mitochondria, and surprisingly found a tenfold higher concentration of the motifs in the AT-rich yeast mitochondrial DNA than in nuclear DNA.
The evolutionary conservation of the G4 DNA motif and its association with specific genome features supports the hypothesis that G4 DNA has in vivo functions that are under evolutionary constraint.
DNA primarily exists as a double helix.
However, DNA can also adopt other structural conformations that have the potential to play critical roles in a range of biological processes.
One such structure is G-quadruplex DNA, which was discovered in the late 1980s when biochemical experiments demonstrated that oligodeoxynucleotides that contain four separated runs of two, three, or four guanines can spontaneously form four-stranded structures CITATION, CITATION.
G4 DNA structures consist of stacked planar G-quartets that are held together by Hoogsteen hydrogen bonding between four guanines from each of the G-tracts.
The guanines can come from a single nucleic acid strand or multiple strands, and the strands may be oriented in a parallel or anti-parallel orientation.
G4 DNA structures are compact, highly stable under physiological pH and salt conditions, resistant to degradation by nucleases, and can have melting temperatures even higher than that of duplex DNA CITATION, CITATION.
G4 DNA structures can be formed from runs of two guanines, but they are less stable than those with longer runs.
The G4 DNA structure is of considerable interest because of its potential to influence a variety of biological processes CITATION, CITATION.
For example, telomeric DNA in most eukaryotic organisms consists of G-rich repeated sequence ending with a 3 single stranded G-rich overhang that can form G-quadruplexes in vitro CITATION, CITATION.
The first direct evidence for the presence of G4 DNA structures in vivo came from studies using G4 DNA-specific antibodies to detect intermolecular structures at ciliate telomeres where their formation and dissolution are cell cycle regulated CITATION - CITATION.
However, as described in detail in this paper, telomeric DNAs are not the only chromosomal sequences with the ability to form G4 DNA structures.
Because experimental characterization of the in vivo functions of G4 DNA structures has proved difficult CITATION, especially at non-telomeric loci, genome-wide computational analyses have played an increasing role in the identification of regions that have the potential to form G4 DNA structures.
The distribution of G4 DNA motifs has been investigated in S. cerevisiae CITATION, human CITATION, CITATION, and a number of prokaryotic genomes CITATION in the hope that the patterns of occurrence will provide insight into the functional roles of these structures.
In each case, a computational search for variations of the G4 DNA motif, usually four tracts of three or more guanines separated by loop regions of any nucleotide, was performed.
Across a wide range of species, G4 DNA motifs were found in telomeres, G-rich micro- and mini-satellites, near promoters, and within the ribosomal DNA CITATION CITATION.
In the human genome, genes that are near G4 DNA motifs fall into specific functional classes; for example, oncogenes and tumor suppressor genes have particularly high or low G4 DNA forming potential CITATION CITATION.
Recently, human G4 DNA motifs were reported be associated with recombination prone regions CITATION and to show mutational patterns that preserved the potential to form G4 DNA structures CITATION.
Computational analysis in S. cerevisiae identified several hundred G4 DNA motifs, and found them to be significantly associated with promoter regions and to a lesser extent with open reading frames CITATION.
Thus, studies in a wide range of organisms have led to the proposal that G4 DNA structures affect multiple cellular processes beyond their roles at telomeres.
However, direct support for formation and function of G4 DNA structures in vivo is still largely unavailable.
In this study, we integrated genome sequence data, experimental analysis, and computational exploration of genome annotations to investigate the conservation and function of G4 DNA structures in S. cerevisiae.
Evolutionary conservation across related species has played a vital role in defining functional elements such as genes and regulatory sites CITATION, CITATION.
We identified sequence motifs with the potential to form G-quadruplex structures in S. cerevisiae and six other fungal species and assessed the evolutionary sequence conservation of the motifs across these seven species.
We found that G4 DNA motifs and the nucleotides comprising them were more evolutionarily conserved than expected by chance; however, they were not as strongly conserved as genes and many known regulatory sites.
Additionally, the patterns of nucleotide conservation within the motifs indicated that the evolutionary constraint was likely the result of pressure to maintain the ability of these motifs to form G4 DNA structures.
This analysis provides strong evidence that many computationally identified G4 DNA motifs form functional G4 DNA structures in vivo.
To characterize possible functions for the structures, we evaluated the association of conserved and non-conserved G4 DNA motifs with a range of genomic features.
These tests corroborated previous observations of the significant associations of G4 DNA motifs with gene promoters and rDNA CITATION, and suggested several new potential biological functions, such as roles in double strand break repair and in the mitochondrial genome.
Transcriptional regulators recognize specific DNA sequences.
Because these sequences are embedded in the background of genomic DNA, it is hard to identify the key cis-regulatory elements that determine disparate patterns of gene expression.
The detection of the intra- and inter-species differences among these sequences is crucial for understanding the molecular basis of both differential gene expression and evolution.
Here, we address this problem by investigating the target promoters controlled by the DNA-binding PhoP protein, which governs virulence and Mg 2 homeostasis in several bacterial species.
PhoP is particularly interesting; it is highly conserved in different gamma/enterobacteria, regulating not only ancestral genes but also governing the expression of dozens of horizontally acquired genes that differ from species to species.
Our approach consists of decomposing the DNA binding site sequences for a given regulator into families of motifs using a machine learning method inspired by the Divide Conquer strategy.
By partitioning a motif into sub-patterns, computational advantages for classification were produced, resulting in the discovery of new members of a regulon, and alleviating the problem of distinguishing functional sites in chromatin immunoprecipitation and DNA microarray genome-wide analysis.
Moreover, we found that certain partitions were useful in revealing biological properties of binding site sequences, including modular gains and losses of PhoP binding sites through evolutionary turnover events, as well as conservation in distant species.
The high conservation of PhoP submotifs within gamma/enterobacteria, as well as the regulatory protein that recognizes them, suggests that the major cause of divergence between related species is not due to the binding sites, as was previously suggested for other regulators.
Instead, the divergence may be attributed to the fast evolution of orthologous target genes and/or the promoter architectures resulting from the interaction of those binding sites with the RNA polymerase.
Whole genome sequences, as well as microarray and chromatin inmunoprecipitation with array hybridization data provide the raw material for the characterization and understanding of the underlying regulatory systems.
It is still challenging, however, to discern the sequence elements relevant to differential gene expression, such as those corresponding to the binding sites of transcriptional factors and RNA polymerase, when they are embedded in the background of genomic DNA sequences that do not play a role in gene expression CITATION.
This raises the question: how does a single regulator distinguish promoter sequences when affinity is a major determinant of differential expression?
Also, how does a regulator evolve given that there appears to be a non-monotonic co-evolution of regulators and targets CITATION CITATION ?
Methods that look for matching to a consensus pattern have been successfully used to identify BSs in promoters controlled by particular TFs CITATION CITATION.
Tools for motif discovery are designed to find unknown, relatively short sequence patterns located primarily in the promoter regions of genomes CITATION.
Because these searches are performed in a context of short signals embedded in high statistical noise, current tools tend to discard a relevant number of samples that only weakly resemble a consensus CITATION.
Moreover, the strict cutoffs used by these methods, while increasing specificity, display lower sensitivity CITATION, CITATION to weak but still functional BSs.
Because the consensus motif reflects a single pattern derived by averaging DNA sequences, it often conceals sub-patterns that might define distinct regulatory mechanisms CITATION.
Overall, the use of consensuses tends to homogenize sequence motifs among promoters and even across species CITATION, CITATION, which hampers the discovery of key features that distinguish co-regulated promoters within and across species.
To circumvent the limitations of consensus methods CITATION, we decomposed BS motifs into sub-patterns CITATION, CITATION by applying the classical Divide Conquer strategy CITATION, CITATION.
We then compared different forms of decomposed BS motifs of a TF into families of motifs from a computational clustering perspective.
In so doing, we extracted the maximal amount of useful genomic information through the effective handling of the biological and experimental variability inherent in the data, and then combined them into an accurate multi-classifier predictor CITATION, CITATION.
Although there is a computational usefulness of the submotifs CITATION, CITATION, it was not clear if these families of motifs were just a computational artifact or if they could provide insights into the regulatory process carried out by a regulator and its targets.
To address this problem, we evaluated the ability of the submotifs to characterize gene expression both within and across genomes.
First, we used submotifs to distinguish between functional and non-functional BSs in genome-wide searches using a combination of ChIP-chip and custom expression microarray experiments.
Then, we determined the evolutionary significance of the submotifs by calculating their rate of evolution CITATION, CITATION and mapping the gain and loss events along the phylogenetic tree of gamma/enterobacteria.
The interspecies variation of orthologous genes, the conservation of the regulatory protein, as well as the cis-features conforming the promoter architecture allowed us to evaluate the major causes of divergences between species CITATION, CITATION .
We applied our approach to analyze the genes regulated by the PhoP/PhoQ two-component system, which mediates the adaptation to low Mg 2 environments and/or virulence in several bacteria species including Escherichia coli species, Salmonella species, Shigella species, Erwinia species, Photorhabdus and Yersinia species.
Two-component systems represent the primary signal transduction paradigm in prokaryotic organisms.
Although proteins encoded by these systems are often well conserved throughout different bacterial species CITATION, CITATION, regulators like PhoP differentially control the expression of many horizontally-acquired genes, which constitute one of the major sources of genomic variation CITATION .
The mechanisms of stress tolerance in sessile animals, such as molluscs, can offer fundamental insights into the adaptation of organisms for a wide range of environmental challenges.
One of the best studied processes at the molecular level relevant to stress tolerance is the heat shock response in the genus Mytilus.
We focus on the upstream region of Mytilus galloprovincialis Hsp90 genes and their structural and functional associations, using comparative genomics and network inference.
Sequence comparison of this region provides novel evidence that the transcription of Hsp90 is regulated via a dense region of transcription factor binding sites, also containing a region with similarity to the Gamera family of LINE-like repetitive sequences and a genus-specific element of unknown function.
Furthermore, we infer a set of gene networks from tissue-specific expression data, and specifically extract an Hsp class-associated network, with 174 genes and 2,226 associations, exhibiting a complex pattern of expression across multiple tissue types.
Our results suggest that the heat shock response in the genus Mytilus is regulated by an unexpectedly complex upstream region, and provide new directions for the use of the heat shock process as a biosensor system for environmental monitoring.
The majority of molluscan species go through two principal developmental phases, a larval embryo followed by a clumping structure, when they are permanently attached to an underwater substrate.
This lifecycle, common amongst marine invertebrates, poses challenges for adaptation and tolerance for a wide range of conditions at the littoral zone, including steep salinity or temperature gradients.
Key model organisms for molluscan biology include species from the genus Mytilus, in particular M. edulis, M. galloprovincialis and M. californianus.
Crucially, the latter species is a target organism for a genome sequencing project, whose results are eagerly expected by the community .
The Mytilus species group provides an ideal model both for fundamental questions of animal adaptation to stress response, as well as biotechnological applications, primarily as a pollution biosensor CITATION.
Its use extends into biomimetics CITATION, in particular protein-based medical adhesives CITATION, with potential applications in fields such as dentistry CITATION.
Moreover, its relatively complex developmental structure and higher taxonomic status as an invertebrate, combined with the fact that it can suffer from mussel haemic neoplasia, renders this organism a potential model for human leukemia and an ideal biomarker for pollution-induced disease CITATION.
In this context, it is important to understand the mechanisms by which mussels tolerate and cope with environmental stress, given that their behavioral options are highly restricted, due to the sessile phase of their lifecycle.
In the past, comparisons between motility and sessility for higher organisms have been primarily confined to animals versus plants CITATION, with follow-up studies focusing on comparisons between large animals, e.g. humans, versus large plants, e.g. trees, and the trade-offs for the tree body plan CITATION.
Less attention has been paid to adaptations by sessile animals, in particular intertidal invertebrates CITATION CITATION, and the molecular mechanisms through which they achieve tolerance to stress.
One exception is represented by heat shock response, a key factor for temperature adaptation that has been studied in this context to a certain extent CITATION, and specifically in Mytilus with regard to the Hsp70 CITATION and Hsp90 CITATION genes.
Transcriptional regulation can be achieved either by an extensive repertoire of paralogs and transcription factors or a complex structure of promoters.
Analysis of comprehensive datasets has clearly demonstrated that transcription factors and transcription-associated proteins are not universally distributed but highly taxon-specific and that relative TF gene content increases with the taxonomic scale CITATION CITATION.
Such comparisons have been later extended by follow-up studies that analyzed TAP complements and their expansion rates in plants CITATION CITATION.
Thus, it is now known that one way by which plants, sessile organisms par excellence, achieve a finer degree of regulation is by the expansion of TF/TAP complements and a gene content strategy.
Yet, it is unclear whether similar trends are followed in sessile animals, since entire genome sequences for those are lacking so far, limiting the range of comparative genome-wide studies that can be performed.
As far as paralogs are concerned, recent studies that have focused on the heat shock response in plants, and in particular Arabidopsis thaliana, have revealed that the process involves up to 21 known TFs and four heat shock protein families CITATION CITATION.
Despite a cursory resemblance to mammals, in Drosophila thermal sensing is achieved by a unique repertoire of genes CITATION, including thermostat systems not exclusively involving heat shock proteins CITATION.
In other words, and probably for different reasons, a gene content strategy might prevail in both model organisms for plants and motile invertebrates.
Thus, it is worth examining what are the mechanisms through which stress response is regulated in sessile marine invertebrates in general, and the Mytilus genus in particular, and which strategy dominates gene expression.
We focus on the Hsp90 family as a case study for stress response in sessile animals and examine the structure and function of the Hsp90 upstream region in M. galloprovincialis.
Previously, two distinct Hsp90 genes with the same genomic organization have been isolated from M. galloprovincialis CITATION, herein called Mghsp90 genes.
Detailed sequence analysis revealed that the two genes contain nine exons and exhibit great similarities in both the 5 non-coding and the coding regions but differ in their 3 non-coding regions, as well as in three introns, due to the presence of repeated sequences CITATION.
The 5 non-coding region of both genes contains a non-translated exon and multiple binding sites for various transcription factors, highly suggestive of potential interactions of these factors with the Hsp90 promoter and subtle patterns of gene regulation CITATION .
A comparative analysis of Hsp90 gene content across all taxa with available sequence data has clearly shown that invertebrate genomes contain a relatively small number of Hsp90 genes, compared to those of vertebrates CITATION.
Thus, it appears that the Mytilus genome might contain a relatively small number of TFs and/or Hsp90 genes, raising the question how the expression of Hsp90 and other heat shock genes is regulated in sessile invertebrates.
In the present work, we perform a detailed analysis of the Mghsp90 upstream region in terms of structure and expression, and reveal the presence of previously undetected sequence elements of unknown function.
Based on tissue-specific expression data, we also delineate the potential associations of Mghsp90 with another 174 genes that are involved in a complex pattern of expression across tissues.
These two discoveries are discussed within the context of existing knowledge and are expected to contribute towards a deeper understanding of the heat shock response in sessile organisms.
In eukaryotes, the interphase nucleus is organized in morphologically and/or functionally distinct nuclear compartments.
Numerous studies highlight functional relationships between the spatial organization of the nucleus and gene regulation.
This raises the question of whether nuclear organization principles exist and, if so, whether they are identical in the animal and plant kingdoms.
We addressed this issue through the investigation of the three-dimensional distribution of the centromeres and chromocenters.
We investigated five very diverse populations of interphase nuclei at different differentiation stages in their physiological environment, belonging to rabbit embryos at the 8-cell and blastocyst stages, differentiated rabbit mammary epithelial cells during lactation, and differentiated cells of Arabidopsis thaliana plantlets.
We developed new tools based on the processing of confocal images and a new statistical approach based on G- and F- distance functions used in spatial statistics.
Our original computational scheme takes into account both size and shape variability by comparing, for each nucleus, the observed distribution against a reference distribution estimated by Monte-Carlo sampling over the same nucleus.
This implicit normalization allowed similar data processing and extraction of rules in the five differentiated nuclei populations of the three studied biological systems, despite differences in chromosome number, genome organization and heterochromatin content.
We showed that centromeres/chromocenters form significantly more regularly spaced patterns than expected under a completely random situation, suggesting that repulsive constraints or spatial inhomogeneities underlay the spatial organization of heterochromatic compartments.
The proposed technique should be useful for identifying further spatial features in a wide range of cell types.
In eukaryotes, the interphase nucleus is organized into distinct nuclear compartments, defined as macroscopic regions within the nucleus that are morphologically and/or functionally distinct from their surrounding CITATION.
Complex relationships between the spatial organization of these compartments and the regulation of genome function have been previously described.
Furthermore, changes in nuclear architecture are among the most significant features of differentiation, development or malignant processes.
Thus, these findings question whether topological landmarks and/or nuclear organization principles exist and, if so, whether these architectural principles are identical in the animal and plant kingdoms.
To investigate nuclear organization principles, multidisciplinary approaches are required based on image analysis, computational biology and spatial statistics.
Spatial distributions of several compartments, which can be proteinaceous bodies or genomic domains, have been analyzed.
Chromosome territories, areas in which the genetic content of individual chromosomes are confined CITATION, CITATION, are usually radially distributed, with gene-rich chromosomes more centrally located than gene-poor chromosomes.
Some studies report that chromosome size could also influence CT location CITATION CITATION.
Centromeres may be close to the nuclear periphery and those located on chromosomes bearing ribosomal genes are generally tethered to the nucleolar periphery CITATION.
Transcription sites, as well as early replicating foci, assumed to correspond to active chromatin, are more centrally located, whereas inactive heterochromatin tends to be at the nuclear periphery.
At a finer level, active genes widely separated in cis or located on different chromosomes can colocalize to active transcription sites CITATION CITATION, whereas proximity to centromeric heterochromatin or to the nuclear periphery is generally associated with gene silencing CITATION CITATION.
Changes in the transcriptional status of genes have been frequently associated with their repositioning in the nucleus relative to their CTs, the nuclear periphery or the repressive centromeric heterochromatin CITATION, CITATION CITATION.
Furthermore, large reorganization in nuclear architecture can accompany some differentiation, development, malignant processes or natural variations CITATION CITATION .
However, it still remains difficult to extract common rules and establish comparisons due to various limitations.
Indeed, most data have been gathered on limited sets of nuclear elements in isolated plant cell nuclei or in nuclei from immortalized animal cell lines outside their physiological environment, except for circulating blood cells.
Little is known about possible differences in nuclear organization of cells within their tissue CITATION.
Some studies compared nuclear organization in primary cells versus cell lines, in cell lines versus tissues, and in 2D culture versus 3D cultures; these studies suggested that tissue architecture is involved in the control of nuclear organization CITATION CITATION.
In addition, data on nuclear organization in plant cell nuclei in situ are rare CITATION, CITATION.
Finally, few three-dimensional studies and quantitative measures have been performed to investigate spatial nuclear organization CITATION CITATION .
The statistics used to analyze the data were mostly based on radial patterns of nuclear elements, such as genes, chromosome territories, and centromeres.
Radial positions have been measured with respect to the nuclear geometric center or the nuclear envelope CITATION, CITATION.
Spatial affinity between several elements has been investigated and spatial correlations have been assessed through central angles, for example between the radii joining homologous chromosome territory centers and the nuclear center CITATION, CITATION.
Alternative approaches based on distances between elements have been developed.
Distances between a small number of elements, like two pairs of homologous alleles, were used for testing spatial attraction or repulsion CITATION.
Remarkably, spatial statistics tools, such as distance functions, that have been developed in ecology or epidemiology for analyzing spatial point patterns CITATION have rarely been applied in nuclear organization studies.
For example, nearest-neighbor distances have been used to analyze large numbers of nuclear elements, such as molecular complexes, PML bodies, or RNA Polymerase II foci CITATION, CITATION.
Alternatively, all pairwise inter-distances have been used to analyze the spatial distribution of chromocenters CITATION and nucleocapsids CITATION .
In spatial statistics, data are usually collected through a sampling window over a single realization of a point process.
This point process is generally considered as unbounded and spatially homogeneous.
Such a theoretical framework makes sense in applications in which the investigated phenomenon extends far beyond the observed region.
By contrast, analyses of nuclear spatial patterns are based on images of entire nuclei: the whole domain of interest is observed.
Furthermore, one should not consider observed nuclear patterns as realizations of spatially homogeneous point processes.
Another difference is that replicated data are available as the analysis is carried out on a sample of nuclei.
Recently, distance functions have been extended to replicated spatially heterogeneous point patterns CITATION, CITATION.
For instance, an extended F-function has been used for analyzing spatial patterns of transmissible spongiform encephalopathy lesions in brain tissue CITATION.
The extended F-function takes into account the expected spatial heterogeneity of the point process intensity.
To estimate this intensity, the replicated patterns are first registered to locate all observed points in a common coordinate system.
However, this type of preliminary registration is not possible for nuclei due to the lack of identifiable nuclear landmarks.
Hence, further developments are required to make spatial statistics tools appropriate for nuclear spatial organization studies.
In this study, we develop an approach to furthering the analysis of nuclear spatial organization.
Spatial distributions of nuclear compartments were quantified using the cumulative distribution functions of nearest-neighbor distances and of distances between arbitrary points within the nucleus and their nearest compartment.
The analysis of G- and F-functions was designed specifically to cope with patterns observed in non-registered and variable domains.
We applied this new approach to the investigation of the 3D distribution of centric/pericentric heterochromatin in five interphase nuclei populations belonging to the animal and plant kingdoms CITATION.
The centric/pericentric compartment was chosen due to its dual structural and regulatory functions.
Indeed, it usually behaves as a transcription repressor and is essential for genome organization and the proper segregation of genetic information during cell division CITATION, CITATION.
This compartment often clusters and forms chromocenters CITATION CITATION.
We studied nuclei of cells at various differentiation stages, in three biological systems: rabbit embryos at the 8-cell and blastocyst stages, differentiated rabbit mammary epithelial cells during lactation, and differentiated cells of A. thaliana plantlets.
We found non-completely random and significantly more regularly spaced patterns than expected under complete randomness of the centric/pericentric heterochromatin compartment in the five differentiated cell populations, suggesting the existence of inter-kingdom nuclear organizational rules and possible nuclear regularities.
Gene regulatory networks consist of direct interactions but also include indirect interactions mediated by metabolites and signaling molecules.
We describe how these indirect interactions can be derived from a model of the underlying biochemical reaction network, using weak time-scale assumptions in combination with sensitivity criteria from metabolic control analysis.
We apply this approach to a model of the carbon assimilation network in Escherichia coli.
Our results show that the derived gene regulatory network is densely connected, contrary to what is usually assumed.
Moreover, the network is largely sign-determined, meaning that the signs of the indirect interactions are fixed by the flux directions of biochemical reactions, independently of specific parameter values and rate laws.
An inversion of the fluxes following a change in growth conditions may affect the signs of the indirect interactions though.
This leads to a feedback structure that is at the same time robust to changes in the kinetic properties of enzymes and that has the flexibility to accommodate radical changes in the environment.
The adaptation of bacteria to changes in their environment involves adjustments in the expression of genes coding for enzymes, regulators, membrane transporters, etc. CITATION CITATION.
These adjustments are controlled by gene regulatory networks ensuring the coordinated expression of clusters of functionally related genes.
The interactions in the network may be direct, as in the case of a gene coding for a transcription factor regulating the expression of another gene.
Most of the time, however, regulatory interactions are indirect, e.g. when a gene encodes an enzyme producing a transcriptional effector CITATION .
A gene regulatory network can thus not be reduced to its transcriptional regulatory interactions: by ignoring indirect interactions mediated by metabolic and signaling pathways we may miss crucial feedback loops in the system.
The network controlling carbon uptake in the bacterium Escherichia coli is a good example because it integrates metabolism, signal transduction, and gene expression.
At the level of gene expression, the network includes intricate feedback loops that arise from indirect interactions between the subsystems.
Global regulators like Crp control expression of enzymes in carbon metabolism CITATION CITATION, while intermediates of the latter pathways control the expression of global regulators.
For instance, the phosphorylation of EIIA activates adenylate cyclase to produce cAMP which is required for the activation of Crp CITATION, CITATION .
The aim of this paper is to develop a method for the systematic derivation of direct and indirect interactions in a gene regulatory network from the underlying biochemical reaction network.
Due to the complexity of the intermediate metabolic and signaling networks, determining indirect interactions is difficult in general.
We show that model reduction based on quasi-steady-state approximations expressing weak assumptions on time-scale hierarchies in the system CITATION CITATION, together with sensitivity criteria from metabolic control analysis CITATION, CITATION, are able to uncover such interactions.
Indeed the MCA formalism uniquely allows to relate systemic sensitivities with the sensitivities of individual reactions to reactants and effectors CITATION, CITATION.
It therefore provides a proper framework for investigating metabolic effects in gene regulation.
We apply our approach to a model of the upper part of the carbon assimilation network in E. coli, consisting of the glycolysis and gluconeogenesis pathways and their genetic and metabolic regulation.
The analysis of the derived gene regulatory network leads to three new insights.
First, contrary to what is often assumed, the network is densely connected due to numerous feedback loops resulting from indirect interactions.
This additional complexity is an important issue for the correct interpretation of data from genome-wide transcriptome studies.
Second, the derived gene regulatory network for carbon assimilation in E. coli is sign-determined, in the sense that the signs of interactions are essentially fixed by weak information on flux directions of biochemical reactions, without explicit specification of kinetic rate laws or parameter values.
Therefore the feedback structure is robust to changes in kinetic properties of enzymes and other biochemical reactions species.
Third, a change in environmental conditions may invert fluxes, and thus the signs of indirect interactions, resulting in a dynamic rewiring of the regulatory network.
Comparison of elastic network model predictions with experimental data has provided important insights on the dominant role of the network of inter-residue contacts in defining the global dynamics of proteins.
Most of these studies have focused on interpreting the mean-square fluctuations of residues, or deriving the most collective, or softest, modes of motions that are known to be insensitive to structural and energetic details.
However, with increasing structural data, we are in a position to perform a more critical assessment of the structure-dynamics relations in proteins, and gain a deeper understanding of the major determinants of not only the mean-square fluctuations and lowest frequency modes, but the covariance or the cross-correlations between residue fluctuations and the shapes of higher modes.
A systematic study of a large set of NMR-determined proteins is analyzed using a novel method based on entropy maximization to demonstrate that the next level of refinement in the elastic network model description of proteins ought to take into consideration properties such as contact order and the secondary structure types of the interacting residues, whereas the types of amino acids do not play a critical role.
Most importantly, an optimal description of observed cross-correlations requires the inclusion of destabilizing, as opposed to exclusively stabilizing, interactions, stipulating the functional significance of local frustration in imparting native-like dynamics.
This study provides us with a deeper understanding of the structural basis of experimentally observed behavior, and opens the way to the development of more accurate models for exploring protein dynamics.
Associated with each protein fold is a set of intrinsically accessible global motions that arise solely from the 3-dimensional geometry of the fold and involve the entire architecture.
For a number of systems it has been shown that these intrinsic motions play an important role in protein function CITATION, facilitating events such as recognition and binding CITATION, CITATION, catalysis CITATION CITATION and allosteric regulation CITATION, CITATION, CITATION.
The time scales of these cooperative motions are usually beyond the reach of conventional MD simulations.
They are modeled instead with coarse-grained techniques that omit the finer details of atomic interactions.
The elastic network model is an example of a coarse-grained model that has enjoyed considerable success in predicting global dynamics of proteins and other macromolecules.
The central idea behind the ENM is that, in the vicinity of a minimum, the potential energy landscape of a biomolecular system can be approximated by the sum of pairwise harmonic potentials that stabilize the native contacts.
In the simplest ENM, the Gaussian network model CITATION, each node of the network is identified by an amino acid, and each edge is a spring that provides a linear restoring force to deviations from the minimum-energy structure.
The system's dynamics is therefore expressed in terms of the normal modes of vibration of the many-bodied system about its equilibrium state; and dynamical information about the protein, such as the expectation values of residue fluctuations or cross-correlations, is uniquely defined by the network topology.
A few prevalent methods are used for constructing ENMs, but most have at their hearts two underlying assumptions: The springs are all at their rest lengths in the equilibrium conformation, and the force constants decrease with the distance between nodes, among other variables.
In the earliest models CITATION, CITATION and the anisotropic network model CITATION CITATION, force constants were taken to be uniform for all nodes separated by a distance less than a specified cutoff distance and zero for greater distances.
In parallel, models were proposed in which the force constants decay exponentially CITATION, CITATION or as an inverse power of distance CITATION, CITATION, or where stronger interactions are assigned to sequentially adjacent residues CITATION, CITATION, CITATION.
Although such modifications can lead to modest improvements in the agreement between ENM predictions and certain experimental data, there is still no clear best method for assigning force constants in an ENM.
A common approach for assessing the performance of ENMs or estimating their force constants has been to compare the ENM-derived autocorrelations of residue motions to the corresponding X-ray crystallographic B-factors or the mean-square fluctuations in residue coordinates observed between NMR models.
Because the slow modes have the largest amplitudes, often the focus of study has been a narrow band of the slowest modes.
The ENM slow modes have indeed been shown to agree well with those predicted by detailed atomic-level force fields and with experimentally determined dynamics CITATION, CITATION.
However, the majority of the dynamical information conveyed by the ENM is contained in the residue cross-correlations, and this information has been largely overlooked during comparisons of ENM results to experimental data.
Further, the subtle and complex dynamics of the structures that lie beneath the gross global motions are ignored when only the slowest modes are considered.
Mid- and high-frequency modes are predicted with relatively lower confidence by ENMs, but these modes may be important for coordinating the finer motions of the molecule while the slower modes orchestrate its global rearrangements CITATION.
Finally, while the ENM-based studies have shown that the network topology is the dominant factor that defines the collective modes, especially those in the low frequency regime, there may be other structural properties that are not accounted for by ENMs but which may provide a more realistic description of equilibrium dynamics, if accurately modeled.
Here we examine the ensembles of structural models determined by NMR for 68 proteins and evaluate for each ensemble the covariance in the deviations of residue-positions from their mean values.
We present a technique for optimizing ENM force constants within a pre-defined network topology so as to provide the most accurate representation of the experimentally observed covariance data.
Our method is based on the concept of entropy maximization: Briefly, when inferring the form of an unknown probability distribution, the one that is least reliant on the form of missing data is that which maximizes the system's entropy subject to constraints imposed by the available data CITATION, CITATION.
This method has been applied to a variety of biological problems, including neural networks CITATION, gene interaction networks CITATION, and protein folding CITATION .
The resulting auto- and cross-correlations in residue fluctuations are used to build an ENM-based model with optimal force constants.
It can be shown that when the constraints of the maximization are pair correlations, the probability distribution takes a Gaussian form.
Further, the only terms that contribute to the probability distribution are those that correspond to pairs with correlations that are explicitly considered as constraints on the entropy maximization.
In terms of the ENM, this means that for a given network topology, there exists a unique set of force constants that exactly reproduces the experimentally observed cross- correlations between all pairs of interacting residues, along with their autocorrelations .
Notably, our technique captures the physical significance of factors such as sequence separation and spatial distance which have been empirically found to influence force constant strengths.
Sequence separation is expressed in terms of contact order, i.e., the number of residues along the sequence between two residues that are connected by a spring in the ENM.
Further, our analysis benchmarked against a test set of 41 NMR ensembles of proteins suggests additional factors, including hydrogen bond formation and secondary structure type, which should also be incorporated in the ENMs for a more accurate description of experimental data.
It also identifies factors that are of little consequence insofar as the collective dynamics near equilibrium conditions are concerned.
Amino acid specificity turns out to be one of them; diffuse, overlapping distributions of OFCs are obtained for different types of amino acids, precluding the assignment of residue-specific OFCs.
A modified version of the GNM, mGNM, that accounts for these factors is proposed and is verified to perform better than existing models especially in reproducing cross-correlations.
Finally, the study highlights the importance of higher modes and the role of frustration in protein dynamics, the implications of which are discussed with regard to model development and protein design.
The idea of date and party hubs has been influential in the study of protein protein interaction networks.
Date hubs display low co-expression with their partners, whilst party hubs have high co-expression.
It was proposed that party hubs are local coordinators whereas date hubs are global connectors.
Here, we show that the reported importance of date hubs to network connectivity can in fact be attributed to a tiny subset of them.
Crucially, these few, extremely central, hubs do not display particularly low expression correlation, undermining the idea of a link between this quantity and hub function.
The date/party distinction was originally motivated by an approximately bimodal distribution of hub co-expression; we show that this feature is not always robust to methodological changes.
Additionally, topological properties of hubs do not in general correlate with co-expression.
However, we find significant correlations between interaction centrality and the functional similarity of the interacting proteins.
We suggest that thinking in terms of a date/party dichotomy for hubs in protein interaction networks is not meaningful, and it might be more useful to conceive of roles for protein-protein interactions rather than for individual proteins.
Protein interaction networks, constructed from data obtained via techniques such as yeast two-hybrid screening, do not capture the fact that the actual interactions that occur in vivo depend on prevailing physiological conditions.
For instance, actively expressed proteins vary amongst the tissues in an organism and also change over time.
Thus, the specific parts of the interactome that are active, as well as their organisational form, might depend a great deal on where and when one examines the network CITATION, CITATION.
One way to incorporate such information is to use mRNA expression data from microarray experiments.
Han et al. CITATION examined the extent to which hubs in the yeast interactome are co-expressed with their interaction partners.
They defined hubs as proteins with degree at least 5, where degree refers to the number of links emanating from a node.
Based on the averaged Pearson correlation coefficient of expression over all partners, they concluded that hubs fall into two distinct classes: those with a low avPCC and those with a high avPCC.
They inferred that these two types of hubs play different roles in the modular organisation of the network: Party hubs are thought to coordinate single functions performed by a group of proteins that are all expressed at the same time, whereas date hubs are described as higher-level connectors between groups that perform varying functions and are active at different times or under different conditions.
The validity of the date/party hub distinction has since been debated in a sequence of papers CITATION CITATION, and there appears to be no consensus on the issue.
Two established points of contention are: Is the distribution of hubs truly bimodal and is the date/party distinction that was originally observed a general property of the interactome or an artefact of the employed data set?
Different statistical tests have suggested seemingly different answers.
However, despite this ongoing debate, the hypothesis has been highly prominent in the literature CITATION, CITATION CITATION.
Here, following up on the work of Batada et al. CITATION, CITATION, we revisit the initial data and suggest additional problems with the statistical methodology that was employed.
In accordance with their results, we find that the differing behaviour observed on the deletion of date and party hubs CITATION, which seemed to suggest that date hubs were more essential to global connectivity, was largely due to a very small number of key hubs rather than being a generic property of the entire set of date hubs.
More generally, we use a complementary perspective to Batada et al. to define structural roles for hubs in the context of the modular organisation of protein interaction networks.
Our results indicate that there is little correlation between expression avPCC and structural roles.
In light of this, the more refined categorisation of date, party, and family hubs, which was based on taking into account differences in expression variance in addition to avPCC CITATION, also appears inappropriate.
A recent study by Taylor et al. CITATION argued for the the existence of intermodular and intramodular hubs a categorisation along the same lines as date and party hubs in the human interactome.
We show that their observation of a binary hub classification is susceptible to changes in the algorithm used to normalise microarray expression data or in the kernel function used to smooth the histogram of the avPCC distribution.
The data does not in fact display any statistically significant deviation from unimodality as per the DIP test CITATION, CITATION, as has already been observed by Batada et al. CITATION, CITATION for yeast data.
We revisited the bimodality question because it was a key part of the original paper CITATION, and in particular because it made a reappearance in Taylor et al. CITATION for human data.
However, it is possible that a date-party like continuum may exist even in the absence of a bimodal distribution, and this is why we also attempt to examine the more general question of whether the network roles of hub proteins really are related to their co-expression properties with interaction partners.
Many real-world networks display some sort of modular organisation, as they can be partitioned into cohesive groups of nodes that have a relatively high ratio of internal to external connection densities.
Such sub-networks, known as communities, often correspond to distinct functional units CITATION CITATION.
Several studies in recent years have considered the existence of community structure in protein-protein interaction networks CITATION CITATION.
A myriad of algorithms have been developed for detecting communities in networks CITATION, CITATION.
For example, the concept of graph modularity can be used to quantify the extent to which the number of links falling within groups exceeds the number that would be expected in an appropriate random network CITATION.
One of the standard techniques to detect communities is to partition a network into sub-networks such that graph modularity is maximised CITATION, CITATION .
We use the idea of community structure to take a new approach to the problem of hub classification by attempting to assign roles to hubs purely on the basis of network topology rather than on the basis of expression data.
Our rationale is that the biological roles of date and party hubs are essentially topological in nature and should thus be identifiable from the network alone.
Once we have partitioned the network into a set of meaningful communities, it is possible to compute statistics to measure the connectivity of each hub both within its own community and to other communities.
One method for assigning relevant roles to nodes in a metabolic network was formulated by Guimer and Amaral CITATION, and we follow an analogous procedure for the hubs in our protein interaction networks.
We then examine the extent to which these roles match up with the date/party hypothesis, finding little evidence to support it.
One might also wonder about the extent to which observed interactome properties are dependent on the particular instantiation of the network being analysed.
Several papers have discussed at length concerns about the completeness and reliability of existing protein interaction data sets, e.g. CITATION CITATION.
Such data have been gathered using multiple methods, the most prominent of which are Y2H and affinity purification followed by mass spectrometry.
In a recent paper, Yu et al. examined the properties of interaction networks that were derived from different sources, suggesting that experimental bias might play a key role in determining which properties are observed in a given data set CITATION.
In particular, their findings suggest that Y2H tends to detect key interactions between protein complexes so that Y2H data sets may contain a high proportion of date hubs whereas AP/MS tends to detect interactions within complexes, so that hubs in AP/MS-derived networks are predominantly highly co-expressed with their partners.
This indicates that a possible reason for observing the bimodal hub avPCC distribution CITATION is that the interaction data sets used information that was combined from both of these sources.
Here we compare several yeast interaction data sets and find both widely differing structural properties and a surprisingly low level of overlap.
Finally, as an alternative to the node-based date/party categorisation, we suggest thinking about topological roles in networks by defining measures on links rather than on nodes.
In other words, one can attempt to categorise interactions between proteins rather than the proteins themselves.
We use a well-known measure of link significance known as betweenness centrality CITATION, CITATION and examine its relation to phenomena such as protein co-expression and functional overlap.
Here as well we find little evidence of a significant correlation with expression PCC of the interactors.
However, there seems to be a reasonably strong relation between link betweenness and functional similarity of the interacting proteins, so that link-centric role definitions might have some utility.
In summary, we have examined the proposed division of hubs in the protein interaction network into the date and party categories from several different angles, demonstrating that prior arguments in favour of a date/party dichotomy appear to be susceptible to various kinds of changes in the data and methods used.
Observed differences in network vulnerability to attacks on the two hub types seem to arise from only a small number of particularly important hubs.
These results strengthen the existing evidence against the existence of date and party hubs.
Furthermore, a detailed analysis of network topology, employing the novel perspective of community structure and the roles of hubs within this context, suggests that the picture is more complicated than a simple dichotomy.
Proteins in the interactome show a variety of topological characteristics that appear to lie along a continuum and there does not exist a clear correlation between their location on this continuum and the avPCC of expression of their interaction partners.
On the other hand, investigating link betweenness centralities reveals an interesting relation to the functional linkage of proteins, suggesting that a framework incorporating a more nuanced notion of roles for both nodes and links might provide a better framework for understanding the organisation of the interactome.
Fundamental properties of phasic firing neurons are usually characterized in a noise-free condition.
In the absence of noise, phasic neurons exhibit Class 3 excitability, which is a lack of repetitive firing to steady current injections.
For time-varying inputs, phasic neurons are band-pass filters or slope detectors, because they do not respond to inputs containing exclusively low frequencies or shallow slopes.
However, we show that in noisy conditions, response properties of phasic neuron models are distinctly altered.
Noise enables a phasic model to encode low-frequency inputs that are outside of the response range of the associated deterministic model.
Interestingly, this seemingly stochastic-resonance like effect differs significantly from the classical SR behavior of spiking systems in both the signal-to-noise ratio and the temporal response pattern.
Instead of being most sensitive to the peak of a subthreshold signal, as is typical in a classical SR system, phasic models are most sensitive to the signal's rising and falling phases where the slopes are steep.
This finding is consistent with the fact that there is not an absolute input threshold in terms of amplitude; rather, a response threshold is more properly defined as a stimulus slope/frequency.
We call the encoding of low-frequency signals with noise by phasic models a slope-based SR, because noise can lower or diminish the slope threshold for ramp stimuli.
We demonstrate here similar behaviors in three mechanistic models with Class 3 excitability in the presence of slow-varying noise and we suggest that the slope-based SR is a fundamental behavior associated with general phasic properties rather than with a particular biological mechanism.
Stochastic resonance has been extensively described in both bi-stable and excitable systems and is a classic example of noise enhanced processing CITATION CITATION.
Briefly, SR involves noise facilitating dynamic state transitions or threshold crossing, while permitting phase-locked response to a subthreshold signal.
The interaction of signal, noise, and response nonlinearity maximizes signal encoding at a nonzero value of noise intensity.
Here, we characterize the novel manner in which SR-like phenomena occur in phasic neuron models.
Phasic neurons are characterized by the absence of repetitive firing to steady current injection and low-frequency input, yet show faithful responses to brief pulsatile and high-frequency signals CITATION CITATION.
In a classical SR system, often exemplified by non-phasic neurons, a signal can be detected without noise simply by making its amplitude adequately large.
In contrast, deterministic phasic neurons will not respond to low-frequency input even if the signal amplitude is very large, making phasic neurons an ideal framework to study noise-gated coding CITATION.
We convey our insights by presenting detailed results for a phasic model CITATION that has been widely used in modeling various auditory brainstem phasic neurons CITATION CITATION that perform precise temporal processing and respond only to rapid transients and coincidences.
We then examine other types of phasic models, showing that our findings are general.
The Class 3 excitability, which is commonly used to define phasic responses CITATION, can be created by different cellular mechanisms, such as recruiting a low-threshold potassium current CITATION CITATION, inactivating the sodium current CITATION, CITATION CITATION, or steepening the activation of the high-threshold fast potassium current CITATION.
The phasic neuron model that is our primary focus here is a Hodgkin-Huxley type model with I KLT CITATION.
Combining the same phasic neuron model and whole-cell recordings in the medial superior olive in gerbil, we have previously shown that adding noise enables phasic neurons to detect low-frequency inputs, which, alone, cause no spiking response CITATION.
Although this behavior seems to be consistent with SR, it is fundamentally different from SR for the reasons listed below.
In a classical SR system, adding a small amount of noise to a subthreshold signal facilitates threshold crossing, such as a spike emission upon crossing a membrane voltage threshold.
When the intensity of the noise is properly chosen, the signal can be encoded by eliciting more spikes around the signal's peak and fewer spikes around its trough.
The larger the amplitude of the subthreshold signal, the better the noise-gated encoding becomes; whereas, for supra-threshold signals noise will only degrade signal encoding.
In this sense, we call the classical SR system an amplitude-based stochastic resonance.
However, we discovered that phasic MSO neurons and a phasic neuron model CITATION responded to the rising, falling, or trough phases, depending on the spectrum of the noise, but not to the signal's peak except for very large noise CITATION.
Here, we report that an essential feature of phasic neurons is that response threshold is better defined in terms of the slope rather than the amplitude of the input.
We further show that the noise-gated signal encoding is sensitive to the slope of the signal, as opposed to its amplitude.
For this reason, we label SR-like phenomena in phasic neurons as a slope-based stochastic resonance .
In this study, we highlight the novelty of a phasic neuron's slope-based SR behavior by contrasting it with the qualitatively distinct amplitude-based SR and coding properties of tonic neurons.
To this end, we first compare the dependence of the signal-to-noise ratio on noise intensity obtained from a tonic model to that from a phasic model.
In addition to analyzing SNRs, we pay attention to the temporal firing patterns, which are often overlooked when SR systems are concerned.
Next, we show that the slope-based SR behavior of the phasic model can be reflected in a highly non-monotonic f-I relation with the compelling feature that firing rate falls continuously to zero with increasing I. Such f-I curves have been reported for phasic neurons and models CITATION, CITATION, CITATION.
Finally, the slope threshold in response to a ramp stimulus, as is observed in noise-free conditions CITATION, is reduced or diminished by the addition of noise.
In total, we report that the influence of noise and any noise-assisted coding performed by phasic neurons is significantly distinct from that of tonic neurons.
The occurrence of Class 3 excitability is often associated with outward currents, i.e., I KLT or I KHT CITATION, CITATION.
It is far less realized that strong inactivation of I Na can also create Class 3 excitability CITATION.
We show that the slope-based SR behavior is observed with phasic models created by manipulating the voltage dependency of either I KHT or I Na when the noise spectrum favors low frequencies.
Our present study, complemented by our previous experimental results CITATION, reveals that phasic neurons can have substantially different behaviors in noisy conditions compared to their behaviors in non-noisy conditions.
The conventional views of phasic neurons being band-pass filters or slope detectors, which are all acquired in idealized conditions with no noise present, should be re-evaluated in noisy conditions.
Integrative approaches to studying the coupled dynamics of skeletal muscles with their loads while under neural control have focused largely on questions pertaining to the postural and dynamical stability of animals and humans.
Prior studies have focused on how the central nervous system actively modulates muscle mechanical impedance to generate and stabilize motion and posture.
However, the question of whether muscle impedance properties can be neurally modulated to create favorable mechanical energetics, particularly in the context of periodic tasks, remains open.
Through muscle stiffness tuning, we hypothesize that a pair of antagonist muscles acting against a common load may produce significantly more power synergistically than individually when impedance matching conditions are met between muscle and load.
Since neurally modulated muscle stiffness contributes to the coupled muscle-load stiffness, we further anticipate that power-optimal oscillation frequencies will occur at frequencies greater than the natural frequency of the load.
These hypotheses were evaluated computationally by applying optimal control methods to a bilinear muscle model, and also evaluated through in vitro measurements on frog Plantaris longus muscles acting individually and in pairs upon a mass-spring-damper load.
We find a 7-fold increase in mechanical power when antagonist muscles act synergistically compared to individually at a frequency higher than the load natural frequency.
These observed behaviors are interpreted in the context of resonance tuning and the engineering notion of impedance matching.
These findings suggest that the central nervous system can adopt strategies to harness inherent muscle impedance in relation to external loads to attain favorable mechanical energetics.
The capability of skeletal muscles to deliver mechanical power is key in determining the neuromechanical performance envelope of organisms.
How fast and how far animals run, fly, swim, or jump is clearly limited by the mechanical power delivered by the muscle-tendon units to skeletal and environmental loads.
Therefore, estimating the mechanical energetics of muscles has been of interest in diverse fields such as organismal biomechanics, biomimetic robotics and prosthetics CITATION CITATION .
Many factors influence the neuromechanical performance of organisms, including the dynamics and mechanical properties of muscle actuators, ii skeletal mechanics, iii neural control and iv influence of loads external to the organism.
Integrative approaches have been proposed to capture the interaction of all, or subsets of these factors.
For example, the connection between muscle impedance and neural control has been studied in depth with respect to postural and dynamic stability CITATION, CITATION, locomotory functions CITATION CITATION, manipulation CITATION, CITATION, and other biomechanical tasks CITATION.
In this work, we adhere to the definition of muscle mechanical impedance as the static and dynamic relation between muscle force and imposed stretch CITATION.
Muscle impedance encompasses muscle stiffness, which is the static relation between muscle force stretch only.
In the context of muscle energetics, most investigations focused on experimentally measuring the power output of individual muscles at a range of frequencies, phases and electrical stimulation parameters, and finding maximal power generating capability of muscles under prescribed motion trajectories.
However, the role of muscle-load interaction on output energetics has not been formalized.
The central premise of this work is that the mechanical energetics of a muscle-actuated system cannot be determined in a meaningful manner without considering the coupling of muscle properties, load dynamics and neural activation.
By considering this coupling explicitly, we arrive at phenomena that cannot be captured using standard workloop testing methodologies, including the opportunity to harness muscle-load interaction in an energetically advantageous manner.
Muscle energetics have been characterized under dynamic conditions, both in vitro CITATION and in vivo CITATION, CITATION, CITATION.
In vitro measurements relied almost invariably on the workloop technique CITATION.
In this approach, isolated muscles are subjected to predetermined periodic length variations in time by means of an external motion source.
At a given phase of the imposed oscillation, an electrical stimulus is delivered synchronously, resulting in periodic muscle contractions.
A plot of muscle contractile force versus displacement results in a cyclic workloop, with the integrated area within the loop being a measure of the net muscle work done.
These and similar measurements have been reproduced in the muscle physiology literature for various muscle groups within various organisms CITATION CITATION, and connections between the muscle function and its mechanical energetics have been made CITATION CITATION.
While such measurements provide useful energetic connections with muscle function, the experimental conditions do not capture representative in vivo conditions because motion profiles are imposed on single isolated muscles with no muscle-load interactions CITATION, and without incorporating the effects of antagonist activity.
In vivo measurements, on the other hand, capture all of the above effects in principle, but lack the experimental flexibility of varying load conditions in an unambiguous manner.
Capturing the effect of muscle-load interaction on muscle energetics is critical.
This interaction can be captured by considering the impedance of the muscles in relation to the impedance of the load.
When a group of muscles acts on a common load, as exemplified by an antagonist pair acting on a common load, each muscle forms part of the load borne by the other muscles in its group.
Because muscle impedance is activation dependent, neural control can be used to modulate the effective load observed by each muscle by modulating the impedance of the opposing muscles, thereby offering the opportunity to create favorable impedance conditions that maximize power transfer to the external environmental load.
This is akin to the notion of impedance matching in engineering systems, where the driving source and the load are matched to provide optimal power transfer.
In the context of neuromuscular control, impedance matching can enable groups of muscles to work synergistically to provide significantly higher energetics than the sum of individual muscles.
Consequently, in this investigation we studied the influence of muscle-load interaction on muscle workloop energetics both computationally and experimentally.
We set up a model problem consisting of a mass-spring-damper system actuated by either a single muscle, or a pair of symmetric, antagonist muscles.
The input to the system can modulate the net force exerted by the two muscles as well as the net impedance.
In the context of this problem, we investigated two hypothesis.
Hypothesis 1 states that the power optimal oscillation frequency of a muscle actuated system is greater than the resonance frequency of the load.
This is in direct contrast to an impedance-free actuator where the optimal oscillation frequency occurs exactly at the resonance of the load.
Hypothesis 2 states that a pair of antagonist muscles can work together to produce more power synergistically than individually by margins that cannot be predicted without explicit incorporation of muscle impedance.
We tested these hypotheses both computationally and experimentally.
Our computational approach relied on optimal control solutions to the workloop maximization problem, which was based on a mathematical model of the problem.
The experimental approach relied on in vitro measurements of workloop energetics of electrically-stimulated, frog muscle acting against emulated mass-spring-damper loads.
Most cellular processes depend on intracellular locations and random collisions of individual protein molecules.
To model these processes, we developed algorithms to simulate the diffusion, membrane interactions, and reactions of individual molecules, and implemented these in the Smoldyn program.
Compared to the popular MCell and ChemCell simulators, we found that Smoldyn was in many cases more accurate, more computationally efficient, and easier to use.
Using Smoldyn, we modeled pheromone response system signaling among yeast cells of opposite mating type.
This model showed that secreted Bar1 protease might help a cell identify the fittest mating partner by sharpening the pheromone concentration gradient.
This model involved about 200,000 protein molecules, about 7000 cubic microns of volume, and about 75 minutes of simulated time; it took about 10 hours to run.
Over the next several years, as faster computers become available, Smoldyn will allow researchers to model and explore systems the size of entire bacterial and smaller eukaryotic cells.
One hurdle to the computational modeling of cellular systems is the lack of adequate tools.
If one assumes that molecules inside cells are well-mixed, and that they behave deterministically, then one can model the chemical reactions that cells use to operate with differential equations.
However, these assumptions are frequently inadequate.
Firstly, most cellular processes depend at least to some extent on intracellular spatial organization.
For example, cell signaling systems transmit signals across significant distances within subcellular compartments and across intracellular membranes, such as the nuclear envelope.
Also, cell division systems segregate one cell into two and regulate the partition of molecular components.
Secondly, many cellular outputs exhibit substantial random variation CITATION, which must arise from random differences in molecular collisions.
Examples range from the random switching of swimming Escherichia coli bacteria between so-called running and tumbling states CITATION to cell-to-cell variation in the operation of cell signaling systems CITATION, CITATION.
More generally, stochastic behavior is likely to affect the outcomes of essentially all cellular processes.
Representation of this complexity requires algorithms and programs that model cellular processes with spatial accuracy CITATION, and that model the chemical reactions by which they operate with stochastic detail CITATION .
Computational biologists have pursued four main approaches to simulating biochemical systems with spatial and stochastic detail.
These differ in how they represent space, time, and molecules, which in turn affects the classes of biological systems that they can simulate appropriately.
The spatial Gillespie method CITATION is based on Gillespie's stochastic simulation algorithms CITATION.
It divides the simulation volume into a coarse lattice of subvolumes, each of which contains many molecules of interest.
This method can be computationally efficient because it tracks the total number of individual classes of molecules per subvolume, rather than individual molecules CITATION.
However, the lattice structure it uses to divide space into subvolumes does not work well for realistic membrane shapes, which require special treatment CITATION.
The microscopic lattice method subdivides space into a much finer lattice, so that each volume can contain zero or one molecule.
In this method, molecules diffuse by hopping between sites and can react with molecules in neighboring sites.
It naturally lends itself to studies of oligomerization and complex formation CITATION, and of the effects of macromolecular crowding on reactions CITATION.
It has not found wide use for studying cell-sized processes due to the facts that it has high computational demands and specific lattice structures affect simulated reaction rates differently CITATION, although recent techniques may circumvent these challenges CITATION.
Particle-based methods, the primary focus of this article, are the most widely used spatial stochastic methods CITATION.
These represent individual molecules with point-like particles that diffuse in continuous space over fixed time steps; molecules can react when they collide.
The fact that these models use continuous space makes realistic membrane geometries relatively easy to represent CITATION, avoids lattice-based artifacts, and offers high spatial resolution.
The need to track individual molecules, however, imposes high computational demands, so particle-based methods are about a factor of two slower than spatial Gillespie methods CITATION.
Finally, Green's function reaction dynamics methods CITATION enable especially accurate particle-based simulation.
GFRD methods step the simulation from the exact time of one individual reaction to the exact time of the next.
This makes these methods ideal for systems that can have long delays between individual reactions, but very computationally intensive for most cellular processes CITATION .
The dominant particle based simulators are ChemCell CITATION, MCell CITATION, CITATION, and Smoldyn CITATION, CITATION.
These programs have many common features, but differ in other features and in the quantitative accuracy of their simulations.
Of the three, ChemCell has the fewest features, but is particularly easy to use and is the only simulator that supports both spatial and non-spatial simulations.
MCell, the oldest program CITATION, has been used the most, produces the highest quality graphics CITATION, and has a number of features that make it particularly well suited to simulating cellular processes involved in synaptic transmission CITATION.
Smoldyn is a relative newcomer, but yields the most accurate results and runs the fastest.
Smoldyn also has a number of attributes, listed in Table 2 and below, which make it well suited to modeling a wide range of cellular processes.
This article focuses on the latest version of Smoldyn, Smoldyn 2.1.
Smoldyn 1.0 embodied several algorithms that were based on Smoluchowski reaction dynamics CITATION.
It and subsequent versions were used to investigate a spatial version of the classic Lotka-Volterra chemical oscillator CITATION, diffusion on hair cell membranes CITATION, protein sequestration in dendritic spines CITATION, diffusion in obstructed spaces, and intracellular signaling in E. coli chemotaxis CITATION CITATION.
Smoldyn 2.1 preserves the original focuses on accuracy and efficiency but offers significantly improved functionality.
In particular, it can represent realistic membrane geometries, simulate diffusion of membrane-bound molecules, and accurately simulate a wide variety of molecule-membrane interactions CITATION.
To make it as general a simulator as possible, Smoldyn 2.1 also supports spatial compartments, rule-based reaction network generation CITATION, CITATION, molecules with excluded volume, conformational spread interactions, and over fifty run-time commands for system manipulation and observation.
We anticipate that Smoldyn will be particularly useful for investigating cellular systems, such as signaling, division, and metabolic systems, studying basic biophysical phenomena, such as the effects of macromolecular crowding on molecular diffusion, and helping to quantify microscopy data CITATION, such as diffusion rates investigated by FRAP .
The suprachiasmatic nucleus of the hypothalamus is a multicellular system that drives daily rhythms in mammalian behavior and physiology.
Although the gene regulatory network that produces daily oscillations within individual neurons is well characterized, less is known about the electrophysiology of the SCN cells and how firing rate correlates with circadian gene expression.
We developed a firing rate code model to incorporate known electrophysiological properties of SCN pacemaker cells, including circadian dependent changes in membrane voltage and ion conductances.
Calcium dynamics were included in the model as the putative link between electrical firing and gene expression.
Individual ion currents exhibited oscillatory patterns matching experimental data both in current levels and phase relationships.
VIP and GABA neurotransmitters, which encode synaptic signals across the SCN, were found to play critical roles in daily oscillations of membrane excitability and gene expression.
Blocking various mechanisms of intracellular calcium accumulation by simulated pharmacological agents reproduced experimentally observed trends in firing rate dynamics and core-clock gene transcription.
The intracellular calcium concentration was shown to regulate diverse circadian processes such as firing frequency, gene expression and system periodicity.
The model predicted a direct relationship between firing frequency and gene expression amplitudes, demonstrated the importance of intracellular pathways for single cell behavior and provided a novel multiscale framework which captured characteristics of the SCN at both the electrophysiological and gene regulatory levels.
In mammals many physiological and behavioral responses are subject to internal time-keeping mechanisms or biological clocks.
Daily rhythms are generated by an internal, self-sustained oscillator located in the suprachiasmatic nucleus of the hypothalamus.
The SCN produces autonomous 24h cycles in gene expression and firing frequency from the synchronization of multiple individual oscillatory signals across the network CITATION.
The single cell gene regulatory mechanism involves a number of interlocking positive and negative feedback loops in which the Period gene occupies the central position CITATION.
The circadian modulation of neural firing affects a number of electrophysiological properties of the cell membrane which also fluctuate over the course of the day CITATION .
In vitro studies of SCN slices and cultures have demonstrated diurnal modulation of neural firing CITATION, resting potential CITATION and membrane resistance CITATION, as well as daily oscillations in a number of ionic currents that include the fast delayed rectifier potassium CITATION, L-type calcium CITATION and the large-conductance Ca 2 -activated potassium CITATION, CITATION channels.
Although individual SCN neurons contain molecular feedback loops that drive such rhythms, membrane excitability and synaptic transmission also play significant roles in generating daily oscillations.
Experimental studies in Drosophila have demonstrated the dependence of core clock oscillations on electrical activity, as electrical silencing resulted in abolishment of circadian oscillations of the free-running molecular clock CITATION.
In mammalian organisms a direct association between membrane excitability and core-clock rhythms has also been reported in multiple studies, providing evidence for a positive correlation between Per gene transcription and neural spike frequency output CITATION CITATION.
For example, activation of GABA A receptors via muscimol enhanced inhibitory postsynaptic currents leads to lower firing rates CITATION, CITATION and suppression of Per1 mRNA CITATION.
Another example involves mice deficient in vasoactive intenstinal peptide receptors known to display lower amplitude oscillations of both core clock genes CITATION and neural firing CITATION .
The mechanisms by which the single cells produce synchronized rhythms in neural firing, gene expression and neuropeptide secretion are postulated to involve intracellular second messengers CITATION.
A candidate second messenger that regulates diverse cellular processes is intracellular calcium.
Cytosolic calcium is known to oscillate over the course of the day preceding rhythms in multiple-unit-activity recordings by a mean phase of 4.5 hr CITATION.
Variations in intracellular calcium concentrations have been demonstrated to induce Per1 gene expression by activating the Ca 2 /calmodulin dependent kinase, which in turn phosphorylates the cAMP-response-element binding protein CITATION.
Reduced Ca 2 concentrations have been shown to abolish daily Per1 mRNA oscillations in SCN slices CITATION.
Cytosolic Ca 2 rhythms also affect neural firing frequency, as dampening of Ca 2 oscillations via blockade of calcium release from ryanodine-sensitive pools results in decreased firing activity CITATION, CITATION .
To our knowledge, detailed cell models with molecular descriptions of gene expression and neural firing coupled by intracellular signaling pathways are not currently available for any circadian system.
In a recent study, a Hodgkin-Huxley type model of SCN neurons was developed and shown to reproduce a significant amount of experimentally observed electrophysiological behavior on a millisecond timescale.
While this study facilitated the formulation of our electrophysiology model by providing guidelines for the mathematical representation of a number of relevant ionic currents, our modeling study was distinct due to its focus on the circadian timescale.
In addition to incorporating single-cell electrophysiological properties, our model has accounted for circadian rhythmicity by coupling electrophysiology to daily oscillations in core-clock gene expression and calcium dynamics.
The objective of the present study was to model couplings between the circadian gene-regulatory pathway, cellular electrophysiology and cytosolic calcium dynamics to evaluate the role of extracellular synaptic stimuli on firing rate behavior over a circadian timescale.
The role of distinct intracellular pathways as well as the directionality of information flow along the network nodes was evaluated by analyzing single cell model behavior following the introduction of various external stimuli.
Calcium dynamics, adapted from a published model CITATION, included the contributions of IP3- and ryanodine stores as well as the flux of Ca 2 in and out of the cell membrane.
Our model has demonstrated the dependence of membrane excitability on synaptic input conveyed by VIP and GABA, and predicted reduced neural firing and Per mRNA oscillation amplitudes as well as shorter circadian periods with reduced cytosolic Ca 2 concentrations.
These results suggest a dual effect of signaling pathways instigated by VIP and calcium that potentially operate as coupling agents between the gene regulatory network and the electrophysiology of SCN neurons.
Alpha-helical transmembrane proteins constitute roughly 30 percent of a typical genome and are involved in a wide variety of important biological processes including cell signalling, transport of membrane-impermeable molecules and cell recognition.
Despite significant efforts to predict transmembrane protein topology, comparatively little attention has been directed toward developing a method to pack the helices together.
Here, we present a novel approach to predict lipid exposure, residue contacts, helix-helix interactions and finally the optimal helical packing arrangement of transmembrane proteins.
Using molecular dynamics data, we have trained and cross-validated a support vector machine classifier to predict per residue lipid exposure with 69 percent accuracy.
This information is combined with additional features to train a second SVM to predict residue contacts which are then used to determine helix-helix interaction with up to 65 percent accuracy under stringent cross-validation on a non-redundant test set.
Our method is also able to discriminate native from decoy helical packing arrangements with up to 70 percent accuracy.
Finally, we employ a force-directed algorithm to construct the optimal helical packing arrangement which demonstrates success for proteins containing up to 13 transmembrane helices.
This software is freely available as source code from LINK.
Alpha-helical transmembrane proteins constitute roughly 30 percent of the proteins encoded in a typical genome and are involved in a wide variety of important biological processes including cell signalling, transport of membrane-impermeable molecules and cell recognition.
Many are also prime drug targets, and it has been estimated that more than half of all drugs currently on the market target membrane proteins CITATION.
Despite significant efforts to predict TM protein topology CITATION, CITATION, CITATION, comparatively little attention has been directed toward developing a method to pack the helices together.
Since the membrane-spanning region is predominantly composed of alpha-helices with a common alignment, this task should in principle be easier than predicting the fold of globular proteins as the longitudinal constraints of helix packing mostly reduces the solution space from three dimensions to two.
However, topologies consisting of large numbers of TM helices as well as structural features including re-entrant, tilted and kinked helices render simple approaches that may work for regularly packed proteins unable to predict the diverse packing arrangements now present in structural databases.
Early attempts to predict TM protein folds were based on sequence similarity to proteins with a known three-dimensional structure, using statistically derived environmental preference parameters combined with experimentally determined features CITATION.
Another method calculated amino acid substitution tables for residues in membrane proteins where the side chain was accessible to lipid.
By comparing observed substitutions obtained from sequence alignments of TM regions, accessibility of residues to the lipid could be predicted.
In combination with a Fourier transform method to detect alpha-helices, the buried and exposed faces could then be discriminated and the presence of charged residues used to construct a three-dimensional model CITATION.
Other methods also made use of exposed surface prediction to allocate helix positions, in combination with an existing framework for globular protein structure prediction involving the combinatorial enumeration of windings over a predefined architecture followed by the selection of preferred folds CITATION.
However, these methods were only suitable for 7 TM helix bundles such as rhodopsin and were unsuitable for other topologies.
More recently, a modified version of the fragment-based protein tertiary structure prediction method FRAGFOLD CITATION was modified to model TM proteins.
FRAGFOLD is based on the assembly of super-secondary structural fragments using a simulated annealing algorithm in order to narrow the search of conformational space by pre-selecting fragments from a library of highly resolved protein structures.
FILM CITATION added a membrane potential to the FRAGFOLD energy terms which was derived from the statistical analysis of a data set of TM proteins with experimentally defined topologies.
Results obtained by applying the method to small membrane proteins of known three-dimensional structure showed it could predict both helix topology and conformation at a reasonable accuracy level.
Despite these good results, the combinatorial complexity of such ab initio protein folding methods means that it is unfeasible to use such approaches for large TM structures, many of which are longer than 150 residues.
Modification of another globular protein ab initio modelling program, ROSETTA CITATION, added an energy function that described membrane intra-protein interactions at atomic level and membrane protein/lipid interactions implicitly, while treating hydrogen bonds explicitly CITATION.
Results suggest that the model captures the essential physical properties that govern the solvation and stability of TM proteins, allowing the structures of small protein domains, up to 150 residues, to be predicted successfully to a resolution of less than 2.5.
A recent enhancement of the algorithm demonstrated that by constraining helix-helix packing arrangements at particular positions based on local sequence-structure correlations for each helix of the interface independently, TM proteins with more complex topologies could be modelled to within 4 of the native structure CITATION .
The prediction of helix-helix interactions, derived from residue contacts and topology, has only recently been investigated in TM proteins due to the relative paucity of TM protein crystal structures.
In contrast, a number of globular protein contact predictors exist based on a variety of machine learning algorithms CITATION, CITATION, and contact prediction has also been used to assess globular protein models submitted to the Critical Assessment of Structure Prediction experiment CITATION.
However, analysis has shown that such globular proteins contact predictors perform poorly when applied to TM proteins, most likely due to differences between TM and globular interaction motifs CITATION.
A number of studies have identified structural and sequence motifs recurring frequently during helix helix interaction in TM proteins.
One investigation analysed interacting helical pairs according to their three-dimensional similarity, allowing three quarters of pairs to be grouped into one of five tightly clustered motifs CITATION.
The largest of these consisted of an anti-parallel motif with left-handed packing angles, stabilised by the packing of small side chains every seven residues, while right-handed parallel and anti-parallel structures showed a similar tendency though spaced at four-residue intervals.
Another study identified a specific aromatic pattern, aromatic-XX-aromatic, which was demonstrated to stabilise helix-helix interactions during assembly CITATION, while others include the GXXXG motif found in glycophorin A CITATION, heptad motifs of leucine residues CITATION, and polar residues through formation of hydrogen bonds CITATION .
The discovery of these recurring motifs, and the likelihood that there are more as yet undiscovered, suggests predictability by a generalised pattern search strategy.
Recently, two methods have been developed that attempt to predict residue contacts and helix-helix interaction.
TMHcon CITATION uses a neural network in combination with profile data, residue co-evolution information, predicted lipid exposure using the LIPS method CITATION, and a number of TM protein specific features, such as residue position within the TM helix, in order to predict helix-helix interaction.
TMhit CITATION uses a two-level hierarchical approach in combination with a support vector machine classifier.
The first level discriminates between contacts and non-contacts on a per residue basis, before the second level determines the structure of the contact map from all possible pairs of predicted contact residues therefore avoiding the high computational cost incurred by the quadratic growth of residue pair prediction.
Here, we present a novel method to predict lipid exposure, residue contacts, helix-helix interactions and finally the optimal helical packing arrangements of TM proteins.
Using molecular dynamics data to label residues potentially exposed to lipid, we have trained and cross-validated a SVM classifier to predict per residue lipid exposure with 69 percent accuracy.
This information is combined with PSI-BLAST profile data and a variety of sequence-based features to train an additional SVM to predict residue contacts.
Combining these results with a priori topology information, we are able to predict helix-helix interaction with up to 65 percent accuracy under stringent cross-validation on a non-redundant test set of 74 protein chains.
We then tested the ability of the method to discriminate native from decoy helical packing arrangement using a decoy set of 2811 structures.
By comparing our predictions with the test set, we were able to identify the native packing arrangement with up to 70 percent accuracy.
All these performance metrics represents significant improvements over existing methods.
In order to visualise the global packing arrangement, we adopted a graph-based approach.
By employing a force-directed algorithm, the method attempts to minimise edge crossing while maintaining uniform edge length, attributes common in native structures.
Finally, a genetic algorithm is used to rotate helices in order to prevent residue contacts occurring across the longitudinal helix axis.
Rates of hospital-acquired infections, such as methicillin-resistant Staphylococcus aureus, are increasingly used as quality indicators for hospital hygiene.
Alternatively, these rates may vary between hospitals, because hospitals differ in admission and referral of potentially colonized patients.
We assessed if different referral patterns between hospitals in health care networks can influence rates of hospital-acquired infections like MRSA.
We used the Dutch medical registration of 2004 to measure the connectedness between hospitals.
This allowed us to reconstruct the network of hospitals in the Netherlands.
We used mathematical models to assess the effect of different patient referral patterns on the potential spread of hospital-acquired infections between hospitals, and between categories of hospitals.
University hospitals have a higher number of shared patients than teaching or general hospitals, and are therefore more likely to be among the first to receive colonized patients.
Moreover, as the network is directional towards university hospitals, they have a higher prevalence, even when infection control measures are equally effective in all hospitals.
Patient referral patterns have a profound effect on the spread of health care-associated infections like hospital-acquired MRSA.
The MRSA prevalence therefore differs between hospitals with the position of each hospital within the health care network.
Any comparison of MRSA rates between hospitals, as a benchmark for hospital hygiene, should therefore take the position of a hospital within the network into account.
Pathogens that typically cause hospital-acquired infections have an opportunistic nature.
These organisms are usually part of the normal bacterial flora of humans and only cause disease when reaching body sites that are normally free from bacterial colonization e.g. when anatomical barriers are breached due to trauma or medical/surgical interventions.
For this reason, severe problems with nosocomial pathogens are mainly seen in the very young and elderly and most frequently in institutions such as hospitals and long-term care facilities where patients are treated for acute or chronic conditions.
Methicillin-resistant Staphylococcus aureus is an antimicrobial resistant variant of S. aureus, a common bacteria frequently colonizing healthy humans and animals.
Emergence of MRSA is due to the acquisition of a large DNA fragment, which seems to be rare CITATION, CITATION.
The expansion of a limited number of MRSA clones that characterizes the current epidemic in hospitals worldwide is therefore believed to be the result of between patient transmission and only to a minor extent due to the de novo emergence in patients exposed to antibiotics.
MRSA has therefore become the marker with which the success or failure of hospital infection control CITATION .
The prevalence of the MRSA differs considerably within and between countries CITATION, CITATION.
Currently about 30 percent of the S. aureus causing bloodstream infections in the UK is resistant to methicillin, against only 1 percent in the Netherlands and Scandinavian countries CITATION.
Although in high endemic countries MRSA infections are frequent in all hospitals, the proportions are highest in large teaching hospitals CITATION, CITATION, which also report the highest frequency of newly occurring MRSA clones CITATION CITATION.
The severity of underlying medical condition of the patients, as well as higher antibiotic use and frequency of invasive procedures have been proposed as the main reasons for this difference CITATION .
Patients can carry MRSA, asymptomatically, for a long time CITATION.
When readmitted, they may introduce the pathogen acquired during a previous admission into a new hospital CITATION.
Failure of one hospital's infection control measures can therefore affect the prevalence in hospitals with which it shares patients CITATION.
Patients are referred to hospitals at different rates depending on the function of hospitals within the health-care system, which likely affect the prevalence at different institutions.
These referral patterns might therefore offer an explanation for high MRSA incidence in hospitals of the tertiary referral level CITATION.
But can referral patterns account for differences in spread between hospitals, and for differences in observed prevalence?
To answer these questions, we have been mapping the health care network based on a large national medical registry, and evaluated the occurrence of hospital-acquired infections in different care categories under simulated epidemic conditions.
The tracing of potentially infectious contacts has become an important part of the control strategy for many infectious diseases, from early cases of novel infections to endemic sexually transmitted infections.
Here, we make use of mathematical models to consider the case of partner notification for sexually transmitted infection, however these models are sufficiently simple to allow more general conclusions to be drawn.
We show that, when contact network structure is considered in addition to contact tracing, standard mass action models are generally inadequate.
To consider the impact of mutual contacts we develop an improvement to existing pairwise network models, which we use to demonstrate that ceteris paribus, clustering improves the efficacy of contact tracing for a large region of parameter space.
This result is sometimes reversed, however, for the case of highly effective contact tracing.
We also develop stochastic simulations for comparison, using simple re-wiring methods that allow the generation of appropriate comparator networks.
In this way we contribute to the general theory of network-based interventions against infectious disease.
Modelling has become a central tool in understanding the epidemiology of infectious disease, and designing control strategies.
One control method, contact tracing, has been considered in a large number of disease contexts.
These include the 2003 SARS pandemic CITATION, CITATION, the 2001 UK FMD epidemic CITATION CITATION, contingency planning for deliberate release of smallpox CITATION, CITATION, and control of sexually transmitted infections CITATION CITATION.
A particular benefit of tracing is that it allows targeting of control, at the cost of effort spent on finding the individuals at risk.
Since contact tracing takes place as a process over the network of interactions between hosts, it is natural to consider network-based models of this process.
Theoretical work has so far dealt with contact tracing as a branching process CITATION, through modifications to mean-field equations CITATION, pairwise approximations CITATION and simulation CITATION.
This work means that the implications of heterogeneous numbers of contacts for the efficacy of contact tracing are reasonably well understood.
For the case of clustering, due to the analytical challenge posed by the existence of short closed loops in the contact network, it has generally been more difficult to make similar progress.
Existing theoretical work has therefore either been restricted to the limiting case of clump structured populations, with all clustering due to completely connected cliques CITATION, or else simulation on exemplar networks CITATION, CITATION, CITATION .
In this work, we derive an improved triple closure for clustered pairwise models that removes two significant problems with existing closure regimes, and use this to make a systematic investigation of the impact of clustering on the efficacy of contact tracing, keeping other network and epidemiological parameters constant as appropriate.
We find that, for many parameter choices, there are intuitive explanations, borne out by modelling, for the increased impact of contact tracing as clustering increases.
This is not, however, a completely general result, meaning that the full implications of clustering for the efficacy of contact tracing are subtle and should be the subject of case by case investigation.
We perform our analysis within the SIS paradigm, meaning that while some of our terminology will be general to all infectious disease epidemiology, other statements will be geared towards the modelling of sexually transmitted infections where recovery/treatment does not confer lasting immunity.
Accumulated experimental observations demonstrate that protein stability is often preserved upon conservative point mutation.
In contrast, less is known about the effects of large sequence or structure changes on the stability of a particular fold.
Almost completely unknown is the degree to which stability of different regions of a protein is generally preserved throughout evolution.
In this work, these questions are addressed through thermodynamic analysis of a large representative sample of protein fold space based on remote, yet accepted, homology.
More than 3,000 proteins were computationally analyzed using the structural-thermodynamic algorithm COREX/BEST.
Estimated position-specific stability and its component enthalpy and entropy were quantitatively compared between all proteins in the sample according to all-vs.-all pairwise structural alignment.
It was discovered that the local stabilities of homologous pairs were significantly more correlated than those of non-homologous pairs, indicating that local stability was indeed generally conserved throughout evolution.
However, the position-specific enthalpy and entropy underlying stability were less correlated, suggesting that the overall regional stability of a protein was more important than the thermodynamic mechanism utilized to achieve that stability.
Finally, two different types of statistically exceptional evolutionary structure-thermodynamic relationships were noted.
First, many homologous proteins contained regions of similar thermodynamics despite localized structure change, suggesting a thermodynamic mechanism enabling evolutionary fold change.
Second, some homologous proteins with extremely similar structures nonetheless exhibited different local stabilities, a phenomenon previously observed experimentally in this laboratory.
These two observations, in conjunction with the principal conclusion that homologous proteins generally conserved local stability, may provide guidance for a future thermodynamically informed classification of protein homology.
Protein structure and function are ultimately determined by thermodynamics.
For example, Anfinsen's seminal work CITATION demonstrated that the native state of a protein exists at a minimum in Gibbs free energy of stability under physiological conditions.
Binding and catalysis are also governed by free energy: the sign and magnitude of the free energy change of each functional reaction controls the reaction's direction and equilibrium extent, respectively CITATION, CITATION .
Gibbs free energy results from the summed, often opposing, contributions of enthalpy and entropy : G H T S. Generally, in the case of proteins, changes in free energy are small as compared to the underlying enthalpic or entropic changes CITATION.
Reactions can be dominated by either enthalpy or entropy, but it is most often the case that a sometimes delicate balance between enthalpy and entropy controls protein structure and function.
Unfortunately for the goal of thermodynamic characterization of protein folds, each of these quantities can be challenging to accurately predict.
While enthalpy can be rationalized in terms of information derived from atomic coordinates CITATION, entropy is harder to estimate, frequently requiring knowledge not apparent from a single structure, such as information about the conformational degeneracy of the protein CITATION CITATION.
Equally as challenging is the task of developing a robust analysis that reports the position-specific stability within the protein, rather than reporting either: the energetic contribution of a residue or the stability of a protein as a whole .
Due in part to the inherent difficulty of accurately computing global and local enthalpy, entropy, and free energy, all protein structure classification strategies of which we are aware do not incorporate thermodynamic information.
It is our hypothesis that this theoretical omission limits the complete understanding of protein fold space.
There may also be practical consequences to such an omission.
For example, it is possible that thermodynamic information, as a protein observable independent of sequence or structure CITATION, could improve computational tools for sequence alignment, fold recognition CITATION, or homology detection, thereby clarifying discrepancies in existing classification schemes that are based on only sequence and structure.
Thermodynamic information may also yield new understanding, not available from current schemes, about evolutionary sequence, structure, and functional relationships CITATION.
One particularly important and as yet unanswered question is the degree to which protein stability and its components are conserved during fold evolution: does the concept of thermodynamic homology meaningfully exist beyond conservative point mutations?
As a step towards integration of thermodynamic information into existing protein classification schemes, the local free energy of stability, enthalpy, and entropy are here computed for a large representative database of protein domains using the previously described COREX/BEST algorithm CITATION CITATION.
Importantly, the diverse proteins studied have accepted evolutionary relationships CITATION and are expertly curated CITATION such that any homologs are remote.
Thus, by experimental design, trivial comparisons between the thermodynamics of closely related proteins are explicitly excluded from this analysis.
The central aim of this work is to assess the degree of thermodynamic conservation among remotely homologous protein domains.
Three findings relating thermodynamics to protein sequence and structure are reported.
First, in accordance with previous work CITATION, it is confirmed that homologous proteins exhibit correlated thermodynamic information.
Second, enthalpy and entropy are less correlated than stability, suggesting that homologous sequence differences result in enthalpic and entropic changes that largely balance to preserve the local stability of an evolved protein as compared to an ancestral one.
Third, based on manual inspection of structural and thermodynamic alignments of homologous and non-homologous pairs of proteins, an organizational framework is postulated to guide the future integration of COREX/BEST thermodynamic information into theories of protein fold evolution.
A longstanding question in molecular biology is the extent to which the behavior of macromolecules observed in vitro accurately reflects their behavior in vivo.
A number of sophisticated experimental techniques now allow the behavior of individual types of macromolecule to be studied directly in vivo; none, however, allow a wide range of molecule types to be observed simultaneously.
In order to tackle this issue we have adopted a computational perspective, and, having selected the model prokaryote Escherichia coli as a test system, have assembled an atomically detailed model of its cytoplasmic environment that includes 50 of the most abundant types of macromolecules at experimentally measured concentrations.
Brownian dynamics simulations of the cytoplasm model have been calibrated to reproduce the translational diffusion coefficients of Green Fluorescent Protein observed in vivo, and snapshots of the simulation trajectories have been used to compute the cytoplasm's effects on the thermodynamics of protein folding, association and aggregation events.
The simulation model successfully describes the relative thermodynamic stabilities of proteins measured in E. coli, and shows that effects additional to the commonly cited crowding effect must be included in attempts to understand macromolecular behavior in vivo.
While reductionist biophysical studies continue to contribute important insights into the properties and functions of biological macromolecules, research attention is increasingly being directed at uncovering the extent to which behavior observed in vitro is likely to reflect that occurring in vivo CITATION, CITATION.
In a physiological setting, all biomolecules must inevitably experience non-specific, unintended interactions with the intracellular milieu and there are good theoretical reasons to expect that, even if such interactions are only steric in nature, significant alterations in macromolecular folding and association equilibria may result CITATION, CITATION.
In order to allow macromolecules to be directly interrogated in vivo therefore, a number of important developments have been made in the experimental fields of hydrogen exchange CITATION, nuclear magnetic resonance CITATION, CITATION, and fluorescence spectroscopies CITATION CITATION .
An alternative to the use of experimental techniques is to assemble a molecular model of an intracellular environment in silico and to use molecular simulation techniques to explore its behavior; if such a model could be shown to be realistic and that is a big if it would have the important advantage of allowing the simultaneous, direct observation of all molecules in the system.
In fact, at least two simulation studies that attempt to model the bacterial cytoplasm have already been reported CITATION, CITATION, producing a number of intriguing results.
Both of these previous studies, however, modeled all cytoplasmic molecules as spheres and it is perhaps to be anticipated therefore that simulations that include structurally detailed macromolecular models might lead to additional insights.
In pursuit of this strategy, we have chosen the gram-negative prokaryote Escherichia coli as a test system, combining quantitative proteomic CITATION and high-resolution structural data CITATION to build a first structurally detailed molecular model of the bacterial cytoplasm.
A central challenge in computational modeling of biological systems is the determination of the model parameters.
Typically, only a fraction of the parameters are experimentally measured, while the rest are often fitted.
The fitting process is usually based on experimental time course measurements of observables, which are used to assign parameter values that minimize some measure of the error between these measurements and the corresponding model prediction.
The measurements, which can come from immunoblotting assays, fluorescent markers, etc., tend to be very noisy and taken at a limited number of time points.
In this work we present a new approach to the problem of parameter selection of biological models.
We show how one can use a dynamic recursive estimator, known as extended Kalman filter, to arrive at estimates of the model parameters.
The proposed method follows.
First, we use a variation of the Kalman filter that is particularly well suited to biological applications to obtain a first guess for the unknown parameters.
Secondly, we employ an a posteriori identifiability test to check the reliability of the estimates.
Finally, we solve an optimization problem to refine the first guess in case it should not be accurate enough.
The final estimates are guaranteed to be statistically consistent with the measurements.
Furthermore, we show how the same tools can be used to discriminate among alternate models of the same biological process.
We demonstrate these ideas by applying our methods to two examples, namely a model of the heat shock response in E. coli, and a model of a synthetic gene regulation system.
The methods presented are quite general and may be applied to a wide class of biological systems where noisy measurements are used for parameter estimation or model selection.
Many biological processes are modeled using ordinary differential equations that describe the evolution over time of certain quantities of interest.
At the molecular level, the variables considered in the models often represent concentrations of chemical species, such as proteins and mRNA.
Once the pathway structure is known, the corresponding equations are relatively easy to write down using widely accepted kinetic laws, such as the law of mass action or the Michaelis-Menten law.
In general the equations will depend on several parameters.
Some of them, such as reaction rates, and production and decay coefficients have a physical meaning.
Others might come from approximations or reductions that are justified by the structure of the system and, therefore, they might have no direct biological or biochemical interpretation.
In both cases, most of the parameters are unknown.
While sometimes it is feasible to measure them experimentally, in many cases this is very hard, expensive, time consuming, or even impossible.
However, it is usually possible to measure some of the other variables involved in the models using PCR, immunoblotting assays, fluorescent markers, and the like.
For these reasons, the problem of parameter estimation, that is the indirect determination of the unknown parameters from measurements of other quantities, is a key issue in computational and systems biology.
The knowledge of the parameter values is crucial whenever one wants to obtain quantitative, or even qualitative information from the models CITATION, CITATION .
In the last fifteen years a lot of attention has been given to this problem in the systems biology community.
Much research has been conducted on the applications to computational biology models of several optimization techniques, such as linear and nonlinear least-squares fitting CITATION, simulated annealing CITATION, genetic algorithms CITATION, and evolutionary computation CITATION, CITATION.
The latter is suggested as the method of choice for large parameter estimation problems CITATION.
Starting with a suitable initial guess, optimization methods search more or less exhaustively the parameter space in the attempt to minimize a certain cost function.
This is usually defined as the error in some sense between the output of the model and the data that comes from the experiments.
The result is the set of parameters that produce the best fit between simulations and experimental data.
One of the main problems associated with optimization methods is that they tend to be computationally expensive and may not perform well if the noise in the measurements is significant.
Considerable interested has also been raised by Bayesian methods CITATION, which can extract information from noisy or uncertain data.
This includes both measurement noise and intrinsic noise, which is well known to play an important role in chemical kinetics when species are present in low copy numbers CITATION.
The main advantage of these methods is their ability to infer the whole probability distributions of the parameters, rather than just a point estimate.
Also, they can handle estimation of stochastic systems with no substantial modification to the algorithms CITATION.
The main obstacle to their application is computational, since analytical approaches are not feasible for non-trivial problems and numerical solutions are also challenging due to the need to solve high-dimensional integration problems.
Nonetheless, the most recent advancements in Bayesian computation, such as Markov chain Monte Carlo techniques CITATION, ensemble methods CITATION, CITATION, and sequential Monte Carlo methods that don't require likelihoods CITATION, CITATION have been successfully applied to biological systems, usually in the case of lower-dimensional problems and/or availability of a relatively high number of data samples.
Maximum-likelihood estimation CITATION, CITATION has also been extensively applied.
More recently, parameter estimation for computational biology models has been tackled in the framework of control theory by using state observers.
These algorithms were originally developed for the problem of state estimation, in which one seeks to estimate the time evolution of the unobserved components of the state of a dynamical system.
The controls literature on this subject is vast, but in the context of biological or biochemical systems the classically used approaches include Luenberger-like CITATION, Kalman filter based, CITATION CITATION, and high-gain observers CITATION.
Other methods have been developed by exploiting the special structure of specific problems CITATION.
State observers can be employed for parameter estimation using the technique of state extension, in which parameters are transformed into states by suitably expanding the system under study CITATION CITATION.
In this context extended Kalman filtering CITATION, CITATION and unscented Kalman filtering CITATION methods have been applied as well.
When the number of unknown parameters is very large, it is often impossible to find a unique solution to this problem.
In this case, one finds several sets of parameters, or ranges of values, that are all equally likely to give a good fit.
This situation is usually referred to as the model being non identifiable, and it is the one that's most commonly encountered in practice.
Furthermore, it is known that a large class of systems biology models display sensitivities to the parameter values that are roughly evenly distributed over many orders of magnitude.
Such sloppiness has been suggested as a factor that makes parameter estimation difficult CITATION.
These and similar results indicate that the search for the exact individual values of the parameters is a hopeless task in most cases CITATION.
However, it is also known that even if the estimation process is not able to tightly constrain any of the parameter values, the models can still be able to yield significant quantitative predictions CITATION .
The purpose of the present contribution is to extend the results on parameter estimation by Kalman filtering by introducing a procedure that can be applied to large parameter spaces, can handle sparse and noisy data, and provides an evaluation of the statistical significance of the computed estimates.
To achieve this goal, we introduce a constrained hybrid extended Kalman filtering algorithm, together with a measure of accuracy of the estimation process based on a FORMULA variance test.
Furthermore, we show how these techniques together can be also used to address the problem of model selection, in which one has to pick the most plausible model for a given process among a list of candidates.
A distinctive feature of this approach is the ability to use information about the statistics of the measurement noise in order to ensure that the estimated parameters are statistically consistent with the available experimental data.
The rest of this paper is organized as follows.
In the Methods Section we introduce all the theory associated with our procedure, namely the constrained hybrid extended Kalman filter, the accuracy measure and its use in estimation refinement, and the application to the model selection problem.
In the Results Section we demonstrate the procedure on two examples drawn from molecular biology.
Finally, in the Discussion Section we summarize the new procedure, we give some additional remarks, and we point out how these findings will be of immediate interest to researchers in computational biology, who use experimental data to construct dynamical models of biological phenomena.
Perception is fundamentally underconstrained because different combinations of object properties can generate the same sensory information.
To disambiguate sensory information into estimates of scene properties, our brains incorporate prior knowledge and additional auxiliary sensory information to constrain perceptual interpretations.
For example, knowing the distance to an object helps in perceiving its size.
The literature contains few demonstrations of the use of prior knowledge and auxiliary information in combined visual and haptic disambiguation and almost no examination of haptic disambiguation of vision beyond bistable stimuli.
Previous studies have reported humans integrate multiple unambiguous sensations to perceive single, continuous object properties, like size or position.
Here we test whether humans use visual and haptic information, individually and jointly, to disambiguate size from distance.
We presented participants with a ball moving in depth with a changing diameter.
Because no unambiguous distance information is available under monocular viewing, participants rely on prior assumptions about the ball's distance to disambiguate their -size percept.
Presenting auxiliary binocular and/or haptic distance information augments participants' prior distance assumptions and improves their size judgment accuracy though binocular cues were trusted more than haptic.
Our results suggest both visual and haptic distance information disambiguate size perception, and we interpret these results in the context of probabilistic perceptual reasoning.
For well over a century CITATION, CITATION psychologists have considered the question of how the brain uses visual angle sensations to make judgments of an object's size, overcoming the confounding effect of its distance - but the topic remains unsettled.
Holway and Boring CITATION found that when strong sensations of an object's distance were made available, human size matching performance at different distances was high, but when distance sensations were removed human perception of an object's size was erroneously dominated its visual angle.
Epstein et al. CITATION surveyed literature regarding the size-distance invariance hypothesis CITATION, which holds that retinal visual angle constrains perception of an object's size and distance such that their ratio holds a constant value, and concluded the size-distance invariance hypothesis was subject to a variety of failures.
Several studies attributed participants' mistaken size perceptions CITATION, CITATION CITATION to misjudgments of physical distance, while others point out that specific experimental design choices and task demands contribute to reported failures of size constancy CITATION CITATION.
Recently Combe and Wexler CITATION reported that size constancy is stronger when the relative distance between observer and object varies due to observer motion, than when due to object motion.
Such findings highlight the unsettled state of current empirical knowledge about human size and distance perception, which is exacerbated by the absence of a unified theoretical account for normative size/distance perception.
We hypothesize that the brain makes size inferences by incorporating multiple sensations based on knowledge of their generative relationship with physical environment properties, and that failures like inaccuracy and systematic biases are due to poverty, unreliability, and/or mistrust, of observed sensations.
Our experiments tackle the issue of how the brain incorporates distance information, in particular binocular and haptic, to jointly perceive of how an object's size is changing.
Size-change perception, which surprisingly has not been studied in the size/distance perception literature, bears close similarity to static size perception because size-change judgments based on retinal image size are ambiguous if information about the object's motion-in-depth is unknown.
However when auxiliary sensations indicating motion-in-depth are available, an observer may rule out size-change/motion combinations that are inconsistent with the auxiliary sensations, and unambiguously infer whether the object is inflating or deflating.
We predicted that despite the inherent novelty of the stimuli, participants' abilities to discriminate whether an object inflated or deflated would depend on the availability and quality of information about its motion-in-depth.
Because binocular and haptic sensations provide information about depth, we predicted that they would each be incorporated for improving size-change judgments.
Thus our study answers two key questions: Does the brain use distance-change information for size-change perception?
What are the roles of binocular and haptic distance-change information?
Our size-change discrimination task presented participants with an object that either inflated or deflated while simultaneously either approaching or receding, and asked them to discriminate whether it inflated or deflated.
Most static size perception tasks use matching paradigms, and our task was advantageous because it allowed us to present a single stimulus per trial, and avoid issues regarding relative comparison of pairs of stimuli.
We provided participants with different types of auxiliary motion-in-depth information, binocular CITATION, CITATION, CITATION and haptic CITATION, CITATION, both in isolation and simultaneously, and examined their inflation/deflation judgments to evaluate how auxiliary distance information influenced perceived size-change.
Evidence for the use of binocular and haptic distance information in size-change perception has not been reported, and previous studies of cue integration CITATION suggest the brain combines haptic and binocular information in proportion to its reliability to jointly improve spatial perception.
We found that when distance-change information was absent, participants' size-change judgments closely matched object's image size-change.
However, when we provided participants with auxiliary distance-change sensations, participants incorporated this additional information to form more accurate size percepts that were consistent with both monocular and auxiliary sensations.
Moreover when both binocular and haptic information was presented, most participants showed greater disambiguation of size than when either was presented in isolation.
These results suggest size-change perception uses knowledge of how multi-modal size and distance sensations are related to interpret the scene.
We interpret these findings in the framework of probabilistic perceptual inference, in which available sensations are combined according to their relationship to scene properties and their respective reliabilities CITATION, CITATION .
Cytokines such as TNF and FASL can trigger death or survival depending on cell lines and cellular conditions.
The mechanistic details of how a cell chooses among these cell fates are still unclear.
The understanding of these processes is important since they are altered in many diseases, including cancer and AIDS.
Using a discrete modelling formalism, we present a mathematical model of cell fate decision recapitulating and integrating the most consistent facts extracted from the literature.
This model provides a generic high-level view of the interplays between NF B pro-survival pathway, RIP1-dependent necrosis, and the apoptosis pathway in response to death receptor-mediated signals.
Wild type simulations demonstrate robust segregation of cellular responses to receptor engagement.
Model simulations recapitulate documented phenotypes of protein knockdowns and enable the prediction of the effects of novel knockdowns.
In silico experiments simulate the outcomes following ligand removal at different stages, and suggest experimental approaches to further validate and specialise the model for particular cell types.
We also propose a reduced conceptual model implementing the logic of the decision process.
This analysis gives specific predictions regarding cross-talks between the three pathways, as well as the transient role of RIP1 protein in necrosis, and confirms the phenotypes of novel perturbations.
Our wild type and mutant simulations provide novel insights to restore apoptosis in defective cells.
The model analysis expands our understanding of how cell fate decision is made.
Moreover, our current model can be used to assess contradictory or controversial data from the literature.
Ultimately, it constitutes a valuable reasoning tool to delineate novel experiments.
Engagement of TNF or FAS receptors can trigger cell death by apoptosis or necrosis, or yet lead to the activation of pro-survival signalling pathway, such as NF B. Apoptosis represents a tightly controlled mechanism of cell death that is triggered by internal or external death signals or stresses.
This mechanism involves a sequence of biochemical and morphological changes resulting in the vacuolisation of cellular content, followed by its phagocyte-mediated elimination.
This physiological process regulates cell homeostasis, development, and clearance of damaged, virus-infected or cancer cells.
In contrast, pathological necrosis results in plasma membrane disruption and release of intracellular content that can trigger inflammation in the neighbouring tissues.
Long seen as an accidental cell death, necrosis also appears regulated and possibly involved in the clearance of virus-infected or cancer cells that escaped apoptosis CITATION .
Dynamical modelling of the regulatory network controlling apoptosis, non-apoptotic cell death and survival pathways could help identify how and under which conditions the cell chooses between different types of cellular deaths or survival.
Moreover, modelling could suggest ways to re-establish the apoptotic death when it is altered, or yet to trigger necrosis in apoptosis-resistant cells.
The decision process involves several signalling pathways, as well as multiple positive and negative regulatory circuits.
Mathematical modelling provides a rigorous integrative approach to understand and analyse the dynamical behaviours of such complex systems.
Published models of cell death control usually focus on one death pathway only, such as the apoptotic extrinsic or intrinsic pathways CITATION, CITATION, CITATION.
A few studies integrate both pathways CITATION, some show that the concentration of specific components contribute to the decision between death and survival CITATION, CITATION while other studies investigate the balance between proliferation, survival or apoptosis in specific cell types along with the role of key components in these pathways CITATION, but no mathematical models including necrosis are available yet.
Moreover, we still lack models properly demonstrating how cellular conditions determine the choice between necrosis, apoptosis and survival, and how and to what extent conversions are allowed between these fates.
Our study aims at identifying determinants of this cell fate decision process.
The three main phenotypes considered are apoptosis, non-apoptotic cell death and survival.
Although the pathways leading to these three phenotypes are highly intertwined, we first describe them separately hereafter, concentrating on the players we chose to include in each pathway.
Summarised in Figure 1A, this description does not intend to be exhaustive, but rather aims at covering the most established processes participating in cell fate decision.
Only the apoptotic caspase-dependent pathway downstream of FAS and TNF receptors is considered here.
Upon engagement by their ligands and in the presence of FADD, a specific Death Inducible Signalling Complex forms and recruits pro-caspase-8.
This leads to the cleavage and activation of caspase-8.
In the so-called type II cells, CASP8 triggers the intrinsic or mitochondria-dependent apoptotic pathway, which also responds to DNA damage directly through the p53-mediated chain of events.
CASP8 cleaves the BH3-only protein BID, which can then translocate to the mitochondria outer membrane.
There, BID competes with anti-apoptotic BH3 family members such as BCL2 for interaction with the proteins BAX or BAK.
Consequently, oligomerisation of BAX results in mitochondrial outer membrane permeabilisation and the release of pro-apoptotic factors.
Once released to the cytosol, cytochrome c interacts with APAF1, recruiting pro-caspase-9.
In presence of dATP, this enables the assembly of the apoptosome complex, responsible for caspase-9 activation, followed by the proteolytic activation of pro-caspase-3 CITATION.
By cleavage of specific targets, the executioner caspases are responsible for major biochemical and morphological changes characteristic of apoptosis.
SMAC/DIABLO is released during MOMP to the cytosol, where it is able to inactivate the caspase inhibitor XIAP CITATION.
CASP3 also participates in a positive circuit by inducing the activation of CASP8 CITATION, CITATION.
In type I cells, CASP8 directly cleaves and activates executioner caspases such as CASP3 .
Exposure to environmental chemicals and drugs may have a negative effect on human health.
A better understanding of the molecular mechanism of such compounds is needed to determine the risk.
We present a high confidence human protein-protein association network built upon the integration of chemical toxicology and systems biology.
This computational systems chemical biology model reveals uncharacterized connections between compounds and diseases, thus predicting which compounds may be risk factors for human health.
Additionally, the network can be used to identify unexpected potential associations between chemicals and proteins.
Examples are shown for chemicals associated with breast cancer, lung cancer and necrosis, and potential protein targets for di-ethylhexyl-phthalate, 2,3,7,8-tetrachlorodibenzo-p-dioxin, pirinixic acid and permethrine.
The chemical-protein associations are supported through recent published studies, which illustrate the power of our approach that integrates toxicogenomics data with other data types.
Humans are daily exposed to diverse hazardous chemicals via skincare products, plastic cups, computers and pesticides to mention but a few sources.
The potential effect of these environmental compounds on human health is a major concern CITATION CITATION.
For example chemicals such as phthalate plasticizers have been widely linked to allergies, reproductive disorders and neurological defects.
Humans are intentionally exposed to drugs used for treatment and cure of diseases.
Many drugs affect multiple targets and may interact or affect the same proteins as environmental chemicals CITATION CITATION.
The mechanism of action of these small molecules is often not completely understood and can be associated to adverse and toxic effects through for example drug-drug interactions CITATION.
There is thus a need to improve our understanding of the underlying mechanism of action of chemicals and the biological pathways they perturb to fully evaluate the impact of small molecules on human health.
An essential step towards deciphering the effect of chemicals on human health is to identify all possible molecular targets of a given chemical.
Various network-oriented chemical pharmacology approaches have been published recently to identify novel protein candidates for drugs, using structural chemical similarity CITATION CITATION.
For example Keiser et al. CITATION applied network analysis to drugs and their targets.
The authors identified unexpected molecular targets such as muscarinic acetylcholine receptor M 3, alpha-2 adrenergic receptor and neurokinin NK2 receptor for methadone, emetine and loperamide, respectively.
Additionally, recent studies have demonstrated that chemicals could be classified based upon their effect on mRNA expression detected by microarrays CITATION CITATION.
Lamb et al. showed that genomic signatures could be used to recognize drugs with common mechanism of action allowing discovery of unknown modes of action.
Despite the explosion of chemical-biological networks, the chemical toxicity remains a major issue in human health.
Analysis of environmental chemicals with similar gene expression profiles is still lacking.
With the recent advances in toxicogenomics, information on gene/protein activity in response to small molecule exposures becomes more available.
This provide necessary data to develop computational systems biology models to predict both high level associations and more detailed associations
In this paper we present a method that can associate chemicals to disease and identify potential molecular targets based on the integration of toxicogenomics data, chemical structures, protein-protein interaction data, disease information and functional annotation.
The core of our procedure is derived from the target hopping concept defined previously CITATION.
But instead of considering only binding activity, we extended the concept to gene expression.
If two proteins are affected with two chemicals, then both proteins are deemed associating in chemical space.
Our approach is not only a statistical model but mimics the true biological system by constructing a network of associations between human proteins defined as Protein-Protein Association Network.
We have validated our network by comparison with two high confidence protein-protein interaction networks, and by assessing the functional enrichment of clusters in the network generated.
The P-PAN revealed both known as well as many novel surprising connections between chemicals and diseases or proteins.
We provide literature support for some of the unexpected associations, such as the connection between diethylhexylphthalate and gamma-aminobutyric acid A receptor beta target CITATION, as well as between apocarotenal, a chemical found in spinach, and necrosis.
This illustrates the usefulness of an approach that integrates toxicogenomics data with other diverse data types.
Many neurons have epochs in which they fire action potentials in an approximately periodic fashion.
To see what effects noise of relatively small amplitude has on such repetitive activity we recently examined the response of the Hodgkin-Huxley space-clamped system to such noise as the mean and variance of the applied current vary, near the bifurcation to periodic firing.
This article is concerned with a more realistic neuron model which includes spatial extent.
Employing the Hodgkin-Huxley partial differential equation system, the deterministic component of the input current is restricted to a small segment whereas the stochastic component extends over a region which may or may not overlap the deterministic component.
For mean values below, near and above the critical values for repetitive spiking, the effects of weak noise of increasing strength is ascertained by simulation.
As in the point model, small amplitude noise near the critical value dampens the spiking activity and leads to a minimum as noise level increases.
This was the case for both additive noise and conductance-based noise.
Uniform noise along the whole neuron is only marginally more effective in silencing the cell than noise which occurs near the region of excitation.
In fact it is found that if signal and noise overlap in spatial extent, then weak noise may inhibit spiking.
If, however, signal and noise are applied on disjoint intervals, then the noise has no effect on the spiking activity, no matter how large its region of application, though the trajectories are naturally altered slightly by noise.
Such effects could not be discerned in a point model and are important for real neuron behavior.
Interference with the spike train does nevertheless occur when the noise amplitude is larger, even when noise and signal do not overlap, being due to the instigation of secondary noise-induced wave phenomena rather than switching the system from one attractor to another .
Rhythmic or almost regular periodic neuronal spiking is found in many parts of the central nervous system, including, for example, thalamic relay cells CITATION CITATION, dopaminergic neurons CITATION, respiratory neurons CITATION, CITATION, locus coeruleus neurons CITATION and dorsal raphe serotonergic neurons CITATION, CITATION.
Periodic behavior is also found in the activity of neuronal populations CITATION, CITATION.
Since stochasticity is a prominent component of neuronal activity at all levels CITATION, CITATION, it is of interest to see what effects noise may have on the repetitive activity of neurons.
There are many kinds of neuronal model which could be used, an immediate dichotomy being provided by Hodgkin's defining classes of type 1 and type 2 neurons CITATION, CITATION.
We have chosen to first examine the behavior of the classic type 2 neural model in its full spatial version CITATION which has been employed in recent studies of reliability CITATION.
The methods we use can be easily extended to more complicated models such as in CITATION CITATION, CITATION, CITATION .
The deterministic spatial Hodgkin-Huxley system, consisting of the cable partial differential equation for membrane voltage and three auxiliary differential equations describing the sodium and potassium conductances is one of the most successful mathematical models in physiology CITATION.
The corresponding system of ordinary differential equations has been the subject of a very large number of studies and analyses, as for example in references CITATION CITATION.
Most neuronal modeling studies, aside from some that use software packages, ignore spatial extent altogether and many of those that include spatial extent do not include a soma and hardly ever an axon, because the inclusion of all three major neuronal components, soma, axon and dendrites, makes for a complicated system of equations and boundary conditions.
A recent study of spike propagation in myelinated fibres used a multi-compartmental stochastic Hodgkin-Huxley model and demonstrated the facilatory effect of noise and that there were optimal channel densities at nodes for the most efficient signal transmission CITATION.
In reality, if solutions and statistical properties are found by simulation, stochastic cable models, including the nonlinear model of Hodgkin and Huxley, are not much more complicated than the corresponding point models, although more computing time is required.
On the other hand, an apparent disadvantage of spatial models is that more parameters must be specified, many of which can at best only be approximately estimated.
The original HH-system for action potential formation and propagation in squid axon contained only sodium ions, potassium ions and leak currents and the distribution of the corresponding ion channels was assumed to be uniform.
That is, the ionic current wasFORMULAand the various channel densities did not vary with distance.
However, there are two reasons why this basic model has been modified in the modeling of more complex cells.
Firstly, ion channel densities do depend on position, and secondly, neurons, especially those in the mammalian central nervous system, often receive many thousands of synaptic inputs from many different sources and each source has a different spatial distribution pattern on the soma-dendritic surface CITATION CITATION.
Thus, spatial models of motoneurons CITATION and cortical pyramidal cells CITATION, CITATION have also used the same components for the ionic current as in the HH-system, but with channel densities that vary over the soma-dendritic and axonal surface.
Most central neurons have many dendritic trunks and an axon, each of which branches many times.
In this article we focus on a cable model with one space dimension, which is most accurate for a nerve cylinder, usually of uniform diameter.
Thus in the first instance our approach is useful to investigate the properties of single axonal or dendritic segments.
This simple geometry can nevertheless be used to gain some insight into the properties of neurons with complex anatomy by appealing to such methods as CITATION mapping from the neuronal branching structure to a cylinder thus reducing the multi-segment problem to solving a cable equation in one space dimension.
Thus single-segment cable models can have relevance for neurons with branching dendritic or axonal trees.
Recent studies of the HH-system of ordinary differential equations with stochastic input have revealed new and interesting phenomena CITATION, CITATION which have a character opposite to that of stochastic resonance CITATION.
In the latter, there is a noise level at which some response variable achieves a maximum.
In particular, in the space-clamped HH system, at mean input current densities near the critical value for repetitive firing, it was found that a small amount of noise could strongly inhibit spiking.
Furthermore, there occurred, for given mean current densities, a minimum in the firing rate as the noise level increased from zero CITATION, CITATION.
Such properties are related to noise-induced delays in firing as found in single HH neurons with periodic input current CITATION or networks of such neurons CITATION, CITATION.
It is of interest to see if these kinds of phenomena extend to the spatial HH-system where in addition many possibilities for the spatial distribution of the mean input and of the noise.
We will demonstrate that the spatial HH system exhibits quite similar but more complex behavior than the ODE system.
The first efficacy trials named STEP of a T cell vaccine against HIV/AIDS began in 2004.
The unprecedented structure of these trials raised new modeling and statistical challenges.
Is it plausible that memory T cells, as opposed to antibodies, can actually prevent infection?
If they fail at prevention, to what extent can they ameliorate disease?
And how do we estimate efficacy in a vaccine trial with two primary endpoints, one traditional, one entirely novel, and where the latter may be influenced by selection bias due to the former?
In preparation for the STEP trials, biostatisticians developed novel techniques for estimating a causal effect of a vaccine on viral load, while accounting for post-randomization selection bias.
But these techniques have not been tested in biologically plausible scenarios.
We introduce new stochastic models of T cell and HIV kinetics, making use of new estimates of the rate that cytotoxic T lymphocytes CTLs; the so-called killer T cells can kill HIV-infected cells.
Based on these models, we make the surprising discovery that it is not entirely implausible that HIV-specific CTLs might prevent infection as the designers explicitly acknowledged when they chose the endpoints of the STEP trials.
By simulating thousands of trials, we demonstrate that the new statistical methods can correctly identify an efficacious vaccine, while protecting against a false conclusion that the vaccine exacerbates disease.
In addition to uncovering a surprising immunological scenario, our results illustrate the utility of mechanistic modeling in biostatistics.
The first generation of vaccines against the human immunodeficiency virus, designed to prevent HIV acquisition by stimulating neutralizing antibodies, failed to protect in efficacy trials CITATION.
Second-generation vaccines have been designed to elicit HIV-specific cellular immune responses CITATION.
These candidates are supported by evidence that so-called killer T cells the cytotoxic T lymphocytes, bearing the CD8 membrane-molecule, that can react to and kill infected target cells play a crucial role in controlling HIV infection CITATION CITATION .
The first efficacy trial, named STEP, of a T cell directed HIV vaccine began in December 2004; it is being conducted by Merck Research Laboratories in collaboration with the HIV Vaccine Trials Network and the Division of AIDS at the US National Institutes of Health.
The candidate vaccine consists of three vectors that can ferry HIV proteins into human cells.
The vaccine elicits broad T cell responses in a majority of vaccinated HIV-uninfected adults CITATION.
The STEP trial will randomize 3,000 HIV uninfected volunteers to receive MRKAd5 or placebo in a 1:1 ratio and follow participants until 100 HIV infections occur.
Mehrotra, Li, and Gilbert CITATION provide details of this trial.
A second STEP trial of MRKAd5 with a nearly identical design will begin in South Africa in 2006.
The co-primary endpoints of the STEP trials are HIV infection and a clinical measure of disease: setpoint viral load.
The terminology reflects the typical course of HIV disease, which appears first as a flu-like illness, progresses through a stable, asymptomatic phase, then progresses to AIDS.
The viral load is typically measured in blood drawn sometime after the primary stage.
Even without preventing infection, a vaccine that suppresses viral load could confer a benefit to the individual, by slowing progression to AIDS CITATION, CITATION and preventing the need for antiretroviral treatment; and to the community by reducing HIV transmission CITATION, CITATION.
The second primary analysis of STEP compares viral load setpoints among HIV-infected subjects in the vaccine and placebo groups.
Besides the unprecedented nature of the trials, the nontraditional design presented a statistical challenge.
Because the subjects included in the viral-load comparison are determined by a post-randomization event, HIV infection, the analysis is susceptible to selection bias CITATION, CITATION.
Specifically, a conventional analysis of viral load differences would not uniquely assess a causal effect of the vaccine, but rather a mixture of causal vaccine-effect and the effects of variables correlated with viral load.
The latter may be unevenly distributed between the infected-vaccinated and infected-placebo groups.
For example, selection bias would occur if the vaccine protects from HIV infection only vaccinees with a relatively strong immune system, which implies the infected vaccine group would be weaker immunologically than the infected placebo group.
Consequently, even if the vaccine has no causal effect on viremia, the viral loads in the vaccine group would be expected to be higher than those in the placebo group.
Failing to account for this selection bias would lead to the incorrect inference that the vaccine harmfully increases viral load.
In preparation for the analysis of the STEP trials, Gilbert, Bosch, and Hudgens CITATION and other investigators CITATION, CITATION developed statistical techniques for assessing a vaccine effect on viral load that allow for plausible levels of selection bias.
However, these papers did not consider underlying biological mechanisms that could account for causal vaccine effects.
Rather, they simulated arbitrary effects and studied the purely statistical operating characteristics of the methodology.
To evaluate the statistical methods in a more biologically relevant manner, we consider here various mechanistic hypotheses for vaccine effects.
At their present stage of development, mathematical models of HIV infection and the immune system have made few compelling predictions, primarily because of uncertainty about which are the most important mechanisms and the values of rate-constants.
Nevertheless, models have attained enough maturity that they can quantitatively reproduce the drop in primary viremia after appearance of HIV-specific CTLs, the lag between peak viremia and peak immune response, the formation of a steady state, and other aspects of HIV infection CITATION CITATION.
In addition, we can exploit a recent estimate of the rate at which HIV-specific CTLs can kill HIV-infected cells CITATION.
Building on these developments, we constructed new stochastic models of primary infection and the immune response and made a surprising observation: it does not appear implausible that CTLs might abort an HIV infection.
By using infection as a co-primary endpoint with setpoint viral load, the designers of the STEP trials explicitly acknowledged this possibility.
For purposes of discussion, let us distinguish prevention from eradication of infection.
By prevention we will mean either that no productively infected target cells arise at all, or the pool does not expand beyond some small number of cells; afterward, it is driven to extinction.
The number of PITs at any time is insufficient to cause disease.
Such a favorable outcome might be described as a transient infection or as sterilizing immunity, depending on which assays are employed to detect exposure.
Eradication we restrict, consistent with common usage, to clearing the infection after primary, symptomatic viremia.
Because CD8 T cells require a priming step and an expansion period before they can clear infected cells, most immunologists regard preventing infection as we have defined it to be unlikely CITATION.
That vaccines against simian immunodeficiency virus have not prevented infection may be a consequence of the large inoculums used in the experiments CITATION.
More relevant to HIV is the observation of T cell memory to HIV antigens in frequently exposed but seronegative sex workers in Kenya CITATION.
One mechanistic interpretation of this finding is that a productive cellular infection was initially established, but was either cleared by CTLs or went extinct due to chance.
The transient infection left behind a small pool of latently infected, resting CD4 T cells that, due to occasional activation events, continuously expose the immune system to HIV antigens, maintaining CTL memory.
However, the investigators could not prove that the HIV-specific CTLs actually protected these women.
But vaccine-derived or adoptively transferred CTLs have prevented infection by other viruses; in particular, Sendai CITATION and Ebola CITATION in mice.
However we may judge the plausibility of prevention by T cells, as we are interested here in the impact on statistical estimation, we have to incorporate some biological mechanism for it into our simulations.
We have combined stochastic and deterministic models so as to simulate the impact of T cells both on the probability of infection given exposure and on viral load assuming infection, in vaccinated or unvaccinated subjects.
Of course, such a concatenation requires more hypotheses, in particular about vaccine action.
Again, because of the extent of uncertainty about these mechanisms, we do not claim to predict the outcome of vaccine trials.
Rather, the models provide cases where an influence of selection bias is present or absent, and when present of a magnitude resulting from biological scenarios rather than ad hoc assumptions.
We can then generate thousands of hypothetical STEP trials, and put the GBH method to the test.
In the STEP trials, two vaccine efficacy parameters will be assessed; one-minus-relative-risk of HIV infection, and the difference in mean viral load setpoint of HIV-infected subjects, where setpoint viral load is defined as the average of two log10 plasma HIV RNA levels measured at month 2 and 3 visits after diagnosis of HIV infection.
The data will be analyzed using an adaption of the GBH method, which estimates the causal vaccine effect on viral load while accounting for plausible levels of selection bias.
This technique was developed from the potential outcomes framework for causal inference CITATION, CITATION.
In this framework, each trial participant has two potential HIV infection outcomes: one under assignment to vaccine and one under assignment to placebo.
Following GBH, a causal effect on viral load can be defined for the subpopulation of always-infected subjects who would become HIV-infected regardless of randomization to vaccine or placebo.
Such an effect is causal because the always-infected subpopulations of the two study arms have identical characteristics except for the vaccine/placebo assignment, and therefore observed differences are directly attributable to vaccination.
The methods estimate the average causal effect of vaccine on viral load, equal to the difference in mean setpoint viral loads for the always-infected subpopulation.
The fundamental difficulty in evaluating the ACE is the lack of knowledge about which infected subjects are in the always-infected group knowing this would require data on subjects' HIV infection outcome both under assignment to vaccine and under assignment to placebo, but for each subject only one of these outcomes is observed.
To address the identifiability problem, GBH made the simplifying assumption that the vaccine does not increase the risk of infection for any subject, and posited a model for whether an infected placebo recipient with setpoint viral load Y would have been infected had they been assigned vaccine.
This model is indexed by a sensitivity parameter, which is the log odds ratio of infection under assignment to vaccine comparing two infected placebo recipients with setpoint viral loads Y and Y 1.
A value 0 reflects the case of no selection bias, in which case the naive analysis assesses a causal vaccine effect, and positive values of reflect selection bias such that the odds of infection is higher for larger setpoint viral loads Y. The parameter is fixed by the investigator at each possible value within a plausible range, and for each, GBH provided procedures for estimating ACE with a confidence interval.
See Materials and Methods for the mathematical details.
For the first STEP trial, a panel of ten experts proposed a plausible range for of log to log; see Discussion section.
To be cautious, our analysis will estimate ACE over a somewhat wider range for, namely log to log; i.e., 1,2 .
Combinatorial regulation of gene expression is ubiquitous in eukaryotes with multiple inputs converging on regulatory control elements.
The dynamic properties of these elements determine the functionality of genetic networks regulating differentiation and development.
Here we propose a method to quantitatively characterize the regulatory output of distant enhancers with a biophysical approach that recursively determines free energies of protein-protein and protein-DNA interactions from experimental analysis of transcriptional reporter libraries.
We apply this method to model the Scl-Gata2-Fli1 triad a network module important for cell fate specification of hematopoietic stem cells.
We show that this triad module is inherently bistable with irreversible transitions in response to physiologically relevant signals such as Notch, Bmp4 and Gata1 and we use the model to predict the sensitivity of the network to mutations.
We also show that the triad acts as a low-pass filter by switching between steady states only in response to signals that persist for longer than a minimum duration threshold.
We have found that the auto-regulation loops connecting the slow-degrading Scl to Gata2 and Fli1 are crucial for this low-pass filtering property.
Taken together our analysis not only reveals new insights into hematopoietic stem cell regulatory network functionality but also provides a novel and widely applicable strategy to incorporate experimental measurements into dynamical network models.
Appropriate spatiotemporal control of gene expression is central to metazoan development.
CITATION.
Combinatorial interactions of regulatory proteins with regulatory regions of DNA and the basal transcriptional machinery form the building blocks of complex gene regulatory networks.
The availability of whole genome sequences as well as advanced bioinformatics and high-throughput experimental techniques have vastly accelerated the identification of candidate regulatory sequences.
However, experiments that can uncover and/or validate the underlying connectivity of GRNs remain both costly and time consuming.
Consequently, our understanding of the functionality of GRNs even for the most studied model organisms remains superficial.
Moreover, simply cataloguing ever increasing numbers of interactions between GRN components is not sufficient to deduce the underlying network architecture or function of individual modules.
Unraveling the dynamical properties of GRNs will be the key to understanding their functionality.
Throughout development, cells progress through a succession of differentiation steps from stem cells via immature progenitors to fully differentiated mature cells, and each of these subtypes is associated with a unique regulatory state of the GRN CITATION.
It is therefore essential to understand dynamical properties of the various regulatory states of GRNs, transitions between them and their interplay with intercellular signaling.
It is unlikely that this goal can be achieved solely using experimental approaches.
However, the development of dynamical models of GRNs offers great potential to interpret existing experimental data in order to gain new mechanistic insights.
Various computational approaches have been used for regulatory network analysis in the past.
Boolean models provide qualitative information about network behavior such as the existence of steady states and network robustness and are most useful for large networks or when experimental information is scarce CITATION, CITATION.
However to examine dynamical aspects, continuous ordinary differential equation models are more appropriate.
These models can be constructed with phenomenological descriptions of gene regulation in the form of Hill functions or based on more detailed biophysical mechanisms and derived using a statistical thermodynamics approach.
Phenomenological models are useful for understanding the general dynamics of network topology.
They are most effective for small to medium sized networks and can also be predictive of cellular behavior CITATION.
Models based on thermodynamics have the advantage of including an hypothesis about the biophysics of the system CITATION, CITATION, CITATION.
Most parameters in these models have a direct biochemical interpretation.
Unfortunately the lack of knowledge about specific biochemical parameters usually makes it difficult to relate results from these models to experimental information about gene expression.
Nevertheless this modeling approach has been shown to be useful in understanding certain bacterial gene regulation modules CITATION and studying the effects of nucleosome dynamics in eukaryotic gene regulation CITATION .
The hematopoietic system has long served as a powerful model to study the specification and subsequent differentiation of stem cells CITATION.
Sophisticated cell purification protocols coupled with powerful functional assays have allowed a very detailed reconstruction of the differentiation pathways leading from early mesoderm via hemangioblasts and hematopoietic stem cells to the multiple mature hematopoietic lineages.
Transcriptional regulators have long been recognized as key hematopoietic regulators but the wider networks within which they operate remain ill defined CITATION.
Detailed molecular characterization of regulatory elements active during the early stages of HSC development has identified specific connections between major regulators CITATION, CITATION, CITATION, CITATION and has led to the definition of combinatorial regulatory codes specific for HSC enhancers CITATION, CITATION, CITATION.
Moreover, these studies identified a substantial degree of cross-talk and positive feedback in the connectivity of major HSC TRs CITATION.
In particular, a triad of HSC TRs forms a regulatory module that appears to lie at the core of the HSC GRN CITATION.
This module consists of the three transcription factor proteins as well as three regulatory elements through which they are connected via cross-regulatory and autoregulatory interactions CITATION, CITATION.
The details of regulatory interactions in this triad are shown in Figure 1B; only significant binding sites in the enhancers are shown for simplicity.
Gata2-3 and Fli1 12 enhancers contain multiple Gata2, Fli1 and Scl binding motifs.
The Scl 19 enhancer contains ETS and GATA binding motifs.
Scl, Gata2 and Fli1 are all essential for normal hematopoiesis in mice CITATION suggesting that the triad is an important sub-circuit or kernel of the GRN that governs hematopoiesis.
The triad architecture is very dense in regulatory connections and possesses multiple direct and indirect positive feedback loops.
Such network topologies are rare in prokaryotes CITATION but have been identified in other stem cell systems such as the Nanog-Oct4-Sox2 triad in the embryonic stem cell GRN CITATION, CITATION.
These observations suggest that the triad design may be associated with stem cell behavior.
This idea prompted further investigation of combinatorial control by the triad TRs CITATION.
Generation of an enhancer library with wild type and mutant enhancers allowed the construction of different combinations of binding motifs in each enhancer.
Wild type and mutant enhancers were sub-cloned into a SV minimal promoter and lacZ reporter vector and tested using stable transfection of hematopoietic progenitor cell lines CITATION.
This analysis produced results such as those schematically illustrated in Figure 1C.
It has been suggested that the dense connectivity and positive feedback loops within stem cell GRN modules play important roles in stabilizing the stem cell phenotype CITATION.
However, the dynamical nature as to how this self-enforcing circuit may be initiated or indeed exited remains unclear.
In this paper we construct a mathematical model of the Scl-Gata2-Fli1 triad module and characterize its dynamical properties using continuous ODE modeling approaches.
We first propose a thermodynamic method of estimating free energies of different configurations of the enhancer regions from the measurements of the transcriptional reporter libraries.
This method together with a proposed biochemical mechanism of distant transcriptional enhancement significantly reduces dimensionality of the network parameter space.
Measurements of protein lifetimes provide experimentally informed timescales to model transient behavior of the network.
We analyze the network response to physiologically relevant signals such as Notch, Bmp4 and Gata1 and show that the network behaves as an irreversible bistable switch in response to these signals.
Our model also predicts the results of various mutations in the enhancer sequences and shows that the triad module can ignore transient differentiation signals shorter than threshold duration.
The combination of a bistable switch with short signal filtering not only provides new mechanistic insights as to how the Scl-Gata2-Fli1 triad may function to control HSC specification and differentiation but also suggests a possibly more general role for this network architecture in the development of other major organ systems.
We present molecular dynamics simulations of unliganded human hemoglobin A under physiological conditions, starting from the R, R2, and T state.
The simulations were carried out with protonated and deprotonated HC3 histidines His146, and they sum up to a total length of 5.6 s. We observe spontaneous and reproducible T R quaternary transitions of the Hb tetramer and tertiary transitions of the and subunits, as detected from principal component projections, from an RMSD measure, and from rigid body rotation analysis.
The simulations reveal a marked asymmetry between the and subunits.
Using the mutual information as correlation measure, we find that the subunits are substantially more strongly linked to the quaternary transition than the subunits.
In addition, the tertiary populations of the and subunits differ substantially, with the subunits showing a tendency towards R, and the subunits showing a tendency towards T. Based on the simulation results, we present a transition pathway for coupled quaternary and tertiary transitions between the R and T conformations of Hb.
Conformational transitions of allosteric proteins are fundamental to a variety of biological functions.
For instance, quaternary transitions in hemoglobin give rise to the cooperativity of ligand binding and have therefore drawn extensive and ongoing scientific interest over many decades CITATION, CITATION.
The end points of the quaternary transition of Hb are referred to as deoxy T state and oxy R state of Hb, which are characterized by low and high oxygen affinity, respectively CITATION, and the cooperativity of ligand binding originates from the dependence of quaternary population on the number of liganded subunits CITATION.
The oxygen affinity of Hb decreases with lower pH, a phenomenon that is referred to as alkaline Bohr effect.
Approximately 40 percent of the Bohr effect has been attributed to the protonation of the terminal His146 residues of the subunits, which are also denoted as HC3 histidines CITATION .
The stereochemical explanation of Hb cooperativity and the characterization of the transition pathway were originally based on the HbCO and deoxyHb crystal structures, corresponding to the R and T state, respectively.
According to these structures, the transition can mainly be described by a 12 15 rotation of the 1 1 dimer with respect to the 1 2 dimer CITATION, CITATION.
Later, a second quaternary structure of liganded Hb, termed R2, was found CITATION, with a 1.1 larger distance between the centers of mass of two subunits as compared to the R structure.
Differences between R and R2 at the 1 2 interface triggered a still unresolved discussion whether R2 is a stable intermediate on a R-R2-T pathway CITATION CITATION.
NMR experiments indicate that liganded Hb in solution is in equilibrium between the R and R2 structures CITATION.
More recently, two additional liganded Hb structures RR2 and R3 were found using the high-salt crystallization conditions of Perutz CITATION, emphasizing that a consensus view on the liganded Hb state in solution is far from being reached.
RR2 represents an intermediate structure between R and R2, whereas the distance between the COMs of the two subunits is reduced by 3.1 in R3 as compared to the R structure.
Extensive efforts aimed to identify the transition pathway of Hb in response to ligand dissociation CITATION.
The kinetics of the R T transition after photodissociation of the CO adduct, HbCO, have been studied using time-resolved spectroscopic techniques including absorption CITATION, Raman CITATION, CITATION, and circular dichroism spectroscopy CITATION, CITATION.
The picture derived from these experiments suggests a multistep R T pathway via several metastable intermediates, with relaxation rates ranging from tens of nanoseconds to tens of microseconds, and with a time constant of 21 s for the overall R T quaternary transition CITATION.
The experiments provide extremely valuable insights into the kinetics of Hb, but they also bear limitations.
They do not directly detect the global quaternary transitions, but mainly measure the formation of hydrogen bonds of aromatic residues, such as the Trp37-Asp94 and the Tyr42-Asp99 H-bonds, which must be interpreted in terms of conformational transitions.
A full-atomistic picture of the R T transition could so far exclusively be derived for a mollusk dimeric hemoglobin using time-resolved X-ray crystallography CITATION.
Such experiments provide an ensemble-averaged picture, whereas Hb may follow heterogeneous transition pathways that may not be fully reflected by the spectra.
Furthermore, in contrast to the well-studied R T transition, little is known about the kinetics of the T R transition because that transition cannot be triggered by photolysis.
Molecular dynamics simulations can provide a full-atomistic picture of Hb and are therefore well suited to complement experimental efforts.
Early MD efforts focused on the photodissociation of CO CITATION, or were restricted to the dynamic treatment of a subset of Hb residues CITATION.
Ramadas and Rifkind considered the response of Hb to the perturbation of the heme on a several 100ps time scale CITATION, and Mouawad and coworkers enforced quaternary transitions within 200ps using a technique called path exploration method CITATION.
In addition, a set of MD simulation of up to 6ns were carried out with a focus on the mechanism of effectors CITATION.
Recently, a single 45-ns simulation of Hb was published without observing any conformational transitions CITATION.
Complementary to the MD studies, a normal mode analysis considered the collective motions intrinsic to the Hb tetramer CITATION, and an elastic network study suggested a T-R2 transition as the preferential quaternary transition pathway CITATION .
So far, no spontaneous quaternary or tertiary transitions of Hb were observed during MD simulations, presumably since previous simulations were restricted to too short time scales.
Here, we apply extensive MD simulations to investigate the deoxy R, R2, and T state of human.
We observe for the first time spontaneous and reproducible quaternary transitions of Hb, as well as tertiary transitions of the and subunits.
Hence, these simulations allow one to study the transition mechanism in atomistic detail.
We find the T-R pathway as the primary quaternary transition pathway.
By analyzing repeated T-R transitions, we find a marked asymmetry between the and subunits.
Based on the simulation results, we present a schematic mechanism underlying the preferential transition pathway between the R and T states of hemoglobin.
Observability of a dynamical system requires an understanding of its state the collective values of its variables.
However, existing techniques are too limited to measure all but a small fraction of the physical variables and parameters of neuronal networks.
We constructed models of the biophysical properties of neuronal membrane, synaptic, and microenvironment dynamics, and incorporated them into a model-based predictor-controller framework from modern control theory.
We demonstrate that it is now possible to meaningfully estimate the dynamics of small neuronal networks using as few as a single measured variable.
Specifically, we assimilate noisy membrane potential measurements from individual hippocampal neurons to reconstruct the dynamics of networks of these cells, their extracellular microenvironment, and the activities of different neuronal types during seizures.
We use reconstruction to account for unmeasured parts of the neuronal system, relating micro-domain metabolic processes to cellular excitability, and validate the reconstruction of cellular dynamical interactions against actual measurements.
Data assimilation, the fusing of measurement with computational models, has significant potential to improve the way we observe and understand brain dynamics.
A universal dilemma in understanding the brain is that it is complex, multiscale, nonlinear in space and time, and we never have more than partial experimental access to its dynamics.
To better understand its function one not only needs to encompass the complexity and nonlinearity, but also estimate the unmeasured variables and parameters of brain dynamics.
A parallel comparison can be drawn in weather forecasting CITATION, although atmospheric dynamics are arguably less complex and less nonlinear.
Fortunately, the meteorological community has overcome some of these issues by using model based predictor-controller frameworks whose development derived from computational robotics requirements of aerospace programs in 1960s CITATION, CITATION.
A predictor-controller system employs a computational model to observe a dynamical system, assimilate data through what may be relatively sparse sensors, and reconstruct and estimate the remainder of the unmeasured variables and parameters in light of available data.
The result of future measured system dynamics is compared with the model predicted outcome, the expected errors within the model are updated and corrected, and the process repeats iteratively.
For this recursive initial value problem to be meaningful one needs computational models of high fidelity to the dynamics of the natural systems, and explicit modeling of the uncertainties within the model and measurements CITATION CITATION .
The most prominent of the model based predictor-controller strategies is the Kalman filter CITATION.
In its original form, the KF solves a linear system.
In situations of mild nonlinearity, the extended forms of the KF were used where the system equations could be linearized without losing too much of the qualitative nature of the system.
Such linearization schemes are not suitable for neuronal systems with nonlinearities of the scale of action potential spike generation.
With the advent of efficient nonlinear techniques in the 1990s such as the ensemble Kalman filter CITATION, CITATION and the unscented Kalman filter CITATION, CITATION, along with improved computational models for the dynamics of neuronal systems CITATION, the prospects for biophysically based ensemble filtering from neuronal systems are now strong.
The general framework of the UKF differs from the extended KF in that it integrates the fundamental nonlinear models directly, along with iterating the error and noise expectations through these nonlinear equations.
Instead of linearizing the system equations, UKF performs the prediction and update steps on an ensemble of potential system states.
This ensemble gives a finite sampling representation of the probability distribution function of the system state CITATION, CITATION CITATION .
Our hypothesis is that seizures arise from a complex nonlinear interaction between specific excitatory and inhibitory neuronal sub-types CITATION.
The dynamics and excitability of such networks are further complicated by the fact that a variety of metabolic processes govern the excitability of those neuronal networks, and these metabolic variables are not directly measurable using electrical potential measurements.
Indeed, it is becoming increasingly apparent that electricity is not enough to describe a wide variety of neuronal phenomena.
Several seizure prediction algorithms, based only on EEG signals, have achieved reasonable accuracy when applied to static time-series CITATION CITATION.
However, many techniques are hindered by high false positive rates, which render them unsuitable for clinical use.
We presume that there are aspects of the dynamics of seizure onset and pre-seizure states that are not captured in current models when applied in real-time.
In light of the dynamic nature of epilepsy, an approach that incorporates the time evolution of the underlying system for seizure prediction is required.
As one cannot see much of an anticipatory signature in EEG dynamics prior to seizures, the same can be said of a variety of oscillatory transient phenomena in the nervous system ranging from up states CITATION, spinal cord burst firing CITATION, cortical oscillatory waves CITATION, in addition to animal CITATION and human CITATION epileptic seizures.
All of these phenomena share the properties that they are episodic, oscillatory, and have apparent refractory periods following which small stimuli can both start and stop such events.
It has recently been shown that the interrelated dynamics of FORMULA and sodium concentration affect the excitability of neurons, help determine the occurrence of seizures, and affect the stability of persistent states of neuronal activity CITATION, CITATION.
Competition between intrinsic neuronal ion currents, sodium-potassium pumps, glia, and diffusion can produce slow and large-amplitude oscillations in ion concentrations similar to what is observed physiologically in seizures CITATION, CITATION .
Brain dynamics emerge from within a system of apparently unique complexity among the natural systems we observe.
Even as multivariable sensing technology steadily improves, the near infinite dimensionality of the complex spatial extent of brain networks will require reconstruction through modeling.
Since at present, our technical capabilities restrict us to only one or two variables at a restricted number of sites, computational models become the lens through which we must consider viewing all brain measurements CITATION.
In what follows, we will show the potential power of fusing physiological measurements with computational models.
We will use reconstruction to account for unmeasured parts of the neuronal system, relating micro-domain metabolic processes to cellular excitability, and validating cellular dynamical reconstruction against actual measurements.
Competence is a transiently differentiated state that certain bacterial cells reach when faced with a stressful environment.
Entrance into competence can be attributed to the excitability of the dynamics governing the genetic circuit that regulates this cellular behavior.
Like many biological behaviors, entrance into competence is a stochastic event.
In this case cellular noise is responsible for driving the cell from a vegetative state into competence and back.
In this work we present a novel numerical method for the analysis of stochastic biochemical events and use it to study the excitable dynamics responsible for competence in Bacillus subtilis.
Starting with a Finite State Projection solution of the chemical master equation, we develop efficient numerical tools for accurately computing competence probability.
Additionally, we propose a new approach for the sensitivity analysis of stochastic events and utilize it to elucidate the robustness properties of the competence regulatory genetic circuit.
We also propose and implement a numerical method to calculate the expected time it takes a cell to return from competence.
Although this study is focused on an example of cell-differentiation in Bacillus subtilis, our approach can be applied to a wide range of stochastic phenomena in biological systems.
Competence is the ability of a cell, usually a bacterium, to bind and internalize transforming exogenous DNA.
Under stressful environments, such as nutrient limitations, some cells enter competence while other cells commit irreversibly to sporulation.
Entry in competence is a transient probabilistic event that facilitates copying of the exogenous DNA CITATION, CITATION.
It has been shown that among a group of cells only a randomly chosen fraction enters in competence CITATION, CITATION.
Proper modeling and correctly accounting for noise in the model of this phenomenon is crucial to understanding the underlying biological explanation.
The few cells that enter competence express a high concentration of the key regulator ComK, which activates hundreds of genes, including the genes encoding the DNA-uptake and recombination systems CITATION CITATION.
Competence is understood as a bistability pattern CITATION, CITATION and the nonlinear system describing the competence regulatory circuit is an excitable dynamical system.
Auto-activation of the regulator ComK is responsible for the bistable response in competence development.
Auto-activation of ComK, is essential and can be sufficient to generate a bistable expression pattern CITATION CITATION.
Specifically, the concentration of an inducer must cross a certain threshold to start the positive feedback.
Different experimental studies concluded that an auto activation of ComK is the only needed factor for bistability to occur in the expression of this protein CITATION, CITATION, CITATION.
In CITATION, Smits et.
al discuss the factors that determine the required threshold for the activation of ComK and deduce that other transcription factors can raise or lower the threshold.
Although many proteins are involved in the regulation of competence, there are two main proteins that play a major role.
S el et al. CITATION propose a deterministic model driven by an additive noise to describe the dynamics of competence regulation.
We use the reduced order Stochastic Differential Equation model presented in CITATION to develop a discrete stochastic model for competence.
Calculating the probability and the expected time for entering and returning from competence, requires solving for the splitting probabilities and the first moment passage time.
The problem of calculating the first passage time has been studied heavily in the literature for the stochastic difference equations, Fokker Planck equations and some special cases of the CME.
For a detailed treatment of this topic see CITATION CITATION and references therein.
Researchers usually use Monte-Carlo simulations to calculate the distribution of the first passage time when working with he CME.
We propose in this work, an alternative approach that makes it possible to calculate the states in which the system will be as time evolves.
The main idea here is to aggregate regions of the state space over which specie evolve into absorbing states.
This technique is useful in analytically computing the distribution of the first passage time, by providing a way to deal with the infinite dimension of the state space over which the system evolves.
The contributions of this paper are threefold.
First, it provides a new method to calculate exact probabilities of biological phenomena where transient behaviors such as competence, which is the topic we chose to study here, occur.
Second, it shows how to calculate sensitivities of the probabilities of passing to the transient state with respect to the system's parameters.
Third, it gives a methodology to calculate the expected time that it takes a cell to return from its transient state.
All these methods can be used to analyze any biological system that has the characteristic of switching between two states, while staying for a while in the unstable state.
In this paper we start by describing the chemical reactions and the deterministic model.
We then generate the Chemical Master Equation of our proposed discrete stochastic model.
The CME characterizes the evolution of the probability density of the different discrete states.
We simulate it using the Stochastic Simulation Algorithm and show how the solution can be approximated using the Finite State Projection method.
We then conduct a sensitivity analysis studying the effect that the various system parameters have on the probability with which a cell enters in competence.
This analysis shows the usefulness of our proposed numerical method in analyzing the roles of the different affinity, transcription and degradation rates, etc., in driving the cellular switching.
Finally, we analyze the roles of these parameters in determining the expected time a cell stays in competence.
The development of systemic approaches in biology has put emphasis on identifying genetic modules whose behavior can be modeled accurately so as to gain insight into their structure and function.
However, most gene circuits in a cell are under control of external signals and thus, quantitative agreement between experimental data and a mathematical model is difficult.
Circadian biology has been one notable exception: quantitative models of the internal clock that orchestrates biological processes over the 24-hour diurnal cycle have been constructed for a few organisms, from cyanobacteria to plants and mammals.
In most cases, a complex architecture with interlocked feedback loops has been evidenced.
Here we present the first modeling results for the circadian clock of the green unicellular alga Ostreococcus tauri.
Two plant-like clock genes have been shown to play a central role in the Ostreococcus clock.
We find that their expression time profiles can be accurately reproduced by a minimal model of a two-gene transcriptional feedback loop.
Remarkably, best adjustment of data recorded under light/dark alternation is obtained when assuming that the oscillator is not coupled to the diurnal cycle.
This suggests that coupling to light is confined to specific time intervals and has no dynamical effect when the oscillator is entrained by the diurnal cycle.
This intringuing property may reflect a strategy to minimize the impact of fluctuations in daylight intensity on the core circadian oscillator, a type of perturbation that has been rarely considered when assessing the robustness of circadian clocks.
Real-time monitoring of gene activity now allow us to unravel the complex dynamical behavior of regulatory networks underlying cell functions CITATION.
However, understanding the collective behavior of even a few molecular actors defies intuition, as it depends not only on the topology of the interaction network but also on strengths and response times of its links CITATION.
A mathematical description of a regulatory network is thus necessary to qualitatively and quantitatively understand its dynamical behavior, but obtaining it is challenging.
State variables and parameters are subject to large fluctuations CITATION, which create artificial complexity and mask the actual network structure.
Genetic modules are usually not isolated but coupled to a larger network, and a given gene can be involved in different modules and pathways CITATION.
It is thus important to identify gene circuits whose dynamical behavior can be modeled quantitatively, to serve as model circuits.
One strategy for obtaining such circuits has been to construct synthetic networks, which are isolated by design CITATION CITATION.
As recent experiments have shown, an excellent quantitative agreement can be obtained by incorporating when needed detailed descriptions of various biochemical processes CITATION .
Another strategy is to study natural gene circuits whose function makes them relatively autonomous and stable.
The circadian clocks that drive biological processes around the day/night cycle in many living organisms are natural candidates, as these genetic oscillators keep track of the most regular environmental constraint: the alternation of daylight and darkness caused by Earth rotation CITATION CITATION.
Informed by experiments, circadian clock models have progressively become more complex, evolving from single loops featuring a self-repressed gene CITATION, CITATION to networks of interlocked feedback loops CITATION CITATION .
Here we report surprisingly good agreement between the mathematical model of a single transcriptional feedback loop and expression profiles of two central clock genes of Ostreococcus tauri.
This microscopic green alga is the smallest free-living eukaryote known to date and belongs to the Prasinophyceae, one of the most ancient groups of the green lineage.
Ostreococcus displays a very simple cellular organization, with only one mitochondrion and one chloroplast CITATION, CITATION.
Its small genome sequence revealed a high compaction and a very low gene redundancy CITATION.
The cell division cycle of Ostreococcus is under control of a circadian oscillator, with cell division occurring at the end of the day in light/dark cycles CITATION.
These daily rhythms in cell division meet the criteria characterizing a circadian clock, as they can be entrained to different photoperiods, persist under constant conditions and respond to light pulses by phase shifts that depend on internal time CITATION .
Very recently, some light has been shed on the molecular workings of Ostreococcus clock by Corellou et al. CITATION.
Since the clock of closely related Arabidopsis has been extensively studied, they searched Ostreococcus genome for orthologs of higher plant clock genes and found only two, similar to Arabidopsis central clock genes Toc1 and Cca1 CITATION.
These two genes display rhythmic expression both under light/dark alternation and in constant light conditions.
A functional analysis by overexpression/antisense strategy showed that Toc1 and Cca1 are important clock genes in Ostreococcus.
Overexpression of Toc1 led to increased levels of CCA1 while overexpression of Cca1 resulted in lower levels of TOC1.
Furthermore CCA1 was shown to bind to a conserved evening element sequence that is required for the circadian regulated activity of Toc1 promoter.
Whether Toc1 and Cca1 work in a negative feedback loop could not be inferred from this study since Ostreococcus clock appeared to rely on more than a simple Toc1/Cca1 negative feedback loop.
Interestingly, Arabidopsis genes Toc1 and Cca1 were the core actors of the first plant clock model, based on a transcriptional loop where TOC1 activates Cca1 and the similar gene Lhy, whose proteins dimerize to repress Toc1 CITATION, CITATION.
However, this model did not reproduce well expression peaks of Toc1 and Cca1 in Arabidopsis CITATION and was extended to adjust experimental data CITATION.
Current Arabidopsis clock models feature several interlocked feedback loops CITATION, CITATION.
This led us to investigate whether the transcriptional feedback loop model where Toc1 activates Cca1 and is repressed by Cca1 would be relevant for Ostreococcus.
We not only found that this two-gene loop model reproduces perfectly transcript profiles of Ostreococcus Toc1 and Cca1 but that excellent adjustment of data recorded under light/dark alternation is obtained when no model parameter depends on light intensity.
This counterintuitive finding suggests that the oscillator is not permanently coupled to light across the 24-hour cycle but only during specific time intervals, which is supported by numerical simulations.
In this article, we propose that the invisibility of coupling in entrainment conditions reflects a strategy to shield the oscillator from natural fluctuations in daylight intensity.
Spike timing is precise in the auditory system and it has been argued that it conveys information about auditory stimuli, in particular about the location of a sound source.
However, beyond simple time differences, the way in which neurons might extract this information is unclear and the potential computational advantages are unknown.
The computational difficulty of this task for an animal is to locate the source of an unexpected sound from two monaural signals that are highly dependent on the unknown source signal.
In neuron models consisting of spectro-temporal filtering and spiking nonlinearity, we found that the binaural structure induced by spatialized sounds is mapped to synchrony patterns that depend on source location rather than on source signal.
Location-specific synchrony patterns would then result in the activation of location-specific assemblies of postsynaptic neurons.
We designed a spiking neuron model which exploited this principle to locate a variety of sound sources in a virtual acoustic environment using measured human head-related transfer functions.
The model was able to accurately estimate the location of previously unknown sounds in both azimuth and elevation in a known acoustic environment.
We found that multiple representations of different acoustic environments could coexist as sets of overlapping neural assemblies which could be associated with spatial locations by Hebbian learning.
The model demonstrates the computational relevance of relative spike timing to extract spatial information about sources independently of the source signal.
Animals must be able to rapidly estimate the location of the source of an unexpected sound, for example to escape a predator.
This is a challenging task because the acoustic signals at the two ears vary with both the source signal and the acoustic environment, and information about source location must be extracted independently of other causes of variability.
Psychophysical studies have shown that source localization relies on a variety of acoustic cues such as interaural time and level differences and spectral cues CITATION.
At a neuronal level, spike timing has been shown to convey information about auditory stimuli CITATION, CITATION, and in particular about source location CITATION, CITATION.
Although it is well accepted that ITDs can be extracted from phase-locked responses, it is unknown how information beyond this could be extracted from the spike timing of neurons.
In addition, the potential computational advantages of a spike timing code in this task are unclear.
The sound S produced by a source propagates to the ears and is transformed by the presence of the head, body and pinnae, and possibly other aspects of the acoustic environment.
It results in two linearly filtered signals F L S and F R S at the two ears, where the filtering depends on the relative position of the head and source.
Because the two signals are obtained from the same source signal, the binaural stimulus has a particular structure, which is indicative of source location.
When these signals are transformed into spike trains, we expect that this structure is transformed into synchrony patterns.
Therefore, we examined the synchrony patterns induced by spatialized sounds in neuron models consisting of spectro-temporal filtering and a spiking nonlinearity, where binaural signals were obtained using a variety of sound sources filtered through measured human head-related transfer functions.
We then complemented the model with postsynaptic neurons responding to both sides, so that synchrony patterns induced by binaural structure resulted in the activation of location-specific assemblies of neurons.
The model was able to precisely encode the source location in the activation of a neural assembly, in a way that was independent of the source signal.
Several influential models have addressed the mechanisms of sound localization at an abstract level CITATION CITATION.
Considerable progress has also been realized in understanding the physiological mechanisms of cue extraction, in particular neural mechanisms underlying ITD sensitivity CITATION CITATION.
These studies mostly used simplified binaural stimuli such as tones or noise bursts with artificially induced ITDs.
Several purely computational models CITATION CITATION address the full problem of sound localization in a virtual acoustic environment with realistic sounds, although these do not suggest how neurons might perform this task.
Here we propose a binaural neural model that performs the full localization task in a more realistic situation, based on the idea that synchrony reflects structural properties of stimuli, which in this setting are indicative of source location.
The epidemic spread of infectious diseases is ubiquitous and often has a considerable impact on public health and economic wealth.
The large variability in the spatio-temporal patterns of epidemics prohibits simple interventions and requires a detailed analysis of each epidemic with respect to its infectious agent and the corresponding routes of transmission.
To facilitate this analysis, we introduce a mathematical framework which links epidemic patterns to the topology and dynamics of the underlying transmission network.
The evolution, both in disease prevalence and transmission network topology, is derived from a closed set of partial differential equations for infections without allowing for recovery.
The predictions are in excellent agreement with complementarily conducted agent-based simulations.
The capacity of this new method is demonstrated in several case studies on HIV epidemics in synthetic populations: it allows us to monitor the evolution of contact behavior among healthy and infected individuals and the contributions of different disease stages to the spreading of the epidemic.
This gives both direction to and a test bed for targeted intervention strategies for epidemic control.
In conclusion, this mathematical framework provides a capable toolbox for the analysis of epidemics from first principles.
This allows for fast, in silico modeling - and manipulation - of epidemics and is especially powerful if complemented with adequate empirical data for parameterization.
Despite huge efforts to improve public health, the spread of infectious diseases is still ubiquitous at the beginning of the 21st century, and there is considerable variability in epidemic patterns between locations.
Although the recent influenza pandemic has been a global challenge, there have nonetheless been differences in its timing in the northern and southern hemisphere due to seasonal effects CITATION, CITATION.
Another prominent example for epidemic variability is the prevalence of sexually transmitted diseases, specifically HIV infections.
Although HIV is endemic in many populations at low levels or restricted to high-risk groups, it has become highly endemic in other parts of the world CITATION, CITATION.
As a consequence, the spread of infectious diseases cannot be understood globally but understood only as the result of several local factors, such as climate and hygiene conditions, population density and structure, and cultural habits and mobility.
Epidemic models aim to capture the mechanisms that link these factors to the emergent epidemics and to promote an understanding of the underlying dynamic processes as a prerequisite for intervention strategies CITATION, CITATION.
A useful abstraction in this context is to regard individuals that may be infected as nodes of a network in which the links are the potentially infectious contacts among individuals.
A major remaining challenge in modern epidemiology is to link the variability of transmission networks to the corresponding emergent epidemics.
Models that are flexible and can be adapted to specific epidemic situations best meet these challenges.
Because we focus on the interplay between transmission network topology and epidemics, we will restrict ourselves to diseases caused by agents that lead to either immunity or death in their host, i.e., in which infection can occur only once.
These epidemics can be described by Susceptible-Infected-Recovered or SIR models CITATION, CITATION.
We refer to the mathematically closely related case, where infection eventually leads to the death of the host, as a SID model.
The original, or classical, SIR model CITATION assumes a mass-action type dynamic and as a consequence describes epidemics in homogeneous, well-mixed populations.
Because this is generally not a good approximation of real world situations, current epidemic models strive for an integrated approach that considers both information about the course of disease and the relevant transmission network CITATION, CITATION.
The models vary in their assumptions, attention to detail, computational costs, and as a consequence, their fields of application.
Compartmental SIR models consider different contact patterns in sub-populations and link them via a contact matrix CITATION, providing a coarse-grained, but often adequate, representation.
Network-based SIR models consider the distribution in each individual's number of infectious contacts FORMULA in the transmission network CITATION CITATION.
These models allow for the study of transmission networks with strong heterogeneity in the number of contacts among individuals, which in some cases also means that they consider correlations in the way contacts are made CITATION, CITATION, or clustering CITATION CITATION.
Although these approaches focus on static networks, a recent approach considers networks with arbitrary degree distributions and transient contacts and allows for the derivation of the temporal evolution in the number of susceptible and infected nodes from a closed set of equations CITATION CITATION.
Finally, pair models are a very general approach CITATION for studying SIR epidemics on heterogeneous networks.
They provide a large amount of flexibility in considering the way contacts are made and maintained CITATION CITATION, but, as a trade-off, they quickly become very computationally demanding CITATION .
An assumption often implicitly made in epidemic models is that the epidemic sweeps through the population at much shorter time scales than the time scale of background demographic processes, i.e., natural birth and death processes are neglected.
This is a good approximation in cases such as the yearly influenza epidemics, but it is hardly adequate for HIV epidemics, which span decades.
To compensate for this limitation, we integrated demographic background processes into recent network epidemic models CITATION, CITATION.
With HIV in mind as a case study, we focus on disease epidemics that lead to death after infection of susceptible individuals, possibly after undergoing several stages of the disease.
In addition to earlier work, our approach also allows for an in depth study of the interplay between epidemic spreading and the structure and dynamics of the underlying transmission network.
Strictly speaking, all approaches discussed only predict the mean behavior of epidemics within the limit of an infinite host population.
However, our comparisons with finite size, agent-based simulations show that this is a good and computationally efficient approximation already for moderate population sizes.
It is shown that existing processing schemes of 3D motion perception such as interocular velocity difference, changing disparity over time, as well as joint encoding of motion and disparity, do not offer a general solution to the inverse optics problem of local binocular 3D motion.
Instead we suggest that local velocity constraints in combination with binocular disparity and other depth cues provide a more flexible framework for the solution of the inverse problem.
In the context of the aperture problem we derive predictions from two plausible default strategies: the vector normal prefers slow motion in 3D whereas the cyclopean average is based on slow motion in 2D.
Predicting perceived motion directions for ambiguous line motion provides an opportunity to distinguish between these strategies of 3D motion processing.
Our theoretical results suggest that velocity constraints and disparity from feature tracking are needed to solve the inverse problem of 3D motion perception.
It seems plausible that motion and disparity input is processed in parallel and integrated late in the visual processing hierarchy.
The representation of the three-dimensional external world from two-dimensional retinal input is a fundamental problem that the visual system has to solve CITATION CITATION.
This is true for static scenes in 3D as well as for dynamic events in 3D space.
For the latter the inverse problem extends to the inference of dynamic events in a 3D world from 2D motion signals projected into the left and right eye.
In the following we exclude observer movements and only consider passively observed motion.
Velocity in 3D space is described by motion direction and speed.
Motion direction can be measured in terms of azimuth and elevation angle, and motion direction together with speed is conveniently expressed as a 3D motion vector in a cartesian coordinate system.
Estimating such a vector locally is highly desirable for a visual system because the representation of local estimates in a dense vector field provides the basis for the perception of 3D object motion, that is direction and speed of moving objects.
This information is essential for interpreting events as well as planning and executing actions in a dynamic environment.
If a single moving point, corner or other unique feature serves as binocular input then intersection of constraint lines or triangulation together with a starting point provides a straightforward and unique geometrical solution to the inverse problem in a binocular viewing geometry.
If, however, the moving stimulus has spatial extent, such as an edge, contour, or line inside a circular aperture CITATION then local motion direction in corresponding receptive fields of the left and right eye remains ambiguous and additional constraints are needed to solve the aperture and inverse problem in 3D.
The inverse optics and the aperture problem are well-known problems in computational vision, especially in the context of stereo CITATION, CITATION, structure from motion CITATION, and optic flow CITATION.
Gradient constraint methods belong to the most widely used techniques of optic-flow computation from image sequences.
They can be divided into local area-based CITATION and into more global optic flow methods CITATION.
Both techniques employ brightness constancy and smoothness constraints in the image to estimate velocity in an over-determined equation system.
It is important to note that optical flow only provides a constraint in the direction of the image gradient, the normal component of the optical flow.
As a consequence some form of regularization or smoothing is needed.
Similar techniques in terms of error minimization and regularization have been offered for 3D stereo-motion detection CITATION CITATION.
Essentially these algorithms extend processing principles of 2D optic flow to 3D scene flow.
Computational studies on 3D motion algorithms are usually concerned with fast and efficient encoding when tested against ground truth.
Here we are less concerned with the efficiency or robustness of a particular implementation.
Instead we want to understand and predict behavioral characteristics of human 3D motion perception.
2D motion perception has been extensively researched in the context of the 2D aperture problem CITATION CITATION but there is a surprising lack of studies on the aperture problem and 3D motion perception.
Any physiologically plausible solution to the inverse 3D motion problem has to rely on binocular sampling of local spatio-temporal information.
There are at least three known cell types in early visual cortex that may be involved in local encoding of 3D motion: simple and complex motion detecting cells CITATION CITATION, binocular disparity detecting cells CITATION sampled over time, and joint motion and disparity detecting cells CITATION CITATION .
It is therefore not surprising that three approaches to binocular 3D motion perception have emerged in the literature: Interocular velocity difference, changing disparity over time, and joint encoding of motion and disparity .
These three approaches have generated an extensive body of research but psychophysical results have been inconclusive and the nature of 3D motion processing remains an unresolved issue CITATION, CITATION.
Despite the wealth of empirical studies on motion in depth there is a lack of studies on true 3D motion stimuli.
Previous psychophysical and neurophysiological studies typically employ stimulus dots with unambiguous motion direction or fronto-parallel random-dot surfaces moving in depth.
The aperture problem and local motion encoding however, which features so prominently in 2D motion perception CITATION CITATION has been neglected in the study of 3D motion perception.
Large and persistent perceptual bias has been found for dot stimuli with unambiguous motion direction CITATION CITATION suggesting processing strategies that are different from the three main processing models CITATION CITATION.
It seems promising to investigate local motion stimuli with ambiguous motion direction such as a line or contour moving inside a circular aperture CITATION because they relate to local encoding CITATION CITATION and may reveal principles of 3D motion processing CITATION .
The aim of this paper is to evaluate existing models of 3D motion perception and to gain a better understanding of binocular 3D motion perception.
First, we show that existing models of 3D motion perception are insufficient to solve the inverse problem of binocular 3D motion.
Second, we establish velocity constraints in a binocular viewing geometry and demonstrate that additional information is necessary to disambiguate local velocity constraints and to derive a velocity estimate.
Third, we compare two default strategies of perceived 3D motion when local motion direction is ambiguous.
It is shown that critical stimulus conditions exist that can help to determine whether 3D motion perception favors slow 3D motion or averaged cyclopean motion.
An important step in understanding gene regulation is to identify the DNA binding sites recognized by each transcription factor.
Conventional approaches to prediction of TF binding sites involve the definition of consensus sequences or position-specific weight matrices and rely on statistical analysis of DNA sequences of known binding sites.
Here, we present a method called SiteSleuth in which DNA structure prediction, computational chemistry, and machine learning are applied to develop models for TF binding sites.
In this approach, binary classifiers are trained to discriminate between true and false binding sites based on the sequence-specific chemical and structural features of DNA.
These features are determined via molecular dynamics calculations in which we consider each base in different local neighborhoods.
For each of 54 TFs in Escherichia coli, for which at least five DNA binding sites are documented in RegulonDB, the TF binding sites and portions of the non-coding genome sequence are mapped to feature vectors and used in training.
According to cross-validation analysis and a comparison of computational predictions against ChIP-chip data available for the TF Fis, SiteSleuth outperforms three conventional approaches: Match, MATRIX SEARCH, and the method of Berg and von Hippel.
SiteSleuth also outperforms QPMEME, a method similar to SiteSleuth in that it involves a learning algorithm.
The main advantage of SiteSleuth is a lower false positive rate.
An important step in characterizing the genetic regulatory network of a cell is to identify the DNA binding sites recognized by each transcription factor protein encoded in the genome.
A TF typically activates and/or represses genes by associating with specific DNA sequences.
Although other factors, such as metabolite binding partners and protein-protein interactions, can affect gene expression CITATION, it is important to identify the sequences directly recognized by TFs to the best of our ability to understand which genes are controlled by which TFs.
A better understanding of gene regulation, which plays a central role in cellular responses to environmental changes, is a key to manipulating cellular behavior for a variety of useful purposes, as in metabolic engineering applications CITATION .
A number of computational methods have been developed for predicting TF binding sites given a set of known binding sites CITATION CITATION.
Commonly used methods involve the definition of a consensus sequence or the construction of a position-specific weight matrix, where DNA binding sites are represented as letter sequences from the alphabet {A, T, C, G}.
More sophisticated approaches further constrain the set of potential binding sites for a given TF by considering, in addition to PWMs, the contribution of each nucleotide to the free energy of protein binding CITATION and additional biologically relevant information, such as nucleotide correlation between different positions of a sequence CITATION or sequence-specific binding energies CITATION.
Perhaps not as widely used as sequence analysis, the idea of employing structural data for predicting TF binding sites has been considered CITATION CITATION.
Most of these methods use protein-DNA structures rather than DNA by itself.
Acquiring training sets large enough to be useful is problematic for even well-studied TFs, for which only small sets of known binding sites are typically available CITATION.
New high-throughput technologies have been used to identify large numbers of binding sites for particular TFs CITATION CITATION, but there remains a need for methods that predict TF binding sites given a small number of positive examples.
Such methods can be used, for example, to complement analysis of high-throughput data.
Binding sites detected by high-throughput in vitro methods, such as protein-binding microarrays CITATION, can be compared with predicted binding sites to prioritize studies aimed at confirming the importance of sites in regulating gene expression in vivo.
The fine three-dimensional structure of DNA is sequence dependent and TF-DNA interactions depend on various physicochemical parameters, such as contacts between nucleotides and amino acid residues and base pair geometry CITATION.
These parameters are not accounted for by conventional methods for predicting TF binding sites, which rely on sequence information alone.
Letter representations of DNA sequences do not capture the biophysics underlying TF-DNA interactions.
Given that a TF does not read off letters from a DNA sequence, but interacts with a particular sequence because of its chemical and structural features, we hypothesized that better predictions of TF binding sites might be generated by explicitly accounting for these features in an algorithm for predicting TF binding sites.
The mechanisms by which TFs recognize DNA sequences can be divided into two classes: indirect readout and direct readout CITATION.
For indirect readout, a TF recognizes a DNA sequence via the conformation of the sequence, which is determined by the local geometry of base pair steps, the distortion flexibility of the DNA sequence, and protein-DNA interactions CITATION, CITATION.
For direct readout, a TF recognizes a DNA sequence through direct contacts between specific bases of the sequence and amino acid residues of the TF CITATION, CITATION.
These two classes of recognition mechanisms are not mutually exclusive.
In this study, we introduce a method, SiteSleuth, for predicting TF binding sites on the basis of sequence-dependent structural and chemical features of short DNA sequences.
By using molecular dynamics methods to calculate these features, we can map a set of known or potential binding sites for a given TF to vectors of structural and chemical features.
We use features of positive and negative examples of TF binding sites to train a support vector machine to discriminate between true and false binding sites.
Negative examples are derived from randomly selected non-coding DNA sequences.
Positive examples are taken from RegulonDB CITATION, which collects information about TFs in Escherichia coli.
Classifiers for E. coli TFs developed through the SiteSleuth approach are evaluated by cross validation, and the classifier for Fis is tested against chromatin immunoprecipitation -chip assays of Fis binding sites CITATION.
Combining ChIP with microarray technology, ChIP-chip assays provide information about DNA-protein binding in vivo on a genome-wide scale CITATION.
We also evaluate the performance of SiteSleuth against four other computational methods: the method of Berg and von Hippel CITATION, MATRIX SEARCH CITATION, Match CITATION, and QPMEME CITATION.
The BvH, MATRIX SEARCH, and Match methods rely on the PWM approach to capture TF preferences for binding sites.
The QPMEME method is similar to SiteSleuth in that it employs a learning algorithm.
In the case of Fis, we show that SiteSleuth generates significantly fewer estimated false positives and provides higher prediction accuracy than the other computational approaches.
Many behavioral phenomena have been found to spread interpersonally through social networks, in a manner similar to infectious diseases.
An important difference between social contagion and traditional infectious diseases, however, is that behavioral phenomena can be acquired by non-social mechanisms as well as through social transmission.
We introduce a novel theoretical framework for studying these phenomena by adapting a classic disease model to include the possibility for automatic non-social infection.
We provide an example of the use of this framework by examining the spread of obesity in the Framingham Heart Study Network.
The interaction assumptions of the model are validated using longitudinal network transmission data.
We find that the current rate of becoming obese is 2FORMULA per year and increases by 0.5 percentage points for each obese social contact.
The rate of recovering from obesity is 4FORMULA per year, and does not depend on the number of non-obese contacts.
The model predicts a long-term obesity prevalence of approximately 42FORMULA, and can be used to evaluate the effect of different interventions on steady-state obesity.
Model predictions quantitatively reproduce the actual historical time course for the prevalence of obesity.
We find that since the 1970s, the rate of recovery from obesity has remained relatively constant, while the rates of both spontaneous infection and transmission have steadily increased over time.
This suggests that the obesity epidemic may be driven by increasing rates of becoming obese, both spontaneously and transmissively, rather than by decreasing rates of losing weight.
A key feature of the SISa model is its ability to characterize the relative importance of social transmission by quantitatively comparing rates of spontaneous versus contagious infection.
It provides a theoretical framework for studying the interpersonal spread of any state that may also arise spontaneously, such as emotions, behaviors, health states, ideas or diseases with reservoirs.
Social network effects are of great importance for understanding human behavior.
People interact with a varying number of individuals and with some individuals more than others, and this affects behavior in fundamental ways.
Sociologists have long studied social influence through networks, and networks now routinely appear in investigations from other fields, including economics CITATION, physics CITATION, public health CITATION and scientific publishing CITATION, CITATION.
Extensive reviews of social networks analysis, including investigations of their structure and their effect on social dynamics, include Mitchell CITATION, Wasserman CITATION ,Watts CITATION, Rogers CITATION, Jackson CITATION, and Smith CITATION.
Networks have also long been known to be important in many areas of biology, including ecological food webs and the evolution of cooperation CITATION CITATION.
Social networks have also been studied as determinants of health, ranging from determining the patterns of infectious disease spread CITATION to the propagation of phenomena such as emotions CITATION CITATION, smoking cessation CITATION, obesity CITATION, suicide CITATION, altruism CITATION, anti-social behavior CITATION, and online health forum participation CITATION.
These studies suggest that on top of the physical environment, the social environment can also be an important contributor to health.
They have lead to suggestions that public health interventions must be designed that work with the network structure and that the network can be exploited to spread health related information CITATION, CITATION .
Within network studies, much work has focused on how information, trends, behaviors and other entities spread between the individuals in social networks.
These processes are generally referred to as contagion.
Such suggestions of contagious dynamics and the possible relevance of network structure can be rigorously examined using mathematical models of contagious processes.
These can then be used to obtain accurate measures of expected prevalences, interventional efficacy, and optimized information flow.
Many previous models have been proposed to study influential interactions between individuals.
Most of these have considered well-mixed populations, although more recent work has focused on network-structured populations.
The most well studied are classic epidemiological models for the spread of microbial infectious diseases CITATION, including spread in network-structured populations CITATION CITATION, CITATION, CITATION.
Various related processes have been used to model social influence, with important contributions including the same epidemiological models CITATION, CITATION, diffusion models CITATION, CITATION CITATION, statistical mechanics type interactions CITATION, CITATION, and threshold models CITATION .
Each of these models, however, has one or more properties that are problematic for studying social contagion.
Many do not capture the probabilistic nature of contagion, or the asymmetry inherent in traditional infectious disease.
Others only consider well-mixed populations, where everyone is influenced by everyone else, ignoring the effect of network structure.
Most models inspired by epidemiology are not directly applicable to the social spread of other phenomenon, because many phenomena that spread by social contagion may also arise spontaneously.
That is, it is possible to adopt a trend or behavior, or obtain information, from an outside source, without directly catching it from a contact in the network.
In other words, on top of the probability of obtaining the infection from each infected contact, there is also a non-zero probability of automatically obtaining the infection, independent of the local network.
This automatic non-social infection is not included in traditional infectious disease models.
Economic models for the diffusion of innovations, based on early work by Bass CITATION, do take into account automatic infection.
Individuals move from susceptible to an infected state by adopting a new product or idea, influenced by both social and non-social factors.
However, these models do not allow for recovery; because the innovation adoptions are assumed to be permanent changes in behavior, individuals never move back to a susceptible state.
This results in the entire population becoming adopters at equilibrium.
This does not reflect the dynamics of many phenomena that spread socially, which may be repeatedly acquired and lost.
Through a balance of infection and recovery, a steady-state with multiple states of individuals coexisting can be reached.
Finally, most previous models make assumptions about the type of interaction between individuals, the particulars of which are not usually validated with real data.
Yet, long term behavior of a model and the prevention strategies it suggests can depend critically on the specifics of the interaction assumptions.
Here, we introduce a new model to study the spread of entities in a social network which has all of the important properties listed above.
We then analyze its characteristics and show how it can be applied in different contexts.
This model is an extension of the classical infectious disease model, combining features from other models mentioned above.
It describes infections that can be contracted both spontaneously and through social transmission, and allows for recovery from infection.
As an example, we focus on the spread of obesity in the Framingham Heart Study network.
The interaction assumptions of the model will be validated using longitudinal network transmission data.
We show how we can quantitatively assess the values for the rate of adopting a trend spontaneously versus by contagion to determine the extent to which social transmission is important.
We use it to predict prevalences and intervention effectiveness.
The results of this model are very different from models with other interaction assumptions, such as the majority rules models.
We will show that transmissive components are often small compared to the automatic component, but may still contribute materially to prevalence levels.
Lastly, we will use pair-wise approximations to generate analytic results for infections in network-structured populations, as well as presenting simulations using a real social network.
Much of the complexity of biochemical networks comes from the information-processing abilities of allosteric proteins, be they receptors, ion-channels, signalling molecules or transcription factors.
An allosteric protein can be uniquely regulated by each combination of input molecules that it binds.
This regulatory complexity causes a combinatorial increase in the number of parameters required to fit experimental data as the number of protein interactions increases.
It therefore challenges the creation, updating, and re-use of biochemical models.
Here, we propose a rule-based modelling framework that exploits the intrinsic modularity of protein structure to address regulatory complexity.
Rather than treating proteins as black boxes, we model their hierarchical structure and, as conformational changes, internal dynamics.
By modelling the regulation of allosteric proteins through these conformational changes, we often decrease the number of parameters required to fit data, and so reduce over-fitting and improve the predictive power of a model.
Our method is thermodynamically grounded, imposes detailed balance, and also includes molecular cross-talk and the background activity of enzymes.
We use our Allosteric Network Compiler to examine how allostery can facilitate macromolecular assembly and how competitive ligands can change the observed cooperativity of an allosteric protein.
We also develop a parsimonious model of G protein-coupled receptors that explains functional selectivity and can predict the rank order of potency of agonists acting through a receptor.
Our methodology should provide a basis for scalable, modular and executable modelling of biochemical networks in systems and synthetic biology.
A goal of biology is to understand the structure and function of the biochemical networks that underpin cellular decision-making.
One organizing principle is that these networks are inherently modular CITATION CITATION, with specific functions ascribed to a subset of proteins in the network.
Yet, like logic gates in electronic circuits, even individual proteins can perform sophisticated computations and integrate multiple inputs CITATION CITATION.
In engineering, a modular approach to the analysis of a system scales well with the size of the system and its complexity.
Indeed, engineers design systems hierarchically with modules comprising other modules.
If molecular biology is similarly modular, which structures are the atomic modules from which larger modules are constructed?
In signalling networks, we may plausibly ascribe this role to protein subunits and domains CITATION, CITATION.
Their function as elementary modules often depends on allosteric transitions: an interaction at one site alters the structure at a distant site via a conformational change.
Indeed, allostery increases the information-processing ability of a network because it transforms proteins from passive substrates to dynamic computational elements CITATION.
A modular approach to the analysis and design of biochemical networks should therefore explicitly describe the computations performed by individual allosteric proteins.
Efforts to tackle complexity in biochemical networks should also exploit the modularity of protein structure.
Protein structure is hierarchical, and a given protein often has domains also present in other proteins or repeated subunits.
For example, many signalling proteins contain SH2 or PDZ domains, and many receptors, ion channels and enzymes are multimers.
In genetic networks, transcription factors are also often multimers or have a common DNA-binding domain, such as a zinc finger or homeobox.
The re-use of protein domains is both a simplifying and confounding feature: once a domain has been characterized, that characterization can be used again, but it is also necessary to model molecular cross-talk between signalling pathways that contain proteins with similar structures.
In vivo, protein interactions can generate both combinatorial and regulatory complexity.
Combinatorial complexity is an explosion in the number of possible species in a system as the number of proteins and interactions in the system increases.
It arises because the number of states of a module dramatically increases as its proteins bind ligands as well as each other and as different residues are covalently modified CITATION, CITATION.
For example, p53, the so-called cellular gatekeeper, has 37 known modification sites and so potentially 2 37 states CITATION.
Thus, a complete description of the system potentially requires a combinatorially large number of chemically distinct species and reactions.
In contrast, regulatory complexity is a combinatorial increase in the number of parameters required to describe the regulatory interactions within a system as the number of interactions increase.
This complexity arises because the strength of protein interactions depends on the state of a module, and each state of the module potentially requires a unique set of parameters to characterize interactions within the module, with other modules in the network, and with molecules external to the network.
Measuring this number of parameters in vivo is challenging.
Rule-based modelling addresses combinatorial complexity and allows biologists to specify the regulatory logic of a system CITATION.
Examples include BioNetGen CITATION, Kappa CITATION, Moleculizer CITATION and StochSim CITATION.
Rather than explicitly enumerating each species and reaction in the network, a rule-based model describes a system as a collection of biomolecules interacting according to a set of rules.
Each rule is a template for a reaction that specifies the reactants, products and all relevant biochemical parameters.
Thus, combinatorially complex systems are compactly described because a large number of distinct reactions are subsumed in the template encoded by a single rule.
An algorithm may automatically infer a complete reaction network prior to simulation or, if the combinatorial complexity is too great, use alternative techniques to simulate the system CITATION, CITATION.
Importantly, some rules also specify contextual conditions that constrain when an interaction can occur and hence encode the regulatory logic of the network.
For example, a rule may allow only a doubly phosphorylated MAP kinase to phosphorylate its substrate.
Rule-based formalisms can describe complex biochemical systems, but inherently offer little guidance on avoiding a number of methodological problems.
First, using rules to specify the regulatory logic of a system does not address the system's regulatory complexity.
Consider G protein-coupled receptors, which allosterically couple an extracellular ligand-binding site to an intracellular G protein-binding site CITATION.
GPCRs can be promiscuous, binding multiple intracellular targets CITATION, CITATION.
Supposing a given GPCR can bind one of L different drugs or endogenous ligands and one of G different G proteins, then in principle we require LG pair-wise cooperativity parameters to describe how each ligand regulates the GPCR's affinity for each G protein.
Thus, the number of regulatory parameters scales with LG, and the number of rules also scales with LG because each parameter is part of a rule with distinct contextual constraints.
Promiscuous allosteric proteins can therefore require a large number of rules and parameters to characterize their interactions.
Second, a module should have a well-described function and be easily re-used and portable between systems, but most rule-based formalisms are not inherently modular.
Modellers typically treat proteins as black boxes and define interactions using biochemical equations.
In such interaction-centric approaches, the regulation of proteins is encoded by rules with ad hoc conditions that no longer apply when the proteins interact with different partners.
These ad hoc rules obfuscate the mechanism underlying allosteric regulation because they do not show explicitly how the intrinsic structural and thermodynamic properties of allosteric proteins generate their functional properties.
In contrast, a biomolecule-centric approach would encode regulatory logic in the proteins themselves.
Fewer changes to rules would then be required to define how a new set of interaction partners regulates the protein's activity.
If a model includes protein domains and subunits, re-use of these components would also be simplified.
Finally, models generated by rule-based methods should be thermodynamically correct.
In biochemical networks, there are often sets of reversible reactions that connect into a closed loop, forming a thermodynamic cycle.
In many of these cycles no free energy is consumed: for example, when proteins bind multiple ligands, when ligands bind several conformations of a protein, or when ion channels bind multiple agonists and have closed, open, and desensitized states.
Thermodynamics imposes a mathematical relationship between the equilibrium constants for all the reactions involved in such cycles: their product must be unity.
Equilibrium constants cannot therefore be assigned independently.
A thermodynamically correct methodology should ensure that a model satisfies this constraint, ideally by construction.
Here, we present a modular and scalable modelling methodology that alleviates the regulatory as well as the combinatorial complexity of biochemical networks.
We first describe our modelling framework, which uses a thermodynamically grounded treatment of allostery in which ligands distinguish only the conformational state of allosteric proteins.
We also introduce a rule-based modelling tool that implements our methodology: the Allosteric Network Compiler.
We use ANC to examine how allostery can make macromolecular assembly more efficacious.
We then show how our modelling framework describes common mechanisms of allostery by mapping the regulatory properties of a protein onto conformational changes in the protein itself and demonstrate how we can ease the analysis of multiple ligands interacting through an allosteric protein.
Next, we discuss how our approach reduces regulatory complexity and thereby increases a model's modularity.
Finally, we use our framework to develop a model of G protein-coupled receptors whose regulatory complexity scales with instead of LG and consequently has greater predictive power.
While our major goal is to introduce a new modular modelling methodology rather than its implementation, we have made ANC and the models we discuss available at: LINK.
The classic algorithms of Needleman Wunsch and Smith Waterman find a maximum a posteriori probability alignment for a pair hidden Markov model.
To process large genomes that have undergone complex genome rearrangements, almost all existing whole genome alignment methods apply fast heuristics to divide genomes into small pieces that are suitable for Needleman Wunsch alignment.
In these alignment methods, it is standard practice to fix the parameters and to produce a single alignment for subsequent analysis by biologists.
As the number of alignment programs applied on a whole genome scale continues to increase, so does the disagreement in their results.
The alignments produced by different programs vary greatly, especially in non-coding regions of eukaryotic genomes where the biologically correct alignment is hard to find.
Parametric alignment is one possible remedy.
This methodology resolves the issue of robustness to changes in parameters by finding all optimal alignments for all possible parameters in a PHMM.
Our main result is the construction of a whole genome parametric alignment of Drosophila melanogaster and Drosophila pseudoobscura.
This alignment draws on existing heuristics for dividing whole genomes into small pieces for alignment, and it relies on advances we have made in computing convex polytopes that allow us to parametrically align non-coding regions using biologically realistic models.
We demonstrate the utility of our parametric alignment for biological inference by showing that cis-regulatory elements are more conserved between Drosophila melanogaster and Drosophila pseudoobscura than previously thought.
We also show how whole genome parametric alignment can be used to quantitatively assess the dependence of branch length estimates on alignment parameters.
Needleman Wunsch pairwise sequence alignment CITATION is known to be sensitive to parameter choices.
To illustrate the problem, consider the eighth intron of the Drosophila melanogaster CG9935-RA gene located on chr4:660,462 660,522.
This intron, which is 61 base pairs long, has a 60 base pair ortholog in Drosophila pseudoobscura.
The ortholog is located at Contig8094 Contig5509:4,876 4,935 in the August 2003 freeze 1 assembly, as produced by the Baylor Genome Sequencing Center.
Using the basic 3-parameter scoring scheme, these two orthologous introns have the following optimal alignment when the parameters are set to M 5, X 5, S 5:
However, if we change the parameters to M 5, X 6, and S 4, then the following alignment is optimal:
Note that a relatively small change in the parameters produces a very different alignment of the introns.
This problem is exacerbated with more complex scoring schemes, and is a central issue with whole genome alignments produced by programs such as MAVID CITATION or BLASTZ/MULTIZ CITATION.
Indeed, although whole genome alignment systems use many heuristics for rapidly identifying alignable regions and subsequently aligning them, they all rely on the Needleman Wunsch algorithm at some level.
Dependence on parameters becomes an even more crucial issue in the multiple alignment of more than two sequences.
Parametric alignment was introduced by Waterman, Eggert, and Lander CITATION and further developed by Gusfield et al. CITATION, CITATION and Fernandez-Baca et al. CITATION as an approach for overcoming the difficulties in selecting parameters for Needleman Wunsch alignment.
See CITATION for a review and CITATION, CITATION for an algebraic perspective.
Parametric alignment amounts to partitioning the space of parameters into regions.
Parameters in the same region lead to the same optimal alignments.
Enumerating all regions is a non-trivial problem of computational geometry.
We solve this problem on a whole genome scale for up to five free parameters.
Our approach to parametric alignment rests on the idea that the score of an alignment is specified by a short list of numbers derived from the alignment.
For instance, given the standard three-parameter scoring scheme, we summarize each alignment by the number m of matches, the number x of mismatches, and the number s of spaces in the alignment.
The triple is called the alignment summary.
As an example, consider the above pair of orthologous Drosophila introns.
The first alignment has the alignment summary while the second alignment has the alignment summary .
Remarkably, even though the number of all alignments of two sequences is very large, the number of alignment summaries that arise from Needleman Wunsch alignment is very small.
Specifically, in the example above, where the two sequences have lengths 61 and 60, the total number of alignments is 1,511,912,317,060,120,757,519,610,968,109,962,170,434,175,129 1.5 10 46.
There are only 13 alignment summaries that have the highest score for some choice of parameters M,X,S. For biologically reasonable choices, i.e., when we require M X and 2S X, only six of the 13 summaries are optimal.
These six summaries account for a total of 8,362 optimal alignments .
Note that the basic model discussed above has only d 2 free parameters, because for a pair of sequences of lengths l,l all the summaries satisfy
This relation holds with l l for the six summaries in Table 1.
Figure 1 shows the alignment polygon, as defined in the section Alignment polytopes, in the coordinates .
In general, for two DNA sequences of lengths l and l, the number of optimal alignment summaries is bounded from above by a polynomial in l and l of degree d/, where d is the number of free parameters in the model CITATION, CITATION.
For d 2, this degree is 0.667, and so the number of optimal alignment summaries has sublinear growth relative to the sequence lengths.
Even for d 5, the growth exponent d/ is only 3.333.
This means that all optimal alignment summaries can be computed on a large scale for models with few parameters.
The growth exponent d/ was derived by Gusfield et al. CITATION for d 2 and by Fernandez-Baca at al. CITATION and Pachter-Sturmfels CITATION for general d. Table 1 can be computed using the software XPARAL CITATION.
This software works for d 2 and d 3, and it generates a representation of all optimal alignments with respect to all reasonable choices of parameters.
Although XPARAL has a convenient graphical interface, it seems that this program has not been widely used by biologists, perhaps because it is not designed for high throughput data analysis and the number of free parameters is restricted to d 3.
In this paper, we demonstrate that parametric sequence alignment can be made practical on the whole-genome scale, and we argue that computing output such as Table 1 can be very useful for comparative genomics applications where reliable alignments are essential.
To this end, we introduce a mathematical point of view, based on the organizing principle of convexity, which was absent in the earlier studies CITATION, CITATION, CITATION.
Our advances rely on new algorithms, which are quite different from what is implemented in XPARAL, and which perform well in practice, even if the number d of free parameters is greater than three.
Convexity is the organizing principle that reveals the needles in the haystack.
In our example, the haystack consists of more than 10 46 alignments, and the needles are the 8,362 optimal alignments.
The summaries of the optimal alignments are the vertices of the alignment polytope.
The alignment polytope is the convex hull of the summaries of all alignments.
Background on convex hulls and how to compute the alignment polytopes are provided in the section From Genomes to Polytopes.
Thus, parametric alignment of two DNA sequences relative to some chosen pair hidden Markov model means constructing the alignment polytope of the two sequences.
The dimension of the alignment polytope is d, the number of free model parameters.
For d 2, the polytope is a convex polygon, as shown in Figure 1 for the pair of introns above.
The basic model is insufficient for genomics applications.
More realistic PHMMs for sequence alignment include gap penalties.
We consider three such models.
The symmetries of the scoring matrices for these models are derived from those of the evolutionary models known as Jukes Cantor, Kimura-2, and Kimura-3.
The models are reviewed in the section Models, alignment summaries, and robustness cones.
Our contribution is the construction of a whole genome parametric alignment in all four models for D. melanogaster and D. pseudoobscura.
Our methods and computational results are described in the next section.
Three biological applications are presented in the section From Polytopes to Biology.
A discussion follows in the Discussion section.
Our main computational result is the construction of a whole genome parametric alignment for two Drosophila genomes.
This result depended on a number of innovations.
By adapting existing orthology mapping methods, we were able to divide the genomes into 1,999,817 pairs of reliably orthologous segments, and among these we identified 877,982 pairs for which the alignment is uncertain.
We computed the alignment polytopes of dimensions two, three, and four for each of these 877,982 sequence pairs, and of dimension five for a subset of them.
The methods are explained in the section Alignment polytopes.
The vertices of these polytopes represent the optimal alignment summaries and the robustness cones.
These concepts are introduced in the section Models, alignment summaries, and robustness cones.
Computational results are presented in the section Computational results.
The orthology mapping problem for a pair of genomes is to identify all orthologous segments between the two genomes.
These orthologous segments, if selected so as not to contain genome rearrangements, can then be globally aligned to each other.
This strategy is frequently used for whole genome alignment CITATION, CITATION, and we adapted it for our parametric alignment computation.
MERCATOR is an orthology mapping program suitable for multiple genomes that was developed by Dewey et al. CITATION.
We applied this program to the D. melanogaster and D. pseudoobscura genomes to identify pieces for parametric alignment.
The MERCATOR strategy for identifying orthologous segments is as follows.
Exon annotations in each genome are translated into amino acid sequences and then compared with each other using BLAT CITATION.
The annotations are based on reference gene sets, and on ab initio predictions.
The resulting exon hits are then used to build a graph whose vertices correspond to exons, and with an edge between two exons if there is a good hit.
A greedy algorithm is then used to select edges in the graph that correspond to runs of exons that are consistent in order and orientation.
The MERCATOR orthology map for D. melanogaster and D. pseudoobscura has 2,731 segments.
However, in order to obtain a map suitable for parametric alignment, further subdivision of the segments was necessary.
This subdivision was accomplished by the additional step of identifying and fixing exact matches of length at least 10 bp .
We derived 1,116,792 constraints, which are of four possible types: exact matching non-coding sequences, ungapped high scoring aligned coding sequences, segment pairs between two other constraints where one of the segments has length zero, so the non-trivial segment must be gapped, and single nucleotide mismatches that are squeezed between other constraints.
We then removed all segments where the sequences contained the letter N. This process resulted in 877,982 pairs of segments for parametric alignment.
The lengths of the D. melanogaster segments range from one to 80,676 base pairs.
The median length is 42 bp and the mean length is 99 bp.
In all, 90.4 percent of the D. melanogaster genome and 88.7 percent of the D. pseudoobscura genome were aligned by our method.
When movement outcome differs consistently from the intended movement, errors are used to correct subsequent movements by updating an internal model of motor and/or sensory systems.
Here, we examine changes to an internal model of the motor system under changes in the variance structure of movement errors lacking an overall bias.
We introduced a horizontal visuomotor perturbation to change the statistical distribution of movement errors anisotropically, while monetary gains/losses were awarded based on movement outcomes.
We derive predictions for simulated movement planners, each differing in its internal model of the motor system.
We find that humans optimally respond to the overall change in error magnitude, but ignore the anisotropy of the error distribution.
Through comparison with simulated movement planners, we found that aimpoints corresponded quantitatively to an ideal movement planner that updates a strictly isotropic internal model of the error distribution.
Aimpoints were planned in a manner that ignored the direction-dependence of error magnitudes, despite the continuous availability of unambiguous information regarding the anisotropic distribution of actual motor errors.
The motor system is exquisitely sensitive to perturbation.
The ability to sense a discrepancy between planned and executed movement and respond accordingly is one of the hallmarks of motor learning CITATION, CITATION, CITATION, CITATION.
Here, we are concerned with the nature of the error signal used to update future movement plans when the result of a movement does not match the intended outcome.
Of course there is an infinite number of statistics of the error signal that the CNS might use to update future motor plans, ranging from a running average of recent errors, to n th-order moments of the distribution of past errors.
We are interested in exploring the limits of what statistics can be modeled by the nervous system.
Previous work has focused on neuromotor corrections to imposed bias, where corrective responses are found opposite to the direction of previous errors, and proportional to prior error extents CITATION, CITATION, CITATION.
This work supports motor learning models in which future motor plans incorporate an inverse of the command that would have produced the previous error.
This deterministic model of motor learning suggests that errors from past movements are subtracted off of future motor plans.
Such models can be traced at least to Helmholtz CITATION, who used this type of model to describe perceptual constancy following eye movements.
However, these deterministic models fail to recognize that the CNS can neither simply read off a motor error from noisy sensory signals, nor can it produce identical motor outcomes with repetitions of motor commands.
The relationships between sensory signal and motor error, and between motor command and motor outcome, must be inferred; those inferences are far from certain.
Recognizing this, current research has examined the role of uncertainty in motor learning CITATION, CITATION.
For example, Sheidt et al. CITATION added a stochastic element to an average force field and found that subjects adapted to the uncertain field strength by tracking its expectation over recent errors.
Here, we are interested in the response to changes in motor uncertainty, and ask whether these responses result from updating an internal model of motor variance; and if so, which aspects of the variance structure of the uncertain error signal are modeled.
In these studies, we increased motor noise anisotropically by stimulating a reflexive motor response known to occur when reaching in the presence of horizontal visual-field motion, or drift CITATION.
From trial to trial observers were shown leftward motion, rightward motion or a static stimulus, in random order.
The motion, if present, began at the halfway-point of the reach, and resulted in a perturbation of the reach in the direction of the visual motion.
Subjects could not plan in advance for any particular drift condition since these were randomly intermixed, nor could they compensate for the drift online because the timing of the reach and drift-onset insured that reaches were completed before feedback correction was possible CITATION.
Because this reflexive manual following response affects only the horizontal component of a reach, it was possible to test which aspects of the new, anisotropic distribution of motor errors was modeled by the CNS.
We test for changes in the internal representation of motor noise by monitoring changes in reach plans toward visible targets, which depend on the details of the information available to the CNS concerning motor uncertainty.
In these experiments, successful reaches to targets earn subjects a monetary bonus; reaches that instead intersect a neighboring region of the screen induce a monetary loss.
In two sessions, each beginning with reaches to targets without penalties, subjects learn their natural and perturbed noise distributions, and then respond to target-penalty pairs later in the session, allowing us to assess their internal representation of motor uncertainty.
Our results indicate that the CNS updates a strictly circular internal model of motor variance, even when the distribution of actual errors is anisotropic.
This result is consistent with recent psychophysical and neurophysiological results CITATION, CITATION, CITATION, CITATION indicating independent encoding of the directions and extents of movement errors, because a system that updates only a circular internal representation of errors is equivalent to a system that monitors only the magnitudes of those errors, ignoring their directions.
Genome wide association studies, which test for association between common genetic markers and a disease phenotype, have shown varying degrees of success.
While many factors could potentially confound GWA studies, we focus on the possibility that multiple, rare variants may act in concert to influence disease etiology.
Here, we describe an algorithm for RV analysis, RareCover.
The algorithm combines a disparate collection of RVs with low effect and modest penetrance.
Further, it does not require the rare variants be adjacent in location.
Extensive simulations over a range of assumed penetrance and population attributable risk values illustrate the power of our approach over other published methods, including the collapsing and weighted-collapsing strategies.
To showcase the method, we apply RareCover to re-sequencing data from a cohort of 289 individuals at the extremes of Body Mass Index distribution.
Individual samples were re-sequenced at two genes, FAAH and MGLL, known to be involved in endocannabinoid metabolism.
The RareCover analysis identifies exactly one significantly associated region in each gene, each about 5 Kbp in the upstream regulatory regions.
The data suggests that the RVs help disrupt the expression of the two genes, leading to lowered metabolism of the corresponding cannabinoids.
Overall, our results point to the power of including RVs in measuring genetic associations.
The Common Disease, Common Variant hypothesis CITATION CITATION postulates that the etiology of common diseases is mediated by commonly occurring genomic variants in a population.
This has served as the basis for genome wide association studies that test for association between individual genomic markers and the disease phenotype.
Using genome-wide panels of common SNPs, GWA studies have been successful in identifying hundreds of statistically significant associations for many common diseases as well as several quantitative traits CITATION CITATION.
Nevertheless, the success of GWA studies has been mixed.
Significant genetic loci have not been detected for several common diseases that are known to have a strong genetic component CITATION.
Additionally, for many common diseases, associations discovered in GWA studies can account for only a small fraction of the heritability of the disease.
While many factors could potentially confound GWA studies, we focus on the possibility that multiple, rare variants may act in concert to influence disease etiology.
The alternative to the CDCV hypothesis, the Common Disease, Rare Variant hypothesis has been the topic of much recent debate CITATION, and has shown promise in explaining disease etiology in multiple studies.
For example, rare variants have been implicated in reduced sterol absorption and, consequently, lower plasma levels of LDL CITATION, CITATION and colorectal cancer CITATION.
While some studies have shown RVs to increase risk, a recent study indicates that RVs also act protectively, with multiple RVs in renal salt handling genes showing association with reduced renal salt resorption and reduced risk of hypertension CITATION.
Additionally, rare mutations in IFIH1 have been shown to act protectively against type 1 diabetes CITATION .
The aforementioned studies and others focused on re-sequencing of the coding regions of candidate genes using Sanger sequencing.
Recent technological advances in DNA sequencing have made it possible to re-sequence large stretches of a genome in a cost-effective manner.
This is enabling large-scale studies of the impact of RVs on complex diseases.
However, several properties of rare variants make their genetic effects difficult to detect with current approaches.
Bodmer and Bonilla provide an excellent review of the properties of RVs, and the differences between rare, and common variant analysis CITATION.
As an example, if a causal variant is rare, and the disease is common, then the allele's Population-Attributable-Risk, and consequently the odds-ratio, will be low.
Additionally, even highly penetrant RVs are unlikely to be in Linkage Disequilibrium with more common genetic variations that might be genotyped for an association study of a common disease.
Therefore, single-marker tests of association, which exploit LD-based associations, are likely to have low power.
If the CDRV hypothesis holds, a combination of multiple RVs must contribute to population risk.
In this case, there is a challenge of detecting multi-allelic association between a locus and the disease.
Methods to detect such associations are only just being developed.
A natural approach is a collapsing strategy, where multiple RVs at a locus are collapsed into a single variant.
Such strategies have low power when causal and neutral RVs are combined.
Madsen and Browning have recently proposed a weighted-sum statistic to detect loci in which disease individuals are enriched for rare variants CITATION.
In their approach, variants are weighted according to their frequency in the unaffected sample, with low frequency variants being weighted more heavily.
Each individual is scored as a sum of the weights of the mutations carried.
The test then determines if the diseased individuals are weighted more heavily than expected in a null-model.
Madsen and Browning show that with FORMULA of variants in a group being causal and a combined odds ratio FORMULA, the weighted-sum statistic detects associations with high power.
While effective, this approach depends upon the inclusion of high proportion of causal rare variants in the formation of the test statistics and strong penetrance to detect significant association.
In their simulations, the PAR of the locus is partitioned equally among all variants, an assumption that may not always hold.
The Combined Multivariate and Collapsing Method, proposed by Li and Leal, combines variants into groups based upon predefined criteria CITATION.
An individual has a 1 for a group if any variant in the group is carried and a 0 otherwise.
The CMC approach then considers each of the groups in a multivariate analysis to explain disease risk.
This combination of the collapsing approach and multivariate analysis results in an increase of power over single-marker and multiple marker approaches.
However, as Li and Leal point out, the method relies on correct grouping of variants.
The power is reduced as functional variants are excluded and non-functional variants are included in a group.
Assignment of SNPs to incorrect groups may, in fact, decrease power below that attainable through single marker analysis.
Indeed, a recent analysis by Manolio and colleagues suggests that new methods might be needed when the causal variants have both low PAR and low penetrance values CITATION .
Here, we focus on a model-free method, RareCover, that collapses only a subset of the variants at a locus.
Informally, consider a locus FORMULA encoding a set FORMULA of rare variants.
RareCover associates FORMULA with a phenotype by measuring the strongest possible association formed by collapsing any subset FORMULA of variants at FORMULA.
At first glance, such an approach has many problems.
First, selecting an optimal subset of SNPs is computationally intensive, scaling as FORMULA.
We show that a greedy approach to selecting the optimal subset scales linearly, making it feasible to conduct associations on a large set of candidate loci.
A second confounding factor is that the large number of different tests at a locus increase the likelihood of false association.
The adjustment required to control the type I error could decrease the power of the method.
However, extensive simulations show otherwise.
Our results suggest that moderately penetrant alleles FORMULA with small PAR FORMULA, and moderately sized cohorts are sufficient for RareCover to detect significant association.
This compares well with the current power of single-marker GWA studies on common variants, and outperforms other methods for RV detection.
We also applied RareCover to the analysis of two genes, FAAH, and MGLL, in the endocannabinoid pathway in a large sequencing study of obese and non-obese individuals.
The endocannabinoid pathway is an important mediator of a variety of neurological functions CITATION, CITATION.
Endocannabinoids, acting upon CB1 receptors in the brain, the gastrointestinal tract, and a variety of other tissues, have been shown to influence food intake and weight gain in animal models of obesity.
Using a selective endocannabinoid receptor antagonist, SR141716 leads to reduced food intake in mice.
Correspondingly, elevation of leptin levels have been shown to decrease concentrations of endogenous CB1 agonists, Anandamide, and 2-AG in mice, thereby reducing food-intake CITATION.
The FAAH and MGLL enzymes serve as regulators of endocannabinoid signaling in the brain CITATION, by catalyzing the hydrolysis of endocannabinoid including anandamide, and 2-AG.
Gene expression studies in lean and obese women show significantly decreased levels of AEA and 2-AG, as well as over-expression of CB1 and FAAH in lean, as opposed to obese women CITATION.
While evidence points to a genetic association of these loci with obesity, multiple recent studies using common SNPs in the FAAH region have failed to confirm an association CITATION CITATION.
A Pro129Thr polymorphism was tentatively associated with obesity in a cohort of Europe and and Asian ancestry, but has not been replicated in other data CITATION .
We tested the hypothesis that multiple, rare alleles at these loci are associated with obesity.
We have used unpublished data from Frazer and colleagues, where the FAAH and MGLL regions were re-sequenced using next generation technologies in 148 obese and 150 non-obese individuals taken as extremes of the body mass index distribution from subjects in a large clinical trial.
The resequencing identified a number of common, and rare variants in the region.
We applied RareCover to determine if multiple RVs, i.e., allelic heterogeneity, mediated the genetic effects of FAAH and MGLL on obesity.
RareCover identified a single region at each locus with permutation adjusted p-values of FORMULA and FORMULA.
In each case, the significant locus was immediately upstream of the gene, consistent with a regulatory function for the rare variants.
Intrinsically disordered regions serve as molecular recognition elements, which play an important role in the control of many cellular processes and signaling pathways.
It is useful to be able to predict positions of disordered regions in protein chains.
The statistical analysis of disordered residues was done considering 34,464 unique protein chains taken from the PDB database.
In this database, 4.95 percent of residues are disordered.
The statistics were obtained separately for the N- and C-termini as well as for the central part of the protein chain.
It has been shown that frequencies of occurrence of disordered residues of 20 types at the termini of protein chains differ from the ones in the middle part of the protein chain.
Our systematic analysis of disordered regions in PDB revealed 109 disordered patterns of different lengths.
Each of them has disordered occurrences in at least five protein chains with identity less than 20 percent.
The vast majority of all occurrences of each disordered pattern are disordered.
This allows one to use the library of disordered patterns for predicting the status of a residue of a given protein to be ordered or disordered.
We analyzed the occurrence of the selected patterns in three eukaryotic and three bacterial proteomes.
Prediction of protein structure and function is one of the general directions in structural genomics.
Of special interest is prediction of the so-called disordered regions of protein chain.
Such disordered regions often play an important functional role.
It should be emphasized that one type of disordered regions are structured only when they bind to other molecules CITATION, , or under changing the conditions of biochemical medium CITATION, CITATION, but the other kinds of disordered regions are always disordered and never become structured.
Disordered regions of protein chains often cause complications upon expression, purification and crystallization of such proteins.
At present, more than 500 proteins with disordered regions are described in the Disprot database CITATION.
These proteins and domains are either entirely unstructured in the native state or have lengthy disordered regions.
At that functionally important protein regions in such proteins are outside of globular domains, i.e. just in the disordered regions CITATION, CITATION .
Since disordered regions of the protein chain play an important role in the protein functioning, much attention is being paid to their examination and prediction CITATION, CITATION.
Indeed it has been shown that disordered proteins have certain properties which distinguish them from proteins with well-defined structures CITATION.
Abundance of intrinsic disorder in PDB was discussed in a recent study CITATION.
Typically, disordered regions have a low aromatic content and high net charge as well as low sequence complexity and high flexibility CITATION CITATION .
Prediction methods aim to identify disordered regions through the analysis of amino acid sequences using mainly the physico-chemical properties of the amino acids CITATION CITATION or evolutionary conservation CITATION CITATION .
It can be suggested that if one and the same pattern corresponds to disordered regions in the protein structures then it is highly probable that such a pattern will be disordered in other proteins..
Search for disordered patterns is an important task for prediction of disordered regions and search for the functioning of the considered motifs.
The identification of essential features within protein domains can greatly facilitate their functional characterization.
There are well established databases on protein motif or domain information, such as PROSITE, InterPro and Pfam CITATION CITATION .
Creation of a library of disordered patterns is one of the primary tasks in this respect.
There is no information about such a library.
Until now we have known the PEST motif which in most cases is a degradation motif CITATION and the RGD motif which can be found in extracellular matrix proteins such as fibronectin, fibrinogen, prothrombin, tenascin, thrombospondin, vitronectin, and etc. CITATION, CITATION.
The exposed RGD motif constitutes a major recognition site for integrin binding CITATION .
In this work we have been interested in stretches of disordered residues.
As a rule such stretches are short loops inside globular domains and present only one type of disorder, because disordered proteins range from molten globules to chains having no structural preferences whatsoever and from 2 3 residues to several hundreds or even thousands of residues CITATION, CITATION CITATION.
We have analyzed disordered regions and have created a library of disordered motifs and their positions in protein chains from the entire Protein Databank CITATION.
Taking into account the consideration of the library of disordered patterns will help in improving accuracies of predictions for residues to be structured or unstructured inside the given region.
Moreover, our new statistics on the occurrence of unstructured residues will be useful for those who are dealing with prediction of the status of residues to be ordered or disordered.
Combining the motif discovery and disorder protein segment identification in the PDB is a new and promising approach for further studying and understanding the functional role of the obtained patterns in different proteomes.
The question about specificity of these patterns is more important for biological functioning.
We have analyzed the occurrence of the obtained patterns in some eukaryotic proteomes and in some bacterial proteomes .
We used a multi-round, two-party exchange game in which a healthy subject played a subject diagnosed with a DSM-IV disorder, and applied a Bayesian clustering approach to the behavior exhibited by the healthy subject.
The goal was to characterize quantitatively the style of play elicited in the healthy subject by their DSM-diagnosed partner.
The approach exploits the dynamics of the behavior elicited in the healthy proposer as a biosensor for cognitive features that characterize the psychopathology group at the other side of the interaction.
Using a large cohort of subjects, we found statistically significant clustering of proposers' behavior overlapping with a range of DSM-IV disorders including autism spectrum disorder, borderline personality disorder, attention deficit hyperactivity disorder, and major depressive disorder.
To further validate these results, we developed a computer agent to replace the human subject in the proposer role and show that it can also detect these same four DSM-defined disorders.
These results suggest that the highly developed social sensitivities that humans bring to a two-party social exchange can be exploited and automated to detect important psychopathologies, using an interpersonal behavioral probe not directly related to the defining diagnostic criteria.
Social interactions among humans reflect the execution of some of the most important and complex behavioral software with which humans are endowed.
Consequently, we should expect the computations involved in human social exchange to be subtle and perhaps even difficult to expose and study in controlled settings.
However, exposing these computations is crucial if we are to improve our characterization and understanding of normal human cognitive function and dysfunction.
In recent years, the components of social exchange in healthy subjects have been probed using interactive economic exchange games CITATION CITATION.
These games typically involve two subjects interacting for one or multiple rounds through the exchange of monetary gestures to one another.
For our purposes here, these games require three classes of computation be intact and functioning in the minds of the interacting subjects.
They require that each subject can compute norms for what is fair in each exchange, detect deviations in monetary gestures that deviate from these norms, and choose actions predicated on such deviations CITATION CITATION.
These experimental probes have been used previously in the area of behavioral economics and neuroeconomics, but here we show that the behavioral gestures elicited in the context of economic exchange games can be used to classify certain psychopathologies.
The twist in our effort here is that we use a data-driven approach examining the reactions of the healthy partner as a kind of biosensor while playing an exchange game with a subject possessing a psychopathology.
Influenza can be transmitted through respirable, inspirable, direct-droplet-spray, and contact modes.
How these modes are affected by features of the virus strain, host population, and environment have only recently come under investigation.
A discrete-event, continuous-time, stochastic transmission model was constructed to analyze the environmental processes through which a virus passes from one person to another via different transmission modes, and explore which factors increase or decrease different modes of transmission.
With the exception of the inspiratory route, each route on its own can cause high transmission in isolation of other modes.
Mode-specific transmission was highly sensitive to parameter values.
For example, droplet and respirable transmission usually required high host density, while the contact route had no such requirement.
Depending on the specific context, one or more modes may be sufficient to cause high transmission, while in other contexts no transmission may result.
Because of this, when making intervention decisions that involve blocking environmental pathways, generic recommendations applied indiscriminately may be ineffective; instead intervention choice should be contextualized, depending on the specific features of people, virus strain, or venue in question.
On June 11, 2009 the WHO declared the H1N1 influenza virus a pandemic.
Health organizations worldwide were prompted to escalate their efforts to minimize transmission within their jurisdictions.
Airports began to monitor incoming passengers while schools increased their already intensive surveillance activities.
Recommendations were established with regard to masks, hygiene, decontamination, and isolation of suspected cases.
This interest in intervention and control of person-to-person transmitted illnesses with multiple potential routes of transmission began to intensify during the emergence of SARS and later the H5N1 virus.
Heightened awareness of the potential for another pandemic influenza led to increased funding to study non-pharmaceutical interventions by the CDC as well as increased efforts in modeling influenza transmission.
These studies were funded in order to better understand optimal intervention and control strategies.
Much insight was gained into influenza mitigation strategies such as border closure, social distancing, antiviral prophylaxis, restriction of public transportation, and school closure CITATION CITATION.
To date, however, little is known about the relative contributions of the different influenza transmission modes and how these might vary due to heterogeneity in viral strain, host, and environment.
This manuscript explores potential effects of these unknown factors by presenting: a transmission model structure that explicitly describes the environmental processes through which viruses pass from one person to another, thereby distinguishing the different modes of transmission; and an analytical approach that explores which factors increase or decrease different modes of transmission under the given model structure.
The model analyzed is an environmental infection transmission system model that elaborates the approach to such models by Li et al. CITATION by formulating the model in a discrete event framework and greatly expanding on the details of the various processes involved.
It does not define contact events with transmission probabilities for each event as most transmission models do CITATION.
A problem with that approach is defining what constitutes a contact.
Instead we define events related to virus excretion, environmental survival, uptake, and causation of infection.
This allows us to address events at a level that is more relevant to possible interventions and the construction of more meaningful causal theory.
To inform relevant intervention options for influenza, we consider four potential modes of transmission: respirable, inspirable, direct-droplet-spray, and contact mediated transmission CITATION CITATION.
In this manuscript we consider each mode as follows.
Respirable transmission occurs when viruses on small particles are inhaled and deposit in the alveolar region of the lower respiratory tract.
Inspirable transmission occurs when viruses on medium size particles are inhaled and deposit in the upper respiratory tract.
Direct-droplet-spray transmission occurs when viruses on large particles from the cough or sneeze of an infected individual deposit directly on a susceptible individual's mucous membranes.
Contact transmission occurs when an infected person contaminates their own hands or contaminates surfaces via their hands or via droplets with virus laden large particles.
Transfer of pathogens may then result in contamination of the hands of others who then may touch their eyes, nose or mouth to self-inoculate, potentially infecting the upper respiratory tract.
We assess how different feasible model parameters influence how much transmission follows these different routes.
For example, different viruses may have different infectivity, survivability, transferability, or shedding profiles.
Similarly, among different populations who have different behaviors, susceptibility profiles, or shedding profiles, the same virus may have different effects depending on the type of population present.
Finally, even with identical viral strains and human populations, environmental venues may have variable host densities, surface area to volume ratios, or host movement patterns that can generate different population level infection outcomes.
These diverse sources of heterogeneity that we address form the corners of the epidemiologic triad .
We assess the effects of these sources of heterogeneity on relative magnitude of influenza transmission modes in a scenario where all individuals move randomly in an identical fashion.
We construct a detailed stochastic individual based model of environmental influenza transmission.
We use values from empirical literature as well as expert judgment to parameterize this model.
We apply upper and lower parameter constraints to 18 parameters, and obtain a Latin hypercube sample of this constrained parameter space.
We analyze the resulting outcome space with respect to how different transmission modes are more or less important in specific contexts.
With this work we contribute to the body of literature discussing the dominant mode of influenza transmission CITATION, CITATION CITATION.
Additionally, this work takes an incremental step forward from previous environmental infection transmission models CITATION, CITATION, CITATION CITATION as: we model all four modes of influenza transmission simultaneously; we do so in an agent based framework rather than with ordinary differential equation based framework; and this model is solely informed parametrically by empirical work no model fitting or optimization procedures were used to parameterize this model.
We explicitly point out where the holes in the empirical literature exist.
We show that depending on the scenario, one mode may be more or less important than another.
Therefore, when intervening, generic recommendations applied indiscriminately may be ineffective; instead intervention choice should be contextualized depending on the specific features of people, virus, or venue in question.
We consider how features related to pathology, behavior, and microbiology in the host, pathogen, and environment alter the magnitude of transmission via each mode.
Circulation is an important delivery method for both natural and synthetic molecules, but microenvironment interactions, regulated by endothelial cells and critical to the molecule's fate, are difficult to interpret using traditional approaches.
In this work, we analyzed and predicted growth factor capture under flow using computer modeling and a three-dimensional experimental approach that includes pertinent circulation characteristics such as pulsatile flow, competing binding interactions, and limited bioavailability.
An understanding of the controlling features of this process was desired.
The experimental module consisted of a bioreactor with synthetic endothelial-lined hollow fibers under flow.
The physical design of the system was incorporated into the model parameters.
The heparin-binding growth factor fibroblast growth factor-2 was used for both the experiments and simulations.
Our computational model was composed of three parts: media flow equations, mass transport equations and cell surface reaction equations.
The model is based on the flow and reactions within a single hollow fiber and was scaled linearly by the total number of fibers for comparison with experimental results.
Our model predicted, and experiments confirmed, that removal of heparan sulfate from the system would result in a dramatic loss of binding by heparin-binding proteins, but not by proteins that do not bind heparin.
The model further predicted a significant loss of bound protein at flow rates only slightly higher than average capillary flow rates, corroborated experimentally, suggesting that the probability of capture in a single pass at high flow rates is extremely low.
Several other key parameters were investigated with the coupling between receptors and proteoglycans shown to have a critical impact on successful capture.
The combined system offers opportunities to examine circulation capture in a straightforward quantitative manner that should prove advantageous for biologicals or drug delivery investigations.
The bioavailability of molecules as they circulate through the bloodstream is a crucial factor in their signaling capability.
Half-life in circulation can determine the effectiveness of a drug simply by regulating the opportunities a molecule has to interact with the vessel wall.
Although in vivo measurements are routinely made by researchers to monitor serum levels of molecules and to determine half-lives, interactions in the microenvironment are not easily measured or observed.
While some molecules may have a long circulation life, many may have only a single opportunity to interact with the blood vessel walls before being filtered through the liver or kidneys.
In addition, even molecules with a long circulation life may still face impediments to direct interaction with the endothelium.
This, for example, is the case with vascular endothelial growth factor when bound to bevacizumab, a monoclonal antibody to VEGF CITATION, CITATION.
Bevacizumab has been shown to increase the circulating concentration of VEGF in cancer patients when compared to patients not undergoing therapy because of the increased half-life of the growth factor-antibody complex; however the complex is unable to bind to VEGF receptors CITATION making delivery of the VEGF questionable.
In order to better understand the vessel microenvironment and to accurately monitor drug interactions in the context of that microenvironment, better tools are needed to provide meaningful measurements that can predict the fate of molecules in circulation.
Many important measurements have and continue to be made using in vitro mammalian tissue culture methods but there are obvious limitations to the traditional two-dimensional culture approach.
In circulation, the influence of flow on whether a molecule remains in the fluid phase or binds to the vessel wall can be a dominant factor.
This influence cannot be ascertained in static tissue culture studies.
For example, the velocity of blood in the aorta is 400 mm/sec while at the capillary level it is less than 1 mm/sec CITATION.
This reduction in velocity allows the exchange processes at the capillary level to take place more efficiently CITATION and it likely also affects the activity of molecules in circulation that rely on cell surface binding in order to fulfill their roles.
While direct measurement of this binding process is difficult, our model makes use of a commercial bioreactor with endothelial-lined hollow tubes operating under pulsatile flow to mimic the vascular environment architecture and to directly measure the loss of molecules as they pass through these hollow fibers.
We have used a single pass method to allow better assessment of the effect of flow in either retaining molecules in the circulation or permitting their interaction with vessels.
Our approach also makes use of a bolus administration, since this is a typical way in which drugs would be delivered in a clinical setting.
The binding of fibroblast growth factor-2 to its cell surface receptor and the role of heparan sulfate proteoglycans in regulating the process have been of research interest for many years because of their role in angiogenesis, the growth of new blood vessels from existing vessels.
Knowledge of how these processes work could aid in the development of new therapeutics to control tumor growth and assist clinically in the treatment of chronic wounds.
In order to understand the mechanism of FGF-2-mediated cell proliferation, a multitude of experimental studies have been undertaken CITATION and, in the past two decades, several computational models of FGF-2 binding to its receptor FGFR and HSPG have been proposed CITATION CITATION.
Insight can be gained through experiment-coupled modeling that could not otherwise be readily obtained.
Nugent and Edelman CITATION were among the earliest researchers to develop a simple model that includes three species, FGF-2, FGFR and HSPG.
They measured kinetic binding rate constants experimentally and used their model to analyze the data thereby providing a foundation for investigating the complexity of FGF-2 binding.
A similar approach was used by Ibrahimi et al CITATION to investigate stepwise assembly of a ternary FGF-2-FGFR-HSPG complex in conjunction with their surface plasmon resonance measurements.
We introduced more complexity into the FGF-2 binding model with the inclusion of heparin binding CITATION, receptor dimerization CITATION, and formation of alternative HSPG-FGFR species CITATION.
Recent models have moved towards including intracellular signaling CITATION.
With the exception of work by Filion and Popel CITATION, CITATION, which included diffusive transport, previous simulation work has been based on a static tissue culture environment that may be quite different from the dynamic in vivo environment of blood vessels.
We introduced a computational model based on a flow environment in which the competitive binding of FGF-2, FGFR, and HSPG in a pulsatile flow environment was addressed to mimic blood vessel-like hollow fibers CITATION, CITATION.
In this paper we use an updated version of that model to explore how specific parameters such as flow rate impact FGF-2 capture and receptor binding, and compare our results with experimental studies.
Insights with regard to the importance of surface coupling and ligand depletion zones within the fluid phase were found.
The described simulation package provides a new and valuable way to investigate growth factor capture and can be easily extended to other biologically relevant molecules and drugs.
Protein folding dynamics is often described as diffusion on a free energy surface considered as a function of one or few reaction coordinates.
However, a growing number of experiments and models show that, when projected onto a reaction coordinate, protein dynamics is sub-diffusive.
This raises the question as to whether the conventionally used diffusive description of the dynamics is adequate.
Here, we numerically construct the optimum reaction coordinate for a long equilibrium folding trajectory of a Go model of a FORMULA-repressor protein.
The trajectory projected onto this coordinate exhibits diffusive dynamics, while the dynamics of the same trajectory projected onto a sub-optimal reaction coordinate is sub-diffusive.
We show that the higher the free energy profile for the putative reaction coordinate, the more diffusive the dynamics become when projected on this coordinate.
The results suggest that whether the projected dynamics is diffusive or sub-diffusive depends on the chosen reaction coordinate.
Protein folding can be described as diffusion on the free energy surface as function of the optimum reaction coordinate.
And conversely, the conventional reaction coordinates, even though they might be based on physical intuition, are often sub-optimal and, hence, show sub-diffusive dynamics.
A free energy surface projected onto one or a small number of coordinates is often used to describe the equilibrium and kinetic properties of complex systems with a very large number of degrees of freedom.
Studies of protein folding are an important case where this type of projected surface has been introduced and coordinates such as the number of native contacts and radius of gyration have been used CITATION CITATION.
Protein folding then is described as diffusion on the projected free energy surface.
Diffusive dynamics is characterized by means square displacement linearly growing with time, FORMULA, where D is the diffusion coefficient.
For a single reaction coordinate diffusive dynamics is completely specified by the free energy profile, i.e. the free energy as a function of the coordinate and coordinate-dependent diffusion coefficient, which conveniently can be computed from conventional and cut based free energy profiles CITATION.
Construction of a good reaction coordinate is challenging.
In many cases, the standard progress variables are not good reaction coordinates, because they do not preserve the barriers on the FES and thus may mask the inherent complexity of the latter CITATION.
A number of methods to construct good reaction coordinates have been suggested CITATION, CITATION CITATION .
Employing the Mori-Zwanzig formalism CITATION, CITATION one can derive generalized Langevin equations, which describe system dynamics projected on the reaction coordinates.
The generalized Langevin equation contains a memory kernel, which leads to non-Markovian dynamics and subdiffusion.
Subdiffusion is characterized by the mean square displacement growing slower than that for diffusion, FORMULA with exponent FORMULA.
To completely specify dynamics in this case one has to compute the memory kernel, which is not trivial, since it requires the solution of a multidimensional partial differential equation CITATION.
Long-term memory in correlation functions and anomalous diffusion in proteins was observed experimentally and theoretically CITATION CITATION.
This raises the question whether the folding dynamics of proteins can be described as simple diffusion on the projected free energy surface, as is often done, or if one has to use more sophisticated descriptions, e.g. generalized Langevin equations CITATION, CITATION, fractional Fokker-Plank equations CITATION or multiscale state space networks CITATION.
Here we show that if the reaction coordinate is properly optimized, then the dynamics projected onto this coordinate is diffusive, while the same dynamics projected onto a sub-optimal coordinate is sub-diffusive.
Catalysis of ADP-ATP exchange by nucleotide exchange factors is central to the activity of Hsp70 molecular chaperones.
Yet, the mechanism of interaction of this family of chaperones with NEFs is not well understood in the context of the sequence evolution and structural dynamics of Hsp70 ATPase domains.
We studied the interactions of Hsp70 ATPase domains with four different NEFs on the basis of the evolutionary trace and co-evolution of the ATPase domain sequence, combined with elastic network modeling of the collective dynamics of the complexes.
Our study reveals a subtle balance between the intrinsic and specific mechanisms shared by the four complexes.
Two classes of key residues are distinguished in the Hsp70 ATPase domain: highly conserved residues, involved in nucleotide binding, which mediate, via a global hinge-bending, the ATPase domain opening irrespective of NEF binding, and not-conserved but co-evolved and highly mobile residues, engaged in specific interactions with NEFs.
The observed interplay between these respective intrinsic and specific interactions provides us with insights into the allosteric dynamics and functional evolution of the modular Hsp70 ATPase domain.
Many proteins are molecular machines.
They function because their three-dimensional structure allows them to undergo cooperative changes in conformation that maintain the native fold while enabling their biological functions.
The changes have been pointed out to be structure-encoded, intrinsically accessible to proteins, as can be deduced from simple physics-based approaches CITATION.
Yet, amino acid specificity is another important property that selectively mediates the interactions with specific partners and ligands CITATION.
Overall, a subtle balance exists between structure-encoded mechanical properties and sequence-encoded specific properties, and this balance must be evolutionarily optimized to achieve precise functioning.
The interplay between these two effects becomes particularly important in the case of a number of proteins or domains that play a modular role in a variety of biomolecular interactions.
The ATPase domain of the Hsp70 family of proteins is a typical example.
This domain plays a critical role in regulating the activities of these molecular chaperones, which, in turn, promote accurate folding, and prevent unwanted aggregation by either unfolding and refolding misfolded proteins or regulating their intracellular trafficking to the protein degradation machinery CITATION CITATION .
Chaperones of the Hsp70 family contain two domains: the N-terminal ATPase domain and the C-terminal substrate-binding domain, which regulate each other's activity via allosteric effects.
ATP hydrolysis at the ATPase domain increases the substrate-binding affinity of the SBD, thus lowering the substrate exchange rate; on the other hand, the dissociation of the ADP produced upon ATP hydrolysis and its replacement by a new ATP trigger the release of substrate by the SBD, and therefore enhance the substrate exchange rate CITATION.
Regulation of substrate-binding affinity by the ATPase domain forms the basis of the chaperone activity of Hsp70s CITATION, CITATION .
The precise functioning of the Hsp70 ATPase domain involves an interaction with two families of co-factors, also called co-chaperones: the J-domain proteins that catalyze ATP hydrolysis CITATION, and the nucleotide exchange factors that assist in the replacement of ADP with ATP, by significantly increasing the ADP dissociation rate CITATION.
A molecular understanding of Hsp70 function requires a systemic analysis of the structural basis and mechanism of interaction with these co-chaperones.
Here we focus on the interaction of their ATPase domain with NEFs.
The Hsp70 ATPase domain is composed of four subdomains: IA and IB in lobe I, and, IIA and IIB in lobe II.
ATP binds the central cleft between the two lobes at the interface between subdomains IIA and IIB such that the geometric and energetic effects of its binding and hydrolysis are efficiently transmitted throughout the ATPase domain.
To date, four classes of NEFs have been identified: GrpE in prokaryotes CITATION, and BAG-1 CITATION, HspBP1 CITATION and Hsp110 CITATION in eukaryotes.
Their diverse three-dimensional structures exhibit a variety of binding geometries and interfacial interactions with the Hsp70 ATPase domain.
In the present study, we examine these interactions, using sequence-, structure- and dynamics-based computations and identify their shared features.
Our analysis provides insights into the generic and specific aspects of ATPase domain-NEF interactions, as well as the molecular machinery and sequence design principles of this highly versatile module, the Hsp70 ATPase domain, thus reconciling robust structure-encoded cooperative dynamics properties and highly correlated amino acid changes that enable specific recognition.
The cerebral cortex is divided into many functionally distinct areas.
The emergence of these areas during neural development is dependent on the expression patterns of several genes.
Along the anterior-posterior axis, gradients of Fgf8, Emx2, Pax6, Coup-tfi, and Sp8 play a particularly strong role in specifying areal identity.
However, our understanding of the regulatory interactions between these genes that lead to their confinement to particular spatial patterns is currently qualitative and incomplete.
We therefore used a computational model of the interactions between these five genes to determine which interactions, and combinations of interactions, occur in networks that reproduce the anterior-posterior expression patterns observed experimentally.
The model treats expression levels as Boolean, reflecting the qualitative nature of the expression data currently available.
We simulated gene expression patterns created by all FORMULA possible networks containing the five genes of interest.
We found that only FORMULA of these networks were able to reproduce the experimentally observed expression patterns.
These networks all lacked certain interactions and combinations of interactions including auto-regulation and inductive loops.
Many higher order combinations of interactions also never appeared in networks that satisfied our criteria for good performance.
While there was remarkable diversity in the structure of the networks that perform well, an analysis of the probability of each interaction gave an indication of which interactions are most likely to be present in the gene network regulating cortical area development.
We found that in general, repressive interactions are much more likely than inductive ones, but that mutually repressive loops are not critical for correct network functioning.
Overall, our model illuminates the design principles of the gene network regulating cortical area development, and makes novel predictions that can be tested experimentally.
The mammalian cerebral cortex is a complex but extremely precise structure.
In adult, it is divided into several functionally distinct areas characterised by different combinations of gene expression, specialised cytoarchitecture and specific patterns of input and output connections.
But how does this functional specification arise?
There is strong evidence that both genetic and activity-dependent mechanisms play a role in the development of these specialised areas, a process also referred to as arealisation.
A genetic component is implicated by the spatial non-uniformity of expression of some genes prior to thalamocortical innervation, as well as the fact that altering expression of some genes early in development changes area position in adult CITATION CITATION.
On the other hand, manipulating thalamocortical inputs, and hence activity from the thalamus, can alter area size or respecify area identity CITATION, CITATION, CITATION.
These results are accommodated in a current working model of cortical arealisation as a multi-stage process where initial broad spatial patterns of gene expression provide a scaffold for differential thalamocortical innervation CITATION.
Patterned activity on thalamocortical inputs then drives more complex and spatially restricted gene expression which, in turn, regulates further area specific differentiation.
This paper focuses on the earliest stage of arealisation: how patterns of gene expression form early in cortical development.
Experiments have identified many genes expressed embryonically that are critical to the positioning of cortical areas in adult.
Although arealisation occurs in a two-dimensional field, most experiments focus on anterior-posterior patterning and hence, here we concentrate on patterning along this axis.
From around embryonic day 8 in mouse, the morphogen Fgf8 is expressed at the anterior pole of the developing telencephalon CITATION, CITATION, CITATION, CITATION CITATION.
Immediately after Fgf8 expression is initiated in mouse, four transcription factors, Emx2, Pax6, Coup-tfi and Sp8 are expressed in gradients across the surface of the cortex CITATION, CITATION, CITATION, CITATION, CITATION.
These four TFs are an appealing research target because their complementary expression gradients could provide a unique coordinate system for arealisation CITATION, equivalent to positional information CITATION, CITATION.
Altered expression of each of Fgf8 and the four TFs shifts area positions in late embryonic stages and in adult CITATION CITATION ; CITATION.
Furthermore, during development, altered expression of each of these genes up- or down-regulates expression of some other genes in the set along the anterior-posterior axis.
A large cohort of experiments has given rise to a hypothesised network of regulatory interactions between these five genes.
However, only one of these interactions has been directly demonstrated CITATION and no analysis has been performed at the systems level.
Interacting TFs are known to be able to form regulatory networks that drive differential spatial development, fulfilling a role for which morphogens are better known CITATION, CITATION.
Feedback loops are the crucial feature that enable the generation of spatial patterns of expression of the genes in the network.
Since TFs regulate the expression of other genes, local differences in expression of a set TFs are a powerful method of generating spatial patterns of growth, differentiation and expression of guidance cues, and developing more complex patterns of gene expression.
The arealisation genes form a regulatory network with many feedback loops which is in principle capable of generating spatial patterns.
Establishing which interactions are critical for correct arealisation is of great interest to the field, but current experimental approaches are limited in their ability to quickly assay the importance of each particular interaction.
Computational modelling of gene regulatory networks is necessary because their complex behaviour is difficult to understand intuitively.
In addition, it offers several other benefits.
Currently, the many hypothesised interactions between arealisation genes are represented as arrow diagrams like that seen in Figure 2A.
Because intuition tends to follow simple causal chains, the presence of many feedback loops makes intuition about the overall behaviour of complex systems unreliable CITATION CITATION.
Consequently, a more formal description than an arrow diagram would test the current conceptual model, and has the potential to give greater understanding and insight, as it has done for many other regulatory networks CITATION CITATION, CITATION CITATION.
The unambiguous descriptions found in mathematical and computational models offer the added benefit of making assumptions explicit and therefore allowing greater scrutiny CITATION.
Computational experiments can also be performed quickly and cheaply relative to laboratory experiments and consequently can be useful for conducting thought experiments which can then be tested experimentally CITATION, CITATION.
In this way, computational modelling and experiments can spur each other on so that both are improved in a synergistic manner CITATION .
Here, we use the Boolean logical approach to model the arealisation regulatory network.
In this approach, variables representing genes and proteins can take only two values, zero or one, representing gene and protein activity being below or above some threshold for an effect.
While continuous models are more realistic, they have many free parameters which are hard to constrain from experimental data, and offer a formidable computational challenge to investigate systematically.
In contrast, Boolean models can be used when only qualitative expression and interaction data are available, as is the case for arealisation.
In Boolean models, at each point in time, the state of a variable depends on the state of its regulators at the previous time step.
A set of logic equations capture the regulatory relationships between variables and dictate how the system evolves in time.
The Boolean idealisation greatly reduces the number of free parameters while still capturing network dynamics and producing biologically pertinent predictions and insights CITATION, CITATION, CITATION.
In our model, we use only two spatial compartments, one representing the anterior pole and another representing the posterior pole.
The anterior and posterior expression levels after Boolean discretisation are shown in Figure 1C.
More than two expression levels and more than two spatial compartments would be more realistic, but would result in an explosion in the number of parameters currently unconstrained by experimental data.
Having only two expression levels and only two compartments allows us to systematically screen a large number of networks, which would be impossible in a more complex model.
In this paper, we simulate the dynamics of all possible networks created by different combinations in interactions between Fgf8, Emx2, Pax6, Coup-tfi and Sp8, and show that only FORMULA of these networks are able to reproduce the expression patterns observed experimentally.
From this analysis, we identify structural elements common to the best performing networks, as well as elements that never appear in the networks that perform well.
These results reveal important logical principles underlying the cortical arealisation gene network, and suggest potential directions for future experimental investigations.
The scientific literature represents a rich source for retrieval of knowledge on associations between biomedical concepts such as genes, diseases and cellular processes.
A commonly used method to establish relationships between biomedical concepts from literature is co-occurrence.
Apart from its use in knowledge retrieval, the co-occurrence method is also well-suited to discover new, hidden relationships between biomedical concepts following a simple ABC-principle, in which A and C have no direct relationship, but are connected via shared B-intermediates.
In this paper we describe CoPub Discovery, a tool that mines the literature for new relationships between biomedical concepts.
Statistical analysis using ROC curves showed that CoPub Discovery performed well over a wide range of settings and keyword thesauri.
We subsequently used CoPub Discovery to search for new relationships between genes, drugs, pathways and diseases.
Several of the newly found relationships were validated using independent literature sources.
In addition, new predicted relationships between compounds and cell proliferation were validated and confirmed experimentally in an in vitro cell proliferation assay.
The results show that CoPub Discovery is able to identify novel associations between genes, drugs, pathways and diseases that have a high probability of being biologically valid.
This makes CoPub Discovery a useful tool to unravel the mechanisms behind disease, to find novel drug targets, or to find novel applications for existing drugs.
A wealth of knowledge concerning the function of genes and their role in biological processes is present in the biomedical literature, embodied in full text articles or the Medline abstract database.
Various text mining approaches have been developed to extract information on gene function from this body of literature CITATION, CITATION and these have been successfully applied to annotate genes and proteins CITATION CITATION and the interpretation of experimental results CITATION CITATION .
A common method to establish relationships between biomedical concepts such as genes and pathways is co-occurrence CITATION.
This method is built on the assumption that biomedical concepts occurring in the same body of text are in some way biologically related.
Co-occurrence-based methods can also be used to discover new, hidden relationships, assuming that if A and C both are connected with B, A and C might also have a relationship, even if there is no published relationship between A and C. Swanson has provided a classic example in his study in which he found that fish-oil intake is beneficial for patients suffering from Raynaud's disease, a finding that was confirmed experimentally a few years later CITATION, CITATION.
Hidden literature relationships can be used to confirm a hypothesis about a relationship between A and C in a so called closed discovery process CITATION CITATION.
In this process the user provides the hypothesis that A is related to C, which is then tested by mining the literature for shared biomedical concepts that support the hypothesis.
Hidden relationships can also be used to generate novel hypotheses about a relationship between A and C, in a so-called open discovery process CITATION, CITATION, CITATION CITATION.
In this process the user provides a starting point A and examines the literature for hidden relationships with other biomedical concepts that are bridged by intermediates that share co-occurrences with A and C .
The tools that are currently available for performing open discovery experiments are often limited to certain biomedical domains, have only limited number of keywords describing the biomedical terms, or retrieve hidden relationships formed by uninformative concepts, such as in vitro or microarray which are biologically less interesting CITATION, CITATION, CITATION.
Moreover, a bottleneck with all open discovery tools is to identify true, biologically informative, hidden relationships from spurious hits.
In a previous paper we described CoPub CITATION, a database of co-occurrences of 250.000 keywords in Medline abstracts.
CoPub is a database in which the statistical relevance of all co-occurrences is pre-computed, which makes it possible to perform statistical analyses of the significance of the retrieved hidden relationships between biomedical concepts.
In addition, CoPub contains several categories of controlled vocabularies such as genes, drugs, or diseases etc. As such, this database is ideally suited for use in the discovery of hidden relationships.
In this paper we describe CoPub Discovery, a method that uses the CoPub database for the open and closed discovery of hidden literature relationships.
Statistical analysis of the results using ROC curves show that with CoPub Discovery true hidden relationships can be distinguished from true negatives.
Application of this method in open ended retrieval of hidden relations yielded novel hypotheses about gene-disease, drug-disease and drug-biological process relationships which were validated bibliographically.
Moreover, we used CoPub Discovery to identify two novel compounds that would interact with cell proliferation.
Experimental validation showed that these compounds dose dependently inhibited T-cell proliferation.
Accurate modelling of biological systems requires a deeper and more complete knowledge about the molecular components and their functional associations than we currently have.
Traditionally, new knowledge on protein associations generated by experiments has played a central role in systems modelling, in contrast to generally less trusted bio-computational predictions.
However, we will not achieve realistic modelling of complex molecular systems if the current experimental designs lead to biased screenings of real protein networks and leave large, functionally important areas poorly characterised.
To assess the likelihood of this, we have built comprehensive network models of the yeast and human proteomes by using a meta-statistical integration of diverse computationally predicted protein association datasets.
We have compared these predicted networks against combined experimental datasets from seven biological resources at different level of statistical significance.
These eukaryotic predicted networks resemble all the topological and noise features of the experimentally inferred networks in both species, and we also show that this observation is not due to random behaviour.
In addition, the topology of the predicted networks contains information on true protein associations, beyond the constitutive first order binary predictions.
We also observe that most of the reliable predicted protein associations are experimentally uncharacterised in our models, constituting the hidden or dark matter of networks by analogy to astronomical systems.
Some of this dark matter shows enrichment of particular functions and contains key functional elements of protein networks, such as hubs associated with important functional areas like the regulation of Ras protein signal transduction in human cells.
Thus, characterising this large and functionally important dark matter, elusive to established experimental designs, may be crucial for modelling biological systems.
In any case, these predictions provide a valuable guide to these experimentally elusive regions.
Many features of biological systems cannot be inferred from a simple sum of their components but rather emerge as network properties CITATION.
Organisms comprise systems of highly integrated networks or accelerating networks CITATION in which all components are integrated and coordinated in time and space.
Given such complexity, the gaps in our current knowledge prevent us from modelling complete living organisms CITATION, CITATION.
Therefore, the development of bio-computational approaches for identifying new protein functions and protein-protein functional associations can play an important role in systems biology CITATION .
The scarce knowledge of biological systems is further compounded by experimental error.
It is common for different high-throughput experimental approaches, applied to the same biological system, to yield different outcomes, resulting in protein networks with different topological and biological properties CITATION.
However, errors are not restricted to high-throughput analysis.
For example, it has been demonstrated that high-throughput yeast two-hybrid interactions for human proteins are more precise than literature-curated interactions supported by a single publication CITATION .
There has been a great deal of work analysing biological networks across different species, giving insights into how networks evolve.
However, many of these publications have yielded disparate and sometimes contradictory conclusions.
Observation of poor overlap in protein networks across species CITATION and divergence amongst organisms CITATION suggest fast evolution.
Significant variation in subunit compositions of the functional modules has also been observed in protein networks across species CITATION.
However, in contrast to these observations, recent work using combined protein-protein interaction data suggests high conservation of the protein networks between yeast and human CITATION.
This approach, based on data combination, stresses the importance of integrating different data sources to reduce the bias associated with errors in functional prediction, and to increase the coverage in network modelling, and has been demonstrated in numerous studies CITATION CITATION .
Increasing the accuracy of networks by integrating different protein interaction data relies on the intuitive principle that combining multiple independent sources of evidence gives greater confidence than a single source.
For any genome wide computational analyses, we expect the prediction errors to be randomly distributed amongst a large sample of true negative interactions.
Hence, it is unlikely that two independent prediction methods will both identify the same false positive data in large interactomes like yeast or human.
In general, we expect the precision to increase proportionally to the number of independent approaches supporting the same evidence.
From the available list of well-known integration methods specifically designed to integrate diverse protein-protein interaction -PPI- datasets, we chose the Fisher method CITATION in order to have a predictor that is independent from the experimental data used to validate it.
Fisher integration method is not a trained or supervised method as, for example, Naive Bayes or SVM methods.
The Fisher method presumes a Gaussian random distribution of the prediction datasets' scores as a null hypothesis and the Fisher integrated score calculation is based on Information Theory statistics CITATION, CITATION.
Therefore, the Fisher integration score is completely independent of the experimental datasets used in this study to validate and compare the predictions.
In this work, we significantly increase the prediction power of binary protein functional associations in yeast and human proteomes by integrating different individual prediction methods using the Fisher integration method.
Three different untrained methods are implemented: GECO ; hiPPI ; and CODA run with two protein domain classifications, CATH CITATION and PFAM CITATION.
The four different prediction datasets obtained by these methods, were integrated using simple integration and Fisher's method as examples of untrained methods.
Similarly ab-initio prediction datasets from STRING CITATION were also integrated using Fisher integration and compared against the integrated prediction datasets from our methods.
Results from the Fisher integration of our prediction datasets were benchmarked and compared against the individual prediction methods and the results from the integrated STRING methods.
In all cases we demonstrate increased performance for the integrated approach with the Fisher integration of GECO, hiPPI, CODAcath and CODApfam datasets yielding the best results.
Protein pairs identified by significant Fisher integration p-values were used to build a protein network model for yeast and human proteomes referred to as the Predictogram.
Additionally, all the protein-protein associations from several major biological databases, including Reactome CITATION, Kegg CITATION, GO CITATION, FunCat CITATION, Intact CITATION, MINT CITATION and HRPD CITATION were retrieved and combined into a network referred to as a Knowledgegram.
As implemented in other pioneering studies CITATION, we built predicted and experimental models for further comparison.
Different network topology parameters were calculated and compared between KG and PG models for two test species Homo sapiens and Sacharomyces cerevisae.
We observe how the networks change as the cut-off on the confidence score of the predictions is varied.
Results of this PG and KG network comparison demonstrate that PG networks resemble KG networks in many of the major topological features and model a substantial fraction of real protein network associations, as previously observed in some bacterial predicted networks CITATION, CITATION .
There have been frequent observations of low overlaps between different experimental high-throughput approaches CITATION.
Our comparison of the PG and KG models also show that the intersection between the two models is small and that the majority of predictions in the PG are novel predictions.
However, the overlap between PG and KG is significantly higher than expected by random in both species supporting a correspondence between the PG and KG screenings of PPI space.
This PG and KG data overlap is significantly larger in yeast than in human, pointing to a better functional characterization of the yeast PPI network and the presence of larger dark areas in the human PPI network still hidden from current experimental knowledge.
We suggest that this novel prediction set may be a valuable estimation of the relative differences in dark matter of uncharacterised protein-protein associations between both specie, and we show that this dark matter contains key elements, such as hubs, with important functional roles in the cell.
By analogy CITATION, dark matter in protein network models refers to predicted protein-protein associations, whose existence has not yet been experimentally verified.
In this study, we suggest that dark matter involves functional associations difficult to characterise by current experimental assays making any network modelling of organisms highly incomplete and therefore inaccurate.
The results are divided into four main sections in which the predicted and experimental PPI models of human and yeast are compared.
The first section analyses the performance of the single and integrated methods predicting the protein associations and determines the correlation between the prediction scores and the degree of accuracy and noise in the predictions.
The second chapter compares the topological network features of the predicted and experimental PPI models at equivalent levels of accuracy and noise.
The third section searches for functional differences between the predicted and experimental models looking for specific functional areas which appear to be illuminated by the prediction methods but elusive to the experimental approaches.
Whilst the final fourth section explores whether the predicted PPI network graphs contain additional context-based information on protein associations beyond the sets of predicted protein pairs used to build the networks.
The identification and classification of genes and pseudogenes in duplicated regions still constitutes a challenge for standard automated genome annotation procedures.
Using an integrated homology and orthology analysis independent of current gene annotation, we have identified 9,484 and 9,017 gene duplicates in human and mouse, respectively.
On the basis of the integrity of their coding regions, we have classified them into functional and inactive duplicates, allowing us to define the first consistent and comprehensive collection of 1,811 human and 1,581 mouse unprocessed pseudogenes.
Furthermore, of the total of 14,172 human and mouse duplicates predicted to be functional genes, as many as 420 are not included in current reference gene databases and therefore correspond to likely novel mammalian genes.
Some of these correspond to partial duplicates with less than half of the length of the original source genes, yet they are conserved and syntenic among different mammalian lineages.
The genes and unprocessed pseudogenes obtained here will enable further studies on the mechanisms involved in gene duplication as well as of the fate of duplicated genes.
Gene duplication is the major source of biological innovation and diversity as it provides the necessary conditions for the appearance of new or more specialized protein functions CITATION.
In eukaryotic genomes, there are two major mechanisms through which coding gene regions duplicate: retrotransposition and non-homologous recombination.
Whereas retrotransposition can lead in rare occasions to a functional mRNA copy CITATION, it usually results in processed pseudogenes.
The present study focuses on gene copies that, on the other hand, arose through non-homologous recombination, which produces intact genes copies.
It is generally agreed that after such gene duplications, there is a period of functional redundancy and, consequently, a partial relaxation of their associated selective constraints.
This allows each copy to accept a higher level of sequence modification and, therefore, explore new or more specialized roles as long as the basic ancestral function is not compromised.
Although this situation can eventually lead to the formation of novel genes, it is generally believed that it normally ends with the silencing of one of the copies by the accumulation of lethal mutations, and the preservation of the other with the same basic ancestral function CITATION.
Non-functional paralogs are then expected to accumulate mutations at a neutral rate and degenerate as unprocessed pseudogenes.
Similarly, apart from duplicated exons that lead to alternatively spliced isoforms CITATION, incomplete duplications of genes that can neither be transcribed nor translated into complete and functional proteins are also expected to undergo neutral degeneration right after their formation, as occurs with the vast majority of processed pseudogenes.
Currently the silencing of genes after duplication is poorly understood.
Its frequency has been indirectly inferred either through theoretical approaches CITATION, CITATION or from the study of functional genes exclusively CITATION, without taking into account the population of dead gene copies, probably due to the lack of consistent annotation for these regions in public databases.
Not only the identification of unprocessed pseudogenes, but also the overall identification and classification of independent gene copies within regions that underwent several rounds of tandem duplications, are not completely solved, as exemplified in a detailed analysis of a particular region of human Chromosome 2 CITATION.
Previous global analyses of dead gene copies in mammals have focused mainly on retrotransposed pseudogenes CITATION CITATION, which appear to be far more abundant and easier to detect than unprocessed pseudogenes.
We have already attempted to define collections of unprocessed pseudogenes in the context of a genome-wide identification of intergenic pseudogenes from several sequenced genomes CITATION, CITATION CITATION.
The estimated number of these regions fluctuated significantly within mammals: between 3,000 and 4,500 per genome.
However, on the basis of our recent and more detailed analysis of the finished human Chromosomes 2 and 4 CITATION, we estimate that the human genome might actually contain no more than 2,000 unprocessed pseudogenes, because previous sets were somewhat inflated by misclassified retrotranscribed pseudogenes.
In addition to these large-scale approximations, several hundred unprocessed pseudogenes also have been identified during the annotation of single human chromosomes and from detailed studies focused on particular gene families or genomic regions.
Despite all these efforts, a considerable fraction of human and mouse unprocessed pseudogenes is likely to be unannotated or incorrectly classified owing to the difficulties in analyzing complex regions with multiple copies of genes.
Using filtering procedures performed on the available assemblies of the human and mouse genomes, we have carried out a consistent and comprehensive search for gene duplicates independent of previous gene annotations.
We have distinguished the potentially active from the non-functional copies in order to construct the first reliable set of unprocessed pseudogenes.
Fly lobula plate tangential cells are known to perform wide-field motion integration.
It is assumed that the shape of these neurons, and in particular the shape of the subclass of VS cells, is responsible for this type of computation.
We employed an inverse approach to investigate the morphology-function relationship underlying wide-field motion integration in VS cells.
In the inverse approach detailed, model neurons are optimized to perform a predefined computation: here, wide-field motion integration.
We embedded the model neurons to be optimized in a biologically plausible model of fly motion detection to provide realistic inputs, and subsequently optimized model neuron with and without active conductances along their dendrites to perform this computation.
We found that both passive and active optimized model neurons perform well as wide-field motion integrators.
In addition, all optimized morphologies share the same blueprint as real VS cells.
In addition, we also found a recurring blueprint for the distribution of g K and g Na in the active models.
Moreover, we demonstrate how this morphology and distribution of conductances contribute to wide-field motion integration.
As such, by using the inverse approach we can predict the still unknown distribution of g K and g Na and their role in motion integration in VS cells.
Neurons in different animals and brain regions feature a wealth of different dendritic morphologies and distributions of ionic conductances CITATION, CITATION.
While the physiological effects of these morphologies and conductance distributions are increasingly understood, how the computational functions these dendrites perform emerge from their morphologies and physiologies is still incompletely known.
Computational function is defined here as input-output transformation, resulting from the physiology and subserving the biological purpose of that neuron.
Notable exceptions to this incomplete understanding are neurons close the sensory input for which the electrophysiological dynamics are recorded during sensory stimulation, the morphology is known and both can be correlated to the neuron's sensory coding.
One such example are the fly lobula plate tangential cells, which responds to visual motion in preferred directions, and which are demonstrated to be wide-field motion detectors CITATION, CITATION .
We have recently developed an inverse approach to elucidate dendritic structure-function relationships.
The underlying assumption of the inverse is that dendritic structure parallels the computational function performed in dendrites.
In the inverse approach, we start with a computational function of interest and optimize model neurons including dendritic morphology to perform this function.
Here, we apply the inverse approach to investigate the neuronal morphology-function relationship in the fly LTPCs.
We focus on a particular type of LPTC, the VS cells that respond to vertical motion.
Briefly, VS cells receive motion sensitive signals from the medulla in a retinotopic organization on their dendrites.
These inputs are noisy insofar they are corrupted by spatial modulation reflecting the activity of individual inputs to the VS cell CITATION.
While the membrane potential of VS cells at the sites of synaptic input reproduces the fast input dynamics, by the time the signal reaches the axon the cells' physiology and anatomy gets rid of the temporal modulations imposed by their presynaptic local motion detectors.
The output of the LPTCs is a smooth signal that encodes the direction of the presented moving stimulus CITATION, CITATION.
Thus the function of a single VS cell is temporal smoothing of motion sensitive inputs to produce a smooth output signal.
This computation is rewarded in the optimization performed in this work.
In line with the argument described in CITATION we assume that the dendritic morphology computes temporal smoothing.
The aim of this study is to identify the morphological building blocks required to perform wide-field motion integration, and to investigate how physiological processes interact with the morphology to perform wide-field motion integration.
In this work, we start by formalizing the notion of temporal smoothing and subsequently optimize different model neurons to perform this computation.
We found that our optimized model neurons share crucial morphological features with real VS cell morphologies and that the intrinsic dynamics show remarkable similarity to the real cells.
Our results provide an alternative line of support for the hypothesis that the VS cell's morphology contributes to the computation of wide-field motion integration in these cells.
Moreover, from our simulations we were able to identify the morphological building blocks required to perform wide-field motion integration and conclude that passive dendrites alone can account for temporal smoothing, and active ion-channels can be used to balance the responses to visual stimulation in the preferred and null-direction.
Due to the similarity of our optimized models and real VS cells we can predict the actual -but still unknown- distribution of three ionic conductances and their role in wide-field integration in VS cells.
We discuss the significance of our results and compare our findings to a related approach.
A neural field model is presented that captures the essential non-linear characteristics of activity dynamics across several millimeters of visual cortex in response to local flashed and moving stimuli.
We account for physiological data obtained by voltage-sensitive dye imaging which reports mesoscopic population activity at high spatio-temporal resolution.
Stimulation included a single flashed square, a single flashed bar, the line-motion paradigm for which psychophysical studies showed that flashing a square briefly before a bar produces sensation of illusory motion within the bar and moving squares controls.
We consider a two-layer neural field model describing an excitatory and an inhibitory layer of neurons as a coupled system of non-linear integro-differential equations.
Under the assumption that the aggregated activity of both layers is reflected by VSD imaging, our phenomenological model quantitatively accounts for the observed spatio-temporal activity patterns.
Moreover, the model generalizes to novel similar stimuli as it matches activity evoked by moving squares of different speeds.
Our results indicate that feedback from higher brain areas is not required to produce motion patterns in the case of the illusory line-motion paradigm.
Physiological interpretation of the model suggests that a considerable fraction of the VSD signal may be due to inhibitory activity, supporting the notion that balanced intra-layer cortical interactions between inhibitory and excitatory populations play a major role in shaping dynamic stimulus representations in the early visual cortex.
Visual cortical activity does not exclusively mirror visual input but rather reflects the contribution of additional recurrent processes involving lateral and local feedback couplings.
Understanding cortical processing requires a theoretical understanding of the underlying activity dynamics, which can be attained by modeling at various levels of abstraction.
Naturally, the chosen level should match the level at which neuronal recordings are made CITATION.
The activity patterns observed using voltage-sensitive dye imaging reflect population activity at the mesoscopic level CITATION, CITATION.
This suggests the application of mean-field models in which large numbers of neurons are averaged.
Moreover, we are interested in the relation of neuronal dynamics to the spatial dimensions of the cortical sheet.
Neural field models CITATION, CITATION CITATION, in which the efficacy of synaptic coupling depends on the notion of distance between neurons or ensembles of neurons, are therefore our preferred choice.
Here, we show that a minimalistic multiple-layer NF models can simulate mean VSD data in space and time with high accuracy.
The model is an abstract functional description of VSD-recorded dynamics.
Thus, it is in the first place phenomenological.
However, its interpretation in biological terms allows to link its structure and parameters to the neuronal functional architecture.
The imaging data that we model showed: Two stationary stimuli presented in rapid succession produce a pattern that signals propagation of activity across the bar's retinotopic representation in early visual cortex.
ii The obtained pattern was different from activity when the bar was flashed alone, and did not match the simple superposition of activities evoked by individually-presented square and bar stimuli.
iii Rather, we observed propagation of a wave front of activity that was also found when a square stimulus moved physically in visual space CITATION .
Based on the VSD imaging data CITATION, we hypothesized that a two-layer neural field CITATION CITATION model can account for the findings i iii.
If so, this would imply that the feedback from higher brain areas is not a principal requirement to produce motion patterns across primary visual cortex upon presentation of a square and a bar flashed in rapid succession .
Voltage sensitive dye imaging measures relative fluorescence changes that are linearly correlated to changes in membrane potentials CITATION, CITATION.
This technique currently allows recording of in vivo cortical activity at sub- as well as suprathreshold level with at least 10 ms temporal resolution and a spatial resolution of 50 FORMULAm across several millimeters of cortex.
Hence, it is well suited to capture the real-time dynamics of millions of neurons at once.
However, the signal does not distinguish between excitatory or inhibitory contributions to the overall activity.
Therefore, our model explicitly assumes that the VSD signal reflects a mixture of activity from excitatory and inhibitory neuronal populations, and thus contrasts with studies that interpret VSD data as mainly reflecting excitatory activity.
We expect to gain insights about the relative contributions of excitatory and inhibitory activities in the VSD signal, because the model allows separate inspection of its inhibitory and excitatory layers.
In the following, we first describe the underlying data and its preprocessing, our model structure, and our parameter identification procedure.
Then we present the results including the model fit in comparison to further simplified models, the model prediction regarding similar yet novel stimuli, and the results of a standard linear stability analysis of the homogeneous solution of the model.
We then follow with a discussion of the findings in relation to alternative modeling approaches, the physiological interpretation of our model, and the role of excitation and inhibition in the model.
Finally, we consider our results in the context of hypotheses concerning the cortical representation of motion and the origin of the line-motion illusion.
In eukaryotic cells, the internalization of extracellular cargo via the endocytic machinery is an important regulatory process required for many essential cellular functions.
The role of cooperative protein-protein and protein-membrane interactions in the ubiquitous endocytic pathway in mammalian cells, namely the clathrin-dependent endocytosis, remains unresolved.
We employ the Helfrich membrane Hamiltonian together with surface evolution methodology to address how the shapes and energetics of vesicular-bud formation in a planar membrane are stabilized by presence of the clathrin-coat assembly.
Our results identify a unique dual role for the tubulating protein epsin: multiple epsins localized spatially and orientationally collectively play the role of a curvature inducing capsid; in addition, epsin serves the role of an adapter in binding the clathrin coat to the membrane.
Our results also suggest an important role for the clathrin lattice, namely in the spatial- and orientational-templating of epsins.
We suggest that there exists a critical size of the coat above which a vesicular bud with a constricted neck resembling a mature vesicle is stabilized.
Based on the observed strong dependence of the vesicle diameter on the bending rigidity, we suggest that the variability in bending stiffness due to variations in membrane composition with cell type can explain the experimentally observed variability on the size of clathrin-coated vesicles, which typically range 50 100 nm.
Our model also provides estimates for the number of epsins involved in stabilizing a coated vesicle, and without any direct fitting reproduces the experimentally observed shapes of vesicular intermediates as well as their probability distributions quantitatively, in wildtype as well as CLAP IgG injected neuronal cell experiments.
We have presented a minimal mesoscale model which quantitatively explains several experimental observations on the process of vesicle nucleation induced by the clathrin-coated assembly prior to vesicle scission in clathrin dependent endocytosis.
The cellular process of endocytosis is important in the biological regulation of trafficking in cells, as well as impacts the technology of targeted drug delivery in nanomedicine CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION.
In eukaryotic cells, the internalization of extracellular cargo via the endocytic machinery is an important regulatory process required for many essential cellular functions, including nutrient uptake and cell-cell communication.
Several experimental CITATION as well as theoretical CITATION, CITATION, CITATION treatments have addressed mechanisms in endocytosis, yet the role of cooperative protein-protein and protein-membrane interactions in the ubiquitous endocytic pathway in mammalian cells, namely clathrin-dependent endocytosis, remains unresolved.
A sequence of molecular events in CDE is responsible for the recruitment of adaptor protein 2, accessory proteins such as epsin, AP180, Eps15, Dynamin, etc., and the scaffolding protein clathrin to the plasma membrane CITATION.
The accessory proteins such as epsin are implicated in membrane bending CITATION.
Polymerization of clathrin triskelia in the presence of adaptor proteins such as AP-2 results in the clathrin coat formation, and tubulating proteins such as epsin interact with both the clathrin coat as well as the bilayer CITATION to stabilize a clathrin-coated budding vesicle.
The involvement of dynamin is believed to be in the vesicle scission step CITATION.
Even though actin is believed to play an important role in the endocytosis process in S. cerevisiae, in mammalian cells, actin repression, at best, has a small effect on endocytosis CITATION .
We focus on the energetic stabilization of a budding vesicle induced by the clathrin-coat assembly.
Recent work CITATION demonstrates that the membrane invagination only begins in the presence of a growing clathrin coat CITATION.
Experiments performed by down-regulating AP-2 expression CITATION, CITATION as well as those involving the inhibition of epsin CITATION either significantly decrease the number of clathrin-coated pits or alter the distribution of coated-intermediates involved in the vesicle-bud formation.
Although the CDE in mammalian cells remains a complex regulatory process, we believe that a critical and self-consistent set of experiments is now emerging which warrants the formulation of physically-based models to quantitatively describe the bioenergetics of protein-induced vesicle formation in CDE CITATION .
Even though models directly addressing CDE in the experimental context have not been proposed, Oster et al. have addressed yeast endocytosis driven by actin CITATION, CITATION.
Moreover, Kohyama et al. CITATION have shown that model two component membranes bud in response to induced spontaneous curvature or the line tension between the two components of the membrane and Frese et al. have investigated the effect of protein shape and crowding on domain formation and curvature in biological membranes CITATION.
A recent mini-review examining the current experimental trend by Lundmark and Carlsson on driving membrane curvature in clathrin-dependent and clathrin-independent endocytosis is also available CITATION.
We formulate a minimal model, by restricting our focus to three proteins in the clathrin-coat assembly : clathrin, epsin and AP-2, and their role in the stabilization of a budding vesicle on the cell membrane.
Mammalian cells have a diverse set of proteins which often serve as surrogates and participate in compensatory mechanisms.
In this regard, our choice for the ingredients for the minimal model represents roles for the scaffolding proteins, curvature inducing proteins and the adaptor proteins.
Recent experiments CITATION, CITATION have reported characteristics of nucleation and growth of clathrin coat: the initiation was observed to occur randomly, but only within subdomains devoid of cytoskeletal elements.
In BSC1 cell lines, such domains appear to be 400 nm in diameter surrounded by a rim of a 200 nm dead zone.
Notably, the nucleation of clathrin coats was observed only in the 400 nm region CITATION with the following salient properties: in the growth phase, the addition of clathrin proceeds at a steady rate of about one triskelion every 2 s,.
Two fates are possible for a growing coat; they either transform into a vesicle, or they abort containing about 10 40 triskelia, which suggests that the coat sizes are bounded.
While we do not consider the process of nucleation and growth of clathrin, based on the above observations, we study the process of one maturing vesicle in the presence of an assembled clathrin coat of a finite size in a membrane patch free of cytoskeletal elements and subject to a pinned boundary condition at the patch boundary.
For our model cell membrane patch not fortified by cytoskeleton, we employ a typical value of bending rigidity of our 20k BT derived from literature CITATION, CITATION ;.
In this respect, we describe a mean-field model which characterizes the membrane patch as a homogeneous phase with effective properties.
Our model is also mean-field in the sense that it applies to just one vesicular intermediate and the effect of neighboring coats is not included.
As noted earlier, our model does not account for the mechanism of clathrin coat nucleation or that of vesicle scission.
Clathrin triskelia and AP-2 polymerize to form a coat CITATION and the stabilizing interactions in the clathrin coat assembly can be quantified using the free energy of the polymerization process.
Based on in vitro equilibrium data of clathrin cage formation, Nossal CITATION estimated the energetics of a fully-closed clathrin/AP-2 basket relative to a dissolved coat to be 20 k BT.
The inclusion of epsin in the clathrin-coat accounts for 23 k BT of energy per bound epsin: the ENTH domain of epsin binds to the PtdInsP 2 lipid head groups on the membrane with a binding energy of 14 k BT per bound epsin CITATION and the CLAP domain of epsin interacts with clathrin/AP-2 with an energy of 9 k BT CITATION.
The ENTH interactions with the membrane require the presence of PIP2, which constitutes about 1 percent of the total phospholipids on the cell membrane CITATION.
To produce a coated vesicle d 50 nm diameter,, the area of the clathrin coat required is d 2 7850 nm 2.
Considering the area per lipid head-group to be 0.65 nm 2, the number of PIP2 molecules in the membrane spanning the area of the coat is 1 percent of 185.
Hence, we note that the ratio of ENTH binding sites to the CLAP binding sites is 185/29 6, and hence as the clathrin coat grows, we expect sufficient number of the corresponding PIP2 binding sites to be present for the ENTH domain of epsin to bind.
For this reason, we are justified in not explicitly considering PIP2 as a necessary/limiting species in our minimal model.
Contemporary theory of spiking neuronal networks is based on the linear response of the integrate-and-fire neuron model derived in the diffusion limit.
We find that for non-zero synaptic weights, the response to transient inputs differs qualitatively from this approximation.
The response is instantaneous rather than exhibiting low-pass characteristics, non-linearly dependent on the input amplitude, asymmetric for excitation and inhibition, and is promoted by a characteristic level of synaptic background noise.
We show that at threshold the probability density of the potential drops to zero within the range of one synaptic weight and explain how this shapes the response.
The novel mechanism is exhibited on the network level and is a generic property of pulse-coupled networks of threshold units.
Understanding the dynamics of single neurons, recurrent networks of neurons, and spike-timing dependent synaptic plasticity requires the quantification of how a single neuron transfers synaptic input into outgoing spiking activity.
If the incoming activity has a slowly varying or constant rate, the membrane potential distribution of the neuron is quasi stationary and its steady state properties characterize how the input is mapped to the output rate.
For fast transients in the input, time-dependent neural dynamics gains importance.
The integrate-and-fire neuron model CITATION can efficiently be simulated CITATION, CITATION and well approximates the properties of mammalian neurons CITATION CITATION and more detailed models CITATION.
It captures the gross features of neural dynamics: The membrane potential is driven by synaptic impulses, each of which causes a small deflection that in the absence of further input relaxes back to a resting level.
If the potential reaches a threshold, the neuron emits an action potential and the membrane potential is reset, mimicking the after-hyperpolarization.
The analytical treatment of the threshold process is hampered by the pulsed nature of the input.
A frequently applied approximation treats synaptic inputs in the diffusion limit, in which postsynaptic potentials are vanishingly small while their rate of arrival is high.
In this limit, the summed input can be replaced by a Gaussian white noise current, which enables the application of Fokker-Planck theory CITATION, CITATION.
For this approximation the stationary membrane potential distribution and the firing rate are known exactly CITATION, CITATION, CITATION.
The important effect of synaptic filtering has been studied in this limit as well; modelling synaptic currents as low-pass filtered Gaussian white noise with non-vanishing temporal correlations CITATION CITATION.
Again, these results are strictly valid only if the synaptic amplitudes tend to zero and their rate of arrival goes to infinity.
For finite incoming synaptic events which are excitatory only, the steady state solution can still be obtained analytically CITATION, CITATION and also the transient solution can efficiently be obtained by numerical solution of a population equation CITATION.
A different approach takes into account non-zero synaptic amplitudes to first calculate the free membrane potential distribution and then obtain the firing rate by solving the first passage time problem numerically CITATION.
This approach may be extendable to conductance based synapses CITATION.
Exact results for the steady state have so far only been presented for the case of exponentially distributed synaptic amplitudes CITATION .
The spike threshold renders the model an extremely non-linear unit.
However, if the synaptic input signal under consideration is small compared to the total synaptic barrage, a linear approximation captures the main characteristics of the evoked response.
In this scenario all remaining inputs to the neuron are treated as background noise.
Calculations of the linear response kernel in the diffusion limit suggested that the integrate-and-fire model acts as a low-pass filter CITATION.
Here spectrum and amplitude of the synaptic background input are decisive for the transient properties of the integrate-and-fire model: in contrast to white noise, low-pass filtered synaptic noise leads to a fast response in the conserved linear term CITATION.
Linear response theory predicts an optimal level of noise that promotes the response CITATION.
In the framework of spike-response models, an immediate response depending on the temporal derivative of the postsynaptic potential has been demonstrated in the regime of low background noise CITATION.
The maximization of the input-output correlation at a finite amplitude of additional noise is called stochastic resonance and has been found experimentally in mechanoreceptors of crayfish CITATION, in the cercal sensory system of crickets CITATION, and in human muscle spindles CITATION.
The relevance and diversity of stochastic resonance in neurobiology was recently highlighted in a review article CITATION .
Linear response theory enables the characterization of the recurrent dynamics in random networks by a phase diagram CITATION, CITATION.
It also yields approximations for the transmission of correlated activity by pairs of neurons in feed-forward networks CITATION, CITATION.
Furthermore, spike-timing dependent synaptic plasticity is sensitive to correlations between the incoming synaptic spike train and the firing of the neuron, captured up to first order by the linear response kernel CITATION CITATION.
For neuron models with non-linear membrane potential dynamics, the linear response properties CITATION, CITATION and the time-dependent dynamics can be obtained numerically CITATION.
Afferent synchronized activity, as it occurs e.g. in primary sensory cortex CITATION, easily drives a neuron beyond the range of validity of the linear response.
In order to understand transmission of correlated activity, the response of a neuron to fast transients with a multiple of a single synaptic amplitude CITATION hence needs to be quantified.
In simulations of neuron models with realistic amplitudes for the postsynaptic potentials, we observed a systematic deviation of the output spike rate and the membrane potential distribution from the predictions by the Fokker-Planck theory modeling synaptic currents by Gaussian white noise.
We excluded any artifacts of the numerics by employing a dedicated high accuracy integration algorithm CITATION, CITATION.
The novel theory developed here explains these observations and lead us to the discovery of a new early component in the response of the neuron model which linear response theory fails to predict.
In order to quantify our observations, we extend the existing Fokker-Planck theory CITATION and hereby obtain the mean time at which the membrane potential first reaches the threshold; the mean first-passage time.
The advantage of the Fokker-Planck approach over alternative techniques has been demonstrated CITATION.
For non-Gaussian noise, however, the treatment of appropriate boundary conditions for the membrane potential distribution is of utmost importance CITATION.
In the results section we develop the Fokker-Planck formalism to treat an absorbing boundary in the presence of non-zero jumps.
For the special case of simulated systems propagated in time steps, an analog theory has recently been published by the same authors CITATION, which allows to assess artifacts introduced by time-discretization.
Our theory applied to the integrate-and-fire model with small but finite synaptic amplitudes CITATION, introduced in section The leaky integrate-and-fire model, quantitatively explains the deviations of the classical theory for Gaussian white noise input.
After reviewing the diffusion approximation of a general first order stochastic differential equation we derive a novel boundary condition in section Diffusion with finite increments and absorbing boundary.
We then demonstrate in section Application to the leaky integrate-and-fire neuron how the steady state properties of the model are influenced: the density just below threshold is increased and the firing rate is reduced, correcting the preexisting mean first-passage time solution CITATION for the case of finite jumps.
Turning to the dynamic properties, in section Response to fast transients we investigate the consequences for transient responses of the firing rate to a synaptic impulse.
We find an instantaneous, non-linear response that is not captured by linear perturbation theory in the diffusion limit and that displays marked stochastic resonance.
On the network level, we demonstrate in section Dominance of the non-linear component on the network level that the non-linear fast response becomes the most important component in case of feed-forward inhibition.
In the discussion we consider the limitations of our approach, mention possible extensions and speculate about implications for neural processing and learning.
A prevailing theory proposes that the brain's two visual pathways, the ventral and dorsal, lead to differing visual processing and world representations for conscious perception than those for action.
Others have claimed that perception and action share much of their visual processing.
But which of these two neural architectures is favored by evolution?
Successful visual search is life-critical and here we investigate the evolution and optimality of neural mechanisms mediating perception and eye movement actions for visual search in natural images.
We implement an approximation to the ideal Bayesian searcher with two separate processing streams, one controlling the eye movements and the other stream determining the perceptual search decisions.
We virtually evolved the neural mechanisms of the searchers' two separate pathways built from linear combinations of primary visual cortex receptive fields by making the simulated individuals' probability of survival depend on the perceptual accuracy finding targets in cluttered backgrounds.
We find that for a variety of targets, backgrounds, and dependence of target detectability on retinal eccentricity, the mechanisms of the searchers' two processing streams converge to similar representations showing that mismatches in the mechanisms for perception and eye movements lead to suboptimal search.
Three exceptions which resulted in partial or no convergence were a case of an organism for which the targets are equally detectable across the retina, an organism with sufficient time to foveate all possible target locations, and a strict two-pathway model with no interconnections and differential pre-filtering based on parvocellular and magnocellular lateral geniculate cell properties.
Thus, similar neural mechanisms for perception and eye movement actions during search are optimal and should be expected from the effects of natural selection on an organism with limited time to search for food that is not equi-detectable across its retina and interconnected perception and action neural pathways.
Neurophysiology studies of the macaque monkey CITATION CITATION support the existence of two functionally distinct neural pathways in the brain mediating the processing of visual information.
The behavior of patients with brain damage has led to the proposal that perception is mediated by the ventral stream projecting from the primary visual cortex to the inferior temporal cortex, and that action is mediated by the dorsal stream projecting from the primary visual cortex to the posterior parietal cortex CITATION CITATION.
Although there has been debate about whether this separation into ventral/dorsal streams implies that the brain contains two distinct neural representations of the visual world CITATION CITATION, there has been no formal theoretical analysis about the functional consequences of the two different neural architectures on an animal's survival.
Visual search requires animals to move their eyes to point the high-resolution region of the eye, the fovea, to potentially interesting regions of the scene to sub-serve perceptual decisions such as localizing food or a predator.
What is the impact of having similar versus different neural mechanisms guiding eye movements and mediating perceptual decisions on visual search performance for an organism with a foveated visual system?
We consider two leading computational models of multiple-fixation human visual search, the Bayesian ideal searcher CITATION CITATION and the ideal saccadic targeting model for a search task of a target in one of eight locations equidistant from initial fixation.
The ideal searcher uses knowledge of how the detectability of a target varies with retinal eccentricity and statistics of the scenes to move the fovea to spatial locations which maximize the accuracy of the perceptual decision at the end of search CITATION.
The saccadic targeting model makes eye movements to the most probable target location CITATION, CITATION which is optimal if the goal was to saccade to the target rather than collect information to optimize a subsequent perceptual decision CITATION.
Depending on the spatial layout of the possible target locations and the visibility map, the IS and MAP strategies lead to similar or diverging eye-fixations.
For example for a steeply varying visibility map both models make eye movements to the possible target locations while for a broader visibility map the ideal searcher tends to make eye movements in between the possible target locations attempting to obtain simultaneous close-to-fovea processing for more than one location.
Covert attention allows both models to select possible target locations and ignore locations that are unlikely to contain the target when deciding on saccade endpoints and making perceptual search decisions CITATION, CITATION.
Perceptual target localization decisions for both models are based on visual information collected in parallel over the whole retina, temporally integrated across saccades, and based on the location with highest sensory evidence for the presence of the target.
Critically, we implemented the models to have two processing pathways, one determining where to move the fovea and the other stream processing visual information to reach a final perceptual decision about the target location.
Rather than having a single linear mechanism or perceptual template, each pathway in the model had its own neural mechanism which is compared to the incoming visual data at each possible target location.
Likelihood ratios CITATION of the observed responses for each of the mechanisms under the hypothesis that the target is present or absent at that location are used to make decisions about where to move the eyes and perceptual decisions .
We used a genetic algorithm as a method to find near-optimal solutions for perception and action mechanisms but also to simulate the effects of the evolutionary process of natural selection on the neural mechanisms driving saccadic eye movements and perceptual decisions during search.
The computational complexity of the ideal Bayesian searcher makes it difficult to virtually evolve the model and thus we used a recently proposed approximation to the ideal searcher that is computationally faster.
The ELM model chooses the fixation location that minimizes the uncertainty of posterior probabilities over the potential target locations.
The decision rule can be simplified to choose the fixation location with the maximum sum of likelihood ratios across potential target locations, each weighted by its squared detectability given the fixation location CITATION.
The ELM model can be shown to approximate the fixation patterns of the ideal searcher CITATION and capture the main characteristics of the fixation patterns of the IS for our task and visibility maps.
The process of virtual evolution started with the creation of one thousand simulated individuals with separate linear mechanisms for perception and eye movement programming.
Each pathway's template for each individual was created from independent random combinations of the receptive fields of twenty four V1 simple cells.
Each simulated individual was allowed two eye movements before making a final perceptual search decision about the location of the target.
Performance finding the target in one of eight locations for five thousand test-images was evaluated and the probability of survival of an individual was proportional to its performance accuracy.
A new generation was then created from the surviving individuals through the process of reproduction, mutation and cross-over.
The process was repeated for up to 500 generations.
What proteins interacted in a long-extinct ancestor of yeast?
How have different members of a protein complex assembled together over time?
Our ability to answer such questions has been limited by the unavailability of ancestral protein-protein interaction networks.
To overcome this limitation, we propose several novel algorithms to reconstruct the growth history of a present-day network.
Our likelihood-based method finds a probable previous state of the graph by applying an assumed growth model backwards in time.
This approach retains node identities so that the history of individual nodes can be tracked.
Using this methodology, we estimate protein ages in the yeast PPI network that are in good agreement with sequence-based estimates of age and with structural features of protein complexes.
Further, by comparing the quality of the inferred histories for several different growth models, we provide additional evidence that a duplication-based model captures many features of PPI network growth better than models designed to mimic social network growth.
From the reconstructed history, we model the arrival time of extant and ancestral interactions and predict that complexes have significantly re-wired over time and that new edges tend to form within existing complexes.
We also hypothesize a distribution of per-protein duplication rates, track the change of the network's clustering coefficient, and predict paralogous relationships between extant proteins that are likely to be complementary to the relationships inferred using sequence alone.
Finally, we infer plausible parameters for the model, thereby predicting the relative probability of various evolutionary events.
The success of these algorithms indicates that parts of the history of the yeast PPI are encoded in its present-day form.
Many biological, social, and technological networks are the product of an evolutionary process that has guided their growth.
Tracking how networks have changed over time can help us answer questions about why currently observed network structures exist and how they may change in the future CITATION.
Analyses of network growth dynamics have studied how properties such as node centrality and community structure change over time CITATION CITATION, how structural patterns have been gained and lost CITATION, and how information propagates in a network CITATION .
However, in many cases only a static snapshot of a network is available without a node-by-node or edge-by-edge history of changes.
Biology is an archetypical domain where older networks have been lost, as ancestral species have gone extinct or evolved into present-day organisms.
For example, while we do have a few protein-protein interaction networks from extant organisms, these networks do not form a linear progression and are instead derived from species at the leaves of a phylogenetic tree.
Such networks are separated by millions of years of evolution and are insufficient to track changes at a fine level of detail.
For social networks, typically only a single current snapshot is available due to privacy concerns or simply because the network was not closely tracked since its inception.
This lack of data makes understanding how the network arose difficult.
Often, although we do not know a network's past, we do know a general principle that governs the network's forward growth.
Several network growth models have been widely used to explain the emergent features of observed real-world networks CITATION, CITATION CITATION.
These models provide an iterative procedure for growing random graphs that exhibit similar topological features as a class of real networks.
For example, preferential attachment has explained many properties of the growing World Wide Web CITATION.
The duplication-mutation with complementarity model was found by Middendorf et al. CITATION to be the generative model that best fit the D. melanogaster protein interaction network.
The forest fire model was shown CITATION to produce networks with properties, such as power-law degree distribution, densification, and shrinking diameter, that are similar to the properties of real-world online social networks.
Although these random graph models by themselves have been useful for understanding global changes in the network, a randomly grown network will generally not isomorphically match a target network.
This means that the history of a random graph will not correspond to the history of a real network.
Hence, forward growth of random networks can only explore properties generic to the model and cannot track an individual, observed node's journey through time.
This problem can be avoided, however, if instead of growing a random graph forward according to an evolutionary model, we decompose the actual observed network backwards in time, as dictated by the model.
The resulting sequence of networks constitute a model-inferred history of the present-day network.
Reconstructing ancestral networks has many applications.
The inferred histories can be used to estimate the age of nodes, to model the evolution of interactions, and to track the emergence of prevalent network clusters and motifs CITATION.
In addition, proposed growth models can be validated by selecting the corresponding history that best matches the known history or other external information.
Leskovec et al. CITATION explore this idea by computing the likelihood of a model based on how well the model explains each observed edge in a given complete history of the network.
This augments judging a model on its ability to reproduce certain global network properties, which by itself can be misleading.
As an example, Middendorf et al. CITATION found that networks grown forward according to the small-world model CITATION reproduced the small-world property characteristic of the D. melanogaster PPI network, but did not match the empirical PPI network in other aspects.
Leskovec et al. CITATION made a similar observation for social network models.
Ancestor reconstruction also can be used to down-sample a network to create a realistic but smaller network that preserves key topological properties and node labels.
This can be used for faster execution of expensive graph algorithms or for visualization purposes.
In the biological network setting, network histories can provide a view of evolution that is complementary to that derived from sequence data alone.
In the social network setting, if a network's owner decides to disclose only a single network, successful network reconstruction would allow us to estimate when a particular node entered the network and reproduce its activity since being a member.
This could have privacy implications that might warrant the need for additional anonymization or randomization of the network.
Some attempts have been made to find small seed graphs from which particular models may have started.
Leskovec et al. CITATION, under the Kronecker model CITATION, and Hormozdiari et al. CITATION, under a duplication-based model, found seed graphs that are likely to produce graphs with specified properties.
These seed graphs can be thought of as the ancestral graphs at very large timescales, but the techniques to infer them do not generalize to shorter timescales nor do they incorporate node labels.
Previous studies of time-varying networks solve related network inference problems, but assume different available data.
For example, the use of exponential random graph models CITATION, CITATION and other approaches CITATION for inferring dynamic networks requires observed node attributes at each time point.
They are also limited because they use models without a plausible biological mechanism and require the set of nodes to be known at each time point.
Wiuf et al. CITATION use importance sampling to compute the most likely parameters that gave rise to a PPI network for C. elegans according to a duplication-attachment model, but they do not explicitly reconstruct ancient networks.
Mithani et al. CITATION only model the loss and gain of edges amongst a fixed set of nodes in metabolic networks.
There has also been some work on inferring ancestral biological networks using gene trees CITATION CITATION.
These approaches play the tape of duplication instructions encoded in the gene tree backwards.
The gene tree provides a sequence-level view of evolutionary history, which should correlate with the network history, but their relationship can also be complementary CITATION.
Further, gene tree approaches can only capture node arrival and loss, do not account for models of edge evolution, and are constrained to only consider trees built per gene family.
Network alignment between two extant species has also been used to find conserved network structures, which putatively correspond to ancestral subnetworks CITATION CITATION.
However, these methods do not model the evolution of interactions, or do so using heuristic measures.
Finally, the study of ancestral biological sequences has a long history, supported by extensive work in phylogenetics CITATION.
Sequence reconstructions have been used to associate genes with their function, understand how the environment has affected genomes, and to determine the amino acid composition of ancestral life.
Answering similar questions in the network setting, however, requires significantly different methodologies.
Here, we propose a likelihood-based framework for reconstructing predecessor graphs at many timescales for the preferential attachment, duplication-mutation with complementarity, and forest fire network growth models.
Our efficient greedy heuristic finds high likelihood ancestral graphs using only topological information and preserves the identity of each node, allowing the history of each node and edge to be tracked.
To gain confidence in the procedure, we show using simulated data that network histories can be inferred for these models even in the presence of some network noise.
When applied to a protein-protein interaction network for Saccharomyces cerevisiae, the inferred, DMC-based history agrees with many previously predicted features of PPI network evolution.
It accurately estimates the sequence-derived age of a protein when using the DMC model, and it identifies known functionally related proteins to be the product of duplication events.
In addition, it predicts older proteins to be more likely to be at the core of protein complexes, confirming a result obtained via other means CITATION .
By comparing the predicted protein ages using different models, we further confirm DMC as a better mechanism to model the growth of PPI networks CITATION compared to the PA model CITATION or the FF model CITATION, which are designed for web and social networks.
Conversely, when applied to a social network, the DMC model does not produce as accurate an ancestral network reconstruction as that of PA. The FF model also outperforms DMC in the social network context at the task of identifying users who putatively mediated the network's growth by attracting new members to join the service.
Thus, models of social network evolution do not transfer well to biological networks, and vice versa a well-studied and expected notion that we confirm through alternative means.
We also used our reconstructed history of the PPI network to make several novel predictions.
For example, we estimate the arrival time of extant and ancestral interactions and predict that newly added extant edges often connect proteins within the same complex and that modules have recently gained many peripheral units.
The history can also be used to track the change of network topological properties over time, such as the clustering coefficient, which we find has been decreasing in recent evolution.
Analysis of the duplication rates over the inferred history suggests that proteins with fewer extant interactions have been involved in the largest number of duplication events, which is in broad agreement with existing belief that proteins with many interactions evolve more slowly CITATION, CITATION.
In addition, the reconstruction predicts paralogous relationships between proteins that are strongly implied by network topology and which partially agree with sequence-based estimates.
Thus, the reconstructed history makes a number of detailed predictions about the relative order of events in the evolution of the yeast PPI, many of which correlate with known biology and many of which are novel.
The ability of these algorithms to reconstruct significant features of a network's history from topology alone further confirms the utility of models of network evolution, suggests an alternative approach to validate growth models, and ultimately reveals that some of the history of a network is encoded in a single snapshot.
Periplasmic binding proteins are a large family of molecular transporters that play a key role in nutrient uptake and chemotaxis in Gram-negative bacteria.
All PBPs have characteristic two-domain architecture with a central interdomain ligand-binding cleft.
Upon binding to their respective ligands, PBPs undergo a large conformational change that effectively closes the binding cleft.
This conformational change is traditionally viewed as a ligand induced-fit process; however, the intrinsic dynamics of the protein may also be crucial for ligand recognition.
Recent NMR paramagnetic relaxation enhancement experiments have shown that the maltose binding protein - a prototypical member of the PBP superfamily - exists in a rapidly exchanging mixture comprising an open state, and a minor partially closed state.
Here we describe accelerated MD simulations that provide a detailed picture of the transition between the open and partially closed states, and confirm the existence of a dynamical equilibrium between these two states in apo MBP.
We find that a flexible part of the protein called the balancing interface motif is displaced during the transformation.
Continuum electrostatic calculations indicate that the repacking of non-polar residues near the hinge region plays an important role in driving the conformational change.
Oscillations between open and partially closed states create variations in the shape and size of the binding site.
The study provides a detailed description of the conformational space available to ligand-free MBP, and has implications for understanding ligand recognition and allostery in related proteins.
Periplasmic Binding Proteins are major components of the bacterial cell envelope that are involved in nutrient uptake and chemotaxis CITATION, CITATION.
Gram-negative bacteria use PBPs to transport ligands into the cytosol by association with a membrane-bound ATP-binding cassette transporter CITATION.
Gram-positive bacteria differ in that they employ a slightly different design, in which the PBPs motif is directly attached to a membrane-anchored receptor.
In addition, several mammalian receptors contain extracellular ligand binding domains that are homologous to PBPs.
These include glutamate/glycine-gated ion channels such as the NMDA receptor; G protein-coupled receptors, including metabotropic glutamate, GABA-B, calcium sensing, and pheromone receptors; and atrial natriuretic peptide-guanylate cyclase receptors.
Many of these receptors are promising drug targets CITATION.
The structures of PBPs have been called a gold mine for studying the general mechanisms of protein-ligand recognition CITATION, as PBPs have been identified that can transport a large variety of substrates, including: carbohydrates, amino acids, vitamins, peptides, or metal ions CITATION.
The affinity of PBPs for diverse substrates also make them ideal templates for the design of diverse in vitro and in vivo biosensors with tailored properties CITATION .
Maltose binding protein is a part of the maltose/maltodextrin system of Escherichia coli, which is responsible for the uptake and efficient catabolism of maltodextrins.
MBP is the prototypical member of the PBP superfamily.
It has been the subject of extensive study due to its importance in various biological pathways CITATION, CITATION, and its utility as an affinity tag for protein expression and purification CITATION.
The protein folds into two domains of roughly equal size: the C terminal domain, and the N terminal domain.
The two domains are connected via a short helix and a two-stranded -sheet that form an interdomain hinge region.
Like other PBPs, the binding site of MBP is located on the interdomain cleft between domains.
X-ray structures of MBP solved in the presence and absence of ligand indicate that the protein undergoes an important conformational change from an open to a closed state in the presence of the ligand, the effect of which is to better stabilize the ligand by reducing the size of the cleft CITATION.
The conformational change has been dubbed the Venus Fly-Trap Mechanism CITATION due to its resemblance to the traps on the carnivorous plant that closes only when stimulated by prey.
An induced-fit mechanism CITATION is often invoked to describe the ligand recognition process.
In this scenario, the ligand participates in remodeling the binding site by interacting directly with the protein.
Alternatively, it is also possible that the apo protein already exists in a mixture of open and closed conformations.
In which case, the ligand would play a more passive role shifting the equilibrium toward the closed state, a mechanism traditionally described as conformational selection, or population shift CITATION.
Computer simulations and NMR studies are often needed to distinguish between the two scenarios, as X-ray structures typically do not provide detailed information about the ensemble of conformations available to the ligand-free protein CITATION .
Until recently, the bulk of our understanding of the mechanism of substrate recognition in MBP came from crystallographic studies that indicated only two possible conformations, a ligand-free open, and a ligand bound closed structure.
In 2007, Tang et al. CITATION reported the first NMR paramagnetic relaxation enhancement measurements on apo MBP.
By attaching a spin label on the NTD and CTD of the apo protein, domain hinge-bending motions could be studied.
These measurements indicated the existence of a dynamic equilibrium between a major open state, and a minor partially closed state.
Because experimental PRE rates for MBP could not be explained either by the X-ray crystal structure of the apo-state, nor by the ligand-bound closed-state, it was possible to postulate that a partially closed structure exists.
The transition between an open, and a partially closed state, was determined to involve a rotation around the hinge region.
The best agreement between computed and experimental PRE and Residual Dipolar Coupling data, was obtained by considering that substrate-free MBP exists in equilibrium between a major, open state and a minor, semi-closed state, populated 5 percent of the time, which corresponds to a very small energy difference between the two states.
The time-scale of the exchange between states was estimated to be between 20 ns to 20 s CITATION .
From a theoretical point of view, it is understood that a pre-existing equilibrium between different PBP conformations could play an important role in facilitating ligand recognition CITATION.
However, it remains a considerable challenge to access, using fully atomistic MD simulations, a detailed statistical analysis of slow conformational dynamics in proteins mediated by such hinge-bending motions.
In the past decade, the development of increasingly efficient simulation algorithms has led to a large number of theoretical studies using Molecular Dynamics simulations to probe the intrinsic dynamics of PBPs CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION.
In 2003, Pang et al. CITATION studied the glutamine binding protein using 5 ns MD simulations.
They observed large vibrations in the apo protein in the direction of a closed structure, and found that the open apo structure was more flexible than the closed structure.
Subsequently, Pang et al. CITATION confirmed that this is a general result by performing a comparative study of different PBPs, which also showed that different PBPs could display slightly different dynamical properties.
The authors also observed that the opening and closure rate in the presence of a substrate could be fast, even though they also noted that obtaining converged sampling for the opening and closure events was challenging on the nanosecond time-scale.
In 2006, Kandt et al. CITATION performed longer MD simulations on BtuF, a protein involved in vitamin B 12 uptake.
Using 12 simulations of 30 50 ns each, they were able to observe the initiation of opening and closing motions in both apo and holo simulations, with larger motions in the apo simulations.
This behavior of the protein was interpreted to be compatible to the Venus Fly-trap model.
The observation of enhanced molecular flexibility in the open state was confirmed by other groups for similar PBPs, such as the iron binding proteins FhuD CITATION, and FitE CITATION, and the heme binding proteins, ShuT, and PhuT CITATION.
In 2009, Loeffler and Kitao CITATION studied GlnBP in the open liganded form, and reported closing events occurring during the simulations.
Taken together, these MD studies on PBPs have helped characterize the intrinsic flexibility of ligand-free PBPs on the nanosecond time-scale.
The consensus of opinion from these calculations is that the ligand recognition proceeds through a Venus flytrap mechanism, and that the apo PBPs structure is very flexible, with a tendency to oscillate along the modes that lead from the open to the closed structure.
In 2005, a simulation study of the MBP protein was carried out by Stockner et al. CITATION.
Using 4 MD simulations of 30 ns, started from both open and closed states, with and without substrate, the authors could show that the ligand-free MBP structure naturally evolves toward a closed state in the presence of a substrate.
Similarly, the closed state was found to evolve toward an open state when the substrate was removed.
The rapid time-scale of this conformational change was consistent with experimental rate constant for sugar binding 1 2 10 7 M 1 s 1 CITATION, which suggests a rate of closure around 30 50 ns.
However, the time-scale of the simulations was too short to observe any pre-existing equilibrium in apo MBP between an open and a partially closed conformer.
This can be explained by the presumed slow exchange rate CITATION between the two conformations of apo MBP.
In this paper, we have used accelerated Molecular Dynamics simulations CITATION of MBP to show that the apo protein exists in a dynamical equilibrium between an open and a semi-closed conformation.
A number of methods have been developed to enhance the sampling of slow conformational changes in proteins, including targeted MD CITATION, and conformational flooding CITATION.
However, within the framework of this study, a key advantage of aMD is that it allows us to study the conformational behavior and dynamics of the protein without using a pre-defined reaction coordinate.
In previous studies, aMD has been successfully employed to study slow time-scale dynamics in proteins, such as HIV-protease CITATION, ubiquitin CITATION, IKBA CITATION and H-Ras CITATION.
The enhanced conformational space sampled by aMD has also been shown to significantly improve the theoretical prediction of experimental NMR observables, such as residual dipolar couplings, scalar J-couplings CITATION and chemical shifts CITATION, which are sensitive to dynamic averaging on the micro- to millisecond time-scale.
In this paper, we show that aMD simulations successfully allow the study of the transition from the open state of apo MBP to the hidden semi-closed conformation.
This provides the first atomistic view of the transition between open and partially closed states in a PBP.
NMR parameters computed from the simulations agree well with experiments.
Free energy calculations, and continuum electrostatics calculations are used to provide new insights into the mechanism and energetics of the exchange between the open and semi-closed states of apo MBP.
Non-intermingling, adjacent populations of cells define compartment boundaries; such boundaries are often essential for the positioning and the maintenance of tissue-organizers during growth.
In the developing wing primordium of Drosophila melanogaster, signaling by the secreted protein Hedgehog is required for compartment boundary maintenance.
However, the precise mechanism of Hh input remains poorly understood.
Here, we combine experimental observations of perturbed Hh signaling with computer simulations of cellular behavior, and connect physical properties of cells to their Hh signaling status.
We find that experimental disruption of Hh signaling has observable effects on cell sorting surprisingly far from the compartment boundary, which is in contrast to a previous model that confines Hh influence to the compartment boundary itself.
We have recapitulated our experimental observations by simulations of Hh diffusion and transduction coupled to mechanical tension along cell-to-cell contact surfaces.
Intriguingly, the best results were obtained under the assumption that Hh signaling cannot alter the overall tension force of the cell, but will merely re-distribute it locally inside the cell, relative to the signaling status of neighboring cells.
Our results suggest a scenario in which homotypic interactions of a putative Hh target molecule at the cell surface are converted into a mechanical force.
Such a scenario could explain why the mechanical output of Hh signaling appears to be confined to the compartment boundary, despite the longer range of the Hh molecule itself.
Our study is the first to couple a cellular vertex model describing mechanical properties of cells in a growing tissue, to an explicit model of an entire signaling pathway, including a freely diffusible component.
We discuss potential applications and challenges of such an approach.
During embryonic development of complex multicellular organisms, spatial reference points need to be established within tissues.
These are often formed by specialized groups of cells that are capable of signaling to neighboring cells.
Such signaling centers define coordinate systems along which newly arising cells can orient themselves and make crucial decisions regarding proliferation, differentiation or migration CITATION, CITATION, CITATION, CITATION, CITATION, CITATION.
Because of their pervasive importance, tissue-organizing centers need to be precisely controlled both spatially and temporally, as well as with respect to their signaling amplitude.
One possible mechanism for spatial control of tissue organizers is to restrict the movement of cells at fixed boundary positions.
This phenomenon is indeed observed, and it involves the separation of groups of cells that have already been spatially instructed to assume distinct identities, for example at segment- or parasegment-boundaries.
Akin to water in oil, the two cell populations are seen to establish and maintain a relatively straight interface to each other, effectively minimizing their contact area.
The minimizing force is assumed to help stabilize the interface against random perturbations that may arise from cell divisions or from arbitrary cell movements; thus, any organizing activity that is associated with the interface is likewise stabilized.
How is this separation, or sorting, of cells of distinct identities achieved?
One line of work attributes this to differential cell adhesion CITATION, CITATION : cell populations might develop distinct adhesive properties; these affinity differences would then allow them to sort out from one another.
Another line of reasoning is based on Differential Interfacial Tension CITATION, CITATION : this hypothesis suggests that cells might actively constrict surfaces that are in contact with neighboring cells, depending on the cellular identity of neighbors and/or depending on signaling events.
Both mechanisms would ultimately lead to physical forces that would help keep the cell populations apart.
The developing wing primordium of Drosophila is particularly well suited to study boundary formation.
It is not required for larval viability, can be manipulated experimentally through an advanced genetic toolkit, and has been well characterized.
The disc contains a compartment boundary that separates anterior from posterior cells; this boundary is inherited from specification events occurring early in the embryo.
The initial embryonic events that give rise to the boundary involve mutual signaling between stripes of cells, mediated by an extensively studied network of genes.
Once established, the cellular identities on both sides of these boundaries are stable throughout larval development and well into adult life.
The compartment boundary in the disc is strictly respected by all cells, even when cells on one side of the boundary are artificially provided with a competitive growth advantage over cells on the other side of the boundary CITATION.
The wing disc itself is a simple, flat, epithelial sheet, and the orientations of cell divisions appear largely random CITATION.
Genetic analysis and computational modeling of this tissue is simplified by the fact that daughter cells arising from cell divisions usually remain in physical contact and do not migrate away from each other.
This has been shown experimentally by tracing descendents of single cells; in most cases such a clone of offspring cells will form a coherent patch of connected cells.
This behavior suggests that the complicated processes of cell intercalation and migration can be neglected, to a first approximation, when studying boundary maintenance in this tissue.
Working with such wing discs, a recent, seminal study has begun to shed light on possible boundary formation mechanisms CITATION.
The authors have directly demonstrated an increased mechanical tension at cell-to-cell interfaces located immediately at the boundary, using laser ablation experiments.
Subsequent computer simulations then revealed that collectively such local forces are sufficient to maintain a stable compartment boundary.
These results are intriguing, but they raise a number of new questions: Boundary formation in the wing disc is known to depend on the secreted and diffusible signaling protein Hedgehog, which is produced by posterior cells and specifically sensed and transduced by anterior cells CITATION, CITATION.
If diffusible Hh indeed somehow influences mechanical tension, what conditions must then be met to ensure a well-defined boundary?
So far, all known transcriptional responses of Hh signaling are occurring several cell-diameters wide into the responding tissue.
How is the response in this case restricted to the immediate boundary region?
Furthermore, experimental suppression of Hh signaling has been shown to lead to ectopic boundary formation distant from the actual boundary CITATION.
Does this mean that the influence of Hh signaling does extend beyond the actual boundary, and if so, why does this not have a noticeable consequence in the wild type situation?
Here, we propose a mechanistic model that can generate a localized outcome of Hh signaling with respect to physical forces and mechanical properties, despite a longer range of the molecular response in terms of target gene expression.
Furthermore, we estimate the distance from the boundary, up to which Hh signaling may be able prime cells for boundary formation; this distance is inferred using both experimental results as well as modeling results, and we estimate it to be at least 10 cell diameters.
We approach the problem by first formulating an explicit, two-dimensional model of Hh production, diffusion and transduction, and by then coupling this setup to a physical model of the growing tissue.
In our modeling approach, cells and their contact surfaces are described as a graph of connected vertexes.
Our model essentially follows the Differential Interface Tension hypothesis; it is a modified version of a model that has been previously established for the very same tissue CITATION.
We observe good compartment boundary formation over a range of simulation parameters, and the modeling outcomes agree qualitatively with experimental perturbations specifically performed for this study.
Duplications of genes encoding highly connected and essential proteins are selected against in several species but not in human, where duplicated genes encode highly connected proteins.
To understand when and how gene duplicability changed in evolution, we compare gene and network properties in four species that are representative of the increase in evolutionary complexity, defined as progressive growth in the number of genes, cells, and cell types.
We find that the origin and conservation of a gene significantly correlates with the properties of the encoded protein in the protein-protein interaction network.
All four species preserve a core of singleton and central hubs that originated early in evolution, are highly conserved, and accomplish basic biological functions.
Another group of hubs appeared in metazoans and duplicated in vertebrates, mostly through vertebrate-specific whole genome duplication.
Such recent and duplicated hubs are frequently targets of microRNAs and show tissue-selective expression, suggesting that these are alternative mechanisms to control their dosage.
Our study shows how networks modified during evolution and contributes to explaining the occurrence of somatic genetic diseases, such as cancer, in terms of network perturbations.
Gene duplicability defines the propensity to retain multiple copies of a gene and varies among species and gene categories.
In yeast, singleton genes, i.e. single copy genes whose duplication is selected against, preferentially encode members of protein complexes CITATION, highly connected CITATION, CITATION and essential CITATION, CITATION proteins.
Similar relationships are maintained also in multicellular species such as worm and fly, where singleton genes encode highly connected CITATION and essential CITATION proteins.
The strict retention of one single copy of these particular gene categories is a consequence of the fragility towards dosage modifications.
Their duplication is deleterious because it interferes with essential cellular functions and with the fine-tuned equilibrium between formation and disruption of protein-protein interactions CITATION, CITATION .
Recent studies showed that the duplicability of mammalian hubs and essential proteins is different from that of other species.
Human hubs CITATION, CITATION and mouse essential proteins that are involved in development CITATION, CITATION, CITATION are preferentially encoded by duplicated genes, while other categories of essential mouse genes can be both singletons and duplicated CITATION.
These differences between human, mouse and the other species suggest that gene duplicability underwent modifications during evolution, which are likely related with the extensive acquisition of novel genes in vertebrates.
Through massive gene duplication followed by diversification of paralogs, vertebrates accommodated the expansion of gene families that are involved in regulation, signal transduction, protein transport, and protein modification CITATION, CITATION.
In this context, it has been proposed that a higher connectivity may favor the functional diversification of paralogs, for example through tissue specialization CITATION.
However, a thorough analysis of which types of genes undergo modification of their duplicability during evolution and how this influences the network properties of the encoded proteins is still missing.
The comparison of gene and network properties between species is the most straightforward approach to verify whether the modification of gene duplicability is indeed related to the expansion of the vertebrate gene repertoire.
Despite the fact that current representations of protein interactomes are still incomplete CITATION, CITATION, CITATION and may include a high fraction of false positives CITATION, the recent completion of interaction screenings in several species finally allows comparative network analyses.
For example, the comparison of human, fly, worm, and yeast networks showed that they maintain a similar structure despite the difference in size CITATION, CITATION.
In addition, regardless of their connectivity, proteins that occupy central positions in the interactomes of Saccharomyces cerevisiae, Drosophila melanogaster and Caenorhabditis elegans are also essential and slow-evolving CITATION.
These studies demonstrate that the comparison of protein and gene properties in different species can be used to infer general evolutionary trends.
To unravel when the differences between duplicability and network properties arose during evolution, we undertake a comparative analysis of genes and networks in four species, Escherichia coli, yeast, fly, and human.
These species display different levels of complexity, defined as the number of genes, cells, and cell types CITATION, and also high quality genomic and interaction data.
We compare connectivity and centrality of all proteins with origin, conservation and duplicability of the corresponding genes.
We identify a core of singleton hubs whose properties are maintained constant from prokaryotes to human, and another group of duplicated hubs that have emerged during the evolution of vertebrates.
Our analysis provides evidence of how the hubs properties modified during evolution and helps in interpreting the occurrence of somatic genetic diseases that are typical of multicellularity, such as cancer, in terms of network perturbations.
In particular, we find that cancer genes are representatives of the two groups of human hubs: one that originated early in evolution and is composed of singleton genes, and the other that appeared later and is enriched in duplicated genes.
Functionally, these two groups correspond to caretakers and gatekeepers, suggesting that these two different ways to initiate tumorigenesis emerged at different times during evolution.
Cortico-basal ganglia-thalamocortical circuits are severely disrupted by the dopamine depletion of Parkinson's disease, leading to pathologically exaggerated beta oscillations.
Abnormal rhythms, found in several circuit nodes are correlated with movement impairments but their neural basis remains unclear.
Here, we used dynamic causal modelling and the 6-hydroxydopamine-lesioned rat model of PD to examine the effective connectivity underlying these spectral abnormalities.
We acquired auto-spectral and cross-spectral measures of beta oscillations from local field potential recordings made simultaneously in the frontal cortex, striatum, external globus pallidus and subthalamic nucleus, and used these data to optimise neurobiologically plausible models.
Chronic dopamine depletion reorganised the cortico-basal ganglia-thalamocortical circuit, with increased effective connectivity in the pathway from cortex to STN and decreased connectivity from STN to GPe.
Moreover, a contribution analysis of the Parkinsonian circuit distinguished between pathogenic and compensatory processes and revealed how effective connectivity along the indirect pathway acquired a strategic importance that underpins beta oscillations.
In modelling excessive beta synchrony in PD, these findings provide a novel perspective on how altered connectivity in basal ganglia-thalamocortical circuits reflects a balance between pathogenesis and compensation, and predicts potential new therapeutic targets to overcome dysfunctional oscillations.
In Parkinson's disease, degeneration of midbrain dopamine neurons severely disrupts neuronal activity in looping circuits formed by cortico-basal ganglia -thalamocortical connections . Studies have shown that excessive oscillations at beta frequencies are a key pathophysiological feature of these Parkinsonian circuits, when recorded at the level of unit activity and/or local field potentials in several key circuit nodes.
These nodes include the frontal cortex, subthalamic nucleus, external globus pallidus and internal globus pallidus . Suppression of pathological beta-activity is achieved by dopamine replacement therapies  and surgical treatments e.g. high-frequency, deep brain stimulation of the STN; where prolonged attenuation after stimulation is observed . Bradykinesia and rigidity are the primary motor impairments associated with beta activity and, following dopamine replacement therapies, improvements in these motor deficits correlate with reductions in beta power . Moreover, a recent report has shown that stimulating the STN at beta frequencies exacerbates motor impairments in Parkinsonian rodents , in line with similar findings in PD patients .
Precisely how dopamine depletion leads to abnormal beta power is unknown.
Recent work in rodents has revealed that excessive beta-activity emerges in cortex and STN after chronic dopamine loss but not after acute dopamine receptor blockade . Here, we examine whether changes in effective connectivity between the nodes of the cortico-basal ganglia-thalamocortical network can account for enhanced beta oscillations following chronic dopamine loss.
To test this hypothesis we used dynamic causal modelling.
This approach allows one to characterise the distributed neuronal architectures underlying spectral activity in LFPs.
DCM is a framework for fitting differential equations to brain imaging data and making inferences about parameters and models using a Bayesian approach.
A range of differential equation models have been developed for various imaging modalities and output data features.
The current library of DCMs includes DCM for fMRI, DCM for event related potentials and DCM for steady state responses.
The current paper is based on DCM-SSR, designed to fit spectral data features .
Using spectral data, recorded simultaneously from multiple basal ganglia nuclei and the somatic sensory-motor cortex, we asked whether systematic changes in re-entrant neural circuits produce the excessive beta oscillations observed in LFPs recorded from the 6-hydroxydopamine -lesioned rat model of PD . We inverted the models using LFP data collected simultaneously from electrodes implanted in frontal cortex, striatum, GPe and STN.
Specifically, we used neural mass models that characterise the main projection cell types at each circuit node as glutamatergic or GABAergic.
Neural mass models describe neuronal dynamics in terms of the average neurophysiological states over populations of neurons.
Inference on effective connectivity differences observed between the Parkinsonian and control cases was based on a posteriori estimates of connectivity and synaptic parameters.
Using these estimates, we characterised the sensitivity of beta oscillations to changes in particular connection strengths to identify candidate connections that may represent therapeutic targets in idiopathic PD.
Measures of functional connectivity have been applied previously to examine frequency-specific signal correlations between nodes in the cortico-basal ganglia-thalamocortical network.
These measures have highlighted excessive coupling between the cortex and STN  and between STN and GPe  in animal models of PD. While functional connectivity and effective connectivity measures share some technical aspects e.g. likelihood models  or Bayesian estimators , the underlying concepts are fundamentally different . The distinction between functional connectivity and effective connectivity emerged from the analysis of electrophysiological time series: Aertsen et al. , used the term effective connectivity to define the neuronal interactions that could explain observed spike trains using a minimum simple neuronal model.
In what follows, we employ such a minimum model approach, using the key elements of known cortico-basal-ganglia-thalamocortical interactions.
Our model predicts the output of this loop circuit in vivo, where we assume observed responses are caused by interactions among neuronal populations or sources, with known neurotransmitters and directed connections.
The starting point for analyses of effective connectivity in this paper is the end point of classical functional connectivity analyses; namely, observed cross-spectral densities.
In other words, we place special emphasis on explaining how functional connectivity emerges in terms of directed connections that rest on a particular model of neuronal interactions.
In what follows, we illustrate this approach when applied to the directed circuitry of a cortico-basal ganglia-thalamocortical system.
Spectro-temporal receptive fields have been widely used as linear approximations to the signal transform from sound spectrograms to neural responses along the auditory pathway.
Their dependence on statistical attributes of the stimuli, such as sound intensity, is usually explained by nonlinear mechanisms and models.
Here, we apply an efficient coding principle which has been successfully used to understand receptive fields in early stages of visual processing, in order to provide a computational understanding of the STRFs.
According to this principle, STRFs result from an optimal tradeoff between maximizing the sensory information the brain receives, and minimizing the cost of the neural activities required to represent and transmit this information.
Both terms depend on the statistical properties of the sensory inputs and the noise that corrupts them.
The STRFs should therefore depend on the input power spectrum and the signal-to-noise ratio, which is assumed to increase with input intensity.
We analytically derive the optimal STRFs when signal and noise are approximated as Gaussians.
Under the constraint that they should be spectro-temporally local, the STRFs are predicted to adapt from being band-pass to low-pass filters as the input intensity reduces, or the input correlation becomes longer range in sound frequency or time.
These predictions qualitatively match physiological observations.
Our prediction as to how the STRFs should be determined by the input power spectrum could readily be tested, since this spectrum depends on the stimulus ensemble.
The potentials and limitations of the efficient coding principle are discussed.
In response to acoustic input signals, neurons in the auditory pathway are typically selective to sound frequency FORMULA and have particular response latencies.
At least ignoring cases with FORMULA kHz, in which neuronal responses often phase lock to the sound waves, a spectro-temporal receptive field is often used to describe the tuning properties of a neuron CITATION, CITATION, CITATION, CITATION.
This is a two-dimensional function FORMULA that reports the sensitivity of the neuron at response latency FORMULA to acoustic inputs of frequency FORMULA for a given stimulus ensemble.
More specifically, in a stimulus ensemble, the power FORMULA of the acoustic input at frequency FORMULA at time FORMULA fluctuates around an average level denoted by FORMULA.
If we let FORMULA denote the neuron's response at time FORMULA, then FORMULA best approximates the linear relationship between FORMULA and FORMULA in this stimulus ensemble asFORMULANote that in this paper, we refer to FORMULA as the input spectrogram, although some authors also include the average input power FORMULA.
Though FORMULA is not a full description of acoustic input, since it ignores features such as the phase of the oscillation in the sound wave, it is the only relevant aspect of the auditory input as far as the STRF is concerned.
Note that if we use FORMULA to denote the deviation of the neural response from its spontaneous activity level, then both FORMULA and FORMULA have zero mean.
We will use this simplification throughout the paper.
In studies in which the temporal dimension is omitted, the STRF is called the spectral receptive field .
Figure 1 cartoons a typical STRF.
This has excitatory and inhibitory regions, reflecting its preferred frequency and response latency.
For example, if FORMULA peaks at frequency FORMULA and time FORMULA, then this neuron prefers frequency FORMULA and should respond to an input impulse FORMULA of this frequency with latency FORMULA.
We will also refer to FORMULA as the receptive field, the filter kernel, or the transfer function from input to neural responses, as these all convey the same or similar meanings.
A neuron's STRF is typically estimated using reverse correlation methods CITATION, CITATION .
However, there are extensive nonlinearities in the signal transformation along the auditory pathway.
Indeed, the STRF formulation of neural responses, though linear in spectral power, is already a second-order nonlinear function of the auditory sound wave.
There are two kinds of nonlinearities when inputs are represented as spectrograms.
The simpler one is a static nonlinearity FORMULA, which when applied to the linear approximation FORMULA of equation enables better predictions of the neural responses CITATION, CITATION.
This static nonlinearity however does not alter the spectro-temporal selectivity of the neuron seen in the linear STRF.
This paper is interested in the more complex nonlinearity that the STRFs are dependent on the stimulus ensemble used to estimate them CITATION, CITATION, CITATION, CITATION.
For example, the STRFs are wider when the stimuli are narrow-band rather than wide-band CITATION, or when the stimuli are animal vocalizations rather than noise CITATION.
The STRF also becomes more band-pass when sound intensity increases.
The dependence of the STRFs on the stimulus ensemble holds, for example, for type IV neurons in the cochlear nucleus of cats CITATION, CITATION, the inferior colliculus of the frog CITATION and the gerbil CITATION, and field L region of the songbird CITATION.
Nonlinearities in the auditory system become progressively stronger further from the periphery.
Despite the nonlinearities, the concept of the STRF is still widely used, not only because it provides a meaningful description of the spectro-temporal selectivity of the neurons in a given stimulus ensemble, but also because it can predict neural responses to novel stimuli reasonably well, as long as the stimuli are drawn from the same stimulus ensemble as that used to estimate the STRF in the first place.
Reasonable predictions from the STRFs have been obtained for the responses of auditory nerves and auditory midbrain neurons CITATION, CITATION, CITATION.
They have also been obtained for responses of the auditory cortical neurons when the stimulus ensemble is composed of biologically more meaningful static or dynamic ripples.
If the linear neural filter is augmented to include the filtering performed by the head and ears, it is also possible to predict the preferred locations of sound sources of auditory cortical neurons based on the linear neural filter for input spectrograms CITATION.
Meanwhile, linear STRF models fail to capture many complex phenomena, particularly in the auditory cortex, and nonlinearities are not limited to being just static or monotonic.
It has been suggested that some auditory cortical neurons process auditory objects in a highly non-linear manner, by selectively responding to a weak object component while ignoring loud components that occupy the same region in frequency space in auditory mixtures of these object components CITATION, and some prefer low over high spectral contrast sounds CITATION.
Strong nonlinearities in the auditory processes have long since motivated nonlinear models of auditory responses .
This paper aims to understand from a computational, rather than a mechanistic, perspective why the auditory encoding transform should depend on the stimulus ensemble in the ways observed.
More specifically, the paper focuses on cases in which STRFs can reasonably capture neural responses, and aims to identify and understand the computational goal of the STRFs for a given stimulus ensemble finding a metric according to which the STRFs are optimal for the ensemble.
This would provide a rationale for how the physiologically measured STRFs should depend on or adapt to the stimulus ensemble.
This paper does not address what linear or nonlinear mechanisms could build the optimal STRFs, or whether or how nonlinear auditory processes enable the adaptation of the STRFs to the stimulus ensemble.
Existing computational models of auditory neurons, including ones with the notion that cochlear hair cells perform independent component analysis to provide an efficient code for inputs using spikes in the auditory nerves CITATION, CITATION, cannot explain the observed dependence of the STRFs on the stimulus ensemble .
Restricting attention to the temporal properties of STRF, Lesica and Grothe CITATION observed that the temporal filter in STRF adapted to the level of ambient noise in the input environment.
In particular, the temporal receptive field in the STRF changed from being bandpass to being low pass with the increase of ambient noise.
They argued using a simple model that such adaptation in the STRF enables more efficient coding of the input information.
This study applies the principles of efficient coding to understand the auditory STRF and its variations with sound intensities and other input characteristics.
It generalizes the work of Lesica and Grothe CITATION to understand the temporal and spectral filtering characteristics of STRF adaptation to changes in noise, signal and correlations in input statistics.
Explicitly, the principle of efficient coding states that the neural receptive fields should enable the neural responses to transmit as much sensory information as possible to the central nervous system, subject to the limitation in neural cost in representing and transmitting information.
This principle has been proposed CITATION and successfully applied to the visual system to understand the receptive fields in the early visual pathway CITATION, CITATION, CITATION, CITATION, CITATION, CITATION.
We will borrow heavily techniques and intuitions from vision to derive and explain the results in this paper.
To make initial progress, it is necessary to start with some simplifying assumptions.
First, we assume that the statistical characteristics of the stimulus ensemble do not change more rapidly than the speed at which the sensory encoding adapts, so that the stimulus ensemble can be approximated as being stationary as far as optimal encoding is concerned.
Knowing when this assumption does not hold tells us when the encoding is not optimal, e.g., when one sees poorly for a brief moment before the visual encoding adapts to a sudden change from a dark room to a bright garden.
Second, for mathematical convenience, we assume that the linear STRF model as in equation can approximate adapted auditory neural responses reasonably well.
As we know from above, this assumption often does not hold, particularly for auditory cortical neurons.
This paper leaves the extension of the optimal encoding to nonlinear cases for future studies.
Third, to derive a closed-form, analytical, solution to the optimal STRF, we assume that the input statistics in the stimulus ensemble can be approximated as being Gaussian, with higher order correlations in the input contributing only negligibly to the inefficiency of the representation in the original sensory inputs.
Although it is known that the natural auditory inputs are far from Gaussian CITATION, as for the case of vision, the discrepancy may have only a limited impact on the input inefficiency, as measured by the amount of information redundancy in the original sensory input CITATION, CITATION, CITATION .
To understand how sensory inputs should be recoded to increase coding efficiency, we start with visual encoding to draw insights and made analogies with auditory encoding.
In vision, large amounts of raw data about the visual world are transduced by photoreceptors.
However, the optic nerve, which transmits the input data to the visual cortex via thalamus, can only accommodate a dramatically smaller data rate.
It has thus been proposed that early visual processes use an efficient coding strategy to encode as much information as possible given the limited bandwidth CITATION, CITATION, in other words, to recode the data such that the redundancy in the data is reduced and consequently the data can be transmitted by the limited bandwidth.
Compression is possible since images are very redundant CITATION, CITATION, CITATION, CITATION, e.g., with strong correlations between visual inputs at nearby points in time and space.
Removing such correlations can cut down the data rate substantially CITATION .
One way to remove the correlations is to transform the raw input FORMULA into a different representation FORMULA in neural responses that would then have a much smaller data rate than FORMULA, yet preserving essential input information.
This transform is often approximated by the visual receptive field, analogous to the auditory STRFs.
For instance, the center-surround receptive fields of the retinal ganglion cells help remove spatial redundancy CITATION, CITATION, CITATION.
They do this by making the ganglion cells preferentially respond to spatial contrast in the input, and so eliminating responses to visual locations whose input is redundant with that of their neighbors.
Consequently, the responses of retinal ganglion cells are much less correlated than those of the photoreceptors, making their representation much more efficient.
One facet of this efficient encoding hypothesis is that the optimal receptive field transform should depend on the statistical properties, such as the correlation structure and intensity, of the input.
This dependence has been used to explain adaptation, to changes in input statistics, of visual receptive field characteristics, such as the sizes of center-surround regions and the color tuning of retinal neurons, or the ocular dominance properties of striate cortical neurons CITATION, CITATION, CITATION, CITATION, CITATION, CITATION.
In the auditory system, information redundancy is also reduced along the auditory pathway CITATION.
Although this redundancy reduction was only investigated in the neural responses to sensory inputs rather than in the coding transform leading to the neural responses, it suggested that coding efficiency is one of the goals of early auditory processes.
More formally, the efficient coding scheme is depicted in Figure 2A.
The input contains sensory signal FORMULA and noise FORMULA.
The net input FORMULA is encoded by a linear transfer function FORMULA into output.FORMULAwhich also contains additional noise FORMULA introduced in the encoding process.
When the input has multiple channels, e.g., many different photoreceptors or hair cells, FORMULA is a vector with many components, as indeed is FORMULA.
Output FORMULA is a vector representing the neural population responses from many neurons.
For output neuron FORMULA, we have FORMULA.
Therefore FORMULA is a matrix, and its FORMULA row FORMULA models the receptive field for output neuron FORMULA as the array of effective weights from input receptors FORMULA to output neuron FORMULA.
In the particular example when input neurons are photoreceptors and output neurons are retinal ganglion cells, FORMULA is the effective connection from photoreceptor FORMULA to ganglion cell FORMULA, and collectively, FORMULA describe the linear receptive field of this ganglion cell.
We consider the problem of finding an optimal FORMULA that maximizes the information extracted by FORMULA about FORMULA, i.e., the mutual information FORMULA CITATION between FORMULA and FORMULA subject to a given cost of the neural encoding, which depends on the responses in a way we will describe shortly.
Therefore, the optimal FORMULA should minimize the objective function:FORMULAwhere FORMULA is a parameter whose value specifies a particular balance between the needs to minimize costs and to maximize extracted information.
Neural costs can arise from various sources, such as the metabolic energy cost for generating neural activities or spikes CITATION and the cost of thicker axons to transmit higher rates of neural firing.
We follow a formulation that has been productive in vision CITATION, CITATION, and model the neural cost asFORMULAwhere FORMULA indicates the average over the stimulus ensemble.
This givesFORMULAIt has been shown CITATION, CITATION, CITATION, CITATION that the FORMULA that provides the most efficient coding according to FORMULA has the following properties.
At high signal-to-noise ratio, FORMULA is such that FORMULA extracts the difference between correlated channels, and thus avoids transmitting redundant information.
Hence, for example, in photopic conditions, retinal ganglion cells have center-surround spatial receptive fields which extract the spatial contrast of the input.
By contrast, at low SNR, FORMULA is a smoothing filter that averages out input noise instead of reducing redundancy.
This avoids spending neural cost on transmitting noise.
Hence, for example, in scotopic conditions, when SNR can be considered as being low, the receptive fields of retinal ganglion cells expand the sizes of their center regions and weaken their suppressive surrounds CITATION.
We will apply this framework to the auditory encoding to understand STRFs and their adaptation to stimulus ensembles.
We have developed a software program that weights and integrates specific properties on the genes in a pathogen so that they may be ranked as drug targets.
We applied this software to produce three prioritized drug target lists for Mycobacterium tuberculosis, the causative agent of tuberculosis, a disease for which a new drug is desperately needed.
Each list is based on an individual criterion.
The first list prioritizes metabolic drug targets by the uniqueness of their roles in the M. tuberculosis metabolome and their similarity to known druggable protein classes.
The second list prioritizes targets that would specifically impair M. tuberculosis, by weighting heavily those that are closely conserved within the Actinobacteria class but lack close homology to the host and gut flora.
M. tuberculosis can survive asymptomatically in its host for many years by adapting to a dormant state referred to as persistence.
The final list aims to prioritize potential targets involved in maintaining persistence in M. tuberculosis.
The rankings of current, candidate, and proposed drug targets are highlighted with respect to these lists.
Some features were found to be more accurate than others in prioritizing studied targets.
It can also be shown that targets can be prioritized by using evolutionary programming to optimize the weights of each desired property.
We demonstrate this approach in prioritizing persistence targets.
The cost of research and development in the pharmaceutical industry has been rising steeply and steadily in the last decade, but the amount of time required to bring a new product to market remains around ten to fifteen years CITATION.
This problem has been labeled an innovation gap, and it necessitates investment in inexpensive technologies that shorten the length of time spent in drug discovery.
The target identification stage is the first step in the drug discovery process CITATION and as such can provide the foundation for years of dedicated research in the pharmaceutical industry.
As with all the other steps in drug discovery, this stage is complicated by the fact that the identified drug target must satisfy a variety of criteria to permit progression to the next stage.
Important factors in this context include homology between target and host ; activity of the target in the diseased state CITATION, CITATION ; and the essentiality of the target to the pathogen's growth and survival CITATION CITATION .
The values of some of these selection criteria can be found easily by querying publicly available bioinformatics resources, including metabolic pathway databases such as KEGG CITATION, protein classification sets such as COGs CITATION, and databases of druggable proteins CITATION, CITATION, CITATION .
Traditional prioritization approaches such as literature searches and mental integration of multiple criteria can quickly become overwhelming for the researcher.
A more effective alternative is computational integration over different criteria to create a ranking function.
In this article, we describe such an application AssessDrugTarget that ranks the genes in a genome according to a given set of weighted criteria.
The acquisition and analysis of datasets including multi-level omics and physiology from non-model species, sampled from field populations, is a formidable challenge, which so far has prevented the application of systems biology approaches.
If successful, these could contribute enormously to improving our understanding of how populations of living organisms adapt to environmental stressors relating to, for example, pollution and climate.
Here we describe the first application of a network inference approach integrating transcriptional, metabolic and phenotypic information representative of wild populations of the European flounder fish, sampled at seven estuarine locations in northern Europe with different degrees and profiles of chemical contaminants.
We identified network modules, whose activity was predictive of environmental exposure and represented a link between molecular and morphometric indices.
These sub-networks represented both known and candidate novel adverse outcome pathways representative of several aspects of human liver pathophysiology such as liver hyperplasia, fibrosis, and hepatocellular carcinoma.
At the molecular level these pathways were linked to TNF alpha, TGF beta, PDGF, AGT and VEGF signalling.
More generally, this pioneering study has important implications as it can be applied to model molecular mechanisms of compensatory adaptation to a wide range of scenarios in wild populations.
Modelling the responses and compensatory adaptations of living organisms to a changing environment is extremely important both in terms of scientific understanding and for its potential impact on global health.
Although computational modelling of ecological systems has been utilised in ecotoxicology, the application of systems biology approaches to non-model organisms in general presents formidable difficulties, partly due to limited sequence information for environmentally relevant sentinel species.
Moreover, the number of samples and the depth of information available are often limited and there may be a lack of truly relevant physiological endpoints.
Thus, omics have proven effective in finding responses of aquatic organisms to model toxicants in laboratory-based experiments CITATION but the environment poses a greater challenge as anthropogenic contaminants are present as complex mixtures and responses will additionally be dependent upon natural life history traits and other environmental factors.
Relatively few omics studies have focussed upon the ecotoxicology of environmentally sampled fish CITATION CITATION.
Although we have previously shown CITATION, CITATION that expression of stress response genes could be used to distinguish fish from environmental sampling sites with different underlying contaminant burdens, this gave little insight to the health outcomes of these molecular differences.
In this context, identifying molecular mechanisms of compensatory and toxic responses from observational data, an approach that has been so successful in clinical studies and in laboratory model organisms, is highly challenging in field studies.
We addressed this challenge by developing a novel network inference strategy based on the integration of multi-level measurements of populations of fish exposed to a diverse spectrum of environmental pollutants.
This provides a useful model for a network biology approach generally applicable to non-model species and represents a breakthrough in the way we study the mechanisms whereby organisms respond to chemical exposure in the environment.
We directed our efforts towards modelling molecular networks representative of populations of the flatfish European flounder sampled from marine environments of North Western Europe, including locations significantly impacted by anthropogenic chemical contaminants.
The study integrated measurements representing a broad spectrum of samples characterized using transcriptomics, metabolomics, conventional biomarkers and analysis of chemicals in sediments from the sampling sites.
Previous studies have shown both anthropogenic contamination and higher prevalence of pre-neoplastic and neoplastic lesions in flounder from the Elbe estuary CITATION and from the Mersey and Tyne CITATION, together with elevated levels of hepatic DNA adducts at these sites CITATION.
Data integration was achieved by implementing a systems biology framework for network reconstruction, starting from cross-species mapping of sequence information to the integration of multi-level datasets within a framework for network inference CITATION and culminating in the identification of network modules predictive of physiological responses to chemical exposure, valuable for marine monitoring CITATION .
The networks we identified demonstrate a remarkable parallel between human liver carcinogenesis and environmental effects on fish liver as well as revealing potentially novel adaptation mechanisms.
The broader application of network biology approaches to other non-model species sampled from the environment is therefore likely to profoundly change our understanding of how living systems are likely to adapt to complex environments.
In recent times, stochastic treatments of gene regulatory processes have appeared in the literature in which a cell exposed to a signaling molecule in its environment triggers the synthesis of a specific protein through a network of intracellular reactions.
The stochastic nature of this process leads to a distribution of protein levels in a population of cells as determined by a Fokker-Planck equation.
Often instability occurs as a consequence of two steady state protein levels, one at the low end representing the off state, and the other at the high end representing the on state for a given concentration of the signaling molecule within a suitable range.
A consequence of such bistability has been the appearance of bimodal distributions indicating two different populations, one in the off state and the other in the on state.
The bimodal distribution can come about from stochastic analysis of a single cell.
However, the concerted action of the population altering the extracellular concentration in the environment of individual cells and hence their behavior can only be accomplished by an appropriate population balance model which accounts for the reciprocal effects of interaction between the population and its environment.
In this study, we show how to formulate a population balance model in which stochastic gene expression in individual cells is incorporated.
Interestingly, the simulation of the model shows that bistability is neither sufficient nor necessary for bimodal distributions in a population.
The original notion of linking bistability with bimodal distribution from single cell stochastic model is therefore only a special consequence of a population balance model.
In the study of cell populations, with vastly improved flow cytometry, access to multivariate distribution measures of cell populations has advanced considerably, calling for a concomitant application of theory sensitive to population heterogeneity.
In this regard, the population balance framework of Fredrickson et al. CITATION has provided the requisite modeling machinery for the same.
While this recognition generally exists in the literature, the modeling of gene regulatory processes has been at the single cell level based on it being viewed as an average cell.
Since gene regulatory processes typically involve a small number of molecules, the reaction network is stochastic in its dynamics, a feature that is included in the single cell analysis.
A further issue of importance, that of bistability, occurs when two levels of gene expression, one high and referred to as on, and the other low and referred to as off exist for a given concentration of the signaling molecule.
This issue is very much a part of the stochastic modeling of the single cell CITATION, CITATION.
Several kinds of stochastic models have been developed; two of them that have been broadly used are the Stochastic Simulation Algorithm CITATION, CITATION, and the Fokker-Planck equation or Stochastic Differential Equations CITATION CITATION.
The Stochastic model certainly cures the drawback of the deterministic model which describes only the averaged behavior on large populations without realizing the fluctuating behaviors in different cells.
Bistability has been studied extensively through experiments, theoretical analysis, and numerical simulations CITATION, CITATION, CITATION CITATION.
A bistable system is characterized by the existence of two stable steady states.
The modes relating to two stable steady states appear as a bimodal distribution of the population.
The coexistence of bistability and bimodal distribution has been shown in many publications CITATION, CITATION, CITATION, CITATION CITATION .
However, almost all of the modeling works on stochastic gene regulation relate to processes at the single-cell level.
The outcome of numerous simulated trajectories of single cell behavior has been interpreted as population behavior.
A cell is assumed to act totally independently of other cells without regard to the fact that the signaling environment is continuously altered by the concerted action of all members of the population.
That no interaction between other cells has been taken into consideration in these models could indeed lead to serious bias.
The drawback of the single cell model may be overcome by applying the Population Balance approach CITATION.
A detailed general framework of the application of population balances to microbial populations was developed by Fredrickson et al. CITATION.
However, the population balance model in the cited work and many others that followed in the literature are based on deterministic behavior of the particulate entities.
Ramkrishna CITATION shows how the PBM can accommodate random particulate behavior described by stochastic differential equations.
In this study, we demonstrate formulating a stochastic gene regulation incorporating PBM, which is capable of tracing time evolution of the behavior of the entire cell population.
A system of pheromone-induced conjugative plasmid transfer CITATION contributing to the dissemination of antibiotic resistance and the virulence of Enterococcus faecalis infections CITATION, CITATION has been simulated in this study as an example of the critical difference between stochastic gene regulation incorporating PBM and single-cell stochastic model.
It is our objective in this paper to formulate population balance models with stochastic gene expression in single cells.
Further, we explore circumstances under which bimodal distributions are observed in protein distributions; in particular we investigate the generally prevailing view in the literature that bistability and bimodality of protein distribution occur concurrently CITATION, CITATION, CITATION, CITATION CITATION.
An exception to this view appears in the work of Karmakar and Bose CITATION, CITATION, who showed that bimodal distributions can arise without bistability when the reaction time of the downstream gene regulation is short relative to the time required for change of DNA conformation.
Other similar publications can also be found in literature CITATION CITATION.
While these cited works show bimodal distributions without bistability, it must be understood that their conclusions are based on mechanistic differences in the behavior of isolated single cells.
In this study, we approach the issue of the relationship between bistability and bimodality from a rational viewpoint; i.e., to examine the nature of protein distribution from cells with and without bistability within the framework of population balances.
Thus, circumstances will be investigated for Figure 1A, in which bimodal distributions can arise without bistability, and for Figure 1B, in which unimodal distributions can arise even when bistability exists.
In this paper we used a general stochastic processes framework to derive from first principles the incidence rate function that characterizes epidemic models.
We investigate a particular case, the Liu-Hethcote-van den Driessche's incidence rate function, which results from modeling the number of successful transmission encounters as a pure birth process.
This derivation also takes into account heterogeneity in the population with regard to the per individual transmission probability.
We adjusted a deterministic SIRS model with both the classical and the LHD incidence rate functions to time series of the number of children infected with syncytial respiratory virus in Banjul, Gambia and Turku, Finland.
We also adjusted a deterministic SEIR model with both incidence rate functions to the famous measles data sets from the UK cities of London and Birmingham.
Two lines of evidence supported our conclusion that the model with the LHD incidence rate may very well be a better description of the seasonal epidemic processes studied here.
First, our model was repeatedly selected as best according to two different information criteria and two different likelihood formulations.
The second line of evidence is qualitative in nature: contrary to what the SIRS model with classical incidence rate predicts, the solution of the deterministic SIRS model with LHD incidence rate will reach either the disease free equilibrium or the endemic equilibrium depending on the initial conditions.
These findings along with computer intensive simulations of the models' Poincar map with environmental stochasticity contributed to attain a clear separation of the roles of the environmental forcing and the mechanics of the disease transmission in shaping seasonal epidemics dynamics.
A plethora of deterministic epidemic models involving susceptible FORMULA, infected FORMULA and recovered FORMULA individuals have been proposed CITATION, CITATION, carefully analyzed CITATION CITATION and confronted with data sets in the biomathematics and ecology literatures CITATION CITATION.
A well defined topic within this mathematical ecology research area is the study of FORMULA-type models with seasonal forcing CITATION CITATION.
These models have proved to be useful for understanding the observed patterns and the natural processes behind human and non-human epidemics CITATION CITATION.
Here, we restrict our attention to the FORMULA and FORMULA models in which we introduce seasonal forcing while varying the structural form of the incidence rates.
Two hypotheses pertaining the RSV and the measles transmission mechanisms were modeled with two simple functional forms of the incidence rates.
We show that in doing so, we are able to attain a clear separation of the roles of the environmental forcing and the mechanics of the disease transmission in shaping the epidemics dynamics.
The construction of deterministic incidence rates functions is a critical building block of epidemiological modeling.
In a seminal paper, Hethcote CITATION showed that because there are many choices for the form of the incidence, demographic structure and the epidemiological-demographic interactions, there really is a plethora of incidence rate functional forms to choose from.
Not surprisingly, the biomathematics literature abound in qualitative mathematical analyses of many of these functional forms CITATION CITATION.
However, biological first principles derivations of incidence rate functional forms are not too common.
As we show in this study, using such first principles derivations greatly enrich the reaches of the practice of confronting models with data while testing biological hypotheses.
Thus, despite the big amount of available functional incidence rates forms CITATION, we believe that the set of models chosen to be confronted with data should be restricted to those forms derivable from first principles.
To illustrate this argument, in this study we first show that a simple probabilistic setting wherein infectious encounters are modeled with a pure birth stochastic process leads to a general nonlinear incidence form proposed previously by Liu CITATION and later analyzed by Hethcote and Van Den Driessche CITATION.
The LHD incidence rate leads to models with qualitatively different dynamics compared with the ones obtained using the classical incidence rate.
In the SIRS model with either incidence rate and seasonal forcing, FORMULA becomes a periodic function of time and the trajectory FORMULA pursuits a moving target thus giving rise to limit cycles.
That moving target is the former endemic equilibrium that bounces back and forth between two points.
In either model, the target switches between that moving point and the disease free equilibrium when FORMULA crosses 1, giving rise to a period doubling bifurcation.
In the SIRS model with classical incidence rate this mechanism does not depend on the initial conditions.
In this work we show that the disease free equilibrium is unconditionally an attractor in the SIRS model with LHD incidence rate.
This leads to a scenario where two regions of attraction can coexist.
The trajectory FORMULA will either reach the disease free equilibrium or have periodic solutions depending on the initial conditions.
Furthermore, after carrying a formal model selection we show that the SIRS model with LHD incidence rate leads to a significant fit improvement over the classical SIRS model with the same seasonal forcing.
Finally, we compared the applicability and generality of the classical and LHD incidence rates functions by fitting them to two measles time series data sets.
Using the later function leads to a vast improvement of model fit in both cases.
Since we were fitting a deterministic SEIR model, we chose to use the data from the two largest cities in the measles data set, where the effects of demographic stochasticity are expected to be less influential in the dynamics of the epidemics CITATION .
Varying the form of the contact rate function while including environmental stochasticity in the SIRS and SEIR models leads to a better understanding of the dynamics of an infectious disease transmission.
Depending on the model and contact rate, the disease free equilibrium is either a saddle point or an attractor.
In the first case, if a trajectory located originally in the basin of attraction of the endemic equilibrium basin of attraction is perturbed with environmental noise, it may transiently visit the DFE stable submanifold and then return to the EE basin of attraction.
If however the DFE and the EE coexist as stable equilibria, a trajectory initially at the EE basin of attraction may end up in the DFE basin of attraction.
The interaction between stochasticity and the different contact rate models was studied using computer intensive simulations of the Poincar map CITATION .
Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks.
A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations.
In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment.
We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons.
As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons.
Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values.
These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration.
Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled.
Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate.
As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information.
This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise.
This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise.
On the contrary, it is a consequence of optimal spike-based inference.
In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.
Our senses furnish us with information about the external world that is ambiguous and corrupted by noise.
Taking this uncertainty into account is crucial for a successful interaction with our environment.
Psychophysical studies have shown that animals and humans can behave as optimal Bayesian observers, i.e. they integrate noisy sensory cues, their own predictions and prior beliefs in order to maximize the expected outcome of their actions CITATION, CITATION, CITATION, CITATION .
Several theoretical investigations have explored the neural mechanisms that could underly such probabilistic computations CITATION, CITATION, CITATION, CITATION, CITATION, CITATION.
In cortical areas, sensory and motor variables are encoded by the joint activity of populations of spiking neurons CITATION, CITATION whose activity is highly variable and weakly correlated CITATION, CITATION.
The timing of individual spikes is unreliable while spike counts are approximately Poisson distributed CITATION.
These characteristics have inspired rate-based models that encode probability distributions in their average firing rates and spike count covariances.
Previous studies have examined analytically and empirically how this information can be encoded in a population code CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, how it can be decoded CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION and how population codes can be combined optimally CITATION, CITATION.
In particular, optimal cue combination reduces to a simple linear combination of neural activities for a broad family of neural variability, including Poisson or Gaussian noise CITATION .
However, most of these studies neglect a crucial dimension of perception: time.
Most sensory stimuli vary dynamically in a natural environment, which requires sensory representations to be constructed, integrated and combined on-line CITATION, CITATION.
Perceptual inference thus cannot be based on rates or spike counts measured during a fixed temporal window, as used in most previous population coding frameworks.
At the same time, reliable decisions typically require an integration of sensory evidence over hundreds of milliseconds CITATION, CITATION, which largely exceeds the integrative time constant of single neurons.
It is unclear how such leaky devices could compute sums of spike counts on the typical time scale of perceptual or motor tasks.
The problem is even more crucial if the decision is delayed compared to the presentation of sensory information.
Sensory variables such as the direction of motion of a stimulus can be retained in working memory for significant periods of time even in the absence of sensory input.
Neural correlates of this working memory appear as persistent neural activity in parietal and frontal brain areas and exhibit firing statistics similar to those found for sensory responses CITATION, CITATION, CITATION.
This persistent activity has been modeled as a stable state of recurrent neural network dynamics CITATION.
However, such attractors correspond to stereotyped patterns of activity that can only represent a single stimulus value.
For example, the memorized position of an object can be encoded by the position of a stable bump of activity CITATION, CITATION.
This would imply though that information about the reliability of the memorized cue is lost and cannot be used for delayed cue combination or decision making.
We hypothesize instead that stimuli are memorized in the same format as sensory input, i.e. as a probability distribution.
The question of how probability distributions can be memorized by a population of neurons remains largely unanswered.
Here, we approach these issues by using a new interpretation of population coding in the context of temporal sensory integration.
We consider spikes, rather than rates, as the basic unit of probabilistic representation.
We show how recurrent networks of leaky integrate-and-fire neurons can construct, combine and memorize probability distributions of dynamic sensory variables.
Spike generation in these neurons results from a competition between an integration of evidence from feed-forward sensory inputs and a prediction from lateral connections.
A neuron therefore acts as a predictive encoder, only spiking if its input cannot be predicted by its own or its neighbors' past activity.
We demonstrate that such networks integrate and combine sensory inputs optimally, i.e. without losing information, and track the stimulus dynamics spike-per-spike even in the absence of sensory input, over timescales much longer than the neural time constants.
This framework thus provides a first comprehensive theory for optimal spike-based sensory integration and working memory.
In contrast to rate models implemented with Poisson spiking neurons, this model does not require large levels of redundancy to compensate for the noise added by stochastic spike generation.
Similar to cortical sensory neurons, model neurons respond with sustained, asynchronous spiking activity.
Spike times are variable and uncorrelated, despite the deterministic spike generation rule.
However, in contrast to rate codes, each spike counts.
The trial to trial variability of spike trains does not reflect an intrinsic source of noise that requires averaging, but is a consequence of predictive coding.
While spike times are unpredictable at the level of a single neuron, they deterministically represent a probability distribution at the level of the population.
This leads us to reinterpret the notions of signal and noise in cortical neural responses.
De novo computational design of synthetic gene circuits that achieve well-defined target functions is a hard task.
Existing, brute-force approaches run optimization algorithms on the structure and on the kinetic parameter values of the network.
However, more direct rational methods for automatic circuit design are lacking.
Focusing on digital synthetic gene circuits, we developed a methodology and a corresponding tool for in silico automatic design.
For a given truth table that specifies a circuit's input output relations, our algorithm generates and ranks several possible circuit schemes without the need for any optimization.
Logic behavior is reproduced by the action of regulatory factors and chemicals on the promoters and on the ribosome binding sites of biological Boolean gates.
Simulations of circuits with up to four inputs show a faithful and unequivocal truth table representation, even under parametric perturbations and stochastic noise.
A comparison with already implemented circuits, in addition, reveals the potential for simpler designs with the same function.
Therefore, we expect the method to help both in devising new circuits and in simplifying existing solutions.
A central concept of Synthetic Biology CITATION is the rational design of synthetic gene circuits by means of modularized, standard parts, which are DNA traits with well-defined functions.
The field aims at adapting methods and ideas such as part composability and abstraction hierarchy from engineering to biology.
Several computational tools embracing these concepts have been developed.
Moreover, some tools permit to realize circuits in a drag and drop way as it is typical in electronics CITATION CITATION.
Nevertheless, de novo design of circuits able to reproduce a target function is not an easy task and its automation represents a major challenge in Synthetic Biology.
Previously, Fran ois and Hakim CITATION showed that small networks characterized by a desired behavior can be obtained by evolutionary optimization of a set of independent circuits.
Similar optimization-based tools like Genetdes CITATION and OptCircuit CITATION use simulated annealing and mixed integer dynamic optimization, respectively.
These approaches yielded interesting circuit designs, but they have several inherent limitations.
Computational complexity requires very simplified models that do not represent basic parts but lump functionalities of entire genes.
Similarly, brute-force optimization can only cope with rather small networks, and it requires dual optimization of circuit structure and of kinetic parameter values.
Hence, more direct, rational design methods are desired.
Here, instead of looking for a general solution to the automatic design challenge, we focus on digital circuits.
These circuits employ Boolean logic where input and output signals can take only two values: 0 and 1.
In the simplest case, a Boolean gate uses two input signals to compute a single logical output.
More complex digital circuits convert FORMULA inputs into a single output.
In both cases, the input-output relation is represented by a truth table where each entry specifies one of the possible FORMULA combinations of input signal values and the corresponding binary output.
In biology, digital circuits are important for several reasons.
First, logical gates such as those determined by the action of two different activators on a promoter are abundant in natural systems.
They are often found in association with feed-forward loop motifs and provide more complicated functionalities such as sign sensitive delays CITATION, CITATION and pulse generation CITATION.
More complex networks of several FFLs interacting with basic Boolean gates control sporulation in B. subtilis CITATION as well as the neuronal system of C. elegans CITATION.
An analysis of possible implementations of logical gates could, thus, help further our understanding of natural biological networks.
In synthetic biology, secondly, complex digital circuits are required for the construction of biosensors and molecular computers.
Biosensors should respond to well-defined external cues that may be specified with a truth table.
The more inputs can be sensed, the better is the ability of the biosensor to discriminate between similar environmental conditions.
Such biosensors could be integrated, for instance, into bioreactors for the production of biofuels CITATION.
Furthermore, they could play an important role in disease treatment Anderson et al. CITATION implemented a biosensor that mimics a logical gate to control bacterial invasion of tumor cells in response to signals from the tumor environment.
Even more complex biosensors could work as molecular computers that perform a diagnosis on the basis of the sensed substances and release drugs if necessary CITATION .
Motivated by these two aspects, several synthetic gene circuits that implement Boolean logic have been realized experimentally in the past years.
Most of these circuits rely on transcriptional control schemes.
In fact, it is well known that bacterial promoters can display logic behavior when controlled by two transcription factors CITATION CITATION.
More complex Boolean promoters have been engineered, for instance, in mammalian cells CITATION.
However, the number of repressors and activators generally used in synthetic biology is low and the rational engineering of transcription factors can be a complex process CITATION .
Alternatively, Boolean gates are achieved in nature by mechanisms of translation control like base-pairing between antisense small-RNAs and the mRNA, or structural mRNA modifications due to the binding of chemical effectors to riboswitches and ribozymes CITATION, CITATION.
These are complex RNA structures made of two modules: an aptamer, where a chemical binds, and an actuator that either undergoes structural modifications in a riboswitch or gets spliced in a ribozyme as a consequence of the chemical binding.
Both riboswitches and ribozymes can either repress or activate translation CITATION.
Furthermore, a tandem riboswitch, where a single actuator is under the control of two aptamers, has been observed in B. clausii CITATION.
With two distinct inputs, it represents a natural Boolean gate located on the mRNA.
Taking these structures as models, similar synthetic RNA constructs have been engineered recently CITATION.
In particular, Win and Smolke CITATION have built complex ribozymes that establish the most common two-input Boolean gates.
Importantly, the design of small RNAs is easy compared to the design of transcription factors.
Despite these individual successes, synthetic biology fundamentally lacks tools and concepts for automatic computational design.
Logical circuits are suitable starting points for automatic design because the target function can be defined easily by a truth table.
Here, we combine approaches from electrical circuit design with our previous model for circuit design with composable parts CITATION, CITATION to develop a method for the automatic design of digital synthetic gene circuits.
It is implemented as an add-on for the process modeling tool ProMoT CITATION, CITATION.
The circuits use a set of standard biological parts and Boolean gates whose kinetic parameters take appropriate default values without invoking any optimization algorithms.
In addition to previously developed building blocks such as two-operator-containing promoters, we consider externally controllable ribosome binding sites.
The method requires only a truth table to directly produce several possible circuit designs that process up to four different inputs to yield a unique, pre-defined output signal.
Design alternatives are ranked according to a complexity score that reflects the efforts for practical implementation.
Simulations on single gates and on networks of different complexity confirm the validity of our approach by highlighting accurate representation of the truth table and robustness of the designed circuits.
The convoluted cortex of primates is instantly recognizable in its principal morphologic features, yet puzzling in its complex finer structure.
Various hypotheses have been proposed about the mechanisms of its formation.
Based on the analysis of databases of quantitative architectonic and connection data for primate prefrontal cortices, we offer support for the hypothesis that tension exerted by corticocortical connections is a significant factor in shaping the cerebral cortical landscape.
Moreover, forces generated by cortical folding influence laminar morphology, and appear to have a previously unsuspected impact on cellular migration during cortical development.
The evidence for a significant role of mechanical factors in cortical morphology opens the possibility of constructing computational models of cortical develoment based on physical principles.
Such models are particularly relevant for understanding the relationship of cortical morphology to the connectivity of normal brains, and structurally altered brains in diseases of developmental origin, such as schizophrenia and autism.
The popular image of the human brain is closely associated with the intricate folds of the cerebral cortex.
This cortical landscape has been described, measured, and interpreted since the age of phrenology CITATION.
The mammalian cerebral cortex evolved and expanded tangentially CITATION CITATION, and folded to accommodate a large surface area, measuring 1,600 2,000 cm 2 in humans, three times larger than the inner surface of the skull CITATION, CITATION .
Classic studies described cortical morphology CITATION, CITATION, frequently with reference to its geometric regularity CITATION CITATION.
Various hypotheses were proposed for the development of convolutions, such as active growth CITATION, pressure and friction of expanding cortex tangentially against the skull or underlying brain structures CITATION, and mechanical bulging from unequal regional expansion CITATION, CITATION CITATION.
Other concepts suggested that cortical buckling results from differential laminar growth CITATION CITATION, or that convolutions are shaped through attached axonal fibers CITATION.
More specifically, it has been proposed, but not yet rigorously tested, that the 3-D shape of the brain reflects the viscoelastic tension exerted by axonal fibers CITATION.
According to this hypothesis, global competition of mechanical forces results in the formation of gyri between densely linked regions, and sulci between weakly connected or unconnected regions.
The axonal tension hypothesis is particularly attractive, since it implies that the characteristic cortical morphology arises automatically from the interconnections of different cortical areas, without the need for individual specification of convolutions.
Moreover, cortical folding through axonal tension implicitly achieves a desirable reduction in the volume of cortical fibers CITATION .
Mechanical factors have been invoked in most models for the development of cortical convolutions CITATION, CITATION, CITATION, CITATION CITATION, but see also a discussion on active growth of convolutions CITATION.
Mechanical forces may also have a role in observed trends in laminar cortical morphology CITATION, CITATION and the deformation of neurons CITATION and blood vessels CITATION in different parts of the cortical landscape.
Nevertheless, the rapid progress of research into the genetic control of brain development during recent decades has sidelined mechanical concepts in favor of genetic models for explaining the morphology of the convoluted cortex CITATION.
Beause of the apparent complexity of cortical morphology and the fact that many aspects of cortical development are still poorly understood, there is a great need for establishing systematic and reliable quantitative data on the architecture of the brain in order to evaluate different morphological concepts.
Here we address two questions related to the role of mechanical factors in cortical morphology: Is the overall pattern of connections in the adult convoluted cortex consistent with the hypothesis that axonal tension underlies the formation of cortical convolutions?
Are systematic variations in adult cortical architecture related to cortical convolutions?
Both questions are addressed through the analysis of quantitative data for connections and architecture of the prefrontal cortex in adult nonhuman primates.
Quantitative tract tracing in animals with a convoluted cortex offers the most detailed and reliable information currently available about the density and trajectories of cortical projections.
Retrograde tract tracing, in particular, is ideal for this purpose because each projection neuron is labeled by transport of a tracer through individual axons from the injection site back to the parent cell body.
A count of labeled neurons, therefore, provides a good estimate of the number of axons that link a given pair of cortices.
Over the last two decades we have systematically accumulated quantitative data on connections of the prefrontal cortex in a gyrencephalic nonhuman primate, the rhesus monkey.
In addition, we have obtained quantitative architectonic data on all prefrontal areas in the same species CITATION.
Using these databases we provide quantitative evidence consistent with a key role of mechanical factors in three interrelated aspects of cortical morphology: the formation of cortical convolutions through axonal tension; the shaping of laminar morphology through cortical folding; and a nonuniform distribution of neurons in cortical layers that may result from the interaction of tensile and compressive folding forces with neuronal migration during development.
Some of these findings were previously reported in conference proceedings CITATION .
In prokaryotes, Shine Dalgarno sequences, nucleotides upstream from start codons on messenger RNAs that are complementary to ribosomal RNA, facilitate the initiation of protein synthesis.
The location of SD sequences relative to start codons and the stability of the hybridization between the mRNA and the rRNA correlate with the rate of synthesis.
Thus, accurate characterization of SD sequences enhances our understanding of how an organism's transcriptome relates to its cellular proteome.
We implemented the Individual Nearest Neighbor Hydrogen Bond model for oligo oligo hybridization and created a new metric, relative spacing, to identify both the location and the hybridization potential of SD sequences by simulating the binding between mRNAs and single-stranded 16S rRNA 3 tails.
In 18 prokaryote genomes, we identified 2,420 genes out of 58,550 where the strongest binding in the translation initiation region included the start codon, deviating from the expected location for the SD sequence of five to ten bases upstream.
We designated these as RS 1 genes.
Additional analysis uncovered an unusual bias of the start codon in that the majority of the RS 1 genes used GUG, not AUG. Furthermore, of the 624 RS 1 genes whose SD sequence was associated with a free energy release of less than 8.4 kcal/mol, 384 were within 12 nucleotides upstream of in-frame initiation codons.
The most likely explanation for the unexpected location of the SD sequence for these 384 genes is mis-annotation of the start codon.
In this way, the new RS metric provides an improved method for gene sequence annotation.
The remaining strong RS 1 genes appear to have their SD sequences in an unexpected location that includes the start codon.
Thus, our RS metric provides a new way to explore the role of rRNA mRNA nucleotide hybridization in translation initiation.
In 1974 Shine and Dalgarno CITATION sequenced the 3 end of Escherichia coli's 16S ribosomal RNA and observed that part of the sequence, 5 ACCUCC 3, was complementary to a motif, 5 GGAGGU 3, located 5 of the initiation codons in several messenger RNAs.
They combined this observation with previously published experimental evidence and suggested that complementarity between the 3 tail of the 16S rRNA and the region 5 of the start codon on the mRNA was sufficient to create a stable, double-stranded structure that could position the ribosome correctly on the mRNA during translation initiation.
The motif on the mRNAs, 5 GGAGGU 3, and variations on it that are also complementary to parts of the 3 16S rRNA tail, have since been referred to as the Shine Dalgarno sequence.
Shine and Dalgarno's theory was bolstered by Steitz and Jakes in 1975 CITATION and eventually experimentally verified, in 1987, by Hui and de Boer CITATION and Jacob et al. CITATION .
Since Shine and Dalgarno's publication, two different approaches have been used to identify and position SD sequences in prokaryotes: sequence similarity and free energy calculations.
Methods based on sequence similarity include searching upstream from start codons for sub-strings of the SD sequences that are at least three nucleotides long CITATION.
Identification errors can arise from this approach for several reasons CITATION.
A threshold of similarity does not exist that can clearly delineate actual SD sequences from spurious sites with a significant, but low, degree of similarity to the SD sequence.
The lack of certainty has led to a number of observations in which gene sequences appear to partition themselves into two categories: those with obvious SD sequences and those without.
The inability of sequence techniques to pinpoint the exact location of the SD sequence poses a problem because its location is believed to affect translation initiation CITATION CITATION .
The second approach, using free energy calculations, is based on thermodynamic considerations of the proposed mechanism of 30S binding to the mRNA and overcomes the limitations of sequence analysis.
Watson Crick hybridization occurs between the 3 -terminal, single-stranded nucleotides of the 16S rRNA and the SD sequence in the mRNA and has a significant effect on translation CITATION, CITATION.
The formation of hydrogen bonds between aligned, complementary nucleotides is the basis of Watson Crick hybridization and results in a more stable, double-stranded structure with lower free energy than the participating single-stranded sequences.
One long-standing implementation of this model, Mfold CITATION, quantifies the degree of hybridization and the stability of RNA secondary structure by calculating the change in energy CITATION CITATION.
This method for estimating free energy has been adapted to identify SD sequences by repeatedly calculating the G values for progressive alignments of the rRNA tail with the mRNA in the region upstream of the start codon CITATION, CITATION, CITATION, CITATION.
All of these studies have observed a trough of negative G upstream of the start codon whose location is largely coincident with the SD consensus sequence.
This second approach can both identify the SD sequence and pinpoint its exact location as that having the minimal G value.
However, the exact location of the SD sequence is dependent on the nucleotide indexing scheme of the algorithm, i.e., on which nucleotide is designated as the 0 position.
To normalize indexing and to further extend free energy analysis through the start codon and into the coding region of genes, we created a new metric, relative spacing.
This metric localizes binding across the entire translation initiation region, relative to the rRNA tail, enabling us to characterize binding that involves the start codon as well as sequences downstream.
RS is also independent of the length of the rRNA tail, and this property allows for comparison of binding locations between species.
By examining sequences downstream from start codons, we could explore mRNAs that lack any upstream region, the leaderless mRNAs CITATION CITATION.
The lack of any 5 untranslated leader in the mRNAs has prompted searches for other sequence motifs that could interact with the 16S rRNA.
One of these, the downstream box hypothesis CITATION, has been disproved CITATION.
Thus, there is a continued search for an explanation for the highly conserved sequences 3 of the initiation codon that have been observed in many leaderless mRNAs CITATION, CITATION, CITATION .
In this study we use the RS metric to identify the positions of minimal G troughs for genes of 18 species of prokaryotes as a test of its usefulness as a means to improve existing annotation tools, i.e., by identifying SD sequences.
We observe 2,420 genes where the strongest binding in the entire TIR takes place one nucleotide downstream from the start codon, at RS 1.
Of these, 624 genes have unusually strong binding.
We then determine if these 624 genes were mis-annotated and conclude that 384 are.
While combinatorial models of transcriptional regulation can be inferred for metazoan systems from a priori biological knowledge, validation requires extensive and time-consuming experimental work.
Thus, there is a need for computational methods that can evaluate hypothesized cis regulatory codes before the difficult task of experimental verification is undertaken.
We have developed a novel computational framework that integrates transcription factor binding site and gene expression information to evaluate whether a hypothesized transcriptional regulatory model is likely to target a given set of co-expressed genes.
Our basic approach is to simultaneously predict cis regulatory modules associated with a given gene set and quantify the enrichment for combinatorial subsets of transcription factor binding site motifs comprising the hypothesized TRM within these predicted CRMs.
As a model system, we have examined a TRM experimentally demonstrated to drive the expression of two genes in a sub-population of cells in the developing Drosophila mesoderm, the somatic muscle founder cells.
This TRM was previously hypothesized to be a general mode of regulation for genes expressed in this cell population.
In contrast, the present analyses suggest that a modified form of this cis regulatory code applies to only a subset of founder cell genes, those whose gene expression responds to specific genetic perturbations in a similar manner to the gene on which the original model was based.
We have confirmed this hypothesis by experimentally discovering six new CRMs driving expression in the embryonic mesoderm, four of which drive expression in founder cells.
A central challenge to determining the structure of genetic regulatory networks is the development of systematic methods for assessing whether a set of transcription factors co-regulates a given set of co-expressed genes.
Although classical genetics approaches allow the identification of key regulating TFs and the determination of their approximate ordering within the genetic hierarchy, demonstrating that a collection of TFs forms a combinatorial code acting to directly drive gene expression has required laborious experimental identification and perturbation of numerous individual cis regulatory modules.
To speed this process, several groups have recently demonstrated that computational approaches can rapidly identify CRMs with considerable accuracy CITATION CITATION, especially when performing computational searches with a collection of TFs known a priori to co-regulate.
This is perhaps best exemplified by the dramatic progress made by several groups in discovering CRMs for genes expressed during segmentation of the Drosophila melanogaster embryo CITATION, CITATION, CITATION, CITATION, a system where years of genetic screens have identified the regulating TFs CITATION.
In most biological systems, however, such a set of co-regulating TFs is either merely hypothesized or entirely unknown.
Therefore, in order for these in silico approaches to effectively identify the cis component of regulation in novel biological systems, additional computational methods are needed that can identify the trans component of regulation .
To address this question in metazoan systems, we have developed an initial statistical framework for evaluating hypothesized transcriptional regulatory models.
As a model system, we have examined the regulation of a class of Drosophila myoblast genes for which a regulatory model has been previously hypothesized CITATION, CITATION and for which extensive transcriptional profiling datasets have been generated CITATION.
Muscle founder cells are a sub-population of mononucleate myoblasts that are specified by the Wingless, Decapentaplegic, and Ras signal transduction cascades acting in combination within the somatic mesoderm CITATION, CITATION.
Prior experimental work using the gene even-skipped to mark a single FC in each embryonic hemisegment provided a detailed model for the integration of these three signaling pathways at the transcriptional level: the TFs activated by the Wg, Dpp, and Ras pathways T cell factor, Mothers against dpp, and Pointed, respectively were demonstrated to bind a transcriptional enhancer driving expression of eve within dorsal FCs CITATION, CITATION, CITATION, CITATION.
Additional tissue specificity was shown to be provided by two mesodermal selector TFs, Twist and Tinman.
Thus, from this single enhancer, a combinatorial model of transcriptional regulation for genes expressed in FCs was hypothesized, where exogenous signaling cues and endogenous tissue-specific TFs jointly establish the appropriate expression domain.
Guided by this genetic analysis of eve expression, a series of gene expression profiles has been determined for purified embryonic myoblasts by Estrada et al. CITATION.
In addition to profiling wild-type cells, these investigators performed expression array analyses of myoblasts in which the Wg, Dpp, Ras, and Notch pathways were variably perturbed by 12 informative gain-of-function and loss-of-function genetic manipulations.
Each of these 12 genetic perturbations was predicted, based on the example of eve, to increase or decrease expression of those genes with localized expression in FCs.
These 12 expression arrays were then combined into a single weighted ranking, which was used to predict additional FC genes.
Estrada et al. CITATION performed over 200 in situ hybridizations on predicted FC genes from the top of this composite FC ordering, and their experiments yielded a list of 159 validated FC genes.
In the present work, we utilize the expression data of Estrada et al. CITATION to evaluate the roles of dTCF/Mad/Pnt/Twi/Tin as generalized regulators of FC gene expression.
A previous computational scan for windows of sequence containing these five TFs successfully identified an additional enhancer for the gene heartbroken that drove expression in dorsal FCs and contained matches to these five transcription factor binding site motifs, demonstrating that the example of eve was not unique CITATION.
However, the generality of the model could not be established by those two examples alone, and we therefore developed a method of quantifying enrichment for these five TFBS motifs in localized windows of non-coding sequences flanking or intronic to FC genes.
Importantly, this approach, which we term CodeFinder, quantifies the relevance of not only each TF individually, but also of all combinations of the given set of TFs.
From this analysis, we hypothesized that the eve TRM is unlikely to apply to all FC genes.
Rather, we found that three TFs Pnt, Twi, and Tin are likely to regulate a specific subset of FC genes that share characteristic changes in their gene expression profiles in response to the genetic perturbations used by Estrada et al. CITATION.
Thus, by combining TFBS and gene expression data, our analysis allows a refinement of the initial model such that a subset of the original TFs appears to regulate a subset of FC genes.
As a test of this hypothesis, we have empirically validated four candidate FC enhancers that conform to our modified TRM .
Ample evidence has accumulated for the evolutionary importance of duplication events.
However, little is known about the ensuing step-by-step divergence process and the selective conditions that allow it to progress.
Here we present a computational study on the divergence of two repressors after duplication.
A central feature of our approach is that intermediate phenotypes can be quantified through the use of in vivo measured repression strengths of Escherichia coli lac mutants.
Evolutionary pathways are constructed by multiple rounds of single base pair substitutions and selection for tight and independent binding.
Our analysis indicates that when a duplicated repressor co-diverges together with its binding site, the fitness landscape allows funneling to a new regulatory interaction with early increases in fitness.
We find that neutral mutations do not play an essential role, which is important for substantial divergence probabilities.
By varying the selective pressure we can pinpoint the necessary ingredients for the observed divergence.
Our findings underscore the importance of coevolutionary mechanisms in regulatory networks, and should be relevant for the evolution of protein-DNA as well as protein-protein interactions.
Initially put forward by Stevens in 1951 CITATION and later advocated by Ohno in his seminal work CITATION, gene duplication followed by functional divergence is now seen as a general mechanism for acquiring new functions CITATION.
Also, regulatory networks are thought to be shaped significantly by genetic duplication CITATION.
For instance, sequence analysis of transcription factor families points to various historical duplication events CITATION, CITATION.
However, very little is known about the subsequent mutational divergence pathways or about the corresponding stepwise phenotypical changes that are subject to selection.
While these issues have not yet been explored experimentally, related generic aspects of mutational plasticity have been addressed theoretically CITATION CITATION.
However, a central obstacle in studying mutational pathways through computer simulations remains the unknown relation between the sequence and binding affinity, for which, in general, a rather abstract mapping has to be assumed.
To describe the formation of a new regulatory interaction after a duplication event, which is our current aim, such an abstract approach would be particularly speculative.
Here we reason that many characteristics of the adaptation of real protein-DNA contacts are hidden in the extensive body of mutational data that has been accumulated over many years.
These measured repression values can be used as fitness landscapes, in which pathways can be explored by computing consecutive rounds of single base pair substitutions and selection.
Here we develop this approach to study the divergence of duplicate repressors and their binding sites.
More specifically, we focus on the creation of a new and unique protein-DNA recognition, starting from two identical repressors and two identical operators.
We consider selective conditions that favor the evolution toward independent regulation.
Interestingly, such regulatory divergence is inherently a coevolutionary process, where repressors and operators must be optimized in a coordinated fashion.
The mere presence of a selective pressure is clearly not a sufficient condition to achieve a new function.
Rather, the evolutionary potential and limitations can be seen as governed by the shape of the actual fitness landscape and the evolutionary search within it.
Studying these intrinsic limitations to divergence represents the overall aim of this work.
Many open questions arise when considering the formation of a new protein-DNA interaction, which may be viewed as the construction of a new lock and uniquely matching key.
For instance, how should the protein be modified step-by-step to recognize a new DNA-binding site that also does not yet exist, or vice versa?
One would expect that complementary mutations need to occur in the protein and DNA-binding site.
Does this mean that temporary losses in fitness must be endured when taking single-mutation steps?
And, how many mutations must minimally accumulate before a noticeable new recognition is obtained on which selection can act?
The latter is an important point: mutations conferring a selective advantage spread more readily through a population CITATION, resulting in a drastic increase of the divergence probability.
These questions are addressed by exhaustively searching the landscape for optimal pathways, as well as by complementary population dynamics simulations.
Previously it has been shown that lac repressor mutants indeed exist that can bind exclusively to mutant lac operators CITATION.
Our simulations reveal that a duplicated repressor-operator pair can readily evolve to achieve such independence of binding, while monotonously increasing its fitness in a step-by-step process.
Moreover, simply following the fittest mutants does predominantly guide the system to the desired global optimum, which indicates funnel-like features in the fitness landscape.
A detailed analysis of the subsequent network changes indicates a generic sequence of events, of which we study the underlying mechanisms by varying the applied selective pressure.
Next, we show that the trajectories we find in the optimal pathway simulations are not rare exceptions, since similar trajectories are followed using a probabilistic scheme for accepting a mutation.
The results further suggest the feasibility of studying regulatory divergence in laboratory evolution experiments, and finally we make a comparison to alternative models for the creation of new regulatory interactions.
Critical to our many daily choices between larger delayed rewards, and smaller more immediate rewards, are the shape and the steepness of the function that discounts rewards with time.
Although research in artificial intelligence favors exponential discounting in uncertain environments, studies with humans and animals have consistently shown hyperbolic discounting.
We investigated how humans perform in a reward decision task with temporal constraints, in which each choice affects the time remaining for later trials, and in which the delays vary at each trial.
We demonstrated that most of our subjects adopted exponential discounting in this experiment.
Further, we confirmed analytically that exponential discounting, with a decay rate comparable to that used by our subjects, maximized the total reward gain in our task.
Our results suggest that the particular shape and steepness of temporal discounting is determined by the task that the subject is facing, and question the notion of hyperbolic reward discounting as a universal principle.
In the limited amount of time available before nighttime, winter, or retirement, we need to make a large number of choices to maximize our total reward gain.
In particular, when choosing between a larger, but delayed, reward, and a smaller, but more immediate reward, we compare the values associated with each reward, and choose the reward associated with the larger value CITATION.
Critical to these choices are the shape and the steepness of the reward values, which monotonically decrease as a function of the delay: the rewards are said to be discounted as a function of the delays .
Two main classes of models that characterize the shape of reward discounting have been proposed: exponential CITATION CITATION and hyperbolic CITATION CITATION.
Although researchers in artificial intelligence favor exponential discounting in uncertain environments, e.g., CITATION, CITATION, CITATION, all behavioral studies that have directly compared the two types of discounting in animals or humans have concluded that hyperbolic discounting better fits delayed reward choice data than does exponential discounting, e.g., CITATION CITATION, CITATION CITATION .
In exponential discounting, the reward value V is given by: FORMULAwhere R is the reward magnitude, D the delay, and k 0 the decay rate.
This equation is equivalently given by: FORMULAwhere is the discount factor, and exp; we note here that a large decay rate corresponds to a small discount factor and vice versa.
Because of constant decay rate, exponential discounting is rational, as it predicts constant preference.
Typical human studies are questionnaire-based: subjects are asked to make a number of choices between small immediate rewards and larger rewards weeks, months, or years in the future, after thinking about the consequences of each alternative CITATION.
In these studies, the hyperbolic discounted reward value is given by: FORMULA
In animal studies, animals are trained to make repeated reward choices, and experience both delays and rewards.
Assuming a constant inter-trial interval, if the animal consistently makes a choice that gives the same reward R after the same delay D, the average reward rate is the hyperbolic function of the delay CITATION : FORMULAwhere T is the sum of all times except the delay in each trial, and V the reward value.
Because of the decreasing decay rate as a function of delay CITATION, hyperbolic discounting has been termed irrational, as it predicts preference reversal and impulsive choice.
For instance, an individual may prefer one apple today to two apples tomorrow, but at the same time prefer two apples in 51 days to one apple in 50 days CITATION.
Hyperbolic discounting is often presented as a struggle between oneself and one's alter ego in the future, or similarly, between a myopic doer and a farsighted planner see CITATION, CITATION.
In what situations is it theoretically advantageous to make delayed reward choices based on exponential or hyperbolic discounting?
Exponential discounting maximizes total gain in situations of constant probability of reward loss per unit time, and exact estimate of the time of the future reward delivery see CITATION, CITATION.
Because hyperbolic discounted value, as given by Equation 4, is the reward rate, it maximizes the total gain in situations of constant delays at each trial .
But does hyperbolic discounting maximize the total gain in foraging-like situations, that is, in situations of repeated forced choices with varying delays to the rewards, constant ITI, and limited total time?
In these situations, the hyperbolic discounting model maximizes the instantaneous reward rate.
But, as the trials are not independent from each other, hyperbolic discounting may not maximize the average reward rate, and thus the total gain.
For instance, in a relatively unfavorable trial with long delays to both rewards, although hyperbolic discounting may favor the large reward, pursuing the small more immediate reward may result in a smaller overall decrease of the average reward rate.
By choosing the small but less-delayed reward, the subject can quickly move to the next more favorable trials.
Thus, we hypothesize that, in these situations, a discounting strategy that values rewards with longer delays less than hyperbolic discounting, as exponential discounting does, would maximize total gain.
The steepness of discounting specifies how far in the future delayed rewards should be considered.
A large decay rate biases individuals to acquire small and more immediate rewards.
Individuals with impulse-control disorders, as well as heroin-, alcohol-, cigarette-, and cocaine-addicted individuals, have steeper discounting functions than controls CITATION, CITATION CITATION.
A small decay rate promotes the acquisition of large and more delayed rewards.
Yet, individuals must obtain some rewards in time; for instance, an animal must find food before it starves, or before it is exhausted, or before winter arrives.
Thus, the discount rate should be carefully adjusted to maximize total gain in task situations of repeated forced choices with varying delays to the rewards and limited total time CITATION, CITATION .
Here, we designed a task that mimics animal foraging to study whether humans could adopt a discounting function whose shape and steepness maximize total gain.
At each trial, subjects had to choose between a smaller more immediate reward and a larger delayed reward, with varying experienced delays to the rewards, and fixed ITI.
To avoid subjects trying to compute explicit reward ratios, or other objective measures of reward discounting, we did not provide direct access to the delay.
Instead, subjects had to select, at each trial, between one of two squares made of 100 small patches.
The stimulus color coded for the monetary reward amount.
At each trial, the initial number of black patches in the white stimulus indicated the small delay D S, and the initial number of black patches in the yellow stimulus indicated large delay D L. The subject was then prompted to choose one of the two stimuli: the stimulus that had been selected in the previous step showed more filled patches, and the other stimulus was identical to that of the previous step.
The stimuli were always displayed for one time step.
This chain of events was repeated until either square was completely filled.
Then a display of the acquired reward was shown during ITI 1.5 s .
In the experiment, total time was limited to five sessions of 210 s each, separated by 15 s to give the subject some rest time.
Thus, each subject had 700 steps available to maximize the total reward.
Because the subjects performed a minimum of one training session of equal duration before the experiment, they were highly familiar with the task.
Subjects were compensated by the total reward earned at the end of the experiment.
We describe comparative patch analysis for modeling the structures of multidomain proteins and protein complexes, and apply it to the PSD-95 protein.
Comparative patch analysis is a hybrid of comparative modeling based on a template complex and protein docking, with a greater applicability than comparative modeling and a higher accuracy than docking.
It relies on structurally defined interactions of each of the complex components, or their homologs, with any other protein, irrespective of its fold.
For each component, its known binding modes with other proteins of any fold are collected and expanded by the known binding modes of its homologs.
These modes are then used to restrain conventional molecular docking, resulting in a set of binary domain complexes that are subsequently ranked by geometric complementarity and a statistical potential.
The method is evaluated by predicting 20 binary complexes of known structure.
It is able to correctly identify the binding mode in 70 percent of the benchmark complexes compared with 30 percent for protein docking.
We applied comparative patch analysis to model the complex of the third PSD-95, DLG, and ZO-1 domain and the SH3-GK domains in the PSD-95 protein, whose structure is unknown.
In the first predicted configuration of the domains, PDZ interacts with SH3, leaving both the GMP-binding site of guanylate kinase and the C-terminus binding cleft of PDZ accessible, while in the second configuration PDZ interacts with GK, burying both binding sites.
We suggest that the two alternate configurations correspond to the different functional forms of PSD-95 and provide a possible structural description for the experimentally observed cooperative folding transitions in PSD-95 and its homologs.
More generally, we expect that comparative patch analysis will provide useful spatial restraints for the structural characterization of an increasing number of binary and higher-order protein complexes.
Protein protein interactions play a key role in many cellular processes CITATION, CITATION.
An important step towards a mechanistic description of these processes is a structural characterization of the proteins and their complexes CITATION CITATION.
Currently, there are two computational approaches to predict the structure of a protein complex given the structures of its components, comparative modeling CITATION CITATION and protein protein docking CITATION CITATION .
In the first approach to modelling a target complex, standard comparative modelling or threading methods build a model using the known structure of a homologous complex as a template CITATION, CITATION.
The applicability of this approach is limited by the currently sparse structural coverage of binary interactions CITATION.
In the second approach, an atomic model is predicted by protein protein docking, starting from the structures of the individual subunits without any consideration of homologous interactions CITATION CITATION.
This docking is usually achieved by maximizing the shape and physicochemical complementarity of two protein structures, through generating and scoring a large set of possible configurations CITATION, CITATION.
Experimental information, such as that obtained from NMR chemical shift mapping, residual dipolar couplings, and cross-linking, can also be used to guide protein docking CITATION CITATION.
While docking is applicable to any two subunits whose structures are known or modeled, both the sampling of relevant configurations and the discrimination of native-like configurations from the large number of non-native alternatives remain challenging CITATION .
Here, we propose a third approach to modeling complexes between two structures.
The approach, called comparative patch analysis, is a hybrid of protein docking and comparative modeling based on a template complex, with a greater applicability than comparative modeling and a higher accuracy than docking.
Comparative patch analysis relies on our prior analysis of the location of binding sites within families of homologous domains CITATION.
This analysis indicated that the locations of the binding sites are often conserved irrespective of the folds of their binding partners.
The structure of the target complex can thus be modeled by restricting protein docking to only those binding sites that are employed by homologous domains.
As a result, comparative patch analysis benefits from knowledge of all interactions involving either one of the two partners.
We find that comparative patch analysis increases the prediction accuracy relative to protein docking.
It is able to correctly identify the binding mode in 70 percent of 20 benchmark complexes, predicting the overall structure with an average improvement in all-atom RMS error of 13.4, compared with protein docking.
In contrast, protein docking correctly identifies the binding mode in 30 percent of the complexes.
We uncovered the underlying energy landscape for a cellular network.
We discovered that the energy landscape of the yeast cell-cycle network is funneled towards the global minimum from the experimentally measured or inferred inherent chemical reaction rates.
The funneled landscape is quite robust against random perturbations.
This naturally explains robustness from a physical point of view.
The ratio of slope versus roughness of the landscape becomes a quantitative measure of robustness of the network.
The funneled landscape can be seen as a possible realization of the Darwinian principle of natural selection at the cellular network level.
It provides an optimal criterion for network connections and design.
Our approach is general and can be applied to other cellular networks.
In the post-genome era, it is crucial to uncover the underlying mechanism of cellular networks to understand their biological function CITATION CITATION.
The underlying nature of cellular networks has been explored by genetic techniques CITATION.
Cellular networks have been found to be generally quite robust and to perform their biological functions against environmental perturbations.
There are increasing numbers of studies on the global topological structures of networks recently CITATION in which the scale-free properties and hierarchical architectures for networks have been found CITATION CITATION.
The hubs, highly connected nodes in the network essential for keeping the network together, might play an important role for the robustness of the network.
However, there are so far very few studies of why the network should be robust and perform the biological function from the physical point of view CITATION CITATION .
Theoretical models of cellular networks have often been formulated with a set of chemical rate equations.
These dynamical descriptions are inherently local.
To probe the global properties, one often has to change the parameters.
The parameter space is huge.
The global robustness therefore is hard to see from this approach.
Here we will explore the nature of networks from another angle and formulate the problem in terms of a potential function or potential energy landscape.
If the potential energy landscape of the cellular network is known, the global properties can be explored CITATION, CITATION.
This is in analogy with the fact that the global thermodynamic properties can be explored when knowing the inherent interaction potentials in the system.
For the set of the normal chemical rate equations describing the cellular networks, F with x being the concentrations of proteins and F being the chemical reaction rate flux, one cannot in general write the right-hand side of these equations as the gradient of a potential energy function.
However, typical chemical reaction network equations are only approximations on the average concentration level.
In the cell, statistical fluctuations coming from the finite number of molecules provide the source of intrinsic internal noise, and the fluctuations from highly dynamical and inhomogeneous environments of the interior of the cell provide the source of the external noise for the networks CITATION CITATION.
Both the internal and external noise play important roles in determining the properties of the network.
In general, one should study the chemical reaction network equations in noisy conditions to model cellular environments more realistically.
One can also study steady-state properties of these chemical reaction equation networks under noisy environments.
The generalized potential for the steady state of the network exists in general CITATION, CITATION CITATION, CITATION.
Once the network problem is formulated in terms of the generalized potential energy function or potential energy landscape, the issue of the global stability or robustness is much easier to address.
In fact, it is the purpose of this paper to study the global robustness problem directly from the properties of the potential landscape of the network.
To explore the nature of the underlying potential landscape of the cellular networks, we will study the yeast cell-cycle network.
One of the most important functions of the cell is the reproduction and growth.
It is therefore crucial to understand the cell cycle and its underlying process.
The cell cycles during development are usually divided into several phases: the G0/G1, S, G2, and M phases.
In most eukaryotic cells, the elaborate control mechanisms over DNA synthesis and mitosis make sure that the crucial events in the cell cycle are carried out properly and precisely.
Physiologically, there are usually three checkpoints for controlling and coordination: G0/G1 before the new round of division, G2 before the mitotic process begins, and M before segregation.
Recently, many of the underlying controlling mechanisms are revealed by genetic techniques such as mutations and gene knockouts.
It has been found that control has been centered around cyclin-dependent protein kinases, which trigger the major events of the eukaryotic cell cycle.
For example, the activation of the cyclin/CDK dimer drives the cells at both the G1 and G2 checkpoints for further progress.
During other phases and checkpoints CDK/cyclin are activated.
Although molecular interactions regulating the CDK activities are known, the mechanisms of the checkpoint controls are still uncertain CITATION CITATION .
In Figure 1, a coarse-grained relationship between cyclin and cdc2 in the cell cycle is illustrated.
In step 1, cyclin is synthesized de novo.
Newly synthesized cyclin may be unstable.
Cyclin combines with cdc2-P to form pre-MPF.
At some point after heterodimer formation, the cyclin subunit is phosphorylated.
The cdc2 subunit is then dephosphorylated to form active MPF.
In principle, the activation of MPF may be opposed by a protein kinase.
Nuclear division is triggered when a sufficient quantity of MPF has been activated, but concurrently active MPF is destroyed in step 6.
Breakdown of the MPF complex releases phosphorylated cyclin, which is subject to rapid proteolysis.
Finally, the cdc2 subunit is phosphorylated, and the cycle repeats itself.
Mathematical models of the cell cycle controls have been formulated with a set of ordinary first-order differential equations mimicking the underlying biochemical processes CITATION CITATION, CITATION.
The models have been applied to the budding yeast cycle and have explained many qualitative physiological behaviors.
The checkpoints can be viewed as fixed points.
Since the intracellular and intercellular signal transduction induces the changes in the regulatory networks, the cell cycle can be described by or mimicked by the dynamics in and out of the fixed points.
Although detailed simulations give some insights towards the issues, due to the limitation of the parameter space search it is difficult to perceive the global or universal properties of the cycle networks.
It is the purpose of the current study to address this issue.
We will develop a global energy landscape theory for the cell cycle network.
This statistical-based approach is good for two reasons.
It is a coarse-grained approach that captures only the most important factors, so that the analysis can be carried out relatively easily, revealing some global properties.
On the other hand, the statistical approach can be very useful and informative when the data are rapidly accumulating.
In this picture, there are many possible states of the network corresponding to different patterns of activation and inhibition of the protein states.
Each checkpoint can be viewed as a basin of attractions of globally low energy states.
The G0/G1 phase states should have the lowest global energy since it is the end of the cycle.
To initiate the new cycle, the network has to receive the signal to activate or pump to the next phase to proceed.
The dynamics of the cell cycle are described as the dynamical motions on the landscape state space from one basin to another.
This kinetic search is not entirely random but directed, since the random search takes cosmological time.
The direction or gradient of the landscape is provided from the tilting towards the G0/G1 phase.
The landscape therefore becomes funneled towards the G0/G1 state, with the bottom of the funnel what we call the native state.
At the end of G0/G1 phase, the network is pumped to high energy excited states at the top of the funnel.
The cell cycle then follows as it cascades through the configurational state space in a directed way, passing several checkpoints, and finally reaching the bottom of the funnel G0/G1 phase again before being pumped again for another cycle.
We will study the global stability by exploring the underlying potential landscape for the yeast cell-cycle network.
The aim of this paper is to provide a framework and a tool to study at the cell network globally.
At the conclusion of this paper we show that the potential landscape of the budding yeast cell cycle is funneled and robust against the perturbation from the kinetic rates and the environmental disturbances through noise.
A central problem in the bioinformatics of gene regulation is to find the binding sites for regulatory proteins.
One of the most promising approaches toward identifying these short and fuzzy sequence patterns is the comparative analysis of orthologous intergenic regions of related species.
This analysis is complicated by various factors.
First, one needs to take the phylogenetic relationship between the species into account in order to distinguish conservation that is due to the occurrence of functional sites from spurious conservation that is due to evolutionary proximity.
Second, one has to deal with the complexities of multiple alignments of orthologous intergenic regions, and one has to consider the possibility that functional sites may occur outside of conserved segments.
Here we present a new motif sampling algorithm, PhyloGibbs, that runs on arbitrary collections of multiple local sequence alignments of orthologous sequences.
The algorithm searches over all ways in which an arbitrary number of binding sites for an arbitrary number of transcription factors can be assigned to the multiple sequence alignments.
These binding site configurations are scored by a Bayesian probabilistic model that treats aligned sequences by a model for the evolution of binding sites and background intergenic DNA.
This model takes the phylogenetic relationship between the species in the alignment explicitly into account.
The algorithm uses simulated annealing and Monte Carlo Markov-chain sampling to rigorously assign posterior probabilities to all the binding sites that it reports.
In tests on synthetic data and real data from five Saccharomyces species our algorithm performs significantly better than four other motif-finding algorithms, including algorithms that also take phylogeny into account.
Our results also show that, in contrast to the other algorithms, PhyloGibbs can make realistic estimates of the reliability of its predictions.
Our tests suggest that, running on the five-species multiple alignment of a single gene's upstream region, PhyloGibbs on average recovers over 50 percent of all binding sites in S. cerevisiae at a specificity of about 50 percent, and 33 percent of all binding sites at a specificity of about 85 percent.
We also tested PhyloGibbs on collections of multiple alignments of intergenic regions that were recently annotated, based on ChIP-on-chip data, to contain binding sites for the same TF.
We compared PhyloGibbs's results with the previous analysis of these data using six other motif-finding algorithms.
For 16 of 21 TFs for which all other motif-finding methods failed to find a significant motif, PhyloGibbs did recover a motif that matches the literature consensus.
In 11 cases where there was disagreement in the results we compiled lists of known target genes from the literature, and found that running PhyloGibbs on their regulatory regions yielded a binding motif matching the literature consensus in all but one of the cases.
Interestingly, these literature gene lists had little overlap with the targets annotated based on the ChIP-on-chip data.
The PhyloGibbs code can be downloaded from LINK or LINK.
The full set of predicted sites from our tests on yeast are available at LINK.
Transcription factors are proteins that bind in a sequence-specific manner to short DNA segments, most commonly in intergenic DNA upstream of a gene, to activate or suppress gene transcription.
Their DNA-binding domains recognize collections of short related DNA sequences.
One generally finds that, although there is no unique combination of bases that is shared by all binding sites, and although different bases can occur at each position, there are clear biases in the distribution of bases that occur at each position of the binding sites.
A common mathematical representation of a motif that takes this variability into account is a so-called weight matrix CITATION, CITATION w, whose components w i give the probabilities of finding base A, C, G, T at position i of a binding site.
The main assumption underlying this mathematical representation is that the bases occurring at different positions of the binding site are probabilistically independent.
This in turn follows, under some conditions CITATION, from the assumption that the binding energy of the protein to the DNA is a sum of pairwise contact energies between the individual nucleotides and the protein.
There are several algorithms that are based on the WM representation that detect, ab initio, binding sites for a common TF in a collection of DNA sequences CITATION CITATION.
These algorithms broadly fall into two classes.
One class, of which MEME CITATION is the typical representative, searches the space of all WMs for the WM that can best explain the observed sequences.
The class of Gibbs sampling algorithms, of which the Gibbs motif sampler CITATION, CITATION is the typical representative, instead samples the space of all multiple alignments of small sequence segments in search of the one that is most likely to consist of samples from a common WM.
A crucial factor for the success of ab initio methods is the ratio of the number of binding sites to the total amount of DNA in the collection of sequences.
That is, the larger the number of binding sites in the set, and the smaller the total amount of DNA, the more likely it is that ab initio methods can discover the binding sites among the other DNA sequences.
In order to ensure a reasonable chance of success one thus needs to provide these methods with collections of sequences that are highly enriched with binding sites for a common TF.
One possibility is to use sets of upstream regions from genes that appear co-regulated in microarray experiments or that were bound by a common TF in ChIP-on-chip experiments.
Another possibility is to use upstream regions of orthologous genes from related organisms.
Here the assumption is that the regulation of the ancestor gene, and thus its binding sites, has been conserved in the orthologs that descend from it.
This latter approach is in general complicated by a number of factors.
When searching for regulatory sites in sequences that are not phylogenetically related, such as upstream regions of different genes from the same organism, one may simply look for short sequence motifs that are overrepresented among the input sequences.
If the set of species from which the orthologous sequences derive are sufficiently diverged, one may simply choose to ignore the phylogenetic relationship between the sequences and treat the orthologous sequences in the same way as sequences that are not phylogenetically related.
This was, for instance, the approach taken by McCue et al. CITATION, CITATION, where the Gibbs motif sampler algorithm CITATION, CITATION was used on upstream regions of proteo- bacteria.
However, this approach is not applicable to datasets containing more closely related species, where some of the sequences will exhibit significant amounts of similarity simply because of their evolutionary proximity.
Moreover, the amount of similarity will depend on the phylogenetic distance between the species, and it is clear that finding conserved sequence motifs between orthologous sequences from closely related species is much less indicative of function than finding sequence motifs that are conserved between distant species.
One will in general thus have to distinguish conservation due to functional constraints from conservation due to evolutionary proximity, and to do this correctly, the phylogenetic relationship between the sequences has to be taken into account.
A second challenge in using orthologous intergenic sequences from multiple species is the nontrivial structure of their multiple alignments.
One typically finds a very heterogeneous pattern of conservation: well-conserved blocks of different sizes and covering different subsets of the species are interspersed with sequence segments that show little similarity with the sequences of the other species.
The technique of phylogenetic footprinting, restricts attention to only those sequence segments in the genome of interest that show significant conservation with the other species.
The conserved regions for multiple genes are then searched for common motifs by a variety of techniques.
It is unclear, however, to what extent regulatory sites are restricted to such conserved segments.
For instance, several studies of Drosophila and yeast CITATION CITATION have shown that there is no strong correlation between where experimentally annotated binding sites occur and whether that region is conserved.
Thus, at least for yeast and flies, considerable information is lost by focusing on the conserved regions only.
We thus decided to retain the entire patchwork pattern of conserved sequence blocks and unaligned segments.
Our strategy is implemented by a Gibbs sampling approach, and a preliminary account of the algorithm was presented in CITATION.
The algorithm operates on arbitrary collections of both phylogenetically related sequences, such as orthologous intergenic regions, and sequences that are not phylogenetically related, such as upstream regions of different genes from the same organism.
The phylogenetically related groups of sequences in the input are pre-aligned into local multiple alignments where clearly similar sequence segments are aligned into blocks and sequence segments of no or marginal similarity are left unaligned CITATION.
Although the algorithm can also take global multiple alignments as input, we believe that these often force phylogenetically unrelated segments into aligned blocks.
This may adversely affect the performance of the algorithm.
We score putative sites within blocks of aligned sequences with an evolutionary model that takes the phylogenetic relationships of the species into account, while putative sites in unaligned segments are treated as independent occurrences.
This Bayesian model defines a probability distribution over arbitrary placements of putative binding sites for multiple motifs, and we sample it with a Monte Carlo Markov chain.
We first use simulated annealing to search for the globally optimal configuration of binding sites.
The motifs in this configuration are then tracked in a further sampling run to estimate realistic posterior probabilities for all the binding sites that the algorithm reports.
Recently a number of other algorithms have been developed that search for regulatory motifs in groups of phylogenetically related sequences.
Probably the first algorithm that was proposed is a generalization of the Consensus algorithm CITATION called PhyloCon CITATION.
PhyloCon operates on sets of co-regulated genes and their orthologs.
It is a greedy algorithm that first finds ungapped alignments of similar sequence segments in sets of orthologous sequences, and then combines these alignments from different upstream regions into larger alignments.
This algorithm does not take any phylogenetic information into account, i.e., closely related sequences are treated the same as distantly related sequences.
Other drawbacks of this algorithm are that it assumes that each motif will have exactly one site in each of the intergenic regions and that it assumes that this site is conserved in all orthologs.
More closely related to PhyloGibbs's approach are two recent algorithms CITATION, CITATION that generalize MEME CITATION to take the phylogenetic relationships between species into account.
The main difference between EMnEM and PhyME is that PhyME uses the same evolutionary model for the evolution of binding sites as PhyloGibbs, which takes into account that binding sites evolve under constraints set by a WM, whereas EMnEM simply assumes an overall slower rate of evolution in binding sites than in background sequences.
Another difference is that PhyME, like PhyloGibbs, treats the multiple alignment more flexibly than EMnEM, which demands a global multiple alignment.
The main difference between PhyloGibbs and these algorithms is of course that PhyloGibbs takes a motif sampling approach, which allows us to search for multiple motifs in parallel, whereas PhyME and EMnEM use expectation maximization to search for one WM at a time.
In the following sections, we first describe our Bayesian model that assigns a posterior probability to each configuration of binding sites for multiple motifs assigned to the input sequences.
We start by describing the model for phylogenetically unrelated sequences, which is essentially equivalent to the model used in the Gibbs motif sampler CITATION, CITATION, and then describe how this model is extended to datasets that contain phylogenetically related sequences.
After that we describe the move set with which we search the state space of all possible configurations, and the annealing and tracking strategy that we use to identify the significant groups of sites.
We then present examples of the performance of ours and other algorithms on both synthetic and real data.
The synthetic datasets consist of mixtures of WM samples and random sequences, which is in accordance with assumptions that all algorithms make.
This allows us to compare the performance of the algorithms in an idealized situation that does not contain the complexities of real data.
These tests also show to what extent binding sites can be recovered for this idealized data as a function of the quality of the WMs, the number of sites available, and the number of species available and their phylogenetic distances.
For our tests on real data we use 200 upstream regions from Saccharomyces cerevisiae that have known binding sites from the collection CITATION, and compare the ability of the different algorithms to recover these sites when running on multiple alignments of the orthologs of these upstream regions from recently sequenced Saccharomyces genomes CITATION, CITATION.
Finally, we run PhyloGibbs on collections of upstream region alignments that were annotated in CITATION to contain binding sites for a common TF based on data from ChIP-on-chip experiments, and we extensively compare PhyloGibbs' results with the annotations in CITATION and with the literature.
We construct a model to study tradeoffs associated with aging in the adaptive immune system, focusing on cumulative effects of replacing naive cells with memory cells.
Binding affinities are characterized by a stochastic shape space model.
System loss arising from an individual infection is associated with disease severity, as measured by the total antigen population over the course of an infection.
We monitor evolution of cell populations on the shape space over a string of infections, and find that the distribution of losses becomes increasingly heavy-tailed with time.
Initially this lowers the average loss: the memory cell population becomes tuned to the history of past exposures, reducing the loss of the system when subjected to a second, similar infection.
This is accompanied by a corresponding increase in vulnerability to novel infections, which ultimately causes the expected loss to increase due to overspecialization, leading to increasing fragility with age.
In our model, immunosenescence is not the result of a performance degradation of some specific lymphocyte, but rather a natural consequence of the built-in mechanisms for system adaptation.
This robust, yet fragile behavior is a key signature of Highly Optimized Tolerance.
The adaptive immune system CITATION of vertebrates has evolved in a manner that enables adaptation to the history of infections over the lifetime of each individual organism.
It consists of a complex, heterogeneous collection of cells that is derived from stem cells in the bone marrow and proliferates in the lymph nodes.
These cells are endowed with the remarkable ability to discriminate between self and nonself agents within the body and to remove the nonself elements CITATION CITATION.
B and T cells are the white blood cells that constitute the adaptive components of the immune system.
They derive their ability to discriminate self from nonself with the binding specificity of their receptors: T cell receptors for T cells, and membrane-bound antibody for B cells.
These receptors are assembled randomly from gene segments, producing a population of naive cells, in which each individual combination has a different binding specificity.
The random combinations of genes give the immune system the ability to produce diverse cells capable of responding to many pathogens.
During an infection, the cells whose receptors recognize the antigen proliferate and differentiate into antigen-removing effector cells and long-lived memory cells.
The memory cells give rise to a more rapid and efficient response to a secondary exposure to the same antigen.
However, due to homeostatic regulation of the lymphocyte population, the growth of memory cells reduces the naive cell population size.
Over time, this has the effect of increasing sensitivity to novel infections.
We introduce a model that captures this tradeoff between resilience to repeated exposure and sensitivity to new pathogens.
The model consists of coupled differential equations for immune-system cell populations, defined in terms of their primary immunological function and their binding characteristics.
The relative population sizes evolve in time, stimulated by episodic infections.
Antigens are drawn from a probability distribution of their characteristics, which enables estimation of the binding affinity of lymphocytes.
We include a constraint on the total number of immune cells in the system, and define an immunological loss function that quantifies disease severity.
The constraint on the number of cells implies that memory, which is specific to infections the system has seen, comes at a price for unseen infections CITATION.
Our model illustrates how the immune system initially increases in effectiveness but eventually becomes overspecialized with age.
Conventional evolutionary game theory predicts that natural selection favours the selfish and strong even though cooperative interactions thrive at all levels of organization in living systems.
Recent investigations demonstrated that a limiting factor for the evolution of cooperative interactions is the way in which they are organized, cooperators becoming evolutionarily competitive whenever individuals are constrained to interact with few others along the edges of networks with low average connectivity.
Despite this insight, the conundrum of cooperation remains since recent empirical data shows that real networks exhibit typically high average connectivity and associated single-to-broad scale heterogeneity.
Here, a computational model is constructed in which individuals are able to self-organize both their strategy and their social ties throughout evolution, based exclusively on their self-interest.
We show that the entangled evolution of individual strategy and network structure constitutes a key mechanism for the sustainability of cooperation in social networks.
For a given average connectivity of the population, there is a critical value for the ratio W between the time scales associated with the evolution of strategy and of structure above which cooperators wipe out defectors.
Moreover, the emerging social networks exhibit an overall heterogeneity that accounts very well for the diversity of patterns recently found in acquired data on social networks.
Finally, heterogeneity is found to become maximal when W reaches its critical value.
These results show that simple topological dynamics reflecting the individual capacity for self-organization of social ties can produce realistic networks of high average connectivity with associated single-to-broad scale heterogeneity.
On the other hand, they show that cooperation cannot evolve as a result of social viscosity alone in heterogeneous networks with high average connectivity, requiring the additional mechanism of topological co-evolution to ensure the survival of cooperative behaviour.
Conventional evolutionary game theory predicts that natural selection favours the selfish and strong CITATION, in spite of existing evidence showing that cooperation is more widespread than theory predicts CITATION.
When cooperation is modelled in terms of the prisoner's dilemma CITATION, the solution of the replicator dynamics equation in infinite, well-mixed populations CITATION CITATION dictates the extinction of cooperators by defectors.
Cooperators become evolutionarily competitive, however, whenever individuals are constrained to interact with few others along the edges of sparse graphs as recently concluded in two independent studies CITATION, CITATION.
Both studies place individuals on the nodes of a static graph, and associate their social ties with the vertices linking the nodes such that, throughout evolution, every individual has the possibility of changing her strategy, but not her social ties.
In CITATION it has been shown that, under strong selection, heterogeneous graphs lead to a significant increase in the overall survivability of cooperation, modelled in terms of the most popular social dilemmas, played on networks of different degrees of heterogeneity CITATION.
For the classical PD in which the act of cooperation involves a cost c to the provider, resulting in a benefit b for the recipient, a simple relation has been obtained in CITATION for a single cooperator to have a chance to survive in a population of defectors, whenever selection is weak : b/c z, where z stands for the average number of ties each individual has.
Both studies show that games on graphs open a window for the emergence of cooperation, showing how social viscosity alone CITATION can contribute to the emergence of cooperation.
However, recent data shows that realistic networks CITATION CITATION exhibit average connectivity values ranging from 2 to 170, with an associated heterogeneity intermediate between single-scale and broad-scale CITATION, which differs from the connectivity values typically used in previous studies CITATION, CITATION.
For instance, the network of movie actors exhibits an average connectivity of 30 CITATION, whereas collaboration networks based on co-authorship of published papers vary from average values of 4, to 9 up to 15 CITATION.
In terms of the simple rule for the evolution of cooperation for graphs, the reported values of z require benefits to often exceed costs by more than one order of magnitude for a single cooperator to survive CITATION.
None of the previous results on strong CITATION and weak CITATION selection on graphs is capable of explaining how cooperation thrives on such social networks.
Other mechanisms have to be at work here that allow for the survival of cooperation.
In most evolutionary models developed so far, social interactions are fixed from the outset.
Such immutable social ties, associated naturally with static graphs, imply that individuals have no control over the number, frequency, or duration of their ties; they can only evolve their behavioural strategy.
A similar observation can be made on studies related to the physical properties of complex networks CITATION CITATION.
The analyzed networks constitute but one static snapshot of networks that have been typically produced by some growth process.
Yet, networks have naturally evolved before and will continue to evolve after the snapshot has been taken.
Indeed, recent longitudinal studies of evolving social networks CITATION indicate that global properties seem to remain rather stable, whereas individual patterns of social ties remain evolving in time.
Hence, assuming a fixed population size and global average connectivity, one may ask: What role do changes in the interaction framework play in the evolution of cooperation and to which extent will the social dilemmas influence the topology and heterogeneity of the evolving network?
Using a minimal model that combines strategy evolution with topological evolution, and in which the requirements of individual cognitive capacities are very small, we investigate under which conditions cooperation may thrive.
Network heterogeneity, which now emerges as a result of an entangled co-evolutionary dynamics, will be shown to play a crucial role in facilitating cooperative behaviour.
Let us consider two types of individuals cooperators and defectors who engage in several of the most popular social dilemmas of cooperation.
They are not required to accumulate information on all other players, only those they are immediately connected with.
Moreover, they are able to decide, on an equal footing with all players, those ties that they want to maintain and those they want to change.
Given an edge with individuals A and B at the extremes, we say that A is satisfied with the edge if the strategy of B is a cooperator, being dissatisfied otherwise.
If A is satisfied, she will decide to maintain the link.
If dissatisfied, then she may compete with B to rewire the link, rewiring being attempted to a random neighbour of B. The intuition behind this reasoning is the following: simple agents, being rational individuals with limited information, tend to interact with other agents that are close by in a social manner CITATION.
In this sense, agent A is more likely to encounter one of the friends of B and become a friend with B's neighbour.
Moreover, selecting a neighbour of an inconvenient partner may turn out to be a good choice, since this partner also tries to establish links with cooperators, making it more likely that the rewiring results in a tie to a cooperator.
Indeed, for all social dilemmas described below, a link with a cooperator maximizes the fitness of any individual, irrespective of its strategy.
Consequently, all individuals naturally seek to establish links with cooperators.
Hence, rewiring to a neighbour of a defector is certainly a good choice for individuals with local information only.
The social dilemmas of cooperation examined in this work are modelled in terms of symmetric two-player games, where the players can either cooperate or defect upon interaction.
When both cooperate, they receive the payoff R. On the other hand, when both defect, they both obtain the payoff P. The two remaining possibilities occur when one defects and the other cooperates, resulting in the payoff T for the defector and S for the cooperator.
Depending on the relative ordering of these four payoff values and assuming that mutual cooperation is preferred over mutual defection, three well-known social dilemmas emerge CITATION the snowdrift game, the stag-hunt game and the prisoner's dilemma.
The dilemma follows from the players' payoff preferences.
In the SG game, the players are referred to as greedy since they prefer unilateral defection to mutual cooperation.
In the SH game, mutual defection is preferred to unilateral cooperation, resulting in an intrinsic fear for the players to cooperate.
Finally, both dilemmas are combined in the PD game, making it the most difficult situation for cooperation to arise.
We adopt the convention of CITATION and normalize the difference between mutual cooperation and mutual defection to 1, making R 1 and P 0, respectively.
As a consequence, we investigate all dilemmas, summarized in Table 1, in a 2-D parameter space, depicted in Figure 2, where the payoff T satisfies 0 T 2 and the payoff S satisfies 1 S 1.
The fitness of each individual corresponds to the total accumulated payoff resulting from pairwise interactions with all her neighbours.
The fact that in our model cooperators and defectors interact via social ties they both decide upon establishes a coupling between individual strategy and population structure: the game payoff induces now an entangled co-evolution of strategy and structure.
Such an adaptive individual behaviour introduces a new time scale, not necessarily equal to the time scale associated with strategy evolution.
Depending on the ratio W e / a, different fates may occur for cooperation.
Indeed, whenever W 0, we recover the results of CITATION, CITATION.
On the other hand, with increasing W, individuals become apt to adapt their ties with increasing efficiency.
In general, however, one expects the two time scales to be of comparable magnitude in realistic situations.
W provides a measure of individuals' inertia to react to their rational choices both at strategy and topological levels: large values of W reflect populations in which individuals react promptly to adverse ties, whereas smaller values reflect some overall inertia for topological change.
In general, the type of response will change from individual to individual.
Hence, W reflects here an average characteristic of the population.
Deleterious mutations are considered a major impediment to adaptation, and there are straightforward expectations for the rate at which they accumulate as a function of population size and mutation rate.
In a simulation model of an evolving population of asexually replicating RNA molecules, initially deleterious mutations accumulated at rates nearly equal to that of initially beneficial mutations, without impeding evolutionary progress.
As the mutation rate was increased within a moderate range, deleterious mutation accumulation and mean fitness improvement both increased.
The fixation rates were higher than predicted by many population-genetic models.
This seemingly paradoxical result was resolved in part by the observation that, during the time to fixation, the selection coefficient of initially deleterious mutations reversed to confer a selective advantage.
Significantly, more than half of the fixations of initially deleterious mutations involved fitness reversals.
These fitness reversals had a substantial effect on the total fitness of the genome and thus contributed to its success in the population.
Despite the relative importance of fitness reversals, however, the probabilities of fixation for both initially beneficial and initially deleterious mutations were exceedingly small .
Modern evolutionary theory recognizes that deleterious mutations may reduce fitness and retard adaptation CITATION CITATION.
Accumulation of deleterious mutations is expected to affect the rate and course of many biological processes such as sexual selection, development of cancer, and senescence CITATION.
The theoretical work underlying these predictions makes an important assumption: the fitness effect of a deleterious mutation is constant until the mutation disappears or fixes.
In the standard infinite population experiencing a combination of natural selection and random mutation, deleterious mutations should not fix, but accumulate to a level perfectly balanced by mutation and selection.
Some processes can lead to deleterious mutations fixing in infinite populations, however.
For example, in Eigen's quasispecies model, high rates of mutation can overwhelm selection and shift the mutation selection balance such that deleterious mutations accumulate to exceedingly high levels CITATION, CITATION.
In finite populations, several processes may also allow deleterious mutation fixation CITATION.
The best studied of these is random genetic drift the stochastic fixation of deleterious mutations in relatively small populations.
Additionally, if recombination is rare and the population size is finite, then deleterious mutations can hitchhike to fixation with independently acting beneficial mutations CITATION, CITATION .
The fixation of deleterious mutations certainly reduces the fitness of populations.
It may be possible, however, for the fitness effect of an initially deleterious mutation to change over time.
In particular, compensatory mutations may evolve that reduce the negative impact of deleterious mutations or, in extreme cases, the resulting fitness may be even higher than the fitness of the ancestor in which the deleterious mutation arose CITATION.
Such compensatory mutations may appear before the deleterious mutation has fixed.
Metaphorically speaking, while fixed deleterious mutations are generally expected to be bad, they may be stepping stones to distant adaptive peaks.
Evolutionary geneticists have long considered mutations that ameliorate or compensate for the deleterious effect of a prior mutation.
The literature on this subject, however, focuses almost exclusively on compensatory mutations occurring after the fixation of the initial deleterious mutation, and therefore does not address the likelihood that the initial mutation will fix in the first place CITATION CITATION.
One possible explanation for this emphasis is convenience both mathematical and experimental.
To greatly simplify the evolutionary dynamics, population-genetic models of adaptation typically assume that selection is much stronger than mutation: strong selection, weak mutation.
Under this assumption, a deleterious mutation will disappear or fix before secondary mutations arise in the genome and thus the fitness effect of a deleterious mutation remains unchanged throughout its evolutionary trajectory to either fixation or loss CITATION .
If the mutation rate is relatively large, however, additional mutations may arise in the genome carrying the initially deleterious mutation before it fixes or is lost.
Such secondary mutations change the genetic background and thus potentially change the fitness effect of the initial deleterious mutation.
The background selection literature has frequently considered the scenario in which a good mutation is driven to extinction by bad mutations.
Our interest, however, is in a process involving epistasis between mutations that results in amelioration of the deleterious effect and, ultimately, the fixation of an initially bad mutation.
Although this process seems to have been largely ignored in the population genetics literature, one notable study by Kimura shows that mutations at two linked alleles, which are singly deleterious but jointly neutral, can both fix relatively rapidly even in large populations CITATION.
There has also been recent interest in the ability of populations to escape from local optima via less fit intermediate genotypes .
Here we use a computational model of asexually replicating RNA molecules to study the fixation of deleterious mutations.
We first observe that initially deleterious mutations fix at a far greater rate than expected for an evolving asexual population and that some populations achieve high mean fitness despite rapidly accumulating deleterious mutations.
We then reconcile this paradox by systematically characterizing the processes leading to fixation, which include random genetic drift, hitchhiking upon independently acting beneficial mutations, and fitness-effect reversals upon secondary mutations.
The intrinsic, or mitochondrial, pathway of caspase activation is essential for apoptosis induction by various stimuli including cytotoxic stress.
It depends on the cellular context, whether cytochrome c released from mitochondria induces caspase activation gradually or in an all-or-none fashion, and whether caspase activation irreversibly commits cells to apoptosis.
By analyzing a quantitative kinetic model, we show that inhibition of caspase-3 and Casp9 by inhibitors of apoptosis results in an implicit positive feedback, since cleaved Casp3 augments its own activation by sequestering IAPs away from Casp9.
We demonstrate that this positive feedback brings about bistability, and that it cooperates with Casp3-mediated feedback cleavage of Casp9 to generate irreversibility in caspase activation.
Our calculations also unravel how cell-specific protein expression brings about the observed qualitative differences in caspase activation.
Finally, known regulators of the pathway are shown to efficiently shift the apoptotic threshold stimulus, suggesting that the bistable caspase cascade computes multiple inputs into an all-or-none caspase output.
As cellular inhibitory proteins frequently inhibit consecutive intermediates in cellular signaling cascades, the feedback mechanism described in this paper is likely to be a widespread principle on how cells achieve ultrasensitivity, bistability, and irreversibility.
Apoptosis, an evolutionary conserved form of cell suicide, allows multicellular organisms to eliminate damaged or excess cells in order to maintain tissue homeostasis.
Dysregulation of apoptosis is associated with various pathological conditions, including cancer and neurodegenerative disorders.
Aspartate-specific cysteine proteases, also known as caspases, are the central executioners of apoptosis.
In most cases, apoptotic stimuli activate initiator caspases, whose substrates, the effector caspases, ultimatively cause cellular demise by cleaving various cellular substrates CITATION .
Figure 1A schematically depicts the so-called extrinsic and intrinsic apoptotic pathways that elicit apoptosis by cleaving and thereby activating caspase-3, the major cellular effector caspase.
The extrinsic pathway is initiated by ligand-binding to death receptors, which then oligomerize and recruit various proteins, including pro-Casp8, into the so-called death-inducing signaling complex.
Formation of the death-inducing signaling complex leads to autoprocessing of pro-Casp8 into active Casp8, which then cleaves Casp3.
Cytotoxic stress or death-receptor stimulated Casp8 engage the intrinsic, or mitochondrial, apoptosis pathway by inducing the translocation of proapoptotic Bcl-2 family members such as Bax and Bid to mitochondria.
This event, which is negatively regulated by antiapoptotic Bcl-2 family members, results in the release of proapoptotic proteins from mitochondria into the cytosol.
Cytosolic cyto c then elicits the oligomerization of Apaf-1 into an active high-molecular-weight complex, the apoptosome, which recruits and stimulates Casp9, and thereby allows activation of effector caspases such as Casp3.
Smac and inhibitors of apoptosis such as X-linked IAP establish an additional layer of regulation in the intrinsic pathway: XIAP inhibits the catalytic activities of Casp9 and Casp3 through reversible binding, and cytosolic Smac relieves this inhibition by sequestering XIAP away from caspases CITATION .
Experimental studies revealed that the qualitative behaviour of caspase activation in the intrinsic pathway depends on the cellular context.
Cyto c added to cytosolic extracts activates Casp3 in an all-or-none fashion in some cells CITATION CITATION, while gradual activation was observed in other systems CITATION CITATION.
As cyto-c release from mitochondria can be a reversible event CITATION, which does not affect mitochondrial function CITATION CITATION, it has been suggested that downstream caspase activation irreversibly commits cells to apoptosis CITATION, CITATION.
Accordingly, cyto c induced Casp3 activation remained elevated even after a strong decline in cytosolic cyto c CITATION or after apo cyto c, an inhibitor of apoptosome formation, was added CITATION.
Furthermore, the time course of caspase activation via the intrinsic pathway equals that for irreversible commitment to apoptosis CITATION, CITATION, and caspase-inhibition allows for long-term cellular recovery and/or proliferation after removal of apoptotic stimuli CITATION, CITATION, CITATION CITATION.
Finally, Fas-treated Jurkat T cells, which enter apoptosis by the intrinsic pathway, escaped commitment to death as judged by maintenance of clonogenic potential if Casp3 was inhibited CITATION.
On the contrary, Casp3 activation was found to be a reversible event in glycochenodeoxycholate-treated hepatocytes CITATION .
These qualitative differences in caspase activation suggest that the intrinsic pathway is bistable in some cells, but monostable in others.
While simple monostable systems respond in a gradual and reversible manner, complex bistable systems exhibit true all-or-none responses and in some cases irreversibility.
Bistability is thought to require a positive circuit, which may be established either by positive feedback or by double-negative feedback.
Once a threshold stimulus is exceeded, such positive circuits allow bistable systems to switch from low activation levels to high activation levels in an all-or-none fashion.
Bistable systems display hysteresis, meaning that different stimulus-response curves are obtained depending upon whether the system began in its off or its on state.
In some cases, the on state is maintained indefinitely after the stimulus is removed, so that the system shows irreversible activation CITATION.
Experimental studies confirmed that bistability indeed occurs in natural and artificial biological networks CITATION CITATION .
Recent mathematical modeling demonstrated that bistability can arise from hidden, or implicit, feedback loops that are usually not explicitly drawn in biochemical reaction schemes CITATION, CITATION.
Similarily, we present a model showing that inhibition of Casp3 and Casp9 by IAPs results in an implicit positive feedback and in bistability.
As cellular inhibitory proteins frequently inhibit consecutive intermediates in cellular signaling cascades, the mechanism described in this paper is likely to be a widespread principle on how cells achieve ultrasensitivity, bistability, and irreversibility .
Our investigation of knotted structures in the Protein Data Bank reveals the most complicated knot discovered to date.
We suggest that the occurrence of this knot in a human ubiquitin hydrolase might be related to the role of the enzyme in protein degradation.
While knots are usually preserved among homologues, we also identify an exception in a transcarbamylase.
This allows us to exemplify the function of knots in proteins and to suggest how they may have been created.
Although knots are abundant and complex in globular homopolymers CITATION CITATION, they are rare and simple in proteins CITATION CITATION.
Sixteen methyltransferases in bacteria and viruses can be combined into the / knot superfamily CITATION, and several isozymes of carbonic anhydrase are known to be knotted.
Apart from these two folds, only a few insular knots have been reported CITATION, CITATION, CITATION, CITATION, some of which were derived from incomplete structures CITATION, CITATION.
For the most part, knotted proteins contain simple trefoil knots that can be represented by three essential crossings in a projection onto a plane.
Only three proteins were identified with four projected crossings .
In this report we provide the first comprehensive review of knots in proteins, which considers all entries in the Protein Data Bank CITATION, and not just a subset.
This allows us to examine knots in homologous proteins.
Our analysis reveals several new knots, all in enzymes.
In particular, we discovered the most complicated knot found to date in human ubiquitin hydrolase, and suggest that its entangled topology protects it against being pulled into the proteasome.
We also noticed that knots are usually preserved among structural homologues.
Sequence similarity appears to be a strong indicator for the preservation of topology, although differences between knotted and unknotted structures are sometimes subtle.
Interestingly, we have also identified a novel knot in a transcarbamylase that is not present in homologues of known structure.
We show that the presence of this knot alters the functionality of the protein, and suggest how the knot may have been created in the first place.
Mathematically, knots are rigorously defined in closed loops CITATION.
Fortunately, both the N- and C-termini of open proteins are typically accessible from the surface and can be connected unambiguously: we reduce the protein to its C - backbone, and draw two lines outward starting at the termini in the direction of the connection line between the center of mass of the backbone and the respective ends CITATION.
The lines are joined by a big loop, and the structure is topologically classified by the determination of its Alexander polynomial CITATION, CITATION.
Applying this method to the Protein Data Bank in the version of January 3, 2006, we found 273 knotted structures in the 32,853 entries that contain proteins.
Knots formed by disulfide CITATION, CITATION or hydrogen bonds CITATION were not included in the study.
In Arabidopsis, tandemly arrayed genes comprise 10 percent of the genes in the genome.
These duplicated genes represent a rich template for genetic innovation, but little is known of the evolutionary forces governing their generation and maintenance.
Here we compare the organization and evolution of TAGs between Arabidopsis and rice, two plant genomes that diverged ~150 million years ago.
TAGs from the two genomes are similar in a number of respects, including the proportion of genes that are tandemly arrayed, the number of genes within an array, the number of tandem arrays, and the dearth of TAGs relative to single copy genes in centromeric regions.
Analysis of recombination rates along rice chromosomes confirms a positive correlation between the occurrence of TAGs and recombination rate, as found in Arabidopsis.
TAGs are also biased functionally relative to duplicated, nontandemly arrayed genes.
In both genomes, TAGs are enriched for genes that encode membrane proteins and function in abiotic and biotic stress but underrepresented for genes involved in transcription and DNA or RNA binding functions.
We speculate that these observations reflect an evolutionary trend in which successful tandem duplication involves genes either at the end of biochemical pathways or in flexible steps in a pathway, for which fluctuation in copy number is unlikely to affect downstream genes.
Despite differences in the age distribution of tandem arrays, the striking similarities between rice and Arabidopsis indicate similar mechanisms of TAG generation and maintenance.
The genomes of Arabidopsis thaliana and Oryza sativa contain substantial proportions of duplicated chromosomal segments, presumably reflecting ancient polyploidy events.
In Arabidopsis, for example, there have been at least three paleopolyploid events CITATION, with the most recent occurring ~25 million years ago CITATION.
The duplicated chromosomal regions retain ~25 percent of their genes as duplicates CITATION, with the remaining duplicate pairs having lost one copy to deletion or pseudogenization.
Surprisingly, the process of gene loss is nonrandom with respect to function, because genes that are retained as duplicates are enriched for functions related to transcription, signal transduction, and development CITATION, CITATION.
Like Arabidopsis, rice also has a history of extensive duplication CITATION, with up to ~60 percent of the genome apparently duplicated by paleopolyploid events CITATION and up to ~50 percent of genes retained as duplicates on duplicated chromosomal segments CITATION .
Although there have been numerous studies to identify genes duplicated via paleopolyploidy, one important source of duplication in plant genomes has not been studied in great detail: tandemly arrayed genes.
TAGs are gene family members that are tightly clustered on a chromosome, and they are frequent in plant genomes.
In A. thaliana, TAGs comprise almost as many genes as those duplicated by paleopolyploid events CITATION.
They also represent a broad functional component of the genome, ranging from genes that encode secondary metabolites CITATION, to disease resistance genes CITATION, to regulatory genes CITATION .
The evolution and organization of TAGs have been studied in Arabidopsis.
TAGs are underrepresented in centromeric regions relative to non-TAG genes, and their prevalence relative to non-TAG genes is positively correlated with recombination rates along chromosomes CITATION.
The evolutionary processes contributing to this correlation are unclear.
The correlation could reflect the generation of TAGs via recombination-mediated processes such as unequal crossing-over, or it could be produced indirectly by interplay among selection, recombination, gene gain, and gene loss.
It is also unclear whether the TAG organization in Arabidopsis is representative of other plant genomes.
TAGs are also likely to differ from dispersed gene families in their process of divergence.
The close physical proximity of TAGs facilitates gene conversion, as recently demonstrated in both yeast CITATION and Arabidopsis CITATION.
One practical ramification is that the synonymous distance between TAGs cannot be easily used as a proxy for the time of the duplication event that gave rise to the two genes CITATION.
Instead, Ks provides insight into either the age of the duplication event or the age of homogenizing gene conversion events CITATION.
Nonetheless, careful study of Ks values among clustered genes could uncover clues to TAG maintenance and diversification.
The completion of the rice genome sequence provides the first opportunity to compare the structure and evolution of TAGs between two plant genomes, Arabidopsis and rice.
The two species diverged ~150 million years ago CITATION but are similar in that they have relatively small genomes and reproduce predominantly by selfing.
Genomic analyses of the rice sequence have already revealed some properties of TAGs i.e., that TAGs compose between 16 percent CITATION and 29 percent of rice genes CITATION and that the preponderance of tandemly duplicated genes are differentiated by relatively low Ks values CITATION.
Nonetheless, TAGs in rice have not been studied in a comparative context nor in the context of genomic features such as chromosomal location and recombination.
In this paper, we address several basic questions about the organization, evolution, and function of TAGs.
First, does the number and distribution of TAGs differ substantially between rice and Arabidopsis?
Second, are TAGs more frequent in high recombination regions in rice, as they are in Arabidopsis?
Third, do the two species exhibit clear similarities or differences in the distribution of Ks among TAGs?
Fourth, do genes in TAGs represent functional biases relative to non-TAG genes?
Finally, can we infer any general mechanisms that contribute to similarities and differences between the distribution of TAGs in the Arabidopsis and rice genomes?
Members of the vascular endothelial growth factor family of proteins are critical regulators of angiogenesis.
VEGF concentration gradients are important for activation and chemotactic guidance of capillary sprouting, but measurement of these gradients in vivo is not currently possible.
We have constructed a biophysically and molecularly detailed computational model to study microenvironmental transport of two isoforms of VEGF in rat extensor digitorum longus skeletal muscle under in vivo conditions.
Using parameters based on experimental measurements, the model includes: VEGF secretion from muscle fibers; binding to the extracellular matrix; binding to and activation of endothelial cell surface VEGF receptors; and internalization.
For 2-D cross sections of tissue, we analyzed predicted VEGF distributions, gradients, and receptor binding.
Significant VEGF gradients were predicted in resting skeletal muscle with uniform VEGF secretion, due to non-uniform capillary distribution.
These relative VEGF gradients were not sensitive to extracellular matrix composition, or to the overall VEGF expression level, but were dependent on VEGF receptor density and affinity, and internalization rate parameters.
VEGF upregulation in a subset of fibers increased VEGF gradients, simulating transplantation of pro-angiogenic myoblasts, a possible therapy for ischemic diseases.
The number and relative position of overexpressing fibers determined the VEGF gradients and distribution of VEGF receptor activation.
With total VEGF expression level in the tissue unchanged, concentrating overexpression into a small number of adjacent fibers can increase the number of capillaries activated.
The VEGF concentration gradients predicted for resting muscle is sufficient for cellular sensing; the tip cell of a vessel sprout is approximately 50 m long.
The VEGF gradients also result in heterogeneity in the activation of blood vessel VEGF receptors.
This first model of VEGF tissue transport and heterogeneity provides a platform for the design and evaluation of therapeutic approaches.
Vascular endothelial growth factor is a key promoter of angiogenesis in vivo and it increases proliferation and migration of endothelial cells cultured in vitro CITATION.
In rats, there are five main splice variants of VEGF, denoted 120, 144, 164, 188, and 205, and the 120 and 164 isoforms are the most prevalent CITATION.
VEGF is expressed at different levels by a variety of cells throughout the body including skeletal muscle CITATION CITATION.
VEGF 164 is secreted as a 45-kDa homodimeric glycoprotein containing an exon-7 encoded domain which allows binding to heparin and neuropilin-1.
VEGF 120 is also a homodimeric glycoprotein but is missing the exon-7 encoded domain.
Because of this domain, only VEGF 164 can bind to the heparan sulfate proteoglycans present in high concentrations in the extracellular matrix and basement membrane spaces, and the two splice variants are responsible for different signaling in both physiological and cancer angiogenesis CITATION, CITATION.
Furthermore, due to the presence of high concentrations of HSPG in the BM that surrounds VEGF-secreting cells, a large amount of VEGF 164 becomes bound and sequestered near sources of VEGF secretion, creating a steep VEGF gradient CITATION.
The cellular response to VEGF occurs when signaling is initiated by the binding of VEGF to its cell surface receptor tyrosine kinases, VEGFR1 and VEGFR2.
VEGF is degraded after it is internalized by these two VEGF receptors.
The receptors and their interactions with VEGF and with each other are discussed in depth in CITATION .
VEGF is involved in both physiological and pathological angiogenesis.
VEGF upregulation is necessary for physiological angiogenesis under conditions of hypoxia via oxygen-sensing mechanisms in the HIF-1 pathway CITATION and increased shear stress in blood vessels CITATION.
In rat extensor digitorum longus muscle during exercise, both hypoxia and increased shear stress induce angiogenesis through overexpression of VEGF by myocytes CITATION.
Because of the importance of VEGF, many clinical trials are under way for both pro- and anti-angiogenic therapies CITATION CITATION.
The FDA has approved anti-VEGF treatments including Avastin, an antibody to VEGF, and Macugen, an RNA aptamer which binds to and sequesters human VEGF 165.
Use of VEGF as a pro-angiogenic treatment for cardiac and limb ischemia has generated intense interest but direct administration of VEGF has not yet produced effective results in humans, and initial trials of transplantation of angiogenic cells have proven effective but require further work CITATION CITATION .
To create an effective VEGF-driven pro-angiogenic therapy, better understanding is needed of both physiological and pathological VEGF-induced angiogenesis.
Increasing VEGF concentration leads to angiogenesis, but concentrations of VEGF beyond critical levels may result in formation of abnormal vessels and hemangiomas CITATION, CITATION.
Thus, normal blood vessels can only be formed if microenvironmental amounts of VEGF are carefully maintained above threshold levels for therapeutic angiogenesis but below threshold levels for abnormal angiogenesis over a prolonged period of time.
Furthermore, responses to VEGF depend on not only VEGF concentration but also VEGF gradients, which enhance VEGF-induced angiogenesis and direct capillary growth CITATION CITATION.
Therefore, it is not sufficient to measure bulk quantities of VEGF in large samples of tissue in order to predict angiogenic behavior.
In the present study, we constructed a computational model to study extracellular diffusion of VEGF in vivo and effects of VEGF upregulation on gradients and receptor binding using the well-characterized rat EDL tissue as a sample environment.
We have previously constructed models studying the kinetics of VEGF binding to receptors on cells in vitro and the effect of the presence of NRP-1 or placental growth factor CITATION, CITATION.
We have also shown using Monte Carlo methods that despite very small concentrations of free VEGF, a continuum description in terms of VEGF concentrations is justified CITATION.
Diffusion of VEGF in vitro has also been studied using a computational model CITATION.
However, to our knowledge, this is the first model of VEGF diffusion in a geometrically complex in vivo environment and it includes: transport of the two most abundant VEGF isoforms through ECM and BM, HSPG binding kinetics, VEGF secretion, and VEGF receptor binding and internalization kinetics.
NRP-1 was not included in this model because there are currently no available measurements of the amount of NRP-1 in skeletal muscle.
Using this model, we predict VEGF distribution and analyze VEGF gradients at a resolution that is currently impossible to measure experimentally.
Use of this model will aid in understanding mechanisms of physiological and therapeutic VEGF overexpression.
This model is general and may be built upon in the future to include additional molecular species as new experimental data are emerging, e.g., neuropilin expression level in skeletal muscle; it can also be applied to specific drug interactions and other tissue types.
Text-mining algorithms make mistakes in extracting facts from natural-language texts.
In biomedical applications, which rely on use of text-mined data, it is critical to assess the quality of individual facts to resolve data conflicts and inconsistencies.
Using a large set of almost 100,000 manually produced evaluations, we implemented and tested a collection of algorithms that mimic human evaluation of facts provided by an automated information-extraction system.
The performance of our best automated classifiers closely approached that of our human evaluators.
Our hypothesis is that, were we to use a larger number of human experts to evaluate any given sentence, we could implement an artificial-intelligence curator that would perform the classification job at least as accurately as an average individual human evaluator.
We illustrated our analysis by visualizing the predicted accuracy of the text-mined relations involving the term cocaine.
Information extraction uses computer-aided methods to recover and structure meaning that is locked in natural-language texts.
The assertions uncovered in this way are amenable to computational processing that approximates human reasoning.
In the special case of biomedical applications, the texts are represented by books and research articles, and the extracted meaning comprises diverse classes of facts, such as relations between molecules, cells, anatomical structures, and maladies.
Unfortunately, the current tools of information extraction produce imperfect, noisy results.
Although even imperfect results are useful, it is highly desirable for most applications to have the ability to rank the text-derived facts by the confidence in the quality of their extraction.
We focus on automatically extracted statements about molecular interactions, such as small molecule A binds protein B, protein B activates gene C, or protein D phosphorylates small molecule E.
Several earlier studies have examined aspects of evaluating the quality of text-mined facts.
For example, Sekimizu et al. and Ono et al. attempted to attribute different confidence values to different verbs that are associated with extracted relations, such as activate, regulate, and inhibit CITATION, CITATION.
Thomas et al. proposed to attach a quality value to each extracted statement about molecular interactions CITATION, although the researchers did not implement the suggested scoring system in practice.
In an independent study CITATION, Blaschke and Valencia used word-distances between biological terms in a given sentence as an indicator of the precision of extracted facts.
In our present analysis we applied several machine-learning techniques to a large training set of 98,679 manually evaluated examples to design a tool that mimics the work of a human curator who manually cleans the output of an information-extraction program.
Our goal is to design a tool that can be used with any information-extraction system developed for molecular biology.
In this study, our training data came from the GeneWays project, and thus our approach is biased toward relationships that are captured by that specific system.
We believe that the spectrum of relationships represented in the GeneWays ontology is sufficiently broad that our results will prove useful for other information-extraction projects.
Our approach followed the path of supervised machine-learning.
First, we generated a large training set of facts that were originally gathered by our information-extraction system, and then manually labeled as correct or incorrect by a team of human curators.
Second, we used a battery of machine-learning tools to imitate computationally the work of the human evaluators.
Third, we split the training set into ten parts, so that we could evaluate the significance of performance differences among the several competing machine-learning approaches.
Electroreceptive fish detect nearby objects by processing the information contained in the pattern of electric currents through the skin.
The distribution of local transepidermal voltage or current density on the sensory surface of the fish's skin is the electric image of the surrounding environment.
This article reports a model study of the quantitative effect of the conductance of the internal tissues and the skin on electric image generation in Gnathonemus petersii.
Using realistic modelling, we calculated the electric image of a metal object on a simulated fish having different combinations of internal tissues and skin conductances.
An object perturbs an electric field as if it were a distribution of electric sources.
The equivalent distribution of electric sources is referred to as an object's imprimence.
The high conductivity of the fish body lowers the load resistance of a given object's imprimence, increasing the electric image.
It also funnels the current generated by the electric organ in such a way that the field and the imprimence of objects in the vicinity of the rostral electric fovea are enhanced.
Regarding skin conductance, our results show that the actual value is in the optimal range for transcutaneous voltage modulation by nearby objects.
This result suggests that voltage is the answer to the long-standing question as to whether current or voltage is the effective stimulus for electroreceptors.
Our analysis shows that the fish body should be conceived as an object that interacts with nearby objects, conditioning the electric image.
The concept of imprimence can be extended to other sensory systems, facilitating the identification of features common to different perceptual systems.
Electroreceptive fish detect nearby objects by processing the information contained in the pattern of electric currents through the skin.
In weakly electric fish, these currents result from a self-generated field, produced by the electric organ discharge.
Local transepidermal voltage or current density is the effective stimulus for electroreceptors.
The distribution of voltage or current on the sensory surface of the fish's skin is the electric image of the surrounding environment CITATION CITATION.
From this image, the brain constructs a representation of the external world.
Therefore, to understand electrolocation it is necessary to know the image-generation strategy used by electrolocating animals.
Theoretical analysis of image generation has yielded realistic models that predict with acceptable accuracy the electrosensory stimulus CITATION CITATION.
One general conclusion of previous reports is that the skin conductance and the conductivity difference between the internal tissues of the fish and the water are the main factors shaping the electric image: the seminal paper by Lissmann and Machin CITATION started a long-lasting controversy about the roles of these factors.
Lissmann and Machin argued that if the fish has approximately the same conductivity as the water and that it does not appreciably distort the perturbing field, the potential distribution around the fish due to the perturbing field can be calculated.
However, several reports CITATION, CITATION, CITATION have indicated that the internal conductivity of freshwater fish is high with respect to the surrounding water, and that the high conductance of internal tissues is critical for enhancing the local EOD field as well as for generating the centre-surround opposition pattern that characterizes electric images and that is coded by primary afferents CITATION .
Experimental studies in pulse gymnotids have confirmed theoretical predictions, showing that the high conductivity of the fish body funnels the self-generated current to the perioral region, where an electrosensory fovea has been described on the basis of electroreceptor density, variety, and central representation CITATION.
This funnelling effect enhances the stimulus at the foveal region.
In addition, two different types of skin have been described in some electric fish of the family Mormyridae: the low-conductance mormyromast epithelium where electroreceptors are present, and the high-conductance non-mormyromast epithelium where electroreceptors are absent CITATION, CITATION.
The mormyromast epithelium is found on the head in front of the gills, as well as along the dorsum of the back and along the ventral surface of the trunk.
The non-mormyromast epithelium is found along the sides of the trunk.
This heterogeneity of skin conductance introduces another factor shaping physical electric images.
This article describes a realistic modelling study of the effect of the internal and skin conductance on electric image generation in G. petersii.
We have calculated the electric image of a metal object on a simulated fish having different magnitudes of conductances for internal tissues and skin.
While the high conductivity of the fish body enhances the electric image by a combination of mechanisms, the skin conductance appears to optimize the transcutaneous voltage modulation by nearby objects.
In contrast, transcutaneous current increases monotonically with skin conductivity.
These results suggest that transcutaneous voltage is the critical proximal stimulus for electroreceptors.
We generalize two concepts: object perturbing field and imprimence, introduced early in electroreception research CITATION, to other sensory systems.
An object perturbs an electric field as if it were adding a new field to the basal one.
This perturbing field can be considered as equivalent to a certain distribution of electric sources.
This distribution is referred to as an object's imprimence.
Cells of the embryonic vertebrate limb in high-density culture undergo chondrogenic pattern formation, which results in the production of regularly spaced islands of cartilage similar to the cartilage primordia of the developing limb skeleton.
The first step in this process, in vitro and in vivo, is the generation of cell condensations, in which the precartilage cells become more tightly packed at the sites at which cartilage will form.
In this paper we describe a discrete, stochastic model for the behavior of limb bud precartilage mesenchymal cells in vitro.
The model uses a biologically motivated reaction diffusion process and cell-matrix adhesion as the bases of chondrogenic pattern formation, whereby the biochemically distinct condensing cells, as well as the size, number, and arrangement of the multicellular condensations, are generated in a self-organizing fashion.
Improving on an earlier lattice-gas representation of the same process, it is multiscale, and the cells are represented as spatially extended objects that can change their shape.
The authors calibrate the model using experimental data and study sensitivity to changes in key parameters.
The simulations have disclosed two distinct dynamic regimes for pattern self-organization involving transient or stationary inductive patterns of morphogens.
The authors discuss these modes of pattern formation in relation to available experimental evidence for the in vitro system, as well as their implications for understanding limb skeletal patterning during embryonic development.
Skeletal pattern formation in the developing vertebrate limb depends on interactions of precartilage mesenchymal cells with factors that control the spatiotemporal differentiation of cartilage.
The most fundamental skeletogenic processes involve the spatial separation of precartilage mesenchyme into chondrogenic and nonchondrogenic domains CITATION, and can be studied in vitro as well as in vivo.
In high-density micromass cultures of chondrogenic embryonic limb mesenchymal cells CITATION, CITATION, as well as in the developing limb itself CITATION, morphogens of the TGF- family induce the local aggregation or condensation of these cells by a process that involves the upregulation of the adhesive extracellular glycoprotein fibronectin CITATION, CITATION.
Cells first accumulate in regions of increased cell fibronectin adhesive interactions CITATION CITATION and then acquire epithelioid properties by upregulation of cell cell adhesion molecules CITATION, CITATION.
Cartilage differentiation follows at the sites of condensation both in vitro and in vivo .
In certain developmental processes, such as angiogenesis and invasion by cancer cells of surrounding tissues, pre-existing multicellular structures become more elaborate.
Precartilage condensation, by contrast, is an example of a developmental process in which cells that start out as independent entities interact to form multicellular structures.
Others in this second category include vasculogenesis, the formation of feather germs, and the aggregation of social amoebae into streams and fruiting bodies.
Both continuous CITATION CITATION and discrete CITATION CITATION models have been used previously to analyze a wide range of pattern formation behaviors in both categories using concepts such as chemotaxis, haptotaxis, and reaction diffusion instability.
Discrete models describe the behaviors and interactions of individual biological entities such as organisms, cells, proteins, etc. They are often applied to microscale events where a small number of elements can have a large impact on a system.
In a previous study CITATION we presented a discrete biological lattice gas model for high-density cultures of precartilage mesenchymal cells derived from the embryonic vertebrate limb.
This model, which was based on the physical notion of a lattice gas, in which individual particles are free to move from point to point on a lattice at discrete time-steps, accurately simulated the formation of patterns of mesenchymal condensations observed in high-density micromass cultures of such cells.
In these simulations, the distribution and relative size of the condensations corresponded to in vitro values when appropriate quantities for cell behavioral parameters were chosen, and the simulated patterns were robust against small variations of these values.
Moreover, the simulated patterns were altered similarly to the cultures when cell density and exposure to or expression of molecular factors represented in the model were altered in a fashion analogous to their counterparts in the living system.
In the earlier model, each of the limb precartilage mesenchymal cells, and each molecule from a core subset of the molecules they secrete, was represented as a single particle on a common grid.
Default motion of the cell particles was random, but cell movement was also biased by the presence of fibronectin particles produced and deposited by the cells according to a set of rules involving TGF- and inhibitor particles.
The latter in turn were produced in a cell-dependent fashion according to a reaction diffusion scheme, the network structure of which was suggested by in vitro experiments CITATION, CITATION, CITATION .
The ability of the model of Kiskowski et al. CITATION to simulate both qualitative and quantitative aspects of precartilage condensation formation and distribution suggested that the core genetic network cell behavioral mechanism that underlies this biological lattice gas might be sufficient to account for pattern formation in the limb cell micromass system and corresponding features of in vivo limb development.
However, the model deviated from biological reality in several important ways.
Mesenchymal cells in vitro are initially surrounded by a small layer of ECM that separates them by less than a cell diameter.
Those that undergo condensation round up, reducing their surface area, but do not move away from adjacent noncondensing cells.
Therefore, unlike the situation in the model of Kiskowski et al. CITATION, mesenchymal condensation in micromass culture does not involve accumulation of cells at particular sites with concomitant depletion of cells in surrounding zones.
The representation of cells, morphogens, and ECM on a common grid is physically unrealistic.
This is not simply a matter of pixel scale: molecular substances can indeed form deposits and gradients on the same linear scale as cells, and a molecular pixel could be considered to correspond to thousands of molecules.
Nonetheless, the dynamics of morphogen transport is continuous and is represented in an inauthentically saltatory fashion by pixel displacement on a grid of the same mesh size as that supporting cell translocation.
Whereas the model of Kiskowski et al. made the assumption that cells halt their motion when they encounter suprathreshold levels of extracellular fibronectin CITATION, this does not agree with measurements CITATION, CITATION indicating that cells actually slightly increase their speed of motion as they enter condensation centers and have a finite probability of escaping from these foci.
Despite the successes of the model of Kiskowski et al. CITATION, it was unknown whether removing its artifactual aspects and replacing them with more realistic assumptions would lead to similarly authentic results.
We have therefore designed a more sophisticated model that overcomes each of the listed deficiencies of the earlier one.
The cells in the new model are extended, multipixel objects that can change shape in the plane and round up by moving pixels into a virtual third dimension.
The model cells are separated by less than a cell diameter, condense without denuding the regions surrounding condensation centers, and are not irreversibly trapped upon entering a center.
Finally, two grids of different mesh size are used for cell and molecular dynamics.
We have found that not only does this improved model reproduce the experimental data accounted for by the model of Kiskowski et al., but that additional morphogenetic features of the micromass culture system are simulated as well.
Moreover, potential dynamic properties of the developmental process not seen in the earlier simulations, and not capable of being distinguished on the basis of existing experimental data, were disclosed in simulations using the new model, which has therefore provided motivation for further empirical tests.
Position determination in biological systems is often achieved through protein concentration gradients.
Measuring the local concentration of such a protein with a spatially varying distribution allows the measurement of position within the system.
For these systems to work effectively, position determination must be robust to noise.
Here, we calculate fundamental limits to the precision of position determination by concentration gradients due to unavoidable biochemical noise perturbing the gradients.
We focus on gradient proteins with first-order reaction kinetics.
Systems of this type have been experimentally characterised in both developmental and cell biology settings.
For a single gradient we show that, through time-averaging, great precision potentially can be achieved even with very low protein copy numbers.
As a second example, we investigate the ability of a system with oppositely directed gradients to find its centre.
With this mechanism, positional precision close to the centre improves more slowly with increasing averaging time, and so longer averaging times or higher copy numbers are required for high precision.
For both single and double gradients, we demonstrate the existence of optimal length scales for the gradients for which precision is maximized, as well as analyze how precision depends on the size of the concentration-measuring apparatus.
These results provide fundamental constraints on the positional precision supplied by concentration gradients in various contexts, including both in developmental biology and also within a single cell.
To determine position in a biological system, some component within the system must have a nonuniform spatial distribution.
Often, this is achieved through the formation of gradients of protein concentration.
Typically, a gradient forms when a protein is manufactured/injected within a small region and subsequently spreads and decays CITATION.
By measuring the local concentration, position relative to the source can be determined.
In developmental biology, where such gradients are used to control patterns of gene expression, gradient proteins are called morphogens.
However, intracellular concentration gradients are also thought to be important for organisation inside single cells.
For a gradient mechanism to be biologically viable, position determination must be precise and therefore robust to noise.
Variability from one copy of the system to another will certainly compromise positional precision.
Production and degradation rates can vary.
The physical size of the system will also vary, and this may affect proper positioning.
Most previous analyses of morphogen gradients have focused on robustness to changes in these extrinsic factors CITATION CITATION between different copies of the system.
However, there will also be intrinsic noise affecting the gradient within a single copy of the system, for example due to the unavoidably noisy nature of the biochemical reactions involved.
This dissection of the fluctuations into extrinsic or intrinsic components mirrors that introduced into the analysis of stochastic gene expression CITATION CITATION.
However, here, intrinsic noise alters not only the overall protein copy numbers, but also crucially the spatiotemporal protein distribution.
Even if all extrinsic variation could be eliminated, intrinsic biochemical noise would still lead to a fundamental limit to the precision of position determination, in a similar way to limits on the precision of protein concentration measurement CITATION, CITATION.
In this paper, we therefore address the question of how precisely a concentration gradient can specify positional information, and calculate the limits on positional precision for a simple, but biologically relevant, gradient formation mechanism with first-order reaction kinetics.
Quantitative measurements, for example on the Bicoid Hunchback system in Drosophila CITATION, have shown that remarkable positional precision can sometimes be obtained.
For this reason, understanding the fundamental limits to the precision of concentration gradients is clearly an important issue in developmental biology.
Our results will be equally relevant to gradients that form within single cells, where protein copy numbers of a few thousand CITATION CITATION will lead to large density fluctuations.
The properties of intracellular protein gradients have been studied by Brown and Kholodenko CITATION.
Recently, a number of these gradients have been observed experimentally in both prokaryotic and eukaryotic systems.
The bacterial virulence factor IcsA forms a polar gradient on the cell membrane of Shigella flexneri CITATION.
MipZ in Caulobacter crescentus forms polar gradients to aid division site selection CITATION.
In Bacillus subtilis, the MinCD complex also forms polar gradients in order to direct division site selection to the mid-plane of the cell CITATION, CITATION.
In Escherichia coli, the oscillatory dynamics of the Min proteins creates a time-averaged gradient that directs cell division placement CITATION CITATION.
Using mechanisms of this sort, division site placement in bacteria can achieve an impressive precision of 1 percent of the cell length CITATION, CITATION.
Cell division in eukaryotic cells is also believed to be regulated by concentration gradients.
For example, in fission yeast, the protein Pom1p forms a cortical concentration gradient emanating from a cell tip, thereby restricting the cell division protein Mid1p to the cell centre CITATION, CITATION.
In eukaryotic cells, gradients of the Ran and HURP proteins aid the formation of the mitotic spindle by biasing microtubule growth toward the chromosomes CITATION CITATION.
Gradients may also play a role in the localization of Cdc42 activation, thereby permitting a coupling between cell shape and protein activation CITATION, CITATION .
Suppose that a biological system needs to identify a particular position along its length, such as the mid-plane to ensure symmetrical cell division.
As concrete examples, MipZ and the MinCD complex act by displacing the essential cell division protein FtsZ from the cell membrane.
Since the concentrations of MipZ/MinCD are higher near the cell poles, FtsZ accumulates near the cell centre.
Below some critical threshold of MinCD or MipZ concentration, enough FtsZ will presumably accumulate to form the division apparatus.
The locations where the concentration gradient crosses these thresholds mark positions within the cell.
In our analysis, we simply postulate the existence of such well-defined critical thresholds, where the gradient sharply switches a downstream signal from on to off.
Clearly, any real gradient cannot act as such a sharp switch in reality, a certain amount of smearing is inevitable.
Furthermore, there will be additional noise in the process of actually measuring the concentration due both to the binding of the gradient proteins to the receptor molecules CITATION, CITATION, and also to the downstream reactions that process this incoming signal CITATION CITATION, CITATION CITATION.
In general, the noise of the output signal of a processing network can be written as the sum of a contribution from the noise in the input signal plus a contribution from the reactions that constitute the processing network.
We assume here that the detector and the processing network are ideal and do not add any noise to the gradient input signal.
As a result, our calculated variation constitutes a lower bound; any real gradient-signalling system will inevitably have a lower precision.
We first considered a system with a single planar morphogen source and linear degradation, thereby producing an exponentially decaying average concentration profile.
While this model is very simple, it remains biologically relevant in both developmental and intracellular contexts.
Gradients of Bicoid in Drosophila and IcsA in Shigella have been quantitatively measured and shown to fit this exponential decay profile on average to high accuracy CITATION, CITATION.
We then calculated the expected distribution of positions where a noisy gradient crosses a concentration threshold.
With typical cellular copy numbers of a few thousand proteins, the system would be unable to identify the correct threshold position from a single measurement.
To achieve reliable position determination, the concentration must be averaged over time.
We show that by averaging measurements, a biological system is able to achieve precision in position determination of a few percent of the system size even with hundreds of protein copies, a result we verified with computer simulations.
Furthermore, we find that the precision of position determination is maximised when a particular choice of the gradient decay length is made.
We also show how the precision depends on the detector size.
For a 2-D gradient, the precision possible after a certain averaging time depends only very weakly on the detector size.
We relate all these results to experimental measurements of gradients in Shigella and fission yeast.
We also considered the ability of gradients from two poles to identify the centre of the system, as in the MipZ and Pom1p gradients discussed above.
Related designs have also been proposed for the control of hunchback positioning in Drosophila CITATION, CITATION, CITATION.
As before, we find that the precision of the system can be optimised by a particular choice of the decay length.
However, if the threshold position is set at the system centre, time-averaging improves precision more slowly than in the single-source model.
For subcellular gradients, we find that a few thousand copies of the gradient proteins may therefore be required for high precision.
Our results strongly constrain the possible concentrations of gradient proteins in two gradient systems.
Many pathogens exist in phenotypically distinct strains that interact with each other through competition for hosts.
General models that describe such multi-strain systems are extremely difficult to analyze because their state spaces are enormously large.
Reduced models have been proposed, but so far all of them necessarily allow for coinfections and require that immunity be mediated solely by reduced infectivity, a potentially problematic assumption.
Here, we suggest a new state-space reduction approach that allows immunity to be mediated by either reduced infectivity or reduced susceptibility and that can naturally be used for models with or without coinfections.
Our approach utilizes the general framework of status-based models.
The cornerstone of our method is the introduction of immunity variables, which describe multi-strain systems more naturally than the traditional tracking of susceptible and infected hosts.
Models expressed in this way can be approximated in a natural way by a truncation method that is akin to moment closure, allowing us to sharply reduce the size of the state space, and thus to consider models with many strains in a tractable manner.
Applying our method to the phenomenon of antigenic drift in influenza A, we propose a potentially general mechanism that could constrain viral evolution to a one-dimensional manifold in a two-dimensional trait space.
Our framework broadens the class of multi-strain systems that can be adequately described by reduced models.
It permits computational, and even analytical, investigation and thus serves as a useful tool for understanding the evolution and ecology of multi-strain pathogens.
Microbial pathogens are tremendously diverse.
Pathogens that cause one and the same disease may differ remarkably in both their genotype and their phenotype, like in HIV/AIDS CITATION, influenza CITATION, malaria CITATION, and meningitis CITATION.
Phenotypically different variants of the same pathogen are called strains.
If several strains exist in a host population, they interact with each other in two ways.
The first type of interaction may be referred to as ecological interference CITATION, CITATION.
For many infectious diseases, a host infected with one strain is removed, for the duration of the disease, from the population of hosts susceptible to the pathogen.
This is because the immune system of the host becomes activated upon infection by the first strain, so that it is hard for a second strain to enter and/or replicate in this host, and the infected host may be physically removed from the susceptible population, by dying or staying at home.
Ecological interference takes place even between unrelated pathogens CITATION .
The second type of interaction, referred to as cross-immunity interference, is specific to different strains of the same pathogen: these can confer full or partial immunity to each other.
This means that a host infected with one strain becomes substantially less susceptible to certain other strains of the pathogen for a prolonged period of time after the initial infection is cleared.
Cross-immunity is highest between phenotypically similar strains.
Since phenotypic similarity usually implies recent common ancestry, a pathogen's ecology is thus intrinsically entangled with its evolution.
Understanding the dynamics of multi-strain pathogens at a general theoretical level turns out to be extremely difficult.
Numerous models have been proposed during the past twenty years.
Although these models share many similarities, they substantially differ in particulars, often resulting in conflicting model predictions.
In consequence, there is little agreement as to how best to gain insights into the ecology and evolution of multi-strain pathogens.
Models of multi-strain pathogens can be either equation- or agent-based.
Agent- or individual-based models have recently become increasingly elaborate and interesting CITATION CITATION, largely due to an increase in computational capabilities.
Since these models, however, are not designed for analytical tractability, we do not dwell on this type of model here.
Virtually all equation-based models of disease dynamics can be traced back to the compartment model introduced by Kermack and McKendrick in 1927 CITATION.
These models are also known as SIR models, reflecting a host population's partitioning into susceptible, infected, and recovered individuals.
A problem that arises immediately when attempting to extend this classical SIR framework to multiple strains is that the number of state variables, and typically also of parameters, increases exponentially with the number of strains CITATION, CITATION.
This presents not only computational challenges but also draws attention to a fundamental conceptual difficulty: even for a moderately large number of strains, the resultant number of state variables quickly surpasses any realistic host population size.
Most compartments in such a model therefore consist of few individuals, if they are occupied at all: effects of demographic stochasticity must then not be neglected.
To avoid this complication, existing approaches to modeling multi-strain pathogens have attempted to reduce the number of model compartments.
Usually, such reductions are valid only under certain sets of assumptions that may or may not be adequate depending on the modeled phenomenon.
Thus, it is important to expand the set of assumptions under which reduced models are applicable.
Our work presented here contributes to this goal.
Traditionally, full models have been developed based on the assumption of reduced susceptibility, which implies that immune hosts are able to block off an infection completely, with a certain probability CITATION, CITATION.
On the other hand, all existing reduced models rely on the assumption of reduced infectivity that implies that all hosts, immune or not, get infected with the same probability, but those that possess immunity become less infectious than those who do not CITATION, CITATION.
The reality, most likely, lies somewhere between these two abstractions.
Nevertheless, as we discuss in the Model section, the reduced susceptibility assumption seems more plausible.
In this study, we develop a state-space reduction approach that can be applied under either of these assumptions, in models with or without coinfections.
Our approach differs from the existing ones in that it produces a collection of models that approximate the full models with the desired degree of accuracy.
The number of variables needed for the resulting approximations grows algebraically with the number n of strains, rather than exponentially: when n is large, the difference between, e.g., n 2 and 2 n, is enormous, with the former growing much more slowly than the latter.
If coinfections and reduced infectivity are assumed, our approach produces a model equivalent to that of Gog and Grenfell CITATION .
To illustrate the utility of our approach, and that of reduced models in general, we demonstrate its application to the phenomenon of drift in influenza A. Using reduced models we are able to simulate up to 400 strains.
Influenza A is a multi-strain pathogen whose epidemiology and evolution display an intricate interaction pattern.
Because the human immune system can produce protective antibodies against influenza's surface glycoprotein hemagglutinin, individuals gain lifelong immunity against each strain of the virus with which they have been infected CITATION, CITATION.
This results in a complex partitioning of the human host population according to the immunity of individuals to different influenza strains.
The ensuing frequency-dependent selection is thought to drive the evolution of influenza A, giving rise to a process known as antigenic drift CITATION.
Lapedes and Farber CITATION have shown that the antigenic space of influenza is approximately five-dimensional.
Subsequently, Smith et al. CITATION argued that the first two principal dimensions are most important.
Moreover, as follows from results by Smith et al., the temporal evolution of influenza's H3N2 subtype proceeds along a single line in the antigenic space, i.e., antigenic clusters corresponding to different years are well separated along the first principal dimension.
This agrees with the observation that the phylogenetic tree of subtype H3N2 possesses a single trunk CITATION.
In other words, even though the H3N2 subtype experiences substantial genetic diversity during each epidemic season, only one progeny strain survives in the longer run.
Accordingly, the number of coexisting H3N2 strains does not grow from year to year.
A few recent studies have attempted to model, and thereby explain, the phenomenon of antigenic drift in influenza A. Apart from individual-based models, most of these studies consider a one-dimensional strain space in which some sort of traveling-wave behavior is observed CITATION, CITATION CITATION.
To constrain the evolution of a virus to one dimension in a two-dimensional strain space, it has been necessary to require that the strain space was essentially unviable except for a relatively thin region along one axis CITATION .
In a recent study, Koelle et al. CITATION took a different approach and succeeded in constraining the diversity of a virus living in a high-dimensional sequence space.
The authors explicitly mapped viral genotypes to phenotypes and showed that the single-trunk phylogeny of influenza A may be a consequence of the neutral network structure of the influenza genotype space.
However, it is an open question which properties of the phenotype space are sufficient to constrain viral diversity in the course of its evolution.
Recker et al. CITATION suggest one explanation.
They argue that the succession of antigenically distinct variants may be an intrinsic feature of the dynamics of a limited set of antigenic types that are always present in the host population and, thus, is decoupled from the genetic evolution of the virus.
Here, we suggest an alternative conceptual scenario that follows the more traditional view that antigenic drift and genetic evolution are tightly connected.
However, we deliberately avoid the problem of mapping genotypes to phenotypes and, instead, assume a relatively simple structure of the phenotype space a rectangular lattice.
Our model offers a straightforward explanation of what could be happening in such a phenotype space in order for the diversity of a virus to be constrained in the long run.
Our work is, thus, complementary to that of Koelle et al. CITATION.
In our two-dimensional phenotype space, each coordinate captures changes in the conformations of an epitope local region on the surface of the hemagglutinin molecule that interacts with the immune system CITATION CITATION.
We then investigate a scenario in which the immune response of hosts depends on two epitopes, and full immune protection is gained against all strains sharing an epitopic conformation with a previous infection.
In this respect, our model is closely related to models studied by Gupta and colleagues CITATION, CITATION, CITATION.
We show that the evolutionary trajectory of the influenza A virus in our model follows a line, even though the model's strain space is two-dimensional.
This finding agrees with the observed single-trunk phylogeny of influenza's H3N2 subtype CITATION .
The primary visual cortex is pre-wired to facilitate the extraction of behaviorally important visual features.
Collinear edge detectors in V1, for instance, mutually enhance each other to improve the perception of lines against a noisy background.
The same pre-wiring that facilitates line extraction, however, is detrimental when subjects have to discriminate the brightness of different line segments.
How is it possible to improve in one task by unsupervised practicing, without getting worse in the other task?
The classical view of perceptual learning is that practicing modulates the feedforward input stream through synaptic modifications onto or within V1.
However, any rewiring of V1 would deteriorate other perceptual abilities different from the trained one.
We propose a general neuronal model showing that perceptual learning can modulate top-down input to V1 in a task-specific way while feedforward and lateral pathways remain intact.
Consistent with biological data, the model explains how context-dependent brightness discrimination is improved by a top-down recruitment of recurrent inhibition and a top-down induced increase of the neuronal gain within V1.
Both the top-down modulation of inhibition and of neuronal gain are suggested to be universal features of cortical microcircuits which enable perceptual learning.
Since Plato's Allegory of the Cave and Kant's Critique of Pure Reason, it is often suggested that our perception of objects in the outer world can never tell us what they really are.
If men had green glasses in place of their eyes, they would perceive the objects as green, and never be able to tell whether this color was intrinsic to the objects or just of our perception.
In a contemporary neuroscientific version of the empiricist's position, one may argue that the perception of visual objects is always distorted by the nonlinearities in the visual pathway, and in particular by the intrinsic circuitry of primary visual cortex.
In fact, any visual input is filtered by the neuronal processing in V1 before reaching consciousness.
For instance, collinear edges are enhanced by the intrinsic V1 circuitry CITATION, and our brightness perception will never match the physical luminance.
Nevertheless, perceptual training without teacher feedback may still improve our brightness discrimination abilities CITATION, casting certain doubts about the strict empirical view.
How then is it possible to reach more veridical perceptions by just pure reason, i.e., by intrinsically adapting the cortical dynamics without being told about the mismatch between percept and true physical quality?
We show in a model that top-down modulation of V1 during unsupervised perceptual learning can suppress intrinsic nonlinearities in V1.
The top-down suppression leads to a faithful neuronal representation of the sensory input.
The underlying neuronal mechanisms are elaborated in an example of brightness discrimination.
In this example, a flanking light bar which is closely aligned in prolongation of a test bar acts as a visual context.
This flanking bar biases the brightness perception of the test bar.
In the presence of the flanking bar, the test bar is perceived to be brighter than it actually is. Clearly, this enhanced brightness perception is helpful when extracting collinear line elements against some noisy background CITATION, CITATION.
However, when the task consists of comparing the brightness of the test bar with a displaced single reference bar, then the collinear flank distorts the brightness comparison CITATION.
The brightness of the test bar is overestimated because the underlying neuronal population representing the test bar within V1 is recurrently excited by the corresponding population representing the collinear flanking bar CITATION.
We show that top-down input can remove this contextual bias by activating recurrent inhibition within V1.
The recurrent inhibition cancels the lateral excitation and linearizes the brightness representation of the test bar, allowing for a faithful perception.
An additional top-down induced gain increase in V1 further enhances the sensitivity to brightness differences.
Perceptual learning, i.e., the change of perception following sensory experiences, is typically explained as a modification of either the feed-forward synaptic pathway to V1 CITATION CITATION, or recurrent connections within V1 CITATION CITATION triggered by repeated practicing.
Because these synaptic modifications would affect any input stream through V1, however, perceptual learning would inevitably deteriorate the information processing in other situations.
Although negative transfer of learning to other tasks is known to appear, perceptual learning is typically task-specific and does not deteriorate perception in other tasks; see, e.g., the reviews CITATION, CITATION.
While improving in brightness discrimination between a context-modulated test bar and a displaced reference bar, for instance, the edge detection capability is expected to not suffer.
In fact, the mutual enhancement of collinear light bars is advantageous for extracting lines in a noisy scene, as required for contour integration in everyday scenes CITATION.
Hence, models of perceptual learning have to explain how improvement on one task is possible without interference with others.
An intriguing possibility is that perceptual learning might be based on modifying a task-dependent top-down input to sensory areas, as opposed to a permanent change of the bottom-up input stream CITATION CITATION.
Taking up this idea we show how top-down signaling from a higher cortical area to V1 could modify the neuronal processing in this lower area, consistent with both electrophysiological recordings in V1 and psychophysical experiments on perceptual learning.
Changes in the synaptic connection strengths between neurons are believed to play a role in memory formation.
An important mechanism for changing synaptic strength is through movement of neurotransmitter receptors and regulatory proteins to and from the synapse.
Several activity-triggered biochemical events control these movements.
Here we use computer models to explore how these putative memory-related changes can be stabilised long after the initial trigger, and beyond the lifetime of synaptic molecules.
We base our models on published biochemical data and experiments on the activity-dependent movement of a glutamate receptor, AMPAR, and a calcium-dependent kinase, CaMKII.
We find that both of these molecules participate in distinct bistable switches.
These simulated switches are effective for long periods despite molecular turnover and biochemical fluctuations arising from the small numbers of molecules in the synapse.
The AMPAR switch arises from a novel self-recruitment process where the presence of sufficient receptors biases the receptor movement cycle to insert still more receptors into the synapse.
The CaMKII switch arises from autophosphorylation of the kinase.
The switches may function in a tightly coupled manner, or relatively independently.
The latter case leads to multiple stable states of the synapse.
We propose that similar self-recruitment cycles may be important for maintaining levels of many molecules that undergo regulated movement, and that these may lead to combinatorial possible stable states of systems like the synapse.
Long-term storage of neuronal information is believed to occur through alterations in synaptic efficacy.
Many mechanisms have been identified for changes in synaptic strength, including modulation of neurotransmitter release, conductivity changes in receptors, changes in numbers of receptors or active synapses, and structural alterations of the synapse.
Among these, the insertion of glutamate receptors of the alpha-amino-3-hydroxy-5-methyl-4-isoxazole propionate subtype into the postsynaptic membrane and the modulation of receptor conductance by phosphorylation are key events in modulating synaptic efficacy.
A fundamental issue challenges all of these mechanisms: how can they last a lifetime?
Synaptic memories can decay in at least three ways: turnover, diffusive exchange, and stochasticity.
Turnover of major postsynaptic molecules ranges from periods of a few minutes to a few days and may be further enhanced by synaptic activity CITATION.
One solution to loss of memory due to molecular turnover is the concept of self-sustaining molecular switches CITATION.
These typically involve some form of molecular feedback giving rise to chemical systems, which can stably settle into one of two states.
Such two-state, or bistable, systems can store information in a binary manner.
Provided there is a steady supply of replacement molecules, molecular turnover can be tolerated, since newly synthesised, na ve molecules become entrained to the current state of the system.
Some current proposals for such bistable synaptic switches include the calcium calmodulin type II kinase hypothesis CITATION, a mitogen-activated protein kinase feedback loop CITATION, CITATION, and, recently, the mammalian target of rapamycin protein synthesis loop CITATION.
Of these, the CaMKII model has been posed in the most detail with the most complete structural correlates.
According to this model, CaMKII at the synapse can undergo autophosphorylation, which leads to activation of the kinase.
The activated kinase molecules catalyze the phosphorylation of yet more CaMKII molecules, resulting in a self-sustaining cycle.
The MAPK feedback loop model also involves a self-sustaining cycle, but in this case several intermediate molecules participate in the loop.
The protein synthesis loop model is based on the observation of local protein translation machinery associated with synapses.
Messenger RNA for several proteins, including the ribosomes themselves, is also present.
Thus, high local protein synthesis creates the machinery for maintaining high levels of synthesis.
This protein synthetic loop is regulated by mTOR.
The second mechanism for decay of synaptic memory is diffusive exchange of synaptic proteins, leading to washout of specific states in the synapse.
Extrapolations from free diffusion constants suggest that diffusive exchange between the synaptic spine and dendrite is likely to be rapid, under 10 s even for proteins CITATION.
The postsynaptic density is an elaborate cytoskeletal and signalling complex that provides anchors for synaptic proteins close to the region of presynaptic neurotransmitter release.
This anchoring solves the problem of free diffusion and washout of active molecules, but introduces the problem of regulating the insertion of molecules into the correct locations.
There is considerable evidence for targeted trafficking of molecules to and from the PSD.
One such trafficking cycle is the insertion and removal of glutamate receptors of the AMPA subtype into the synaptic membrane CITATION.
A striking and physiologically important example of receptor insertion is the conversion of silent synapses, lacking AMPA receptors, into active synapses with a full complement of receptors.
The delivery of AMPARs to the synaptic membrane involves two streams: a constitutive pathway involving glutamate receptor heteromers 2 and 3, and an activity-dependent pathway involving GluR12 CITATION.
Based on current evidence, the activity-dependent insertion of GluR12 into the synaptic membrane is stimulated by phosphorylation on Ser845 CITATION, CITATION.
There is also evidence for such phosphorylation being implicated in synaptic plasticity CITATION, CITATION.
CaMKII also translocates to the PSD upon calmodulin binding and stimulation CITATION.
Thus, in addition to their known involvement in synaptic plasticity, AMPARs and CaMKII have mechanisms for activity-dependent recruitment to the PSD in a manner that acts counter to washout processes CITATION.
This combination of attributes makes these molecules interesting candidates for analysing molecular memory mechanisms.
Nevertheless, over the long term, even anchoring events are reversible and additional processes must be considered for stability.
A third important obstacle to stable memory formation is biochemical stochasticity.
This causes uncertainty in the outcome of biochemical reactions involving small numbers of molecules.
Such fluctuations are severe at the synapse, where many important signalling molecules are present in low numbers, that is, less than 100 molecules.
In a typical synaptic volume of 0.1 fl CITATION there are an estimated five free Ca 2 ions.
Under stochastic conditions, there is a finite probability of spontaneous state flips in bistable molecular switches CITATION, CITATION.
The lifetime of the stable states depends both on reaction rates and on the number of molecules.
For example, the proposed MAPK switch does not fare well in synaptic volumes, and spontaneously flips state on the time scale of minutes CITATION.
Nevertheless, these time estimates are highly dependent on assumptions about diffusion, anchoring, and the levels of noise in other synaptic pathways.
Putting these themes together, a plausible synaptic memory mechanism might look like a bistable molecular switch that is resistant to turnover, incorporates traffic of molecules to and from the PSD, and is unlikely to spontaneously flip state even when small synaptic molecule numbers are taken into account.
In this study we report a novel glutamate receptor based switch that emerges from a consideration of its traffic and satisfies these criteria.
We also examine a possible CaMKII switch in the context of these criteria.
Finally, we integrate these switches to explore how multiple synaptic states may arise CITATION .
Hypoxia induces the expression of genes that alter metabolism through the hypoxia-inducible factor.
A theoretical model based on differential equations of the hypoxia response network has been previously proposed in which a sharp response to changes in oxygen concentration was observed but not quantitatively explained.
That model consisted of reactions involving 23 molecular species among which the concentrations of HIF and oxygen were linked through a complex set of reactions.
In this paper, we analyze this previous model using a combination of mathematical tools to draw out the key components of the network and explain quantitatively how they contribute to the sharp oxygen response.
We find that the switch-like behavior is due to pathway-switching wherein HIF degrades rapidly under normoxia in one pathway, while the other pathway accumulates HIF to trigger downstream genes under hypoxia.
The analytic technique is potentially useful in studying larger biomedical networks.
Molecular oxygen is the terminal electron acceptor in the mitochondrial electron transport chain.
Hypoxia, or oxygen deficiency, induces a number of metabolic changes with rapid and profound consequences on cell physiology.
A hypoxia-induced shortage of energy alters gene expression, energy consumption, and cellular metabolism to allow for continued energy generation despite diminished oxygen availability.
A molecular interaction map of the hypoxia response network has been proposed CITATION CITATION on the basis of analyzing conserved components between nematodes and mammals.
The key element in this network, hypoxia-inducible factor, is a master regulator of oxygen-sensitive gene expression CITATION CITATION.
HIF is a heterodimeric transcription factor which consists of one of the three different members and a common constitutive ARNT subunit which is also known as HIF.
The system also includes an enzyme family: prolyl hydroxylases, which directly sense the level of oxygen and hydroxylate HIF by covalently modifying the HIF subunits.
It is very likely that reactive oxidative species, which are a byproduct of mitochondrial respiration, are also involved in oxygen sensing by neutralizing a necessary cofactor, Fe 2, for the hydroxylation of HIF by a PHD CITATION CITATION.
There are three members in this enzyme family: PHD1, PHD2, and PHD3.
The hydroxylated HIF is then targeted by the von Hippel-Lindau tumor-suppressor protein for the ubiquitination-dependent degradation.
Hypoxia response element is the promoter of the hypoxia-regulated genes, and the occupancy of HRE controls the expression levels of these genes.
The cascade in Figure 1 consists of an input and an output as the core network.
The network is characterized by a switch-like behavior, namely the sharp increase of HIF when the oxygen decreases below a critical value, followed by a sharp increase of HRE occupancy.
It was observed experimentally on many cell lines including Hela cells CITATION and Hep 3B cells CITATION that HIF increases exponentially as the oxygen concentration decreases.
The past two decades have seen a growing body of work on the use of mathematical modeling to help uncover both general principles behind molecular networks and to provide quantitative explanations of particular network phenomena CITATION that may one day have sufficient predictive power to accurately model large subnetworks of the cell.
In this sense, Kohn et al. CITATION have successfully modeled the switch-like response characteristics of HRE occupancy, by numerically integrating a system of ordinary differential equations involving a score of molecular species related to hypoxia.
The large model, however, does not identify the smaller components that are actually responsible for the switch-like response and that may occur in other such networks.
Furthermore, a numerical solution does not provide the type of insight that mathematical formulas can.
At the same time, it is virtually impossible to solve symbolically the type of nonlinear differential equations that model reactions.
In this context, methods are desirable that are both tractable, that reduce a system to its key components, and that are not solely reliant on numerical solution.
Extreme pathway analysis is one such recently developed method CITATION CITATION.
In this method, the dynamics of interactions between species are formulated as a Boolean network in which the state of a gene is represented as either transcribed or not transcribed.
Upregulation and downregulation of genes are captured through an appropriate sign and a scaling constant.
The Boolean network is then formulated as a matrix of interaction rules that is then analyzed to help reveal key components and their contributions to the dynamic behavior CITATION.
The theory of matrices then allows us to look for vectors that characterize the matrix in ways that are helpful for further analysis.
The EPA technique, in particular, finds vectors that correspond to the boundaries of the space of steady-state solutions to the differential equations.
We note that similar methods, such as flux balance analysis and elementary modes analysis, have been developed in other contexts CITATION CITATION.
They essentially yield the same results CITATION, which have been verified by ExPa CITATION and CellNetAnalyzer CITATION, CITATION.
These methods provide a way out of the intractable complexity of sizable molecular networks CITATION CITATION .
Our contribution is to go beyond this type of matrix approach and provide a detailed quantitative analysis that explains the observed behavior in the models.
This is achieved by combining elementary pathway identification via EPA, which depends solely on the network topology, and the detailed analytical as well as numerical analysis of the governing differential equations in the model, which allows studies of the phase space spanned by the mostly unknown rate constants in the differential equations.
Specifically, EPA is first used in our approach to decompose the original network into several underlying pathways.
Following this, we make some reasonable approximations to facilitate analytic solution.
We show that this analytic solution, in the case of the hypoxia network, explains the switch-like behavior.
This explanation is confirmed by comparing the numerical output of the simplified model with the numerical output of the complete differential equation model.
A second contribution of this paper is to highlight a particular mechanism of pathway-switching or pathway branching effect CITATION that appears to cause the sharp response to oxygen concentration.
In particular, we examine the flux redistribution among the elementary pathways as a function of oxygen concentration.
We also identify the key molecular species involved in the subcomponent of the network and show quantitatively how the response of this subcomponent exactly matches the overall response and thus is responsible for it.
For hypoxia, our analysis suggests that the cycle of abundant production and efficient degradation of HIF plays the main role in the sharp response.
Animals with rudimentary innate abilities require substantial learning to transform those abilities into useful skills, where a skill can be considered as a set of sensory motor associations.
Using linear neural network models, it is proved that if skills are stored as distributed representations, then within-lifetime learning of part of a skill can induce automatic learning of the remaining parts of that skill.
More importantly, it is shown that this free-lunch learning is responsible for accelerated evolution of skills, when compared with networks which either cannot benefit from FLL or cannot learn.
Specifically, it is shown that FLL accelerates the appearance of adaptive behaviour, both in its innate form and as FLL-induced behaviour, and that FLL can accelerate the rate at which learned behaviours become innate.
Both evolution and learning may be considered as different types of adaptation.
Learning occurs within a lifetime, whereas genetic change occurs across lifetimes CITATION.
Whereas genetic change ensures that a task can be executed innately, learning permits even the most rudimentary innate ability to be honed into a useful skill.
In an environment that fluctuates from generation to generation, learning permits an innate ability to be adapted to the particular physical environment into which each generation is born.
If the environment ceases to fluctuate, then genetic assimilation CITATION can transform a rudimentary innate ability, which requires much learning, into an innate skill, which requires minimal learning.
This transformation is more likely to occur if the cost of learning is high CITATION, CITATION, and, in this case, computer simulations suggest that learning can accelerate the rate of genetic assimilation CITATION via the Baldwin effect CITATION.
However, if learning is sufficiently inexpensive, then genetic change may not occur at all CITATION, CITATION.
Overall, there appears to be a tradeoff between learning and genetic assimilation, such that learning can subsidize genetic change, especially if learning is inexpensive.
All but the most primitive organisms learn in order to survive, and organisms which learn quickly are at a selective advantage relative to those that learn slowly.
Therefore, a mechanism which reduces the time required to learn a given behaviour confers a selective advantage.
One candidate for such a mechanism is FLL CITATION, CITATION .
As explained below, FLL ensures that in the process of learning one set of associations or behaviours another set of associations is usually learned.
These associations could comprise either perceptual skills, or motor skills .
Before considering how FLL can accelerate evolution of certain types of behaviours, FLL will be described in its original context of spontaneous recovery of memory in humans CITATION and in neural network models CITATION.
Note that FLL is not unique to a specific class of network architectures, although it does assume that associations are learned using a form of supervised learning.
In humans, FLL has been demonstrated using a task in which participants learned the positions of letters on a nonstandard computer keyboard CITATION.
After a period of forgetting, participants relearned a proportion of these letter positions.
Crucially, it was found that this relearning induced recovery of the non-relearned letter positions.
More recently, a set of theorems provided a formal characterization of FLL in linear neural network models CITATION.
In essence, FLL occurs in neural network models because each association is distributed amongst all connection weights between units.
After partial forgetting, relearning some of the associations forces all of the weights closer to pre-forgetting values, resulting in improved performance even on non-relearned associations; a general proof is provided in CITATION.
A geometric demonstration of FLL for a network with two connection weights is given in Figure 1.
Networks with multiple input and output units can be considered without loss of generality CITATION .
The protocol used to examine FLL in neural networks is as follows.
A network with n input units and one output unit has n connection weights.
This network learns a set A of m n associations, where A A 1 A 2 comprises two subsets A 1 and A 2 of n 1 and n 2 associations, respectively.
After the associations A have been learned and then partially forgotten, performance error on subset A 1 is measured.
Finally, only A 2 is relearned, and then performance error on A 1 is remeasured.
FLL occurs if relearning A 2 improves performance on A 1.
It has been proven that the probability of FLL approaches unity as the number of weights increases CITATION.
For the sake of brevity, this is reflected in phrases such as learning A 2 usually improves performance on A 2 in this paper.
Many cellular systems rely on the ability to interpret spatial heterogeneities in chemoattractant concentration to direct cell migration.
The accuracy of this process is limited by stochastic fluctuations in the concentration of the external signal and in the internal signaling components.
Here we use information theory to determine the optimal scheme to detect the location of an external chemoattractant source in the presence of noise.
We compute the minimum amount of mutual information needed between the chemoattractant gradient and the internal signal to achieve a prespecified chemotactic accuracy.
We show that more accurate chemotaxis requires greater mutual information.
We also demonstrate that a priori information can improve chemotaxis efficiency.
We compare the optimal signaling schemes with existing experimental measurements and models of eukaryotic gradient sensing.
Remarkably, there is good quantitative agreement between the optimal response when no a priori assumption is made about the location of the existing source, and the observed experimental response of unpolarized Dictyostelium discoideum cells.
In contrast, the measured response of polarized D. discoideum cells matches closely the optimal scheme, assuming prior knowledge of the external gradient for example, through prolonged chemotaxis in a given direction.
Our results demonstrate that different observed classes of responses in cells are optimal under varying information assumptions.
Recently, there has been considerable research demonstrating the critical role played by random fluctuations in cellular signaling systems CITATION CITATION.
Stochastic variations are found in the external signaling molecules CITATION as well as in the intracellular components CITATION.
They arise because of the small number of molecules involved in signaling and play significant roles in gene regulatory networks CITATION CITATION as well as in prokaryotic CITATION, CITATION and eukaryotic signal transduction pathways CITATION, CITATION, CITATION .
The proper functioning of cellular signaling networks requires mechanisms that can tolerate the effects of noise CITATION.
However, questions remain as to how to evaluate the performance and efficiency of these cellular decision-making systems.
How well does the signaling network of a cell make decisions based on the signaling cues available?
Can improvements be made by altering the parameters or structure of the network?
How efficiently are resources used?
Here we argue that rate distortion theory CITATION, a branch of information theory, can be used to evaluate the effectiveness of such systems.
The application of information theory to the study of biology has been under way for some time CITATION CITATION and has received considerable attention in the fields of neuroscience CITATION and genetics CITATION CITATION.
However, the full breadth of this utility for biological signaling systems, in general, has not been realized, primarily because of the difficulty of defining information in general biological systems.
Here we use rate distortion theory as a tool to study performance cost tradeoffs in general spatial gradient sensing mechanisms, similar to those found in many eukaryotic cells, including neutrophils and amoebae.
Rate distortion theory provides bounds on the rate at which information must be transmitted through a system to achieve a given performance criterion.
Our results demonstrate that, depending on the prior knowledge that a cell has about its chemoattractant environment, different optimal chemotaxis strategies exist.
Furthermore, we show that differences in the observed behaviors of unpolarized and polarized chemotactic cells correspond to these various optimally efficient decision-making processes.
Transposable elements are mobile, repetitive sequences that make up significant fractions of metazoan genomes.
Despite their near ubiquity and importance in genome and chromosome biology, most efforts to annotate TEs in genome sequences rely on the results of a single computational program, RepeatMasker.
In contrast, recent advances in gene annotation indicate that high-quality gene models can be produced from combining multiple independent sources of computational evidence.
To elevate the quality of TE annotations to a level comparable to that of gene models, we have developed a combined evidence-model TE annotation pipeline, analogous to systems used for gene annotation, by integrating results from multiple homology-based and de novo TE identification methods.
As proof of principle, we have annotated TE models in Drosophila melanogaster Release 4 genomic sequences using the combined computational evidence derived from RepeatMasker, BLASTER, TBLASTX, all-by-all BLASTN, RECON, TE-HMM and the previous Release 3.1 annotation.
Our system is designed for use with the Apollo genome annotation tool, allowing automatic results to be curated manually to produce reliable annotations.
The euchromatic TE fraction of D. melanogaster is now estimated at 5.3 percent, and we found a substantially higher number of TEs than previously identified.
Most of the new TEs derive from small fragments of a few hundred nucleotides long and highly abundant families not previously annotated.
We also estimated that 518 TE copies are inserted into at least one other TE, forming a nest of elements.
The pipeline allows rapid and thorough annotation of even the most complex TE models, including highly deleted and/or nested elements such as those often found in heterochromatic sequences.
Our pipeline can be easily adapted to other genome sequences, such as those of the D. melanogaster heterochromatin or other species in the genus Drosophila.
Transposable elements are mobile, repetitive DNA sequences that constitute a structurally dynamic component of genomes.
The taxonomic distribution of TEs is virtually ubiquitous: they have been found in nearly all eukaryotic organisms studied, with few exceptions.
TEs represent quantitatively important components of genome sequences, and there is no doubt that modern genomic DNA has evolved in close association with TEs.
TEs show high species specificity, and the number and types of TE can differ quite dramatically between even closely related organisms.
There is abundant circumstantial evidence that TEs may transfer horizontally between species by mechanisms that remain obscure.
The forces controlling the dynamics of TE spread within a species are also poorly understood, as are the systemic effects of the elements on their host genomes.
Insertions of individual TEs may lead to genome restructuring, mutations in genes, or changes in gene regulation.
Some TE insertions may even have become domesticated to play roles in the normal functions of the host.
Despite their manifold effects, abundance, and ubiquity, we understand very little about most aspects of TE biology.
One way of furthering our knowledge of TE biology is through the computational analysis of TEs in the growing number of complete genomic sequences.
By detailed comparison of the abundance and distribution of TEs in entire genomes, we can infer the fundamental biological properties of TEs that are shared or that differ among species.
However, meaningful inferences about TE biology based on computationally derived TE annotations can only be done if we are confident about the results of these analyses.
The hallmark of a strong result in computational biology should be its robustness to the particular method used.
The annotation of TEs, however, typically relies on the results of a single computational program, RepeatMasker, which recent studies indicate may be neither the most efficient nor the most sensitive approach for TE annotation CITATION.
By contrast, recent advances in the field of gene annotation indicate that high-quality gene models can be produced by combining multiple independent sources of computational evidence CITATION CITATION.
With the recent development of several new methods for TE and repeat detection CITATION CITATION, it is now possible to apply a similar combined evidence approach to elevate the quality of TE annotations to a level comparable to that of gene models.
To achieve this aim, we have developed a TE annotation pipeline that integrates results from multiple homology-based and de novo TE identification methods.
Currently, our pipeline uses the combined computational evidence derived from RepeatMasker, BLASTER CITATION, TBLASTX CITATION, all-by-all BLASTN CITATION, RECON CITATION, TE-HMM CITATION, and previously published TE annotations CITATION.
We have designed our system to use an evidence-model framework and the Apollo genome annotation tool CITATION, allowing computational evidence to be manually curated in an efficient manner to produce reliable TE models.
The pipeline allows rapid and thorough annotation of complex TE models, providing key structural details that allow insights into the origin of highly deleted and/or nested elements.
In contrast to simply masking repeats, our method provides the means to a complete and accurate annotation of TEs, supported by multiple sources of computational evidence, a goal that has important implications for experimental studies of genome and chromosome biology.
As a test case we have chosen to annotate the euchromatic genomic sequence of the fruit fly, Drosophila melanogaster.
The 116.8-Mb Release 3 genome sequence of D. melanogaster is among the highest quality genome sequences and is a particularly well suited sequence for genome-wide studies of TEs, since repetitive DNA sequences have been finished to high quality and systematically verified by restriction fingerprint analysis CITATION.
Moreover, the Release 3.1 annotation of D. melanogaster includes a manually curated set of TE annotations CITATION that can be used as a benchmark for developing and refining TE annotation methodologies.
Controlled tests performed here on the Release 3 sequence show that a combined-evidence approach has superior performance over individual TE detection methods, and that a substantially larger fraction of the genome is composed of TEs than previously estimated.
We have applied our pipeline to the new 118.4-Mb Release 4 sequence, which has closed several of the gaps in Release 3 and has extended the sequence of the pericentomeric regions, to produce a systematic re-annotation of TEs in the D. melanogaster genome.
The euchromatic TE fraction is now estimated at 5.3 percent, and we found a substantially higher number of TEs than previously identified.
We also estimated that 518 TE copies are inserted into at least one other TE, forming a nest of elements.
Our pipeline can be easily adapted to other genome sequences, and could markedly increase the efficiency of annotating genomic regions with complex or abundant TE insertions such as heterochromatic sequences.
In the presence of exogenous mortality risks, future reproduction by an individual is worth less than present reproduction to its fitness.
Senescent aging thus results inevitably from transferring net fertility into younger ages.
Some long-lived organisms appear to defy theory, however, presenting negligible senescence and extended lifespans.
Here, we investigate the possibility that the onset of vitality loss can be delayed indefinitely, even accepting the abundant evidence that reproduction is intrinsically costly to survival.
For an environment with constant hazard, we establish that natural selection itself contributes to increasing density-dependent recruitment losses.
We then develop a generalized model of accelerating vitality loss for analyzing fitness optima as a tradeoff between compression and spread in the age profile of net fertility.
Across a realistic spectrum of senescent age profiles, density regulation of recruitment can trigger runaway selection for ever-reducing senescence.
This novel prediction applies without requirement for special life-history characteristics such as indeterminate somatic growth or increasing fecundity with age.
The evolution of nonsenescence from senescence is robust to the presence of exogenous adult mortality, which tends instead to increase the age-independent component of vitality loss.
We simulate examples of runaway selection leading to negligible senescence and even intrinsic immortality.
Senescence, usually treated as synonymous with aging, refers to a deterioration in physiological condition with age, manifest as an increase in mortality and a decline in fertility.
Since this phenomenon is detrimental to reproductive success, natural selection might be expected to cause its postponement or elimination from the life history of organisms.
Its apparent ubiquity in the natural world, therefore, has been treated as a challenge for evolutionary theory CITATION.
There is a general acceptance that this challenge has, in principle, been met, and modern understanding of the widespread occurrence of senescence in nature has been hailed as one of the great triumphs of evolutionary thinking CITATION .
Discussions of the evolution of senescence broadly follow one of two paradigms, based either in classical population genetics or in physiological ecology CITATION.
The first emphasizes the accumulation of late-acting deleterious mutations on hypothesized genes with age-specific expression CITATION, CITATION.
More generally, this paradigm hypothesizes an antagonistic pleiotropy of age-specific genes in which mutations confer a fitness benefit early in life at the cost of some deleterious effect later CITATION CITATION.
The key insight of this perspective is that the force of selection on the additive component of genetic variance necessarily declines with age, so that an early-age cost is more strongly selected against than an equivalent late-age cost, and, a fortiori, an early-age benefit more than compensates for a late-age cost CITATION.
Hamilton concludes: ...for organisms that reproduce repeatedly, senescence is to be expected as an inevitable consequence of the working of natural selection CITATION .
This population genetic analysis has been challenged recently both theoretically and empirically.
First, a declining force of selection is only guaranteed for mutations with additive effects, and it has been suggested that mutations with proportional effects for which the force of selection need not decline with age may be more relevant CITATION.
Second, Hamilton himself concedes: To what extent and in exactly what way life schedules will be moulded by natural selection depends on what sort of genetical variation is available CITATION.
Thus, the more such genes there are, the more evolutionary pressure there will be toward compressed life histories.
However, despite much empirical work over several decades, evidence for the availability of genes with the necessary age-specific effects appears to be thin .
In contrast to the classical population genetic approach, the disposable soma theory CITATION, CITATION CITATION is based firmly within physiological ecology.
Thus, it is claimed that birth and death schedules are the result of the action of integrated physiological processes concerned with the optimal partitioning of available resources between reproduction and somatic maintenance or growth.
In particular, there is an inherent cost of reproduction in which an early-age reproductive benefit incurs a late-age cost in decreased survival, possibly in the form of latent damage that is only unmasked later in life CITATION.
Relevant genetic mutations must have effects that are manifest at this physiological level.
This is potentially a much more constraining paradigm, though apparently more strongly supported by current evidence CITATION, CITATION.
Nevertheless, given this tradeoff between early fecundity and longevity, it has again generally been concluded that senescence is inevitable CITATION.
In particular, immortality has long been considered theoretically impossible because of the inevitability of senescent aging CITATION CITATION .
Yet this understanding of the evolution of senescence fails to account for organisms showing negligible or even negative senescence CITATION, CITATION.
These are species such as the freshwater Hydra vulgaris CITATION with period survival that remains constant or increases with adult age.
They include some organisms with apparently indefinite lifespans such as the Great Basin Bristlecone Pine Pinus longaeva CITATION, CITATION, which continues producing viable cones at well over 4,000 y old, the Quaking Aspen Populus tremuloides CITATION, and the Creosote Bush Larrea tridentata CITATION, both of which have clonal clusters at least 10,000 y old.
These and other examples CITATION have recently led to a reversal of the traditional perspective in which the problem was to explain the evolution of senescence from nonsenescence.
On the contrary, given the ubiquity of senescence in nature, and the abundance of explanations for its presence, it seems very unlikely that the majority of today's organisms are descended from nonsenescent ancestors.
Rather, an important issue now is to provide an evolutionary account for those organisms that appear to exhibit little or no senescence, but which almost certainly have evolved from ancestors that did exhibit senescence.
Rising to this challenge, a theoretical analysis of the costs of senescent aging CITATION has shown that, although senescence is often favored by a high and sustained early vitality, nonsenescing strategies are locally optimal if vitality loss in the presence of senescence would otherwise be sufficiently fast.
Similarly, an optimization model CITATION has shown how negative senescence can evolve for species that grow in body size throughout their lives, if this growth carries proportionate benefits in increasing reproductive output and decreasing mortality.
Both these analyses are concerned with the optimal tradeoff between fecundity and mortality, and so lie within the disposable soma paradigm.
These analyses have not modeled density dependence, except implicitly as a limiting case in which population growth is set to zero.
In this paper, we construct explicit models, within the disposable soma paradigm, for a very general class of organisms including those without indefinite somatic growth.
These reveal density dependence in recruitment as a sufficient driver for the evolution of nonsenescent life histories from senescent ancestors.
Density-limited recruitment sets up a balance of opposing selective forces that underpins the direction of evolution toward either compressed or spread reproductive life.
Thus, on the one hand, future reproduction is worth less than present reproduction to an individual's fitness, given a future extrinsic mortality risk CITATION.
On the other hand, future reproduction by an individual's mature offspring may be worth more to its inclusive fitness than its own present reproduction, if otherwise viable offspring face an extrinsic mortality risk before recruitment.
The crucial advance that we make is prefigured by Abrams CITATION, who showed that faster senescence is favored by positive or zero density-independent growth, and also by density-dependent adult mortality, whereas slower senescence requires density-dependent fecundity.
Our advance on his analysis is to show how the slower senescence can take the form of a runaway selection to negligible senescence, and even intrinsic immortality.
Indeed, density-dependent recruitment reflects the widely prevailing ecological condition of bottom-up regulation in crowded habitats.
We show that it is unwise either to ignore it, or to represent it only implicitly as zero population growth, because of its ubiquity in nature and its significant consequences for the evolution of senescence.
Here, we perform an optimization analysis of vitality evolution as a fitness tradeoff between compression into earlier life and spread into later life in the context of density-dependent recruitment, which accords with the abundant evidence that reproduction is intrinsically costly to survival CITATION, CITATION, CITATION, CITATION CITATION.
For populations at recruitment-regulated equilibrium, we demonstrate generic conditions under which natural selection itself increases the extrinsic recruitment losses, by successive genomic invasions increasing the level of crowding within the population.
Stronger density dependence means fewer recruitment opportunities into the adult population and, therefore, a natural selection that is weighted toward maximizing generation length over early-age vitality.
This positive feedback leads to the novel result that density regulation can trigger selection for ever-reducing senescence.
We develop a model that shows the potential for runaway selection of reduced senescence to arise across a wide range of age-specific vitality profiles, including accelerating loss from an early or late onset, and constant aging.
We find that natural selection can favor evolution of nonsenescence, and even immortality, from senescence in the presence of exogenous mortality, without a requirement for special life-history characteristics such as increasing intrinsic fecundity with age.
Simulations of this process are given for various scenarios, including stochastic environments.
The first four Results sections develop our analytical framework.
Section 1 outlines the assumptions we make about the action of density-dependent recruitment.
Section 2 specifies precisely the relation between the concepts that we use of vitality and senescent and nonsenescent aging.
Our approach is to define these concepts in terms of instantaneous rates, rather than on the rate of change of age-specific reproductive value.
The section Model: Invasion of Mutations outlines our assumptions concerning the effects of mutations on the key life-history parameter controlling the rate of senescence, and states our main result concerning the possibility of evolution from a highly compressed life history to a highly spread life history.
The section Example describes a specific example of evolution from positive senescence to non- senescence.
Finally, the section Simulations outlines stochastic simulations of the model.
Supporting material is provided in the Methods section and in Text S1.
We conclude with a discussion of the model predictions for life-history conditions and biotic environments that favor negligible senescence.
When incorporated into a polypeptide chain, proline differs from all other naturally occurring amino acid residues in two important respects.
The dihedral angle of Pro is constrained to values close to 65 and Pro lacks an amide hydrogen.
Consequently, mutations which result in introduction of Pro can significantly affect protein stability.
In the present work, we describe a procedure to accurately predict the effect of Pro introduction on protein thermodynamic stability.
Seventy-seven of the 97 non-Pro amino acid residues in the model protein, CcdB, were individually mutated to Pro, and the in vivo activity of each mutant was characterized.
A decision tree to classify the mutation as perturbing or nonperturbing was created by correlating stereochemical properties of mutants to activity data.
The stereochemical properties including main chain dihedral angle and main chain amide H-bonds were determined from 3D models of the mutant proteins built using MODELLER.
We assessed the performance of the decision tree on a large dataset of 163 single-site Pro mutations of T4 lysozyme, 74 nsSNPs, and 52 other Pro substitutions from the literature.
The overall accuracy of this algorithm was found to be 81 percent in the case of CcdB, 77 percent in the case of lysozyme, 76 percent in the case of nsSNPs, and 71 percent in the case of other Pro substitution data.
The accuracy of Pro scanning mutagenesis for secondary structure assignment was also assessed and found to be at best 69 percent.
Our prediction procedure will be useful in annotating uncharacterized nsSNPs of disease-associated proteins and for protein engineering and design.
Proline is unique among the 20 naturally occurring amino acid residues.
On the one hand, because Pro lacks an amide proton the main chain amide N is incapable of forming H-bonds.
Hence, substituting a residue involved in a main chain H-bond with Pro could destabilize the protein.
This property has previously been exploited to obtain information about residues involved in secondary structure CITATION CITATION.
On the other hand, the rigid pyrrolidine ring constrains the main chain dihedral angle to a narrow range of values close to 65.
It has also been observed CITATION CITATION that Pro restricts the conformation of the residue preceding it in a protein sequence.
The Ramachandran map of the pre-proline residue has a large excluded area between 40 50.
This restricts the conformation of the L and regions.
There is also a small leg of density in the region that is unique to pre-proline residues.
Hence, Pro can potentially increase protein stability because it decreases the conformational entropy of the denatured state.
In addition, Pro is usually conserved in proteins and often plays an important role in protein structure and function CITATION, CITATION, CITATION .
Previous studies on Pro mutants of different proteins have shown that the thermodynamic effects of introducing Pro depend on various factors including residue position, value of the original residue, H-bonding of the amide group of the original residue, and electrostatic or hydrophobic interactions of the original residue CITATION, CITATION, CITATION CITATION.
However, it is not yet clear whether the introduction of Pro at a given position in a protein will have a perturbing or nonperturbing effect on the thermodynamic stability of the protein.
The aim of the present work is to generate an algorithm based on Pro scanning mutagenesis data which can be used to predict the perturbing/nonperturbing effect of Pro substitution at a given position for any globular protein.
We also examine the utility of Pro scanning mutagenesis to infer protein secondary structure.
The experimental system used in this study, controller of cell division or death B protein, is a 101 residue, homodimeric protein encoded by F plasmid.
The protein does not contain any disulfides or metal ions.
The protein is an inhibitor of DNA gyrase and is a potent cytotoxin in Escherichia coli.
Transformation of normal E.coli cells with plasmid expressing the wild-type CcdB gene results in cell death.
If the protein is inactivated through mutation, cells transformed with the mutant genes will survive.
In this work we attempted to replace each of 101 amino acids of homodimeric CcdB with Pro using high throughput mega-primer based site-directed mutagenesis.
A total of 77 mutants could be generated.
Mutant phenotype was assayed as a function of expression level by monitoring the presence or absence of cell growth as a function of inducer concentration.
Based on an analysis of CcdB Pro scanning mutagenesis, phenotypic data, and its correlation with various structural parameters, a decision tree was created to classify Pro substitutions of a protein into perturbing and nonperturbing mutations.
The decision tree was further validated on a large phenotypic dataset of 163 Pro mutants of T4 lysozyme at two different temperatures, a nonsynonymous single nucleotide polymorphism database of Pro substitutions which are associated with various diseases and on Pro substitutions extracted from the ProTherm database and literature.
Spike timing dependent plasticity is a learning rule that modifies synaptic strength as a function of the relative timing of pre- and postsynaptic spikes.
When a neuron is repeatedly presented with similar inputs, STDP is known to have the effect of concentrating high synaptic weights on afferents that systematically fire early, while postsynaptic spike latencies decrease.
Here we use this learning rule in an asynchronous feedforward spiking neural network that mimics the ventral visual pathway and shows that when the network is presented with natural images, selectivity to intermediate-complexity visual features emerges.
Those features, which correspond to prototypical patterns that are both salient and consistently present in the images, are highly informative and enable robust object recognition, as demonstrated on various classification tasks.
Taken together, these results show that temporal codes may be a key to understanding the phenomenal processing speed achieved by the visual system and that STDP can lead to fast and selective responses.
Temporal constraints pose a major challenge to models of object recognition in cortex.
When two images are simultaneously flashed to the left and right of fixation, human subjects can make reliable saccades to the side where there is a target animal in as little as 120 130 ms CITATION.
If we allow 20 30 ms for motor delays in the oculomotor system, this implies that the underlying visual processing can be done in 100 ms or less.
In monkeys, recent recordings from inferotemporal cortex showed that spike counts over time bins as small as 12.5 ms and only about 100 ms after stimulus onset contain remarkably accurate information about the nature of a visual stimulus CITATION.
This sort of rapid processing presumably depends on the ability of the visual system to learn to recognize familiar visual forms in an unsupervised manner.
Exactly how this learning occurs constitutes a major challenge for theoretical neuroscience.
Here we explored the capacity of simple feedforward network architectures that have two key features.
First, when stimulated with a flashed visual stimulus, the neurons in the various layers of the system fire asynchronously, with the most strongly activated neurons firing first a mechanism that has been shown to efficiently encode image information CITATION.
Second, neurons at later stages of the system implement spike timing dependent plasticity, which is known to have the effect of concentrating high synaptic weights on afferents that systematically fire early CITATION, CITATION.
We demonstrate that when such a hierarchical system is repeatedly presented with natural images, these intermediate-level neurons will naturally become selective to patterns that are reliably present in the input, while their latencies decrease, leading to both fast and informative responses.
This process occurs in an entirely unsupervised way, but we then show that these intermediate features are able to support categorization.
Our network belongs to the family of feedforward hierarchical convolutional networks, as in CITATION CITATION.
To be precise, its architecture is inspired from Serre, Wolf, and Poggio's model of object recognition CITATION, a model that itself extends HMAX CITATION and performs remarkably well with natural images.
Like them, in an attempt to model the increasing complexity and invariance observed along the ventral pathway CITATION, CITATION, we use a four-layer hierarchy in which simple cells gain their selectivity from a linear sum operation, while complex cells gain invariance from a nonlinear max pooling operation .
Nevertheless, our network does not only rely on static nonlinearities: it uses spiking neurons and operates in the temporal domain.
At each stage, the time to first spike with respect to stimulus onset is supposed to be the key variable, that is, the variable that contains information and that is indeed read out and processed by downstream neurons.
When presented with an image, the first layer's S1 cells, emulating V1 simple cells, detect edges with four preferred orientations, and the more strongly a cell is activated, the earlier it fires.
This intensity latency conversion is in accordance with recordings in V1 showing that response latency decreases with the stimulus contrast CITATION, CITATION and with the proximity between the stimulus orientation and the cell's preferred orientation CITATION.
It has already been shown how such orientation selectivity can emerge in V1 by applying STDP on spike trains coming from retinal ON- and OFF-center cells CITATION, so we started our model from V1 orientation-selective cells.
We also limit the number of spikes at this stage by introducing competition between S1 cells through a one-winner-take-all mechanism: at a given location corresponding to one cortical column only the spike corresponding to the best matching orientation is propagated.
Note that k-winner-take-all mechanisms are easy to implement in the temporal domain using inhibitory GABA interneurons CITATION .
These S1 spikes are then propagated asynchronously through the feedforward network of integrate-and-fire neurons.
Note that within this time-to-first-spike framework, the maximum operation of complex cells simply consists of propagating the first spike emitted by a given group of afferents CITATION.
This can be done efficiently with an integrate-and-fire neuron with low threshold that has synaptic connections from all neurons in the group.
Images are processed one by one, and we limit activity to at most one spike per neuron, that is, only the initial spike wave is propagated.
Before presenting a new image, every neuron's potential is reset to zero.
We process various scaled versions of the input image.
There is one S1 C1 S2 pathway for each processing scale.
This results in S2 cells with various receptive field sizes.
Then C2 cells take the maximum response of S2 cells over all positions and scales, leading to position and scale invariant responses.
This paper explains how STDP can set the C1 S2 synaptic connections, leading to intermediate-complexity visual features, whose equivalent in the brain may be in V4 or IT.
STDP is a learning rule that modifies the strength of a neuron's synapses as a function of the precise temporal relations between pre- and postsynaptic spikes: an excitatory synapse receiving a spike before a postsynaptic one is emitted is potentiated whereas its strength is weakened the other way around CITATION.
The amount of modification depends on the delay between these two events: maximal when pre- and postsynaptic spikes are close together, and the effects gradually decrease and disappear with intervals in excess of a few tens of milliseconds CITATION CITATION.
Note that STDP is in agreement with Hebb's postulate because presynaptic neurons that fired slightly before the postsynaptic neuron are those that took part in firing it.
Here we used a simplified STDP rule where the weight modification does not depend on the delay between pre- and postsynaptic spikes, and the time window is supposed to cover the whole spike wave.
We also use 0 and 1 as soft bounds, ensuring the synapses remain excitatory.
Several authors have studied the effect of STDP with Poisson spike trains CITATION, CITATION.
Here, we demonstrate STDP's remarkable ability to detect statistical regularities in terms of earliest firing afferent patterns within visual spike trains, despite their very high dimensionality inherent to natural images.
Visual stimuli are presented sequentially, and the resulting spike waves are propagated through to the S2 layer, where STDP is used.
We use restricted receptive fields and weight-sharing.
Starting with a random weight matrix, we present the first visual stimuli.
Duplicated cells are all integrating the spike train and compete with each other.
If no cell reaches its threshold, nothing happens and we process the next image.
Otherwise for each prototype the first duplicate to reach its threshold is the winner.
A one-winner-take-all mechanism prevents the other duplicated cells from firing.
The winner thus fires and the STDP rule is triggered.
Its weight matrix is updated, and the change in weights is duplicated at all positions and scales.
This allows the system to learn patterns despite changes in position and size in the training examples.
We also use local inhibition between different prototype cells: when a cell fires at a given position and scale, it prevents all other cells from firing later at the same scale and within an s/2 s/2 square neighborhood of the firing position.
This competition, only used in the learning phase, prevents all the cells from learning the same pattern.
Instead, the cell population self-organizes, each cell trying to learn a distinct pattern so as to cover the whole variability of the inputs.
If the stimuli have visual features in common, the STDP process will extract them.
That is, for some cells we will observe convergence of the synaptic weights, which end up being either close to 0 or to 1.
During the convergence process, synapses compete for control of the timing of postsynaptic spikes CITATION.
The winning synapses are those through which the earliest spikes arrive CITATION, CITATION, and this is true even in the presence of jitter and spontaneous activity CITATION.
This preference for the earliest spikes is a key point since the earliest spikes, which correspond in our framework to the most salient regions of an image, have been shown to be the most informative CITATION.
During the learning, the postsynaptic spike latency decreases CITATION, CITATION, CITATION.
After convergence, the responses become selective CITATION to visual features of intermediate complexity similar to the features used in earlier work CITATION.
Features can now be defined as clusters of afferents that are consistently among the earliest to fire.
STDP detects these kinds of statistical regularities among the spike trains and creates one unit for each distinct pattern.
The paradigm of biological membranes has recently gone through a major update.
Instead of being fluid and homogeneous, recent studies suggest that membranes are characterized by transient domains with varying fluidity.
In particular, a number of experimental studies have revealed the existence of highly ordered lateral domains rich in sphingomyelin and cholesterol.
These domains, called functional lipid rafts, have been suggested to take part in a variety of dynamic cellular processes such as membrane trafficking, signal transduction, and regulation of the activity of membrane proteins.
However, despite the proposed importance of these domains, their properties, and even the precise nature of the lipid phases, have remained open issues mainly because the associated short time and length scales have posed a major challenge to experiments.
In this work, we employ extensive atom-scale simulations to elucidate the properties of ternary raft mixtures with CHOL, palmitoylsphingomyelin, and palmitoyloleoylphosphatidylcholine.
We simulate two bilayers of 1,024 lipids for 100 ns in the liquid-ordered phase and one system of the same size in the liquid-disordered phase.
The studies provide evidence that the presence of PSM and CHOL in raft-like membranes leads to strongly packed and rigid bilayers.
We also find that the simulated raft bilayers are characterized by nanoscale lateral heterogeneity, though the slow lateral diffusion renders the interpretation of the observed lateral heterogeneity more difficult.
The findings reveal aspects of the role of favored lipid lipid interactions within rafts and clarify the prominent role of CHOL in altering the properties of the membrane locally in its neighborhood.
Also, we show that the presence of PSM and CHOL in rafts leads to intriguing lateral pressure profiles that are distinctly different from corresponding profiles in nonraft-like membranes.
The results propose that the functioning of certain classes of membrane proteins is regulated by changes in the lateral pressure profile, which can be altered by a change in lipid content.
The understanding of lipid membrane structures and their role in cellular functions has developed significantly since the introduction of the classical fluid-mosaic model by Singer and Nicolson CITATION.
The fluid-mosaic model predicted that cellular membranes are fluid and characterized by random distribution of molecular components in the membrane, resulting in lateral and rotational freedom.
The more recent picture is considerably more elaborate, however.
A large number of experimental results converge toward the idea that lateral domains enriched in sphingomyelin and cholesterol exist in biological membranes.
These nanosized domains, called functional lipid rafts, have been suggested to take part in various dynamic cellular processes such as membrane trafficking, signal transduction, and regulation of the activity of membrane proteins CITATION CITATION.
The existence of stable lipid rafts in biological membranes is under intense scrutiny, and their existence is actually under debate since the lipid rafts, if they do exist, are probably too small to be resolved by techniques such as fluorescence microscopy CITATION.
Direct evidence of rafts in vivo is mainly based on monitoring the motions of membrane proteins CITATION or on differential partitioning of fluorescent probes in membrane environments CITATION.
It is, however, difficult to perform experiments using living cells, which complicates measurements of physical quantities of the rafts, such as the exact lipid composition, characteristic size, and lifetime CITATION, CITATION.
In model membranes, the coexistence of domains in the liquid ordered and the liquid disordered phase is widely accepted CITATION, CITATION.
For example, the l d phase may be formed by an unsaturated phosphatidylcholine, while the formation of the l o phase is promoted by a mixture of SM and CHOL.
As for rafts, the current understanding of lipid rafts in biological membranes suggests a granular structure of nanometer-scale domains of various compositions CITATION, CITATION, CITATION rather than a large-scale phase separation.
The exact nature of the underlying interactions that lead to lipid immiscibilities in membranes is under debate CITATION, CITATION.
CHOL is particularly important as it has been shown to increase the conformational order of acyl chains and reduce the bilayer area, hence significantly increasing the packing density of the lipids CITATION CITATION.
CHOL is particularly effective in reducing the void space within the acyl chain region of the lipids CITATION, which is related to suppressed area compressibility and increased bending rigidity of the membrane with increasing CHOL concentrations.
However, the lateral diffusion rates are not expected to slow down by more than a factor of 2 3 when the l d phase is compared with CHOL-induced l o phase CITATION, CITATION.
Also, CHOL has recently been reported to significantly alter the lateral pressure profile of membranes CITATION.
This is important, as changes in the lateral pressure profiles have been suggested to be related to changes in membrane protein structure and activity CITATION .
Considering that the smallest estimates for the sizes of rafts fall in the range of nanometers CITATION, CITATION, they make an accessible subject for computational studies.
Though, in spite of the considerable importance of rafts, it is somewhat surprising that only a few atom-scale simulations have dealt with ternary mixtures of CHOL, SM, and PC CITATION, CITATION, concentrating mainly on small-scale structural properties and local interactions between the lipids.
In particular, there are no previous atom-level computational studies of rafts aiming to characterize the nature of their structural and dynamical features.
For example, the nanometer scale structure within raft domains and its interplay with CHOL-induced effects are not understood.
Further, the resulting large-scale properties, such as membrane elasticity in ternary raft-like lipid mixtures, are not understood either.
Finally, and perhaps most importantly, the lateral pressure profiles associated with rafts are completely unknown.
The concept of the lateral pressure profile across the lipid membrane is exceptionally significant, since it describes the pressure exerted on molecules embedded in a membrane.
Cantor has proposed that incorporation of molecules into membrane and changes in lipid content would alter the lateral pressure profile across a membrane, and hence changes in the pressure profile would induce changes in membrane protein structure CITATION, CITATION.
Experimental studies of this issue are remarkably difficult, however: currently there is only one study that employed fluorescent probes to gauge the overall shape of the lateral pressure profile CITATION.
Evidently, detailed atomistic simulations are called for.
The state-of-the-art extent of the simulations conducted in this work, 15 20 nm in lateral dimensions and 100 ns in time, enables a reliable quantitative analysis of the properties of raft-like membranes not accomplished before.
We employ large-scale atom level simulations for three mixtures of palmitoyloleoylphosphatidylcholine, PSM, and CHOL.
The molar fractions are POPC:PSM:CHOL 1:1:1, 2:1:1, and 62:1:1 for systems that we call S A, S B, and S C, respectively.
Based on a recent experimental phase diagram CITATION, these mixtures are expected to display the coexistent l o and l d phase domains or a single l d phase.
Here, we illustrate the distinct nature of raft-like domains in three parts.
First, we consider the elastic, thermodynamic, and dynamic properties of rafts that turn out to be very different from those of nonraft-like membranes.
Second, we provide evidence that the presence of PSM and CHOL in raft-like membranes leads to strongly packed and rigid bilayers, characterized by significant nanoscale lateral heterogeneity within the raft domains.
These findings express the prominent role of favored lipid lipid interactions within rafts and highlight the significant role of CHOL in promoting the formation of rafts.
Third, we provide compelling evidence that the lateral pressure profiles can be altered by a change in lipid content.
In particular, we show how the presence of PSM and CHOL leads to intriguing lateral pressure profiles that are distinctly different from corresponding lateral pressure profiles in nonraft-like membranes, proposing that lipid membranes may regulate the functioning of certain classes of membrane proteins such as mechanosensitive channels through changes in lipid composition, and hence the lateral pressure profile.
Persistent activity states, observed in several neocortical areas after the removal of a sensory stimulus, are believed to be the neuronal basis of working memory.
One of the possible mechanisms that can underlie persistent activity is recurrent excitation mediated by intracortical synaptic connections.
A recent experimental study revealed that connections between pyramidal cells in prefrontal cortex exhibit various degrees of synaptic depression and facilitation.
Here we analyze the effect of synaptic dynamics on the emergence and persistence of attractor states in interconnected neural networks.
We show that different combinations of synaptic depression and facilitation result in qualitatively different network dynamics with respect to the emergence of the attractor states.
This analysis raises the possibility that the framework of attractor neural networks can be extended to represent time-dependent stimuli.
Working memory enables us to hold the trace of a fleeting stimulus for a few seconds after it is gone, thus enabling the manipulation of information over time.
Recordings from neurons in monkeys performing working memory tasks reveal stimulus-selective spiking activity that persists after the removal of the stimulus.
These persistent activity states are considered to be the neuronal substrate of working memory CITATION .
The sustained persistent activity is believed to be achieved by excitatory interpyramidal connections that are either prewired or formed during the learning of the task CITATION.
In vitro studies of such connections in the cortex revealed pronounced short-term plasticity effects CITATION.
In the sensory areas of the cortex, the dominant effect is synaptic depression, expressed as a rapid decay of synaptic efficacy following the presynaptic firing CITATION.
Several theoretical studies investigated the effects of synaptic depression on the existence and stability of attractor states.
Wang et al. CITATION recently performed experiments to investigate short-term synaptic plasticity in the prefrontal cortex, one of the cortical areas where persistent activity is observed CITATION.
They found that interpyramidal connections in this area exhibit various degrees of synaptic facilitation, with three different classes of connections identified.
While synaptic facilitation was recently mentioned as a stabilizing factor for network attractors CITATION, there is as yet no systematic study of its effect on the dynamics of recurrent neural networks undergoing the transition from background to persistent states after the presentation of a stimulus.
In this contribution, we consider an attractor neural network with connections that have already been formed by learning several stimuli CITATION, CITATION.
We assume that the network comprises a set of neuronal populations, each responding primarily to a certain stimulus.
This scheme, via Hebbian learning, can strengthen the synaptic connections within a population and form a stable activity state.
Drawing on recent experimental results CITATION, we assume that the neurons within each population differ in the dynamic properties of their synapses and thus exhibit different temporal response profiles to the same stimuli.
This firing can then lead to a further differentiation of synaptic strengths within the population, whereby neurons with similar synaptic dynamics are connected more strongly to one another than to ones with dissimilar synaptic dynamics.
We thus consider a network comprising several attractor populations, each divided into subpopulations with different synaptic dynamics.
These populations interact via both excitatory dynamic synapses and inhibition to generate rich dynamics in response to external stimuli.
Since the synaptic dynamics differ between subpopulations, we expect them to respond differently to different temporal profiles of the input, which could result in a greater computational power for the network.
The topology of cellular circuits is key to understand their robustness to both mutations and noise.
The reason is that many biochemical parameters driving circuit behavior vary extensively and are thus not fine-tuned.
Existing work in this area asks to what extent the function of any one given circuit is robust.
But is high robustness truly remarkable, or would it be expected for many circuits of similar topology?
And how can high robustness come about through gradual Darwinian evolution that changes circuit topology gradually, one interaction at a time?
We here ask these questions for a model of transcriptional regulation networks, in which we explore millions of different network topologies.
Robustness to mutations and noise are correlated in these networks.
They show a skewed distribution, with a very small number of networks being vastly more robust than the rest.
All networks that attain a given gene expression state can be organized into a graph whose nodes are networks that differ in their topology.
Remarkably, this graph is connected and can be easily traversed by gradual changes of network topologies.
Thus, robustness is an evolvable property.
This connectedness and evolvability of robust networks may be a general organizational principle of biological networks.
In addition, it exists also for RNA and protein structures, and may thus be a general organizational principle of all biological systems.
The biochemical parameters that determine the behavior of cellular systems from proteins to genome-scale regulatory networks change continually.
Such change has two principal sources.
One of them is genetic and consists of mutations.
The other is nongenetic; it is exemplified by noise internal to the organism and by environmental change.
In contrast to mutations, which are relatively rare, internal noise is ubiquitous and substantial.
Much of it consists of stochastic variation in gene expression and expression regulation CITATION CITATION.
Such noise makes all biochemical parameters affecting a circuit's behavior appear to fluctuate randomly.
Environmental change, such as a change in temperature, salinity, or nutrient availability, can similarly affect many parameters at once.
These observations suggest that biological circuits are not fine-tuned to exercise their functions only for precise values of their biochemical parameters.
Instead, they must be able to function under a range of different parameters.
In other words, they must be robust to parameter change.
These insights have lead to explorations of circuit robustness in processes ranging from bacterial chemotaxis to embryonic development CITATION CITATION .
Quantitative models of cellular circuits help us to understand processes as different as circadian rhythms CITATION CITATION, the cell cycle CITATION, organismal development CITATION, CITATION, CITATION, CITATION, CITATION CITATION, bacterial chemotaxis CITATION, and the behavior of synthetic circuitry CITATION CITATION.
Several classes of models are used to represent such biological networks.
The first class comprises differential equation models.
The continuous state variables in these equations correspond to the concentrations or activities of gene products.
The interactions of these gene products are represented through biochemical parameters such as binding affinities of transcriptional regulators to DNA, dissociation constants of ligand-receptor complexes, or kinetic rate constants of enzymes.
A nearly universal problem is that quantitative information about these biochemical parameters is absent, even for experimentally well-studied systems.
In other words, some knowledge of the topology of a circuit who interacts with whom may exist, but the strengths of the interactions are usually unknown.
Even where measurements of biochemical parameters are available, they are often order-of-magnitude estimates rather than quantitative measurements with known precision.
This difficulty leads one naturally to a second class of models in which only the qualitative nature of the state variables is considered.
Our focus here is not to consider any one circuit but many circuit architectures or topologies.
Because of the incessant changes of biochemical parameters and the lack of quantitative information about their values, such an approach is appropriate for studying fundamental properties of cellular circuits; in particular, one may ask what features are responsible for the robustness of a circuit architecture or topology CITATION, CITATION, CITATION, CITATION, CITATION.
In this work, we carry out an analysis for a model of transcriptional regulation networks with important functions in developmental processes.
Despite its level of abstraction, this model has proven highly successful in explaining the regulatory dynamics of early developmental genes in the fruit fly Drosophila as well as in predicting mutant phenotypes CITATION, CITATION CITATION.
It has also helped to elucidate why mutants often show a release of genetic variation that is cryptic in the wild-type, and how adaptive evolution of robustness occurs in genetic networks of a given topology CITATION CITATION.
Most recently, it has helped explain how sexual reproduction can enhance robustness to recombination CITATION .
The model CITATION is concerned with a regulatory network of N transcriptional regulators, which are represented by their expression patterns S at some time t during a developmental or cell-biological process and in one cell or domain of an embryo.
The time scale of the model's expression dynamics is the time scale characteristic for transcriptional regulation, which is on the order of minutes, and not on the order of days, weeks, or months, as for complete development from zygote to adult.
The model's transcriptional regulators can influence each other's expression through cross-regulatory and autoregulatory interactions, which are encapsulated in a matrix w. The elements w ij of this matrix indicate the strength of the regulatory influence that gene j has on gene i. This influence can be either activating, repressing, or absent.
Put differently, the matrix w represents the genotype of this system, while the expression state is its phenotype.
We model the change in the expression state S of the network as time t progresses according to the difference equation where is a constant, and is a sigmoidal function whose values lie in the interval.
This equation reflects the regulation of gene i's expression by other genes.
We are here concerned with networks whose expression dynamics start from a prespecified initial state S at some time t 0 during development, and arrive at a prespecified stable equilibrium or target expression state S. We will call such networks viable networks.
The initial state is determined by regulatory factors upstream of the network, which may represent signals from the cell's environment or from other domains of an embryo.
Transcriptional regulators that are expressed in the stable equilibrium state S affect the expression of genes downstream of the network.
As a modeling assumption, we think of their expression as critical for the course of development.
Thus, deviations from S are highly deleterious.
It is because our work starts from such a developmental framework that S and S play a central role; this is in contrast with most studies determining the generic properties of random Boolean networks CITATION, CITATION, CITATION, CITATION, CITATION CITATION.
We here examine the relationship between robustness and network topology for millions of networks with different topologies.
Topology is synonymous with the structure of the matrix w, because each of w's nonzero entries corresponds to one regulatory interaction among the circuit's genes.
Changes in topology correspond to the loss of a regulatory interaction, or to the appearance of a new regulatory interaction that was previously absent.
Such topological changes can occur on very short evolutionary time scales, in particular in higher eukaryotes with large regulatory regions CITATION.
This underscores the need to study their effects on network robustness.
In our analysis, we first ask how robustness to mutations and noise varies within an ensemble of networks with different topologies.
Subsequently, and more importantly, we also ask whether highly robust topologies can evolve from topologies with low robustness through gradual topological changes.
Protein point mutations are an essential component of the evolutionary and experimental analysis of protein structure and function.
While many manually curated databases attempt to index point mutations, most experimentally generated point mutations and the biological impacts of the changes are described in the peer-reviewed published literature.
We describe an application, Mutation GraB, that identifies, extracts, and verifies point mutations from biomedical literature.
The principal problem of point mutation extraction is to link the point mutation with its associated protein and organism of origin.
Our algorithm uses a graph-based bigram traversal to identify these relevant associations and exploits the Swiss-Prot protein database to verify this information.
The graph bigram method is different from other models for point mutation extraction in that it incorporates frequency and positional data of all terms in an article to drive the point mutation protein association.
Our method was tested on 589 articles describing point mutations from the G protein coupled receptor, tyrosine kinase, and ion channel protein families.
We evaluated our graph bigram metric against a word-proximity metric for term association on datasets of full-text literature in these three different protein families.
Our testing shows that the graph bigram metric achieves a higher F-measure for the GPCRs, protein tyrosine kinases, and ion channel transporters.
Importantly, in situations where more than one protein can be assigned to a point mutation and disambiguation is required, the graph bigram metric achieves a precision of 0.84 compared with the word distance metric precision of 0.73.
We believe the graph bigram search metric to be a significant improvement over previous search metrics for point mutation extraction and to be applicable to text-mining application requiring the association of words.
With the advent of ultra high throughput screening and high-density array technology, the biological community has come to appreciate the value of unbiased surveys of complex biological systems.
Bioinformatics tools have become an integral part of the analysis of these extensive datasets.
When complex data is collected centrally, the analysis can be straightforward.
When data is collected in a distributed fashion, investigators must agree on a centralized data-deposition strategy or we must develop tools to interrogate the published literature and extract relevant information.
Manually curated online databases have developed to meet this need, but they are difficult to maintain and scale.
Accordingly, the biological text-mining field has evolved to identify and extract information from the literature for database storage and access.
Two types of tasks predominate in biological text mining: the extraction of gene and protein names CITATION CITATION and the extraction of interactions between proteins CITATION CITATION.
The BioCreAtIvE challenge was CITATION focused on name extraction CITATION with the additional task of functional annotation CITATION.
Other text-mining applications focus on hypothesis generation CITATION, probing protein subcellular localization CITATION, and pathway discovery CITATION .
Recent work has also focused on the extraction of protein point mutations from biomedical literature CITATION CITATION.
Protein point mutations, the substitution of a wild-type amino acid with an alternate one, can be important to our understanding of protein function, evolutionary relationships, and genetic disorders.
From a functional perspective, researchers introduce point mutations into proteins to assay the importance of a particular residue to protein function.
Evolution relies upon mutations or polymorphisms in DNA, a mechanism for creating diversity in protein sequences.
While the term mutation is used to imply deleterious changes, and polymorphism means a difference within species, for text-mining purposes we refer to a point mutation as a substitution of a different amino acid for the reference amino acid.
dbSNP CITATION and the Human Gene Mutation Database CITATION are two of many databases that catalog point mutations and their downstream effects.
These databases are manually curated, which limits the speed of input into the database and the breadth of information represented, but does aid in the incorporation of complex information that is difficult for text-mining tools to parse.
The task of point mutation extraction can be decomposed into two subtasks.
First, it is necessary to identify the protein and mutation terms discussed within an article.
After these entities are identified, an association must be made between the point mutation and its correct protein of origin.
This problem is trivial when a paper discusses a single protein but increasingly complex when multiple proteins are present.
In our evaluation of Mutation Graph Bigram, we downloaded 589 full-text PDF articles related to the GPCR, tyrosine kinase, and ion channel protein families from PubMed-provided links.
Using our dictionary-based protein term identification method, we counted 350 articles out of the total 589 that contained a point mutation that could have belonged to multiple proteins.
A few methods for point mutation extraction have been developed.
Rebholz-Schuhmann et al. CITATION describe a method called MEMA that scans Medline abstracts for mutations.
Baker and Witte CITATION CITATION describe a method called Mutation Miner that integrates point mutation extraction into a protein structure visualization application.
Our own group has presented MuteXt CITATION, a point mutation extraction method applied to G protein coupled receptor and nuclear hormone receptor literature.
MEMA and MuteXt use a straightforward dictionary search to identify protein/gene names and a word proximity distance measurement to disambiguate between multiple protein terms.
Both methods, while providing a simple and successful method for point mutation extraction, were limited in two areas.
First, the word distance measurement is not always correct in disambiguating between protein terms.
Second, MEMA was evaluated on a set of abstracts, which are intrinsically more limited than the full-text article.
In our literature set, the abstracts contained only 15 percent of the point mutations found in the full text.
The point mutations were also validated against OMIM CITATION, which only contains disease-related point mutations.
MuteXt was trained and evaluated on GPCR and intranuclear hormone receptor literature and contained customizations in the algorithm for dealing with problematic protein naming and amino acid numbering cases.
Mutation Miner approaches the problem differently.
This method identifies and relates proteins, organisms, and point mutations using NLP analysis at a sentence level.
An entity pair is assigned if both entities match noun phrase patterns.
This method would work well if all point mutations were described in conjunction with associated proteins and organisms at the sentence level, which we have observed is not always the case.
Mutation Miner also incorporates protein sequence information, but for use in annotating protein 3-D structures with mutation information instead of point mutation validation.
Our method improves on MEMA, MuteXt, and Mutation Miner by using a novel graph bigram metric that incorporates frequency and location of terms to disambiguate between proteins and searches full-text information.
Like MuteXt, Mutation GraB utilizes the Swiss-Prot protein database CITATION for sequence validation, which intrinsically contains more sequence variation than OMIM.
We addressed the utility of our application by standardizing the algorithm for all protein families and by evaluating our method on three different protein family literature sets covering 589 articles.
More detailed comparisons with MEMA and Mutation Miner are described in the Discussion section.
For our task of associating point mutations to protein terms, it is not sufficient to minimally tag a protein name in the literature; we must also find its correct gene identifier in a corresponding database.
The BioCreAtIvE challenge addressed this problem with the 1B subtask of identifying a protein/gene mentioned in the text and annotating it with its correct gene identifier.
Solutions for this challenge ranged from rule-based methods CITATION to machine-learning approaches CITATION to a combination of both.
Unfortunately, some of these methods may not be applicable to our point mutation extraction task.
The participants in the BioCreAtIve challenge were provided a large set of annotated sentences categorized under three different organisms; human, yeast, and fly.
Some solutions for the subtask 1B consisted of learning the training data for each organism, then applying the learned functions to a test set also divided by organism.
This approach is suboptimal for our task for two reasons.
First, because point mutations are frequently analyzed at a protein family and superfamily level, methods trained on protein names from organism-specific lexicons would not be well-suited for analysis across many species.
Second, our goal is to create a broadly applicable methodology for point mutation extraction that can be utilized on any categorization of proteins.
Machine-learning approaches benefit from large detailed annotated training sets.
In our experience, the manual labor involved in annotating the amount of text necessary to learn protein family specific nomenclature on the scale presented by BioCreAtIve is likely to undermine the benefits of automated point mutation extraction.
Methods relying solely on rule-based features for protein-name identification generally perform at a lower precision and recall than methods incorporating machine learning.
However, since rule-based methods do not necessarily require annotated training data, they are advantageous when such data is unavailable or difficult to acquire.
Our approach to protein term identification is similar to other rule-based approaches CITATION, CITATION, CITATION.
We first create a dictionary using the names and synonyms of proteins in a protein family; the protein names are retrieved from their respective Swiss-Prot and Entrezgene entries.
The terms in the dictionary are then searched for in the journal literature.
Depending on the character length and composition of these terms, we search by different regular expressions with varying levels of specificity.
A further description of this is detailed in the Methods section.
In muscle, force emerges from myosin binding with actin.
This actomyosin binding depends upon myofilament geometry, kinetics of thin-filament Ca 2 activation, and kinetics of cross-bridge cycling.
Binding occurs within a compliant network of protein filaments where there is mechanical coupling between myosins along the thick-filament backbone and between actin monomers along the thin filament.
Such mechanical coupling precludes using ordinary differential equation models when examining the effects of lattice geometry, kinetics, or compliance on force production.
This study uses two stochastically driven, spatially explicit models to predict levels of cross-bridge binding, force, thin-filament Ca 2 activation, and ATP utilization.
One model incorporates the 2-to-1 ratio of thin to thick filaments of vertebrate striated muscle, while the other comprises only one thick and one thin filament.
Simulations comparing these models show that the multi-filament predictions of force, fractional cross-bridge binding, and cross-bridge turnover are more consistent with published experimental values.
Furthermore, the values predicted by the multi-filament model are greater than those values predicted by the two-filament model.
These increases are larger than the relative increase of potential inter-filament interactions in the multi-filament model versus the two-filament model.
This amplification of coordinated cross-bridge binding and cycling indicates a mechanism of cooperativity that depends on sarcomere lattice geometry, specifically the ratio and arrangement of myofilaments.
Muscle contraction is initiated by Ca 2 binding to troponin and the subsequent movement of tropomyosin on the thin filament, enabling myosin to cyclically attach and detach to actin CITATION CITATION.
Underlying this process are myriad factors that contribute to the magnitude and time course of force production.
These factors include the geometry of filaments in the sarcomere, the mechanical properties of the filaments and cross-bridges, the kinetics of thin-filament activation by Ca 2, and the kinetics of cross-bridge cycling.
Because contractile proteins interact in a highly structured, compliant lattice, mechanical coupling exists between myosins along the thick-filament backbone, between actin monomers or regulatory proteins along the thin filament, and between thick and thin filaments following cross-bridge formation.
Thus, kinetic processes responsible for contraction are linked at the molecular level.
Considerable evidence shows that Ca 2 and cross-bridge binding at one location in the sarcomere can influence these processes at proximal regions of the sarcomere, implying that coupled kinetics of thin-filament activation and cross-bridge cycling determine the level of force generated in striated muscle.
Most models do not explicitly consider that spatial properties of muscle may influence contraction CITATION, CITATION, CITATION, CITATION, CITATION CITATION.
Of the muscle contraction models containing both spatial and temporal variables, some provide either spatial predictions of steady-state conditions CITATION, CITATION or temporal predictions of cross-bridge and thin-filament state without any spatial detail CITATION.
In contrast, a few recent spatially explicit models predict both spatial and temporal behavior CITATION CITATION, with some simulations indicating that elasticity of the myofilament lattice contributes to coordination between cross-bridges that enhances cross-bridge binding CITATION, CITATION, CITATION.
This cross-bridge induced cross-bridge recruitment becomes a potential mechanism of cooperativity that results from realignment between compliant myofilaments following myosin binding to actin.
Previous spatially explicit models CITATION CITATION lacked a Ca 2 regulatory cycle, spatially coordinated Ca 2 activation along the thin filament, and the physiological ratio of thick to thin filaments.
These thin-filament components are particularly important for contraction because regions activated by Ca 2 binding to troponin largely determine the spatial distribution of bound cross-bridges.
The current study adopts a spatially explicit model of regulatory proteins along the thin filament, in contrast to prior studies CITATION, CITATION.
These additions enable investigating how force is controlled by two, coupled, spatial, and temporal processes: Ca 2 binding to activate the thin filament and subsequent myosin binding to the proximal, activated region of the thin filament.
Spatial and temporal aspects of contraction may be profoundly influenced by the coupled behavior between myosins throughout the compliant myofilament lattice, as nearly 70 percent of muscle compliance resides in the thick and thin filaments CITATION CITATION.
This significant compliance implies that cross-bridges do not operate independently while generating force CITATION, CITATION, CITATION.
Moreover, recent measurements CITATION CITATION improve estimates about cross-bridge rate functions, depending on distortion and load, suggesting that the extent of realignment between compliant thick and thin filaments may affect kinetics of cross-bridge cycling.
Within this compliant system, however, the consequences of sarcomere lattice structure on cross-bridge dynamics remain unclear.
This study compares multiple models that have identical thin-filament and cross-bridge kinetics, but different model geometries, to examine the consequences of sarcomere lattice structure on Ca 2 -regulated contraction.
Consistent with previous models CITATION, CITATION, CITATION, motions and forces occur solely along the longitudinal axis of filaments in these current models.
This one-dimensional assumption permits a system of linear equations to describe force-generating interactions between filaments.
At the core of each model is a three-state cross-bridge cycle coupled with a three-state thin-filament regulatory model to control actomyosin binding through Ca 2 -sensitive kinetics.
Initial model comparisons occurred between four different models.
Of these geometric options, only one multi-filament model yields predictions that were consistent with the range of published values for muscle contraction.
Therefore, we focus on comparing this multi-filament model with a two-filament model .
Throughout this study, we specifically consider contraction in the absence of cooperative, kinetic feedback between thin-filament activation or cross-bridge binding CITATION, CITATION, CITATION, CITATION CITATION.
Thus, any differences in simulation predictions between the multi-filament and two-filament models depend solely on differences between model geometry.
Simulation results show that additional inter-filament interactions in the multi-filament model lead to greater fractional binding of cross-bridges, force production, and cross-bridge turnover compared with the two-filament model.
Importantly, these increases are larger than predicted by normalizing for the additional filaments in the multi-filament model.
These results indicate that there is a mechanism of cooperativity dependent upon sarcomere lattice structure.
Specifically, multi-filament lattice structure further coordinates cross-bridge binding to enhance cross-bridge recruitment and turnover without any requirements for cooperative feedback mechanisms attributed to thin-filament activation.
Additional studies investigating other mechanisms of cooperativity acting via kinetic feedback pathways to amplify thin-filament activation or cross-bridge binding are ongoing in our lab and in others CITATION, CITATION.
Findings from the current study, however, imply that certain lattice geometries facilitate greater cross-bridge binding and turnover, which may be an important mechanism of cooperativity contributing to muscle performance.
Earlier aspects of this work have been published previously CITATION, CITATION, CITATION .
All materials enter or exit the cell nucleus through nuclear pore complexes, efficient transport devices that combine high selectivity and throughput.
NPC-associated proteins containing phenylalanine glycine repeats have large, flexible, unstructured proteinaceous regions, and line the NPC.
A central feature of NPC-mediated transport is the binding of cargo-carrying soluble transport factors to the unstructured regions of FG nups.
Here, we model the dynamics of nucleocytoplasmic transport as diffusion in an effective potential resulting from the interaction of the transport factors with the flexible FG nups, using a minimal number of assumptions consistent with the most well-established structural and functional properties of NPC transport.
We discuss how specific binding of transport factors to the FG nups facilitates transport, and how this binding and competition between transport factors and other macromolecules for binding sites and space inside the NPC accounts for the high selectivity of transport.
We also account for why transport is relatively insensitive to changes in the number and distribution of FG nups in the NPC, providing an explanation for recent experiments where up to half the total mass of the FG nups has been deleted without abolishing transport.
Our results suggest strategies for the creation of artificial nanomolecular sorting devices.
The contents of the eukaryotic nucleus are separated from the cytoplasm by the nuclear envelope.
Nuclear pore complexes are large protein assemblies embedded in the nuclear envelope and are the sole means by which materials exchange across it.
Water, ions, small macromolecules CITATION, and small neutral particles can diffuse unaided across the NPC CITATION, while larger macromolecules will generally only be transported efficiently if they display a particular transport signal sequence, such as a nuclear localization signal or nuclear export signal.
Macromolecular cargoes carrying these signal sequences bind cognate soluble transport factors that facilitate the passage of the resulting transport factor cargo complexes through the NPC.
The-best studied transport factors belong to a family of structurally related proteins, collectively termed -karyopherins, although other transport factors can also mediate nuclear transport, particularly the export of mRNAs.
NPCs can pass cargoes up to 30 nm diameter, at rates as high as several hundred macromolecules per second each transport factor cargo complex dwelling in the NPC for a time on the order of 10 ms CITATION, CITATION .
Here we focus on karyopherin-mediated import, although our conclusions pertain to other types of nucleocytoplasmic transport as well, including mRNA export.
During import, karyopherins bind cargoes in the cytoplasm via their nuclear localization signals.
The karyopherin cargo complexes then translocate through NPCs to the nucleoplasm, where the cargo is released from the karyopherin by RanGTP, which is maintained in its GTP-bound form by a nuclear factor, RanGEF.
The high affinity of RanGTP binding for karyopherins allows it to displace cargoes from the karyopherins in the nucleus.
Subsequently, karyopherins with bound RanGTP travel back through the NPC to the cytoplasm, where conversion of RanGTP to RanGDP is stimulated by the cytoplasmic factor RanGAP.
The energy released by GTP hydrolysis is used to dissociate RanGDP from the karyopherins, which are then ready for the next cycle of transport.
Importantly, this GTP hydrolysis is the only step in the process of nuclear import that requires an input of metabolic energy.
Overall, the energy obtained from RanGTP hydrolysis is used to create a concentration gradient of karyopherin cargo complexes between the cytoplasm and the nucleus, so that the process of actual translocation across the NPC occurs purely by diffusion CITATION, CITATION CITATION, CITATION CITATION .
Conceptually, nuclear import can be divided into three stages: first, the loading of cargo onto karyopherins in the cytoplasm, second, the translocation of karyopherin cargo complexes through the NPC, and, third, the release of cargo inside the nucleus.
The first and last stages have been the subject of numerous studies, and are relatively well understood, being soluble-phase reactions amenable to biochemical characterization.
The intermediate stage of transport is much less understood.
Nevertheless, it is clear that the ability of karyopherins to bind a particular class of NPC-associated proteins containing phenylalanine glycine repeats, known collectively as FG nups, is a key feature of the transport process, and allows them to selectively and efficiently pass with their cargoes through the NPC.
In particular, experiments in which the FG nup binding sites on the karyopherins were mutated show that disrupting the binding of karyopherins to FG nups impairs transport CITATION, CITATION, CITATION, CITATION, CITATION.
Current estimates of the binding affinity of karyopherins to most FG nups are in the range 1 1,000 nM, depending on the FG nup and karyopherin type CITATION CITATION.
Each FG nup usually carries a small region that anchors it to the body of the NPC, and a larger region characterized by multiple FG repeats.
These FG repeat regions are natively disordered flexible chains or filaments that contain binding sites for transport factors and also appear to set up a barrier at the entrance of the NPC for macromolecules that cannot bind them CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, CITATION CITATION.
The detailed physicochemical nature of this barrier is still under active study, although FG nups have been shown in vitro to form flexible polymer brushes when grafted to a surface CITATION or gels in bulk solution CITATION.
Importantly, it has been repeatedly demonstrated that individual FG repeat regions can have a long reach, on the order of many tens of nm, within the NPC CITATION, CITATION, CITATION, CITATION.
What is still needed is a quantitative theoretical explanation that can account for the observed characteristics of facilitated nuclear cytoplasmic transport.
Here, we develop a diffusion-based theory to explain the mechanism of the intermediate stage of nucleocytoplasmic transport i.e., translocation through the NPC.
A useful theory of NPC-mediated transport should provide insight into several major unresolved questions, including: How does the NPC achieve high transport efficiency of cargoes of variable sizes and in both directions, through only diffusion of the transport factor cargo complexes?
How does binding of transport factors to FG nups facilitate transport efficiency while maintaining a high throughput CITATION, CITATION, CITATION, CITATION, CITATION ? NPCs largely exclude nonspecific macromolecules in favor of transport factor bound cargoes.
How is this high degree of selectivity achieved?
Neither deletion of up to half the mass of the FG nups' filamentous unfolded regions, nor deletion of asymmetrically disposed FG nups' filamentous regions that potentially set up an affinity gradient, abolish transport CITATION.
Directionality of transport across the NPC can even be reversed by reversing the concentration gradient of RanGTP CITATION.
How can we account for such a high degree of robustness?
Several theoretical models have been proposed for the mechanism of transport through the NPC.
These include the Brownian Affinity Gate model CITATION, CITATION, Selective Phase models CITATION, CITATION, CITATION, CITATION, the Oily Spaghetti model CITATION, Affinity Gradient models CITATION, CITATION, CITATION, CITATION, CITATION, the Dimensionality Reduction model CITATION, and most recently a Two-Gate model CITATION.
All these models can be thought of as viewing the NPC as a Virtual Gate CITATION, CITATION, where the FG nups set up a barrier for entrance into the NPC and transport through the NPC involves facilitated diffusion controlled by association and disassociation of transport receptors with FG nups.
They differ only in specific assumptions, such as the conformation and spatial deployment of the FG nups, their physicochemical state, or the distribution of affinities of binding sites .
The aim of the present paper is to establish a general quantitative framework for NPC transport that is consistent with well-established structural and functional properties of the NPC and its components.
We explain how the binding of karyopherins to the FG nups' flexible filaments inside the NPC can give rise to efficient transport.
We demonstrate that competition for the limited space and binding sites within the NPC leads to a novel, highly selective filtering process.
Finally, we explain how the flexibility of the FG nups could account for the high robustness of NPC-mediated transport with respect to structural changes CITATION.
We conclude by discussing verifiable experimental predictions of the model.
The role that mechanistic mathematical modeling and systems biology will play in molecular medicine and clinical development remains uncertain.
In this study, mathematical modeling and sensitivity analysis were used to explore the working hypothesis that mechanistic models of human cascades, despite model uncertainty, can be computationally screened for points of fragility, and that these sensitive mechanisms could serve as therapeutic targets.
We tested our working hypothesis by screening a model of the well-studied coagulation cascade, developed and validated from literature.
The predicted sensitive mechanisms were then compared with the treatment literature.
The model, composed of 92 proteins and 148 protein protein interactions, was validated using 21 published datasets generated from two different quiescent in vitro coagulation models.
Simulated platelet activation and thrombin generation profiles in the presence and absence of natural anticoagulants were consistent with measured values, with a mean correlation of 0.87 across all trials.
Overall state sensitivity coefficients, which measure the robustness or fragility of a given mechanism, were calculated using a Monte Carlo strategy.
In the absence of anticoagulants, fluid and surface phase factor X/activated factor X activity and thrombin-mediated platelet activation were found to be fragile, while fIX/FIXa and fVIII/FVIIIa activation and activity were robust.
Both anti-fX/FXa and direct thrombin inhibitors are important classes of anticoagulants; for example, anti-fX/FXa inhibitors have FDA approval for the prevention of venous thromboembolism following surgical intervention and as an initial treatment for deep venous thrombosis and pulmonary embolism.
Both in vitro and in vivo experimental evidence is reviewed supporting the prediction that fIX/FIXa activity is robust.
When taken together, these results support our working hypothesis that computationally derived points of fragility of human relevant cascades could be used as a rational basis for target selection despite model uncertainty.
The role that mechanistic mathematical modeling and systems biology will play in molecular medicine and clinical development remains uncertain.
Kitano suggested that understanding of critical questions in biology required the integration of experimental and computational research CITATION.
Assmus et al. and others maintained that analysis of the dynamics of human relevant networks using predictive computer models and high-throughput data generation would play an increasingly important role in medical research and the elucidation of disease mechanisms CITATION, CITATION.
However, parametric and structural uncertainty remains an open challenge to mechanistic modeling in medicine.
Strategies that integrate experimental and computational techniques have had success at elucidating network structures.
Arm and Arkin reviewed experimental and computational techniques to uncover molecular interaction networks CITATION.
The central experimental advancements in the area of protein protein network identification have been the yeast two-hybrid system CITATION, CITATION and quantitative mass spectrometry proteomic techniques to determine protein complexes CITATION, CITATION.
Young and coworkers explored protein DNA interactions using the chromatin immunoprecipitation technique CITATION where likely transcription factor binding sites were determined using a combination of chromatin immunoprecipitation chips and DNA microarrays.
Time-lagged correlation matrices CITATION, CITATION, genetic programming techniques CITATION, and network decomposition strategies have also been used with time-series concentration measurements to estimate reaction network structures CITATION .
Sensitivity analysis has been used to integrate model identification and discrimination with optimal experimental design and knowledge discovery.
Cho et al. used sensitivity analysis to study TNF- mediated NF B signalling where parametric uncertainty was addressed using Monte Carlo sensitivity analysis; using the best-guess parameter set, a family of random parameter sets was generated where sensitivity coefficients were calculated for each member of the random family CITATION.
Cho et al. went on to develop a unifying framework, building upon the earlier work of Kholodenko et al. and Sontag et al. to unravel the functional interactions in biomolecular networks using a stimulus response strategy and metabolic control analysis CITATION CITATION.
Kremling et al. investigated the benchmark problem of growth of a microorganism in a continuous bioreactor subject to feed shifts using sensitivity-based model identification and discrimination strategies; they determined optimal experimental design and perturbation strategies to identify and discriminate between rival model formulations CITATION.
Gadkar et al. identified signal transduction models from time-course measurements using a nonlinear scheme to estimate missing protein measurements from measured values CITATION.
They went further and proposed strategies to calculate D-optimal experimental designs that maximized the experimental information used to identify signal transduction models as well as an iterative strategy to explore model structure CITATION, CITATION.
Sensitivity analysis has also been used to explore the robustness and fragility of metabolic and signaling networks.
Robustness, the ability to maintain system performance in the face of perturbation and uncertainty, is a desirable feature in both biological as well as man-made networks, machines, and systems CITATION.
Conversely, fragility, i.e., extreme sensitivity to small perturbations, is a very undesirable trait that could lead to catastrophic system failure following seemingly innocuous perturbations, e.g., a Boeing 777 crashing because of minor software failures or microscopic alterations in a few integrated chips CITATION.
Stelling et al. reviewed several examples of robustness in biological networks CITATION, while Leibler first computationally predicted and later experimentally verified robust features of chemotaxis control networks CITATION, CITATION.
Bullinger and coworkers explored the robustness of models of programmed cell death or apoptosis CITATION, while Stelling et al. computationally identified points of robustness and fragility, using Monte Carlo sensitivity analysis and overall state sensitivity coefficients, in models of circadian rhythm CITATION .
In this study, we use tools from systems biology, namely mathematical modeling and sensitivity analysis, to explore the working hypothesis that mechanistic models of human relevant cascades, despite model uncertainty, can be computationally screened for points of fragility, i.e., sensitive mechanisms, and that these mechanisms could serve as a rational basis for therapeutic target selection.
We test our working hypothesis by computationally screening a mechanistic model of the well-studied coagulation cascade developed and validated from literature sources.
After model validation, using 21 published datasets generated from two different quiescent in vitro coagulation models, we use Monte Carlo sensitivity analysis to computationally screen the model for sensitive mechanisms in the presence and absence of natural anticoagulants.
We then contrast the predicted fragile mechanisms with literature to determine if they are consistent with experimental investigation, thereby proving or disproving our working hypothesis.
While the current development is restricted to coagulation, the broader strategy is general and could be applied to an arbitrary network.
Coagulation, mediated by a family of serine proteases and a key group of blood cells, both of which are normally inactive in the circulation, is directly relevant to human health and has been suggested by Somogyi and Greller to be an ideal candidate for in silico drug discovery CITATION.
Insufficient coagulation is manifested in disorders such as haemophilia A, haemophilia B, or von Willebrand disease CITATION, CITATION.
Conversely, unwanted clotting can be a serious complication following surgical intervention and is directly involved in coronary artery diseases, which collectively account for 38 percent of all deaths in North America CITATION .
The salient features of the coagulation cascade included in our model, shown schematically in Figure 1 and presented in detail in Table 1, are reviewed here.
Several extensive reviews of the underlying biochemistry and cell biology of coagulation can be found elsewhere CITATION CITATION.
There are two pathways that lead to activation of the master protease thrombin and eventually to a clot the intrinsic and extrinsic cascades.
It is generally believed that the extrinsic cascade is the main mechanism of thrombinogenesis in the blood CITATION CITATION.
Upstream coagulation factors are activated by materials exposed because of vessel injury chief among these tissue factors CITATION ; TF and activated factor VIIa present in the blood form a complex that activates factor X and fIX.
FXa activates downstream factors, including fV, fVIII, and fIX.
FXa can also, along with FVa, form a complex on the surface of activated platelets that converts prothrombin to thrombin.
TF FVIIa is not the only mechanism to activate fX; FIXa and FVIIIa can complex on the surface of activated platelets and catalyze the formation of FXa.
Platelet localization at the wound site occurs through specific interactions between the platelet and the subendothelium, primarily through recognition of exposed materials such as collagen, fibronectin, and von Willebrand factor.
Localized platelets are activated by external signals such as adenosine diphosphate and thrombin.
Thrombin irreversibly activates platelets through a family of transmembrane receptors on the platelet surface called protease-activated receptors CITATION, CITATION.
Thrombin, in addition to playing a key role in platelet activation, catalyzes the conversion of fibrinogen to fibrin.
Fibrin, with the help of FVIIIa, forms a cross-linked mesh inside the platelet plug that stops blood flow.
Thrombin also activates upstream coagulation factors, thereby forming a strong positive feedback that ensures rapid activation.
Three control points that inhibit thrombin formation are considered in the model.
TF pathway inhibitor downregulates FXa formation and activity by sequestering free FXa and TF FVIIa in an FXa-dependent manner.
Antithrombin III neutralizes all serine proteases generated during the coagulation response, making it perhaps the most powerful control element in the cascade.
Thrombin itself plays an inadvertent role in its own inhibition by binding the surface protein thrombomodulin, expressed on normal vasculature CITATION.
The FIIa TM complex catalyzes the conversion of protein C to activated PC ; APC attenuates the coagulation response by the proteolytic cleavage of fV/FVa and fVIII/FVIIIa CITATION .
Accumulating infections of highly pathogenic H5N1 avian influenza in humans underlines the need to track the ability of these viruses to spread among humans.
A human-transmissible avian influenza virus is expected to cause clusters of infections in humans living in close contact.
Therefore, epidemiological analysis of infection clusters in human households is of key importance.
Infection clusters may arise from transmission events from the animal reservoir, humans who were infected by animals, or humans who were infected by humans.
Here we propose a method of analysing household infection data to detect changes in the transmissibility of avian influenza viruses in humans at an early stage.
The method is applied to an outbreak of H7N7 avian influenza virus in The Netherlands that was the cause of more than 30 human-to-human transmission events.
The analyses indicate that secondary human-to-human transmission is plausible for the Dutch household infection data.
Based on the estimates of the within-household transmission parameters, we evaluate the effectiveness of antiviral prophylaxis, and conclude that it is unlikely that all household infections can be prevented with current antiviral drugs.
We discuss the applicability of our method for the detection of emerging human-to-human transmission of avian influenza viruses in particular, and for the analysis of within-household infection data in general.
Outbreaks of highly pathogenic H5N1 avian influenza in Southeast Asia, Europe, and Africa have devastating consequences for poultry CITATION, CITATION, and have resulted in numerous infections in humans CITATION CITATION.
Although these infections from the animal reservoir continue to accumulate, the virus does not seem to spread extensively among humans.
Nevertheless, a fear is that these human infections may ultimately spark an influenza pandemic CITATION CITATION.
Indeed, recent clusters of infections in human households hint at the possibility of virus transmission from humans who were infected by poultry to their household contacts CITATION, CITATION.
These suggestions are strengthened by the observation of mutations in recent H5N1 viruses that seem to predispose the virus for more efficient transmission in mammals, including humans CITATION CITATION .
It is likely that a virus with pandemic potential will present itself initially through an increase in the number of infections in humans who have been in close contact with the case infected by animals.
Therefore, rapid detection and control of clusters of infections is of key importance CITATION, CITATION.
Such clusters may result from multiple introductions from the animal reservoir, multiple transmission events from humans who were infected by animals, or multiple transmission events from humans who were themselves infected by humans.
Obviously, evidence for is the most worrisome as it indicates that the virus has acquired the ability to spread efficiently in humans.
It is often thought that pathogens from the animal reservoir that have made the jump to a new host species are usually not well-adapted for sustained transmission in the new host, and that transmissibility in a new species will gradually increase over time by the process of adaptation by means of natural selection CITATION CITATION.
Interestingly, however, in the case of H5N1 avian influenza in humans, the evidence so far does not seem to fit this prediction CITATION CITATION.
Mechanisms that could be responsible for the lack of efficient secondary human-to-human transmission could be due to a dose effect whereby humans infected by animals receive a higher infection dose than humans infected by humans, or to behavioural changes after infection that limit spread of the virus after it has been detected.
In this paper we develop a method to detect and quantify different routes of virus transmission in a household setting.
Our main aim is to investigate whether within-household pathogen transmission has been restricted to transmission from the primary infected individual or whether there is evidence that the transmission chain has extended beyond the first generation of human-to-human infections.
Our analyses are based on theoretical developments on the distribution of the final size of an epidemic in finite populations, which allow construction of flexible methods to analyse within-household transmission chains.
We apply the method to a recent study of within-household transmission of highly pathogenic avian influenza of the H7N7 subtype that caused a large epidemic in poultry in The Netherlands in 2003.
Shortly after the detection of virus circulation, the Dutch authorities undertook an aggressive control strategy that consisted of an animal movement ban in the affected regions, tracing and screening suspected flocks, and culling of infected and contiguous flocks.
In all, a total of 255 flocks became infected during a period of nine weeks, and more than 30 million birds were culled CITATION, CITATION.
Subsequent studies of poultry workers revealed that at least 86 infections from the animal reservoir to humans had taken place CITATION CITATION.
In addition, more than 30 household contacts of the infected poultry workers who had not been in direct contact with poultry were reported positive.
These reports indicate that human-to-human transmission did occur from individuals infected from the animal reservoir.
Here we analyse data of the transmission chains in 24 households that led to 33 human-to-human transmission events, measuring the extent of onward transmission from humans who were infected by humans.
We complement the statistical analyses by systematic power analyses to obtain insight into the study size needed to be able to find significant secondary human-to-human transmission, given that it is present.
Although we have applied the method to a specific dataset, we believe that our method is of general interest as it enables rapid estimation of within-household transmission rates based on data that are easily gathered for most infectious diseases.
For instance, our method of analysis is not restricted to the analysis of emerging pandemic influenza, but it can just as well be used to estimate different routes of within-household transmission rates of human influenza A viruses CITATION CITATION and, importantly, to assess the potential effectiveness of control measures.
Noncoding RNAs are important functional RNAs that do not code for proteins.
We present a highly efficient computational pipeline for discovering cis-regulatory ncRNA motifs de novo.
The pipeline differs from previous methods in that it is structure-oriented, does not require a multiple-sequence alignment as input, and is capable of detecting RNA motifs with low sequence conservation.
We also integrate RNA motif prediction with RNA homolog search, which improves the quality of the RNA motifs significantly.
Here, we report the results of applying this pipeline to Firmicute bacteria.
Our top-ranking motifs include most known Firmicute elements found in the RNA family database.
Comparing our motif models with Rfam's hand-curated motif models, we achieve high accuracy in both membership prediction and base-pair level secondary structure prediction.
Of the ncRNA candidates not in Rfam, we find compelling evidence that some of them are functional, and analyze several potential ribosomal protein leaders in depth.
Recent discoveries of novel noncoding RNAs such as microRNAs and riboswitches suggest that ncRNAs have important and diverse functional and regulatory roles that impact gene transcription, translation, localization, replication, and degradation CITATION CITATION.
In the last few years, several groups have performed genome-scale computational ncRNA predictions based on comparative genomic analysis.
In particular, Barrick et al. CITATION used a pairwise, BLAST-based approach to discover novel riboswitch candidates in bacterial genomes, many of which now have been experimentally verified.
Similar studies have been conducted in various bacterial groups CITATION CITATION.
More recent work has extended these searches to eukaryotes CITATION CITATION, discovering a large number of known microRNAs while producing thousands of novel ncRNA candidates.
With some exceptions, such as CITATION and CITATION, these approaches follow a similar paradigm, which is to search for conserved secondary structures on multiple-sequence alignments that are constructed based on sequence similarity alone.
Typically, these schemes use measures such as mutual information between pairs of alignment columns to signal base-paired regions.
However, the signals such methods seek, namely compensatory base-pair mutations, are exactly the signals that may cause sequence-based alignment methods to misalign, or alternatively refuse to align, homologous ncRNA sequences.
Even local misalignments may weaken this key structural signal, making the methods sensitive to alignment quality, which is especially problematic on diverged sequences.
In this paper, we present a novel structure-oriented computational pipeline for genome-scale prediction of cis-regulatory ncRNAs.
It exploits, but does not require, sequence conservation.
The pipeline differs from previous methods in three respects.
First, it searches in unaligned upstream sequences of homologous genes, instead of well-aligned regions constructed by sequence-based methods.
Second, we predict RNA motifs in unaligned sequences using a tool called CMfinder CITATION, which is very sensitive to RNA motifs with low sequence conservation, and robust to inclusion of long flanking regions or unrelated sequences.
Finally, we integrate RNA motif prediction with RNA homology search.
For every predicted motif, we scan a genome database for more homologs, which are then used to refine the model.
This iterative process improves the model and expands the motif families automatically.
In this study, we apply this pipeline to discover ncRNA elements in prokaryotes.
We chose prokaryotes mainly because of the large number of fully sequenced genomes and the great sequence divergence among the species, which can be well-exploited by our approach.
Our approach has two key advantages.
First, it is efficient and highly automated.
Earlier steps are more computationally efficient than later steps, and we can apply filters between steps so that poor candidates are eliminated from subsequent analysis.
Thus, even though we use some computationally expensive algorithms, the pipeline is scalable to larger problems.
Besides providing RNA motif prediction, the pipeline also integrates gene context and functional analysis, which facilitates manual biological evaluation.
Second, this pipeline is highly accurate in finding prokaryotic ncRNAs, especially RNA cis-regulatory elements.
To demonstrate the performance of this approach, we report our search results in Firmicutes, a Gram-positive bacterial division that includes Bacillus subtilis, a relatively well-studied model organism with many known ncRNAs.
The method exhibits low false-positive rates on negative controls, and low false-negative rates on known Firmicute ncRNAs.
The RNA family database CITATION, a partially hand-curated database of noncoding RNAs, includes 13 ncRNA families categorized as cis-regulatory elements with representatives in B. subtilis.
Of these, 11 are included among our top 50 predictions and a 12th appears somewhat lower in our ranking.
Two other Rfam families are also represented among our top 50 predictions.
In addition, both the secondary structure prediction and identified family members are in excellent agreement with Rfam annotation.
For the 14 Rfam families mentioned above, we achieved 91 percent specificity and 84 percent sensitivity on average in identifying family members, and 77 percent specificity and 75 percent sensitivity in secondary structure prediction.
Many promising novel ncRNA candidates were also discovered and are discussed below.
An important element of the developing field of proteomics is to understand protein-protein interactions and other functional links amongst genes.
Across-species correlation methods for detecting functional links work on the premise that functionally linked proteins will tend to show a common pattern of presence and absence across a range of genomes.
We describe a maximum likelihood statistical model for predicting functional gene linkages.
The method detects independent instances of the correlated gain or loss of pairs of proteins on phylogenetic trees, reducing the high rates of false positives observed in conventional across-species methods that do not explicitly incorporate a phylogeny.
We show, in a dataset of 10,551 protein pairs, that the phylogenetic method improves by up to 35 percent on across-species analyses at identifying known functionally linked proteins.
The method shows that protein pairs with at least two to three correlated events of gain or loss are almost certainly functionally linked.
Contingent evolution, in which one gene's presence or absence depends upon the presence of another, can also be detected phylogenetically, and may identify genes whose functional significance depends upon its interaction with other genes.
Incorporating phylogenetic information improves the prediction of functional linkages.
The improvement derives from having a lower rate of false positives and from detecting trends that across-species analyses miss. Phylogenetic methods can easily be incorporated into the screening of large-scale bioinformatics datasets to identify sets of protein links and to characterise gene networks.
Evidence that two or more traits co-evolve across a range of species can be used to test hypotheses about the common selective pressures acting on the traits, and about the functional or adaptive relationship between them.
Correlated evolution is increasingly being applied at the genetic level on the premise that genes that are gained and lost together , or that show similar expression patterns or rates of evolution , may form a functional linkage.
This provides a computational approach that can screen large genomic datasets for functional links  and help to identify the functions of uncharacterised genes.
Such analyses can also be used to describe metabolic networks , and discover gene modules or clusters of genes engaged in a common function .
Genes and their expression patterns evolve in a phylogenetic context such that functional links of adaptive value tend to be conserved and inherited by descendant species.
Among closely related species, shared phylogenetic inheritance can also produce correlated gene profiles for genes that are not linked.
Two or more genes might arise independently in a common ancestor and be retained in evolutionary descendants owing to their individual adaptive functions.
Figure 1 shows how this can produce spurious evidence of a functional link when measured across species.
By comparison, multiple independent phylogenetic events of the gain/loss of pairs of genes make a compelling statistical case for a functional link.
Phylogenetic methods have uses beyond merely accounting for shared inheritance: they make it possible to investigate ancestral states and to identify the probable temporal ordering of changes in two traits.
Knowledge of which of two traits changed first in the evolutionary history the phylogeny describes can be used to test ideas about cause and effect or the dependency of one trait on another .
Our interest is to evaluate whether incorporating phylogenetic information improves the identification of functional gene links.
The need to take account of phylogenetic relationships in comparative studies has long been appreciated in evolutionary biology  but has received less attention in bioinformatics studies . We apply the phylogenetic-statistical method Discrete , for assessing correlated evolution among pairs of discrete traits, to data on the presence and absence of pairs of genes.
The method identifies independent events of correlated evolution on a phylogeny by comparing the statistical likelihood of the observed data under two alternative scenarios, one in which the two genes are allowed to evolve on the phylogeny independently, and another in which they co-evolve.
Trait evolution is modelled as a continuous-time Markov process, and evidence for the model of correlated evolution is assessed by means of the likelihood ratio statistic.
Our dataset consists of a phylogeny of 15 eukaryote species for which complete or nearly complete sequenced genomes are available.
There is no limit to the number of species that can be used, but it is important to use fully sequenced and well-annotated genomes to ensure that genes determined to be absent are in fact not in the genome.
We compare the phylogenetic method's predictions to predictions derived from across-species correlations; the latter have been used in bioinformatics investigations to predict functional gene links . We use the Munich Information Center for Protein Sequences  database of annotated complexes of yeast proteins as a known criterion measure.
The MIPS functional links have been determined by low-throughput laboratory procedures and therefore provide a reliable collection of functional links in this species.
We find that incorporating phylogenetic information improves predictions by up to 35 percent over across-species correlations in detecting functional links, and increasingly so for pairs of genes with greater phylogenetic evidence of a functional link.
The number of times a pair of genes has been independently gained or lost on the phylogeny is a strong predictor of functional linkage, such that protein pairs with at least two to three correlated events are almost certainly functionally linked.
Advances in the computational identification of functional noncoding polymorphisms will aid in cataloging novel determinants of health and identifying genetic variants that explain human evolution.
To date, however, the development and evaluation of such techniques has been limited by the availability of known regulatory polymorphisms.
We have attempted to address this by assembling, from the literature, a computationally tractable set of regulatory polymorphisms within the ORegAnno database.
We have further used 104 regulatory single-nucleotide polymorphisms from this set and 951 polymorphisms of unknown function, from 2-kb and 152-bp noncoding upstream regions of genes, to investigate the discriminatory potential of 23 properties related to gene regulation and population genetics.
Among the most important properties detected in this region are distance to transcription start site, local repetitive content, sequence conservation, minor and derived allele frequencies, and presence of a CpG island.
We further used the entire set of properties to evaluate their collective performance in detecting regulatory polymorphisms.
Using a 10-fold cross-validation approach, we were able to achieve a sensitivity and specificity of 0.82 and 0.71, respectively, and we show that this performance is strongly influenced by the distance to the transcription start site.
Our ability to identify the molecular mechanisms responsible for specific genetic traits within our population will be enhanced by our imminent ability to decipher each individual's genome.
This is evident from recent advances in sequencing and genotyping technologies, which allow an increasing number of variants to be sampled for association and linkage and contribute a growing number of sources of variation and their frequencies to public databases each year.
As new variants are identified, each becomes a molecular window into our past, present, and future each aids in tracing our genetic heritage and in charting the footsteps of our common evolution, and possesses the potential to predict disease or drug susceptibilities, ideally acting as an early-warning system in preventative medical practice.
However, our ability to catalog genotypes has far outstripped our ability to implicate them in phenotypes.
Currently, more than 6 million unique single-nucleotide polymorphisms are included in version 126 of dbSNP CITATION ; of these SNPs, only a very small fraction have been associated with a phenotype using genetic association or linkage analysis.
This is because association studies are costly, time-consuming, and dependent on the frequency of the genotype in the sampled population.
Furthermore, many SNPs are not necessarily expected to have a function.
To select candidates for functional validation, computational methods have been developed to identify SNPs that alter the protein-coding structure of genes CITATION CITATION.
These types of computational methods tend to prioritize putative functional SNPs by identifying those SNPs that alter a protein's amino acid sequence, are located within well-conserved regions or functional protein domains, and alter the biochemical structure of the protein.
However, very few methods identify regulatory SNPs that alter the expression of genes.
Such rSNPs have been implicated in the etiology of several human diseases, including cancer CITATION, CITATION, depression CITATION, systemic lupus erythematosus CITATION, perinatal HIV-1 transmission CITATION, and response to type 1 interferons CITATION.
This work aims to extend computer-based techniques to identify this particular class of functional variants within the core promoter regions of human genes.
Conventional computational approaches to rSNP classification have predominantly relied on allele-specific differences in the scoring of transcription factor weight matrices as supplied from databases such as TRANSFAC and Jaspar CITATION, CITATION, CITATION.
SNPs located within matrix positions possessing high information content are assumed more likely to be functional.
Support for this hypothesis to date, however, has been restricted to single-case examples.
Furthermore, a recent study has failed to detect significant weight matrix signals in 65 percent of regulatory polymorphisms CITATION.
However, the prevailing hypothesis in computational regulatory element prediction has been that the majority of predictions using unrestricted application of matrix-based approaches are false positives.
By extending this technique and using phylogenetic footprinting between mouse and human, it was demonstrated that from ten SNPs that show significant allele-specific differences in Jaspar predictions, seven also demonstrated electrophoretic mobility shift differences CITATION.
However, only two of the seven had a marked effect in reporter gene assays.
Conservation alone has also been demonstrated as a poor discriminant of function in a study of regulatory polymorphisms in Eukaryotic Promoter Database promoters, where zero of ten experimentally validated regulatory variants were in conserved binding sites CITATION .
A substantial challenge with developing strategies for identifying functional noncoding variants has been the shortage of characterized regulatory variants.
Few studies have successfully identified the causative variant after a susceptibility haplotype is identified.
To address this problem, we have assembled the largest openly available collection of functional regulatory polymorphisms within the ORegAnno database CITATION.
From this dataset, we have examined several features of these SNPs as they relate to polymorphisms of unknown function within the promoter regions of associated genes.
Our hypothesis is that using a combination of regulatory and population genetics properties, the discriminative efficacy of individual properties can be evaluated, and significant predictors of rSNP function can be chosen.
Within our assayed set, we have found that the best discriminants are the distance to the transcription start site, local repetitive density and content, sequence conservation, minor allele frequency and derived allele frequency, and CpG island presence.
Notably, the unrestricted application of a matrix-based approach is demonstrated to be one of the least effective classifiers.
We have used this dataset of rSNPs and their properties to train a support vector machine classifier.
Two approaches were used to train the classifier: one in which the properties of all rSNPs were compared with that of all the ufSNPs, and one in which each property value of the positive SNPs and ufSNPs within an associated gene were compared with the average values for each property within that gene.
The All approach is designed to determine if there are any properties that are important across the test set, while the Group approach is designed to determine if there are important directional shifts in values within a promoter that may discriminate functional SNPs from ufSNPs.
In a 10-fold cross-validated test, the SVM achieves a receiver operating characteristic value of 0.83 0.05 for the All analysis and 0.78 0.04 for the Group analysis .
Cell polarity is a general cellular process that can be seen in various cell types such as migrating neutrophils and Dictyostelium cells.
The Rho small GTPases have been shown to regulate cell polarity; however, its mechanism of emergence has yet to be clarified.
We first developed a reaction diffusion model of the Rho GTPases, which exhibits switch-like reversible response to a gradient of extracellular signals, exclusive accumulation of Cdc42 and Rac, or RhoA at the maximal or minimal intensity of the signal, respectively, and tracking of changes of a signal gradient by the polarized peak.
The previous cell polarity models proposed by Subramanian and Narang show similar behaviors to our Rho GTPase model, despite the difference in molecular networks.
This led us to compare these models, and we found that these models commonly share instability and a mass conservation of components.
Based on these common properties, we developed conceptual models of a mass conserved reaction diffusion system with diffusion driven instability.
These conceptual models retained similar behaviors of cell polarity in the Rho GTPase model.
Using these models, we numerically and analytically found that multiple polarized peaks are unstable, resulting in a single stable peak, and that sensitivity toward changes of a signal gradient is specifically restricted at the polarized peak.
Although molecular networks may differ from one cell type to another, the behaviors of cell polarity in migrating cells seem similar, suggesting that there should be a fundamental principle.
Thus, we propose that a mass conserved reaction diffusion system with diffusion-driven instability is one of such principles of cell polarity.
Eukaryotic cells such as neutrophils and Dictyostelium cells respond to temporal and spatial gradients of extracellular signals with directional movements CITATION CITATION.
This process, known as chemotaxis, is a fundamental cellular process CITATION, CITATION CITATION.
In a migrating cell, specific molecular events take place at the front and back edges CITATION, CITATION, CITATION, CITATION.
The spatially distinctive molecular accumulation inside cells is known as cell polarity.
The front back polarity usually has one axis, and this uniqueness is an important property because a migrating cell with two fronts could not move effectively CITATION.
Another behavior of the front back polarity is higher sensitivity of the front to a gradient of extracellular signals CITATION, CITATION.
This would also be important because the direction of movement should be controlled at the front edge.
Many molecules that are involved in chemotaxis in mammalian cells have been identified CITATION, CITATION.
Some molecules, including phosphoinositide 3-kinase, phosphatidylinositol 3,4,5-triphosphate, Cdc42, Rac, and F-actin, are specifically localized at the front, whereas others, including phosphatase and tensin homologue deleted on Chromosome 10 and RhoA, are at the back of migrating cells CITATION, CITATION, CITATION, CITATION CITATION.
The Rho family of small GTPases in particular play a central role in chemotaxis and in establishing cell polarity CITATION CITATION.
However, the mechanism of generating spatial accumulation of the Rho GTPases in cell polarity has yet to be clarified.
Many mathematical models that account for gradient sensing and signal amplification in cell polarity have been proposed CITATION.
The local excitation and global inhibition model has been proposed to explain spatial gradient sensing CITATION, CITATION.
Some models involve positive feedback loops for amplified accumulation of signaling molecules CITATION CITATION.
A reaction diffusion model that includes local self-enhancement and long-range antagonistic effects has been proposed for directional sensitivity CITATION.
Most of the reported models of cell polarity, which involve the detailed parameters such as concentrations or rate constants, have been constructed with many parameters and equations.
Although these detailed models are at least partially successful in reproducing experimental observations in cell polarity, the theoretical essence underlying cell polarity has not been explicitly demonstrated; thus, a simple conceptual model that can be used for analytical study is needed to extract common principles in cell polarity.
Although the reported models consist of distinct molecular species or networks, it should be especially emphasized that many of them are able to exhibit similar behaviors of cell polarity regardless of their different frameworks.
This fact indicates that a common principle should underlie the models, and a conceptual model is suitable for extracting common principles in cell polarity.
Because the Rho small GTPases are key regulators for cell polarity CITATION, CITATION, we first developed a reaction diffusion model of the Rho GTPases on the basis of an earlier model CITATION to examine the spatial properties of the Rho GTPases.
We found that the interaction of the Rho GTPases per se can generate specific spatial accumulation of the Rho GTPases, and that our model shows important behaviors of cell polarity.
We also found that our model exhibits behaviors similar to the model by Narang and Subramanian CITATION, CITATION, which is based on the molecular networks that are different from ours.
This suggests that common principles should underlie both models.
We found that a mass conservation of components and diffusion-driven instability are commonly conserved in the Narang and Subramanian models and in our model.
Based on these common properties, we established conceptual models of a mass conserved reaction diffusion system, and found that such properties can account for the critical behaviors of cell polarity.
These results strongly suggest that a mass conservation of components with diffusion-driven instability is one of the fundamental principles of cell polarity.
Computational methods for discovery of sequence elements that are enriched in a target set compared with a background set are fundamental in molecular biology research.
One example is the discovery of transcription factor binding motifs that are inferred from ChIP chip measurements.
Several major challenges in sequence motif discovery still require consideration: the need for a principled approach to partitioning the data into target and background sets; the lack of rigorous models and of an exact p-value for measuring motif enrichment; the need for an appropriate framework for accounting for motif multiplicity; the tendency, in many of the existing methods, to report presumably significant motifs even when applied to randomly generated data.
In this paper we present a statistical framework for discovering enriched sequence elements in ranked lists that resolves these four issues.
We demonstrate the implementation of this framework in a software application, termed DRIM, which identifies sequence motifs in lists of ranked DNA sequences.
We applied DRIM to ChIP chip and CpG methylation data and obtained the following results.
Identification of 50 novel putative transcription factor binding sites in yeast ChIP chip data.
The biological function of some of them was further investigated to gain new insights on transcription regulation networks in yeast.
For example, our discoveries enable the elucidation of the network of the TF ARO80.
Another finding concerns a systematic TF binding enhancement to sequences containing CA repeats.
Discovery of novel motifs in human cancer CpG methylation data.
Remarkably, most of these motifs are similar to DNA sequence elements bound by the Polycomb complex that promotes histone methylation.
Our findings thus support a model in which histone methylation and CpG methylation are mechanistically linked.
Overall, we demonstrate that the statistical framework embodied in the DRIM software tool is highly effective for identifying regulatory sequence elements in a variety of applications ranging from expression and ChIP chip to CpG methylation data.
DRIM is publicly available at LINK.
This paper examines the problem of discovering interesting sequence motifs in biological sequence data.
A widely accepted and more formal definition of this task is: given a target set and a background set of sequences, identify sequence motifs that are enriched in the target set compared with the background set.
The purpose of this paper is to extend this formulation and to make it more flexible so as to enable the determination of the target and background set in a data driven manner.
Discovery of sequences or attributes that are enriched in a target set compared with a background set has become increasingly useful in a wide range of applications in molecular biology research.
For example, discovery of DNA sequence motifs that are overabundant in a set of promoter regions of co-expressed genes can suggest an explanation for this co-expression.
Another example is the discovery of DNA sequences that are enriched in a set of promoter regions to which a certain transcription factor binds strongly, inferred from chromatin immuno-precipitation on a microarray CITATION measurements.
The same principle may be extended to many other applications such as discovery of genomic elements enriched in a set of highly methylated CpG island sequences CITATION .
Due to its importance, this task of discovering enriched DNA subsequences and capturing their corresponding motif profile has gained much attention in the literature.
Any approach to motif discovery must address several fundamental issues.
The first issue is the way by which motifs are represented.
There are several strategies for motif representation: using a k-mer of IUPAC symbols where each symbol represents a fixed set of possible nucleotides at a single position or using a position weight matrix, which specifies the probability of observing each nucleotide at each motif position.
Both representations assume base position independence.
Alternatively, higher order representations that capture positional dependencies have been proposed.
While these representations circumvent the position independence assumption, they are more vulnerable to overfitting and lack of data for determining model parameters.
The method described in this paper uses the k-mer model with symbols above IUPAC.
The second issue is devising a motif scoring scheme.
Many strategies for scoring motifs have been suggested in the literature.
One simple yet powerful approach uses the hypergeometric distribution for identifying enriched motif kernels in a set of sequences and then expanding these motifs using an EM algorithm CITATION.
The framework described in this paper is a natural extension of the approach of CITATION.
YMF CITATION, CITATION is an exhaustive search algorithm which associates each motif with a z-score.
AlignACE CITATION uses a Gibbs sampling algorithm for finding global sequence alignments and produces a MAP score.
This score is an internal metric used to determine the significance of an alignment.
MEME CITATION uses an expectation maximization strategy and outputs the log-likelihood and relative entropy associated with each motif.
Once a scoring scheme is devised, a defined motif search space is scanned and motifs with significantly high scores are identified.
To determine the statistical significance of the obtained scores, many methods resort to simulations or ad hoc thresholds.
Several excellent reviews narrate the different strategies for motif detection and use quantitative benchmarking to compare their performance CITATION CITATION.
A related aspect of motif discovery, which is outside the scope of this paper, focuses on properties of clusters and modules of TF binding sites.
Examples of approaches that search for combinatorial patterns and modules underlying TF binding and gene expression include CITATION CITATION .
Drug molecules not only interact with specific targets, but also alter the state and function of the associated biological network.
How to design drugs and evaluate their functions at the systems level becomes a key issue in highly efficient and low side-effect drug design.
The arachidonic acid metabolic network is the network that produces inflammatory mediators, in which several enzymes, including cyclooxygenase-2, have been used as targets for anti-inflammatory drugs.
However, neither the century-old nonsteriodal anti-inflammatory drugs nor the recently revocatory Vioxx have provided completely successful anti-inflammatory treatment.
To gain more insights into the anti-inflammatory drug design, the authors have studied the dynamic properties of arachidonic acid metabolic network in human polymorphous leukocytes.
Metabolic flux, exogenous AA effects, and drug efficacy have been analyzed using ordinary differential equations.
The flux balance in the AA network was found to be important for efficient and safe drug design.
When only the 5-lipoxygenase inhibitor was used, the flux of the COX-2 pathway was increased significantly, showing that a single functional inhibitor cannot effectively control the production of inflammatory mediators.
When both COX-2 and 5-LOX were blocked, the production of inflammatory mediators could be completely shut off.
The authors have also investigated the differences between a dual-functional COX-2 and 5-LOX inhibitor and a mixture of these two types of inhibitors.
Their work provides an example for the integration of systems biology and drug discovery.
Nonsteriodal anti-inflammatory drugs are widely used for the treatment of musculoskeletal pain and other conditions.
In the US, more than 1 percent of the population uses NSAIDs daily CITATION, and the market for NSAIDs now amounts to more than $6 billion annually worldwide CITATION.
Although NSAIDs do alleviate the aches and pains, these drugs have undesirable side effects on the gastrointestinal tract and the central nervous system in addition to the potential exacerbation of conditions such as asthma CITATION.
The findings that cyclooxygenase-2 plays a major role in inflammation, and that inhibition of COX-1 causes gastrointestinal toxicity and mild bleeding diathesis CITATION, had suggested that selective COX-2 inhibitor would be an effective anti-inflammatory drug with low gastrointestinal side effects CITATION.
Ironically, the unexpected cardiovascular side effects of selective COX-2 inhibitors have surfaced CITATION, CITATION.
Thus, on September 30, 2004, Merck Company announced a voluntary withdrawal of the company's COX-2 inhibitor, VIOXX CITATION.
Other FDA-approved COX-2 inhibitors, such as celecoxib and valdecoxib, are being re-evaluated CITATION CITATION.
Despite years of studies, safe anti-inflammatory drug design remains a great challenge.
Failures in anti-inflammatory drug design illustrate the limitations of the current drug discovery paradigm.
A steady waning in the productivity of the pharmaceutical industry in the past decade has been observed.
This decline coincides with the introduction of target-based drug discovery CITATION.
Recently, medicinal chemists have started to think about drug discovery from a systems biology perspective CITATION, CITATION.
Studying the cross-talks between biological responses rather than one by one may provide a better understanding of disease development and achieve accurate evaluation on drug efficacy and toxicity CITATION, CITATION.
This new approach has been applied to safe drug design CITATION, CITATION.
For example, the former SmithKline Beecham focused on the blood coagulation cascade biochemical network CITATION, CITATION.
Armed with a good understanding of the disease from the regulatory network level, the company used model predictions to develop a fully humanized anti Factor IX antibody that has entered clinical trials.
Rajasethupathy et al. have recently reviewed advances in the practical applications of systems biology to drug discovery CITATION.
These researchers promote the development of network-based drug design, which devises drug-treatment strategies from the level of the disease system using computational models and high-throughput experiments.
In this paper, we study the dynamic properties of the arachidonic acid metabolic network in human polymorphonuclear leukocytes in the hope of gaining more insights into anti-inflammatory drug design.
An ordinary differential equation model of the AA metabolic network was developed.
Flux analysis and simulation on the addition of exogenous AA were performed to study the network balance.
The therapeutic effects of anti-inflammatory inhibitors were simulated, and the difference between dual functional COX-2 and 5-lipoxygenase inhibitors and the mixture of these two types of inhibitors was studied.
Corresponding experiments on the introduction of exogenous AA, COX-2, and 5-LOX inhibitors were performed and were found to be consistent with model predictions.
Our work shows that flux balance is important for the efficacy and safety of the drugs.
Compared with traditional single-target drugs, drugs against multiple targets can control the network balance and lead to safer treatment.
To maintain a stable intracellular environment, cells utilize complex and specialized defense systems against a variety of external perturbations, such as electrophilic stress, heat shock, and hypoxia, etc. Irrespective of the type of stress, many adaptive mechanisms contributing to cellular homeostasis appear to operate through gene regulatory networks that are organized into negative feedback loops.
In general, the degree of deviation of the controlled variables, such as electrophiles, misfolded proteins, and O 2, is first detected by specialized sensor molecules, then the signal is transduced to specific transcription factors.
Transcription factors can regulate the expression of a suite of anti-stress genes, many of which encode enzymes functioning to counteract the perturbed variables.
The objective of this study was to explore, using control theory and computational approaches, the theoretical basis that underlies the steady-state dose response relationship between cellular stressors and intracellular biochemical species in these gene regulatory networks.
Our work indicated that the shape of dose response curves depends on changes in the specific values of local response coefficients distributed in the feedback loop.
Multimerization of anti-stress enzymes and transcription factors into homodimers, homotrimers, or even higher-order multimers, play a significant role in maintaining robust homeostasis.
Moreover, our simulation noted that dose response curves for the controlled variables can transition sequentially through four distinct phases as stressor level increases: initial superlinear with lesser control, superlinear more highly controlled, linear uncontrolled, and sublinear catastrophic.
Each phase relies on specific gain-changing events that come into play as stressor level increases.
The low-dose region is intrinsically nonlinear, and depending on the level of local gains, presence of gain-changing events, and degree of feedforward gene activation, this region can appear as superlinear, sublinear, or even J-shaped.
The general dose response transition proposed here was further examined in a complex anti-electrophilic stress pathway, which involves multiple genes, enzymes, and metabolic reactions.
This work would help biologists and especially toxicologists to better assess and predict the cellular impact brought about by biological stressors.
Cells in vivo must maintain a relatively stable intracellular milieu in an extracellular environment that is constantly changing and is potentially unpredictable.
Notably, many intracellular biomolecules need to be held within closely regulated ranges of concentrations for normal cell functions.
Examples of these biochemical species, which could be detrimental and/or beneficial to cellular health, are electrophiles, reactive oxygen species, DNA adducts, misfolded proteins, O 2, and glucose.
When external stressors cause these molecules to deviate from their basal operating concentrations for an extended period of time, normal cell functions become disrupted, and cell cycle arrest and apoptosis may ensue CITATION.
Homeostatic regulation of vital intracellular biochemical species appears to operate primarily via gene regulatory networks that respond specifically to particular types of physical/chemical insults, such as electrophilic chemicals, heat shock, hypoxia, and hyperosmolarity CITATION CITATION.
As with many manmade control devices, such as thermostats and automobile cruise controls, these homeostatic gene regulatory networks are usually organized into negative feedback circuits that can be generalized into a common control scheme.
The output of the system, referred to as controlled variable, is the biochemical species that is perturbed by external stressors and therefore needs to be tightly controlled.
The system contains specific transcription factors that serve as transducers to either directly or indirectly sense the level of the controlled variable.
In this fashion, alterations in the concentration of the controlled variable affect the activity or abundance of the transcription factor.
Activated transcription factors then upregulate expression of individual or suites of anti-stress genes, many of which encode enzymes that participate in an array of interconnected biochemical reactions to counteract the perturbation of the controlled variable.
Control and dynamic system theory has benefited applied fields such as electronic and mechanical engineering for many decades, and in recent years increasing efforts have been made to apply similar concepts to biological systems including adaptive responses CITATION CITATION.
Our goal is to understand nature's design principle for anti-stress cellular homeostasis and to improve prediction of the disrupting effects of biological stressors.
Of practical importance for risk assessment at the cellular level is the steady-state dose response relationship between stressor levels and various measurable biochemical endpoints including the controlled variables, transcription factors, and gene expression.
Cell responses in the low-dose region are particularly relevant to human health risk assessment, and it is traditionally difficult to explain and predict dose response behaviors in this region due to uncertainty and subtlety of the curvature.
To accurately describe and fully understand complex dose response behaviors, the underlying biochemical networks will have to be examined through quantitative models.
With respect to the mathematical approaches involved, theoretical development in quantitative analysis of controls in biochemical networks, including metabolic control analysis and biochemical systems theory, has proven to be of great value CITATION CITATION.
Using numerical simulation and concepts from MCA, BST, and classical control theory, the present study focused on understanding the quantitative basis for the steady-state dose response in an anti-stress gene regulatory network.
While some of the conclusions presented in this paper may seem implicitly familiar, or even obvious, to engineers, they nonetheless provide an important framework by which biologists and especially toxicologists can improve the accuracy with which they evaluate the influence of biological stressors on intracellular control processes under different exposure conditions.
Alternative splicing and gene duplication both are processes that diversify the protein repertoire.
Recent examples have shown that sequence changes introduced by AS may be comparable to those introduced by GD.
In addition, the two processes are inversely correlated at the genomic scale: large gene families are depleted in splice variants and vice versa.
All together, these data strongly suggest that both phenomena result in interchangeability between their effects.
Here, we tested the extent to which this applies with respect to various protein characteristics.
The amounts of AS and GD per gene are anticorrelated even when accounting for different gene functions or degrees of sequence divergence.
In contrast, the two processes appear to be independent in their influence on variation in mRNA expression.
Further, we conducted a detailed comparison of the effect of sequence changes in both alternative splice variants and gene duplicates on protein structure, in particular the size, location, and types of sequence substitutions and insertions/deletions.
We find that, in general, alternative splicing affects protein sequence and structure in a more drastic way than gene duplication and subsequent divergence.
Our results reveal an interesting paradox between the anticorrelation of AS and GD at the genomic level, and their impact at the protein level, which shows little or no equivalence in terms of effects on protein sequence, structure, and function.
We discuss possible explanations that relate to the order of appearance of AS and GD in a gene family, and to the selection pressure imposed by the environment.
Alternative splicing and gene duplication are two main contributors to the diversity of the protein repertoire with enormous impact on protein sequence, structure, and function CITATION CITATION.
Interestingly, several recent studies point to a direct equivalence between AS and GD.
There are some cases where alternative splice variants in one organism are similar to gene duplicates in another organism CITATION CITATION.
For example, the eukaryotic splicing factor U2AF35 has at least two functional splice variants in human, U2AF35a and U2AF35b, which differ by seven amino acids in the RNA recognition motif.
The fugu orthologue U2AF35-a has no splice variant; instead there is a duplicate gene U2AF35-b with changes identical to those found in the human splice variant U2AF35b CITATION .
Further, the changes introduced to a sequence are constrained by the need to preserve a stable and functional three-dimensional fold CITATION.
Indeed, structural studies have shown that insertions and deletions between gene duplicates tend to happen at sequence locations where they are less damaging CITATION, such as loops at solvent-accessible locations.
These restrictions will apply irrespective of the source of the changes and thus may introduce a certain degree of similarity between the sequence changes associated with GD and AS.
Finally, recent studies have shown that AS and GD are inversely correlated on a genome-wide scale CITATION, CITATION, i.e., small gene families tend to have more genes with alternative splice variants than do large families.
These findings together i.e., anecdotal examples, structural constraints, and anticorrelation at the genomic level suggest that AS and GD are interchangeable sources of functional diversification CITATION.
Genes with AS would not need to produce additional variants in the form of duplicates, and vice versa.
Here, we first tested the anticorrelation between AS and GD with respect to sequence divergence, function, and gene expression.
Second, we studied the interchangeability hypothesis at the protein structure level and asked to what extent AS and GD introduce changes to the sequence that are equivalent in their nature and effect on structure and function.
To this end, we conducted a large-scale comparison of the effects of AS and GD on human and mouse proteins.
For the vast majority of cases, the two processes result in different protein modifications with different functional implications.
This finding, while consistent with the different molecular mechanisms underlying both phenomena, contradicts the anticorrelation observed at the genomic level.
We discuss some possible explanations for this paradox.
Whole-genome transporter analyses have been conducted on 141 organisms whose complete genome sequences are available.
For each organism, the complete set of membrane transport systems was identified with predicted functions, and classified into protein families based on the transporter classification system.
Organisms with larger genome sizes generally possessed a relatively greater number of transport systems.
In prokaryotes and unicellular eukaryotes, the significant factor in the increase in transporter content with genome size was a greater diversity of transporter types.
In contrast, in multicellular eukaryotes, greater number of paralogs in specific transporter families was the more important factor in the increase in transporter content with genome size.
Both eukaryotic and prokaryotic intracellular pathogens and endosymbionts exhibited markedly limited transport capabilities.
Hierarchical clustering of phylogenetic profiles of transporter families, derived from the presence or absence of a certain transporter family, showed that clustering patterns of organisms were correlated to both their evolutionary history and their overall physiology and lifestyles.
Membrane transport systems play essential roles in cellular metabolism and activities.
Transporters function in the acquisition of organic nutrients, maintenance of ion homeostasis, extrusion of toxic and waste compounds, environmental sensing and cell communication, and other important cellular functions CITATION.
Various transport systems differ in their putative membrane topology, energy coupling mechanisms, and substrate specificities CITATION.
Among the prevailing energy sources are adenosine triphosphate, phosphoenolpyruvate, and chemiosmotic energy in the form of sodium ion or proton electrochemical gradients.
The transporter classification system represents a systematic approach to classify transport systems according to their mode of transport, energy coupling mechanism, molecular phylogeny, and substrate specificity CITATION CITATION.
Transport mode and energy coupling mechanism serve as the primary basis for classification because of their relatively stable characteristics.
There are four major classes of solute transporters in the transporter classification system: channels, primary transporters, secondary transporters, and group translocators.
Transporters of unknown mechanism or function are included as a distinct class.
Channels are energy-independent transporters that transport water, specific types of ions, or hydrophilic small molecules down a concentration or electrical gradient; they have higher rates of transport and lower stereospecificity than the other transporter classes.
Primary active transporters couple the transport process to a primary source of energy.
Secondary transporters utilize an ion or solute electrochemical gradient, e.g., proton/sodium motive force, to drive the transport process.
E. coli LacY lactose permease CITATION, CITATION is probably one of the best characterized secondary transporters CITATION.
Group translocators modify their substrates during the transport process.
For example, E. coli MtlA mannitol PTS transporter phosphorylates exogenous mannitol using phosphoenolpyruvate as the phosphoryl donor and energy source and releases the phosphate ester, mannitol-1-P, into the cell cytoplasm CITATION, CITATION.
Each transporter class is further classified into individual families and subfamilies according to their function, phylogeny, and/or substrate specificity CITATION .
Since the advent of genomic sequencing technologies, the complete sequences of over 200 prokaryotic and eukaryotic genomes have been published to date, representing a wide range of species from archaea to human.
There are also more than 1,100 additional genome sequencing projects currently underway around the world CITATION, CITATION.
Convenient and effective computational methods are required to handle and analyze the immense amount of data generated by the whole-genome sequencing projects.
An in-depth look at transport proteins is vital to the understanding of the metabolic capability of sequenced organisms.
However, it is often problematic to annotate these transport proteins by current primary annotation methods because of the occurrence of large and complex transporter gene families, such as the ATP-binding cassette superfamily CITATION, CITATION and the major facilitator superfamily CITATION, CITATION, and the presence of multiple transporter gene paralogs in many organisms.
We have been working on a systematic genome-wide analysis of cellular membrane transport systems.
Previously, we reported a comprehensive analysis of the transport systems in 18 prokaryotic organisms CITATION, CITATION and in yeast CITATION.
Here we expand our analyses to 141 species and compare the fundamental differences in membrane transport systems in prokaryotes and eukaryotes.
Phylogenetic profiling of transporter families and predicted substrates was utilized to investigate the relevance of transport capabilities to the overall physiology of prokaryotes and eukaryotes.
Previous modeling studies have identified the vaccination coverage level necessary for preventing influenza epidemics, but have not shown whether this critical coverage can be reached.
Here we use computational modeling to determine, for the first time, whether the critical coverage for influenza can be achieved by voluntary vaccination.
We construct a novel individual-level model of human cognition and behavior; individuals are characterized by two biological attributes that they use when making vaccination decisions.
We couple this model with a population-level model of influenza that includes vaccination dynamics.
The coupled models allow individual-level decisions to influence influenza epidemiology and, conversely, influenza epidemiology to influence individual-level decisions.
By including the effects of adaptive decision-making within an epidemic model, we can reproduce two essential characteristics of influenza epidemiology: annual variation in epidemic severity and sporadic occurrence of severe epidemics.
We suggest that individual-level adaptive decision-making may be an important causal factor in driving influenza epidemiology.
We find that severe epidemics cannot be prevented unless vaccination programs offer incentives.
Frequency of severe epidemics could be reduced if programs provide, as an incentive to be vaccinated, several years of free vaccines to individuals who pay for one year of vaccination.
Magnitude of epidemic amelioration will be determined by the number of years of free vaccination, an individuals' adaptability in decision-making, and their memory.
This type of incentive program could control epidemics if individuals are very adaptable and have long-term memories.
However, incentive-based programs that provide free vaccination for families could increase the frequency of severe epidemics.
We conclude that incentive-based vaccination programs are necessary to control influenza, but some may be detrimental.
Surprisingly, we find that individuals' memories and flexibility in adaptive decision-making can be extremely important factors in determining the success of influenza vaccination programs.
Finally, we discuss the implication of our results for controlling pandemics.
Previously, both complex CITATION CITATION and simple models CITATION CITATION of influenza transmission dynamics have been analyzed to determine what proportion of the population would need to be vaccinated to prevent influenza epidemics and pandemics.
However, none of these modeling studies have shown whether this critical coverage can actually be reached.
Here we investigate, by modeling vaccination decisions made by individuals, whether the critical coverage can be achieved through voluntary vaccination.
We construct an individual-level model of human cognition and behavior and link it to an epidemic model of influenza that includes vaccination dynamics.
We assume that the decision of each individual is based upon self-interest such that s/he wishes to avoid catching influenza, preferably without having to be vaccinated.
Since protective immunity against influenza lasts less than one year CITATION, individuals must decide every year whether or not to participate in a voluntary vaccination program.
Individuals who get vaccinated protect themselves from infection, but if they do not get vaccinated they may still avoid infection if sufficient numbers of their peers get vaccinated.
This poses a yearly dilemma for the self-interested individual of whether vaccination is necessary.
We model each individual's strategy for making yearly vaccination decisions as an adaptive process of trial and error.
We track both individual-level decisions and population-level variables.
We use our model to address the following question: can influenza epidemics be prevented by voluntary vaccination?
Our individual-level adaptive decision-making model is inspired by Minority Game methodology.
A Minority Game models how noncommunicating selfish individuals reach a collective behavior with respect to a common dilemma under adaptation of each one's expectations.
In the past decade, Minority Games CITATION have been used to model inductive reasoning systems CITATION and financial markets CITATION.
Our constructed model consists of a population of N individuals acting in their own self-interest who do not communicate their vaccination decisions to each other.
Every year, these individuals independently decide whether or not to get vaccinated against influenza using a risk-free, highly effective vaccine CITATION.
We assumed that the vaccine presents no real risk and that individuals do not perceive any risk from vaccination.
Individuals in the model are characterized by two biological attributes that they use when making vaccination decisions.
Individuals can adapt their vaccination behavior for the current season on the basis of their memories of the consequences of their past vaccination decisions: i.e., they use cognition to make decisions.
We couple our individual-level model of adaptive decision-making with a model of influenza vaccination dynamics.
Our coupled models show the effect of individual-level vaccination decisions on influenza epidemiology and, conversely, the effect of influenza epidemiology on individual-level vaccination decisions.
We first use our model to assess whether vaccination programs without incentives could achieve the critical coverage levels necessary to control influenza epidemics.
We then assess the potential epidemiological impact of two public heath programs that use incentives to encourage vaccination.
There are two major classes of incentive-based public health programs that can be investigated with our coupled models.
The first class uses incentives to correlate vaccination decisions for the same individual over many influenza seasons.
The second class uses incentives to correlate vaccination decisions amongst individuals in the population in one influenza season.
Many additional incentive-based vaccination programs can be formulated by combining the defining characteristics of these two classes.
The first public health program that we investigate is an example of the first class of incentive-based programs.
This program offers free vaccination for y number of years to an individual who pays for vaccination in the first year.
We assume that the individual gets vaccinated each year during the y years of free vaccination, but that s/he also evaluates the necessity of vaccination every year.
At the end of y years, each individual in the program then uses their evaluations to decide whether or not to re-enroll in the program.
If they choose to re-enroll, they pay for vaccination that season and receive free vaccinations for a further y years.
The second public health program that we analyze is an example of the second class of incentive-based programs.
This program vaccinates a family for free if the head of the family pays for her/his own vaccination.
We assume that the head of the family decides every year whether to re-enroll in the program depending upon how many of her/his family members were infected in the previous season.
Membrane fusion is critical to biological processes such as viral infection, endocrine hormone secretion, and neurotransmission, yet the precise mechanistic details of the fusion process remain unknown.
Current experimental and computational model systems approximate the complex physiological membrane environment for fusion using one or a few protein and lipid species.
Here, we report results of a computational model system for fusion in which the ratio of lipid components was systematically varied, using thousands of simulations of up to a microsecond in length to predict the effects of lipid composition on both fusion kinetics and mechanism.
In our simulations, increased phosphatidylcholine content in vesicles causes increased activation energies for formation of the initial stalk-like intermediate for fusion and of hemifusion intermediates, in accordance with previous continuum-mechanics theoretical treatments.
We also use our large simulation dataset to quantitatively compare the mechanism by which vesicles fuse at different lipid compositions, showing a significant difference in fusion kinetics and mechanism at different compositions simulated.
As physiological membranes have different compositions in the inner and outer leaflets, we examine the effect of such asymmetry, as well as the effect of membrane curvature on fusion.
These predicted effects of lipid composition on fusion mechanism both underscore the way in which experimental model system construction may affect the observed mechanism of fusion and illustrate a potential mechanism for cellular regulation of the fusion process by altering membrane composition.
Membrane fusion plays a key role in cellular function, allowing processes as diverse as neurotransmitter release, secretion of peptide hormones, and infection by enveloped viruses.
Fusion is also critical in allowing intracellular transport.
The cellular membranes participating in these fusion reactions contain complex mixtures of lipids and proteins.
The composition and potentially the organization of these mixtures are both variable and closely regulated by the cell CITATION CITATION .
A variety of experimental and computational model systems CITATION CITATION have been used to gain insight into the basic physical properties underlying membrane fusion.
Such model systems are of necessity much simpler in nature than the physiologic context for fusion, containing one or a few lipid species and omitting or simplifying the protein environment.
In designing and interpreting model systems for fusion, it is important to consider how variations in the lipid mixtures chosen may affect the results.
Investigating how changes in lipid composition affect the kinetics and mechanism of fusion also provides fundamental insight by illuminating which aspects of fusion are robust to small perturbations to the model system used and which are highly model-dependent.
Experimental data on lipid vesicle fusion indicate that variation in lipid composition plays an important role in determining fusogenicity.
Much initial work concentrated on the effects on fusion by lipids that prefer either positive or negative membrane curvature CITATION, CITATION.
Perhaps most extensively characterized is the association between increased fusogenicity and increased fraction of phosphatidylethanolamine CITATION, a lipid headgroup thought to promote negative membrane curvature.
Plasma membranes in fusogenic contexts have a ratio of phosphatidylcholine to PE headgroups measured at between 0.9 and 2.0 CITATION CITATION.
The presence of PE has been identified as a critical factor for fusion CITATION CITATION, and many systems used to develop structural models for fusion intermediates contain a substantially higher proportion of PE than physiologic levels CITATION, CITATION, CITATION.
Similarly, theoretical treatments of proposed fusion intermediates suggest that PE is important to the energetic favorability of the stalk-like state that has been proposed as an early fusion intermediate CITATION, CITATION, CITATION CITATION.
Other lipid species likely play an additional modulatory role in fusion, and the PC:PE ratio in physiological membranes also varies independently in the inner and outer leaflets CITATION, CITATION.
In this work, we examine the effect of PC:PE ratio as a first-order approximation of lipid composition effects; we first treat uniform composition in the inner and outer membrane leaflets and then examine asymmetric compositions.
Membrane curvature has long been believed an important modulator of the fusion process.
Simply conceived, the curvature strain of a highly curved membrane can provide a driving force that reduces the energetic barrier for the fusion process.
Most physiological fusion occurs between membranes that have a relatively large radius of curvature or are even concave relative to the fusion site.
However, the local curvature at the site of fusion has been a subject of much discussion, with several theories positing a membrane protrusion with high local curvature at the site of fusion CITATION CITATION.
Recent experimental evidence suggests that synaptotagmin, one of the neuronal fusion proteins, preferentially binds to curved membranes, induces tubulation of model membranes with a diameter of 17.5 nm, and accelerates vesicle fusion rates CITATION .
This important finding provides more direct evidence that highly curved membrane structures such as we simulate here may play an important role in physiological fusion and that induction of curvature may be a major function of the fusion protein apparatus.
The possibility that physiological fusion may involve highly curved membranes raises the questions of to what degree this curvature is required for fusion and how curvature may differentially affect the formation and stability of fusion intermediates.
Here, we report initial results in that regard; we refer the reader to the forthcoming work by Lee and Schick for a more extensive investigation of this phenomenon from a field-theoretic perspective .
Because the structures and detailed kinetics of fusion intermediates are difficult to observe experimentally, computational simulations of fusion play an important role in generating detailed structural and kinetic models for fusion that are consistent with available experimental data.
Coarse-grained simulations allow a substantial increase in computational tractability at the expense of some fidelity to experiment; the degree of fidelity varies with the particular approximations made in constructing the model.
The lipid model developed by Marrink and Mark CITATION provides a quantitative approximation of experimentally observable lipid parameters such as area per head group, diffusion, and bilayer width, and represents a good compromise between computational tractability and level of atomistic detail.
Initial results reported for membrane fusion with this model CITATION demonstrate the fusogenicity of 15 nm vesicles composed of either pure palmitoyloleoyl-PE or a dipalmitoyl-PC:dipalmitoyl-PE mixture, also observing faster fusion pore formation in the DPPC:DPPE mixture than in pure POPE.
Simulated pure DPPC vesicles were not observed to form fusion pores.
In subsequent work, we have used this Marrink-Mark coarse-grained model to systematically predict intermediates and kinetics for the fusion of small POPE vesicles CITATION.
Our simulations suggested a branching fusion pathway in which vesicles first react to form a stalk-like intermediate but then either rapidly form a fusion pore from the stalk-like state or fuse via an intermediate with an expanded hemifusion diaphragm that is metastable on the microsecond timescale.
Previous theoretical work has suggested these two pathways as alternative hypotheses; we have predicted that these pathways may coexist within a single system.
In our initial report, we predicted pure POPE vesicles to react via both pathways but to primarily undergo fusion via the late-hemifused intermediate using the indirect pathway.
We now extend this model to address how more complex mixtures of lipids may affect the reaction mechanism and kinetics of fusion.
Physiologic membrane fusion takes place in membranes consisting of a complex CITATION, CITATION CITATION and likely inhomogeneous CITATION CITATION mixture of multiple lipid species and proteins.
Experimental and computational model systems for fusion are far simpler, containing well-defined lipid mixtures; in the case of computational studies, single-component membranes are routinely used CITATION, CITATION, CITATION.
Given this gap in complexity between model systems used for mechanistic studies of fusion and the physiologic situation they are meant to reproduce, we wish to understand how fusion mechanisms and properties depend on membrane composition.
We have therefore performed thousands of simulations of vesicle fusion at different lipid compositions to systematically predict how membrane fusion kinetics and intermediates depend on lipid composition.
This molecular-dynamics approach yields similar results to those recently obtained using self-consistent field theory methods CITATION.
Our molecular-dynamics analyses, the field-theoretic results of Schick and coworkers, and prior continuum-mechanics work by Kozlov expand computational models to more complex membrane compositions to better approximate experimental models and physiologic scenarios.
Yeast two-hybrid screens are an important method for mapping pairwise physical interactions between proteins.
The fraction of interactions detected in independent screens can be very small, and an outstanding challenge is to determine the reason for the low overlap.
Low overlap can arise from either a high false-discovery rate or a high false-negative rate.
We extend capture recapture theory to provide the first unified model for false-positive and false-negative rates for two-hybrid screens.
Analysis of yeast, worm, and fly data indicates that 25 percent to 45 percent of the reported interactions are likely false positives.
Membrane proteins have higher false-discovery rates on average, and signal transduction proteins have lower rates.
The overall false-negative rate ranges from 75 percent for worm to 90 percent for fly, which arises from a roughly 50 percent false-negative rate due to statistical undersampling and a 55 percent to 85 percent false-negative rate due to proteins that appear to be systematically lost from the assays.
Finally, statistical model selection conclusively rejects the Erd s-R nyi network model in favor of the power law model for yeast and the truncated power law for worm and fly degree distributions.
Much as genome sequencing coverage estimates were essential for planning the human genome sequencing project, the coverage estimates developed here will be valuable for guiding future proteomic screens.
All software and datasets are available in Datasets S1 and S2, Figures S1 S5, and Tables S1 S6, and are also available from our Web site, LINK.
Maps of pairwise protein protein interactions are being generated in increasing numbers by the two-hybrid method CITATION.
Genome-scale two-hybrid screens have now been conducted for Saccharomyces cerevisiae CITATION, CITATION, Caenorhabditis elegans CITATION, and Drosophila melanogaster CITATION.
More recently, screens have been reported for herpesviruses and human CITATION CITATION.
These datasets have stimulated large-scale analysis of the topology of protein interaction networks.
Limitations in the data, both false positives and false negatives, continue to make it difficult to infer network properties CITATION CITATION, including distinctions as basic as the difference between Erd s-R nyi, power law CITATION CITATION, and other network degree distributions CITATION .
A recent review points out the challenges in estimating false-positive rates, false-negative rates, and completion to full coverage of protein interaction networks CITATION.
Virtually every published method falls back to an estimate based on intersections of datasets.
For false-positive rates, these methods have large variance when assays have little overlap, and indeed could not be used to analyze the existing large-scale maps for worm and fly.
Estimates for false-negative rates based on overlap of datasets may have even larger uncertainty.
Finally, global estimates of false-positive and false-negative rates say little about protein-specific properties, including whether certain classes of proteins behave well or badly in two-hybrid screens.
The goal of this work is to develop and apply a statistical model for two-hybrid pairwise interaction screens.
Previous methods typically summarize the presence or absence of an interaction as a 1/0 binary variable, and possibly split off a high-confidence core dataset.
The method we describe reaches back to the raw counts of observed bait prey clones.
This frees the statistical method from the need for an external gold standard of true-positive and true-negative interactions, or even a second dataset.
It permits protein-specific predictions that for the first time permit tests of hypotheses that some classes of proteins are more or less likely to have nonspecific interactions.
Finally, estimates of false-negative rates permit statistically grounded confidence intervals for the total number of pairwise interactions present in model organism proteomes.
A flowchart of a two-hybrid screen orients the discussion by showing where true-positive interaction partners can be lost and where false-positive, spurious interactions may arise.
In a two-hybrid assay, one protein is fused to the binding domain of a yeast transcription factor, and a second protein is fused to the activation domain.
Physical interactions between bait and prey proteins reconstitute transcription factor activity.
Due to the expense of the assay, not every protein may be selected to be made into a bait or prey construct.
Furthermore, some constructs may not be functional at all due to improper folding or incompatibility with the two-hybrid system.
These missing interactions are important to consider when estimating the total number of interactions in a proteome.
High-throughput two-hybrid screens have used multiplexed pairwise tests, either by testing a single bait versus a pool of preys CITATION, CITATION, or by pooling both baits and preys CITATION.
Unnormalized prey pools can be generated from mRNA extracted from growing cells.
With access to clone collections, pools can be normalized by designing baits and preys individually for each protein or protein domain, then mixing preys in equal proportion.
The yeast screen considered here CITATION tested 62 normalized bait pools versus 62 normalized prey pools, each pool having approximately 96 genes.
The fly screen and worm screen each tested one bait in turn versus both normalized and unnormalized pools.
The testing occurs by using mating or transformation to express both the bait and prey construct in a single yeast cell.
True-positive interactions drive reporter genes that permit the yeast cell to grow in selective media.
Yeast cells whose bait prey constructs do not interact are expected to drop out during the population expansion.
True positives may also be lost during the population expansion for at least two reasons.
First, the mating or transformation may lack enough cells to ensure that every combination is tested.
Second, a particular construct may have domain-specific misfolding, making it functional for some interactions but nonfunctional for others.
True interactions that are not represented in the cells following the population expansion are systematic false negatives for a particular screen.
False negatives due to insufficient mating/transformation and due to nonfunctional domains could in principle be discriminated by repeating the mating or transformation step and the selective population expansion.
Without this additional step, however, losses during the population expansion combine to yield a systematic false-negative rate termed 1 p syst, with p syst representing the true-positive rate for an interacting pair to survive the population expansion.
Some cells expressing noninteracting proteins may also survive the population expansion, and the final population of cells will be a mixture of true positives and false positives.
In Figure 1, the mass fraction of true-positive cells is 1, and of false-positive cells is. The ratio of false positives to the total number of true negatives is the false-positive rate.
Usually, however, the ratio is with respect to the total number of observed interactions, defined as the false-discovery rate and synonymous with the parameter .
An ongoing point of contention in two-hybrid screens is the possibility that two proteins that never interact in vivo in the host organism might have a strong, reproducible interaction in vitro in the engineered two-hybrid system.
Conversely, proteins with a strong two-hybrid interaction might nevertheless fail to interact in vivo.
For the purposes of this work, we assume that such cases are rare and we classify any pair of proteins with a reproducible two-hybrid interaction as a true positive.
While the total false-positive fraction may be large, it represents a sum over many different false-positive pairs.
Most models, including ours, assume that any particular false positive is rare, with vanishing probability of observing a specific false-positive interaction more than once.
Interactions detected in pooled screens often require sequencing to identify the interacting partners, although advanced pooling designs may improve deconvolution efficiency CITATION.
Cost constraints limit the number of interactions that can be sampled for sequencing.
If the number of clones selected for sequencing is smaller than the number of true interaction partners of a bait, some true partners will certainly be lost.
Limited sampling depth also truncates the observed degree distribution for baits.
The false-negative rate due to undersampling is termed 1 p samp in Figure 1.
False-discovery rates have typically been estimated by comparing datasets CITATION CITATION, suggesting up to 50 percent false positives, but these analyses can confound false-positive and false-negative error sources.
Estimated error rates have large uncertainty because few interactions are observed in multiple datasets.
For example, comparing the Uetz and Ito two-hybrid datasets for yeast reveals only 9.1 percent of the total interactions in common CITATION, and comparing the two-hybrid interactions with mass spectrometry interactions reveals only 0.6 percent in common CITATION.
Similarly, comparison of two fly screens reveals few interactions in common CITATION, CITATION.
Cross-species comparisons have also revealed little overlap in the reported interactions CITATION, although protein and network evolution are additional confounding factors.
Efforts to estimate the true number of interaction partners of a protein have used contingency tables for observing an interaction in multiple screens.
These methods require that all the interactions be true positives, for example by excluding singleton observations CITATION, which can reduce the estimated interaction count.
A notable exception is previous work in the context of mass spectrometry of protein complexes CITATION, which used a Bayesian model to infer global parameters for screen-specific false-positive and false-negative rates.
These parameters then provided posterior estimates for the probability of a true interaction given results of one or more screens.
This work is important in using the number of trials and successes, rather than a single summary yes/no observation, in its probability model; it serves as motivation for developing similar models for the more complicated two-hybrid sampling process involving strong protein-specific effects.
Quantitative predictions of the amount of work required to identify some fraction of true interactions would be analogous to formulas for genome sequencing CITATION and would be useful for planning new experiments CITATION.
The new work presented here uses the raw screening data to estimate the false-negative rate from undersampling, together with the false-positive rate.
A schematic illustrates the sampling process.
Interactions are sampled with replacement from two sets, one representing true positives and the other true negatives.
The observations are the number of times that each interaction is sampled, which we summarize with three variables: n, the total number of samples drawn; w, the number of unique interactions within the n samples; and s, the number of interactions observed exactly once.
From these observations we are to estimate the unknown values of k, the total number of true interaction partners, and f, the number of false positives within the sample n. We also estimate the parameter representing the fraction of false positives in the mixture, as well as parameters representing the probability distribution for k. For simplicity, the illustration suggests sampling interactions in the entire network; in reality, this sampling process occurs separately for each bait, and the estimation of k and f is performed separately for each bait.
This estimation problem is akin to estimating population sizes or species counts from capture recapture experiments, estimating vocabulary size from word counts, estimating the number of distinct alleles at a particular locus, and estimating the number of facts in the scientific literature CITATION CITATION.
Classic capture recapture theory permits heterogeneous capturability rates, here analogous to different probabilities of observing each true interaction partner of a bait.
The canonical estimator has a simple form: w s 2/2k 2 CITATION CITATION, where k 2 is the number of partners observed exactly twice.
The classic estimator fails in the two-hybrid setting because it does not account for false positives.
To our knowledge, false positives have never been discussed in the capture recapture setting.
False positives will vastly inflate the interaction count by adding to the number of singleton observations, s, and to the total observed count, w. The standard estimator has high variance when the number of observations is small, yielding a small value for the denominator k 2.
The estimator fails to converge when each partner is observed only once, yielding n w s, k 2 0, and .
We present a front-to-back statistical model for both false-positive and false-negative error rates in two-hybrid screens.
A glossary of model terms is provided.
The overall approach is to start by estimating the parameters of a mixture model for true positives and false positives following the population expansion.
This permits us to estimate bait-specific false-discovery rates and false-negative rates due to undersampling.
We can then back-calculate the false-negative rate due to systematic effects.
Putting the results together yields an overall estimate for the false-negative rate of a screen and a basis for comparing interaction lists produced by different efforts.
Along the way we examine issues that our model is able to address quantitatively: selecting the best model for the protein degree distribution; correlating false-discovery rates with bait properties such as sticky or promiscuous domains or hydrophobic regions; and determining the relative performance of prey libraries generated from cDNA libraries or ORFeome collections.
Five independent groups have reported microarray studies that identify dozens of rhythmically expressed genes in the fruit fly Drosophila melanogaster.
Limited overlap among the lists of discovered genes makes it difficult to determine which, if any, exhibit truly rhythmic patterns of expression.
We reanalyzed data from all five reports and found two sources for the observed discrepancies, the use of different expression pattern detection algorithms and underlying variation among the datasets.
To improve upon the methods originally employed, we developed a new analysis that involves compilation of all existing data, application of identical transformation and standardization procedures followed by ANOVA-based statistical prescreening, and three separate classes of post hoc analysis: cross-correlation to various cycling waveforms, autocorrelation, and a previously described fast Fourier transform based technique CITATION CITATION.
Permutation-based statistical tests were used to derive significance measures for all post hoc tests.
We find application of our method, most significantly the ANOVA prescreening procedure, significantly reduces the false discovery rate relative to that observed among the results of the original five reports while maintaining desirable statistical power.
We identify a set of 81 cycling transcripts previously found in one or more of the original reports as well as a novel set of 133 transcripts not found in any of the original studies.
We introduce a novel analysis method that compensates for variability observed among the original five Drosophila circadian array reports.
Based on the statistical fidelity of our meta-analysis results, and the results of our initial validation experiments, we predict many of our newly found genes to be bona fide cyclers, and suggest that they may lead to new insights into the pathways through which clock mechanisms regulate behavioral rhythms.
Most organisms exhibit rhythms of behavior and physiology that occur with circadian or daily periods.
Such rhythms are driven by endogenous biological clocks regulated via the rhythmic expression of a core set of pacemaker genes CITATION CITATION.
A salient feature of many circadian genes, conserved across a wide span of evolutionary divergence, is the cyclic expression of their mRNAs CITATION, CITATION.
Several studies have exploited this characteristic to identify novel clock-related genes.
Five such reports, based in the model organism Drosophila melanogaster, utilize microarray technology to discover clock-related genes that exhibit cyclic mRNA expression in fly heads CITATION, CITATION CITATION.
These cumulatively identify hundreds of rhythmic transcripts; yet, there is a striking lack of overlap among the lists of identified genes.
This raises doubts as to the fidelity of reported expression patterns CITATION CITATION.
Given the importance of rhythmic transcription to circadian clock function, we revisited these studies, attempting to identify the root causes of existing incongruities and to find transcripts that exhibit truly cyclic expression.
Our analyses suggest that compilation and statistical prescreening of all available data lead to substantial reductions in the false discovery rate.
In addition, we find data quality and algorithm choice to play essential roles in determining the transcripts ultimately detected by any particular analysis.
We introduce a novel procedure, as well as improvements to previously published techniques, that identify a core set of rhythmically expressed transcripts with high statistical fidelity.
Intriguingly, our list includes 133 transcripts not found in the original reports.
Correlated changes of nucleic or amino acids have provided strong information about the structures and interactions of molecules.
Despite the rich literature in coevolutionary sequence analysis, previous methods often have to trade off between generality, simplicity, phylogenetic information, and specific knowledge about interactions.
Furthermore, despite the evidence of coevolution in selected protein families, a comprehensive screening of coevolution among all protein domains is still lacking.
We propose an augmented continuous-time Markov process model for sequence coevolution.
The model can handle different types of interactions, incorporate phylogenetic information and sequence substitution, has only one extra free parameter, and requires no knowledge about interaction rules.
We employ this model to large-scale screenings on the entire protein domain database.
Strikingly, with 0.1 trillion tests executed, the majority of the inferred coevolving protein domains are functionally related, and the coevolving amino acid residues are spatially coupled.
Moreover, many of the coevolving positions are located at functionally important sites of proteins/protein complexes, such as the subunit linkers of superoxide dismutase, the tRNA binding sites of ribosomes, the DNA binding region of RNA polymerase, and the active and ligand binding sites of various enzymes.
The results suggest sequence coevolution manifests structural and functional constraints of proteins.
The intricate relations between sequence coevolution and various selective constraints are worth pursuing at a deeper level.
Coevolution is prevalent at species, organismic, and molecular levels.
At the molecular level, selective constraints operate on the entire system, which often require coordinated changes of its components.
The most well-known example is the compensatory substitution of nucleic acid pairs in RNA secondary structures CITATION CITATION.
Interacting nucleotides vary between AU, CG, and GU pairs in different species in order to maintain the hydrogen bonds.
Coordinated changes of amino acid residues have also been investigated.
Typically these studies acquired one family of aligned sequences and examined covariation between aligned positions or of the entire sequences.
Some of these have applied different covariation metrics including correlation coefficients CITATION CITATION, mutual information CITATION CITATION, and the deviance between marginal and conditional distributions CITATION.
These studies demonstrate that sequence covariation is powerful in detecting protein protein interactions CITATION, CITATION, ligand-receptor bindings CITATION, CITATION, and the folding structure of single proteins CITATION, CITATION.
In addition to direct physical interactions, distant coevolving amino acid residues are reported to be energetically coupled CITATION or subject to the functional constraints of the proteins CITATION .
A major drawback of many covariation metrics is the lack of phylogenetic information.
The sequences manifesting the same level of covariation may arise from either a few independent substitutions in early ancestors or correlated changes along multiple lineages CITATION, CITATION.
In RNA structure prediction, many authors have thereby extended the continuous-time Markov process of sequence substitution CITATION to coevolving nucleic acid pairs CITATION, CITATION, CITATION, CITATION.
However, direct application of these models to protein coevolution is intractable due to the large number of parameters in the CTMP of amino acid pairs.
This problem was addressed by replacing amino acids in a CTMP with simplified, surrogate alphabet sets such as the presence/absence of a protein in each species CITATION or the charge and size of amino acid groups CITATION.
Yet this simplification deviates from the standard CTMP of sequence substitution, in which a rich set of empirical models are available.
All the previous studies of detecting protein coevolution target a few proteins or protein domains, such as myoglobin CITATION, PGK CITATION, Ntr family CITATION, PDZ domain family CITATION, Gag, Hsp90, and GroEL proteins CITATION.
The availability of large-scale protein sequences and their phylogenetic information allows us to perform a systematic screening on all the known protein families.
Such large-scale screening will give comprehensive information of coevolution among all the protein domains and provide insight about their physical/functional couplings.
We propose a general coevolutionary CTMP model which requires neither simplification of states nor prior knowledge about interactions, and has only one extra free parameter.
Sequence substitution of the two sites is modeled by a continuous-time Markov process.
The null model hypothesizes that two sites evolve independently.
The alternative model is obtained from the null model by reweighting the independent substitution rate matrix to favor double over single changes.
We apply this model to all the inter- and intra-domain position pairs in all the known protein domain families in Pfam database CITATION.
Strikingly, from a large number of pairwise comparisons the coevolving domain pairs are highly enriched with domains in the same proteins, protein complexes, or possessing the same functions.
Moreover, the coevolving positions demonstrate a tendency of spatial coupling and are mapped to functionally important sites of their proteins.
Mitochondria are eukaryotic organelles that originated from the endosymbiosis of an alpha-proteobacterium.
To gain insight into the evolution of the mitochondrial proteome as it proceeded through the transition from a free-living cell to a specialized organelle, we compared a reconstructed ancestral proteome of the mitochondrion with the proteomes of alpha-proteobacteria as well as with the mitochondrial proteomes in yeast and man. Overall, there has been a large turnover of the mitochondrial proteome during the evolution of mitochondria.
Early in the evolution of the mitochondrion, proteins involved in cell envelope synthesis have virtually disappeared, whereas proteins involved in replication, transcription, cell division, transport, regulation, and signal transduction have been replaced by eukaryotic proteins.
More than half of what remains from the mitochondrial ancestor in modern mitochondria corresponds to translation, including post-translational modifications, and to metabolic pathways that are directly, or indirectly, involved in energy conversion.
Altogether, the results indicate that the eukaryotic host has hijacked the proto-mitochondrion, taking control of its protein synthesis and metabolism.
Mitochondria are organelles that are found in virtually all eukaryotic cells.
In addition to their role in energy conversion, mitochondria are involved in many processes from intermediate metabolism, such as synthesis of heme groups CITATION, steroids CITATION, amino acids, and iron-sulphur clusters CITATION.
Phylogenetic analyses of mitochondrial genes indicate that all mitochondria derive from a single alpha-proteobacterial ancestor, the so-called proto-mitochondrion CITATION.
During the transformation of proto-mitochondrion to organelle, its proteome underwent a series of modifications, including, among others, the acquisition of a protein import machinery and an ADP/ATP carrier, leading to a situation in which only a minority of mitochondrial proteins can be traced back to an alpha-proteobacterial ancestor CITATION, CITATION.
Similarly, large transformations of the mitochondrial metabolism are thought to have occurred in the course of mitochondrial evolution CITATION, CITATION.
According to a recent reconstruction CITATION, the proto-mitochondrion possessed an aerobic metabolism comprising a considerable variety of pathways, such as fatty-acid synthesis and degradation, the respiratory chain, and the Fe-S cluster assembly pathways.
Some studies have focused on the subsequent evolution from the alpha-proteobacteria of some mitochondrial pathways such as the electron transport chain CITATION, CITATION.
However, no comprehensive analysis has been performed so far to analyze the proteomic transition of mitochondria at a larger scale.
It is still largely unknown, for example, which aspects of the proteome of modern mitochondria resemble that of its bacterial ancestor or to what extent the current metabolic diversity observed in mitochondria from different organisms was achieved through the differential gain or differential loss of proteins.
To address these questions, we compared ancient and modern mitochondrial proteomes and their inferred metabolic pathways.
To reconstruct the proteome of the proto-mitochondrion, we have used a similar approach to the one used previously for a smaller set of genomes CITATION.
The rationale behind this approach is that proto-mitochondrial proteins are eukaryotic proteins with an alpha-proteobacterial ancestry and that they can be detected by constructing phylogenies of eukaryotic proteins and examining those for a monophyletic relation between alpha-proteobacterial proteins and eukaryotic proteins.
Metabolic pathways from modern mitochondria were inferred from recent proteomics surveys of highly pure, isolated mitochondria from yeast and human.
A comparison of the functional classification of these proteomes indicates that only in classes corresponding to translation, post-translation modification, and protein folding and metabolism do current-day mitochondria resemble the proto-mitochondrion.
Other classes have either disappeared or have been replaced by proteins of non alpha-proteobacterial origin.
Focusing on the metabolic transition, we compared the inferred ancestral mitochondrial metabolism with the metabolism of present-day mitochondria as it can be inferred from comprehensive mitochondrial proteomics.
By comparing the three reconstructed metabolic pathways, we trace the main lines of the metabolic transition from the early endosymbiont to the modern organelle, as well as the later divergence of fungal and metazoan mitochondrial metabolic pathways.
Altogether, our results indicate a continuously increasing bias toward energy conversion from the alpha-proteobacteria to the proto-mitochondrion, and from the proto-mitochondrion to current-day mitochondrion, a significant retargeting of metabolic enzymes of alpha-proteobacterial origin to other cellular compartments and a complete eukaryotic takeover of replication, transcription, mitochondrial division and signal transduction, and gene regulation.
Early identification of adverse effect of preclinical and commercial drugs is crucial in developing highly efficient therapeutics, since unexpected adverse drug effects account for one-third of all drug failures in drug development.
To correlate protein drug interactions at the molecule level with their clinical outcomes at the organism level, we have developed an integrated approach to studying protein ligand interactions on a structural proteome-wide scale by combining protein functional site similarity search, small molecule screening, and protein ligand binding affinity profile analysis.
By applying this methodology, we have elucidated a possible molecular mechanism for the previously observed, but molecularly uncharacterized, side effect of selective estrogen receptor modulators.
The side effect involves the inhibition of the Sacroplasmic Reticulum Ca2 ion channel ATPase protein transmembrane domain.
The prediction provides molecular insight into reducing the adverse effect of SERMs and is supported by clinical and in vitro observations.
The strategy used in this case study is being applied to discover off-targets for other commercially available pharmaceuticals.
The process can be included in a drug discovery pipeline in an effort to optimize drug leads and reduce unwanted side effects.
Early identification of the adverse effects of preclinical and commercial drugs is crucial in developing highly efficient therapeutics, since unexpected adverse drug effects contribute to one-third of all drug failures in the late stage of drug development CITATION.
Conventional practices for identifying off-targets rely on a counterscreen of compounds against a large number of enzymes and receptors in vitro CITATION CITATION.
Computational approaches could not only save time and costs spent during in vitro screening by providing a candidate list of potential off-targets but also provide insight into understanding the molecular mechanisms of protein drug interactions.
It has been shown that potential off-targets can be identified in silico by establishing the structure activity relationship of small molecules CITATION CITATION.
However, the success of ligand-based methods strongly depends on the availability and coverage of the chemical structures used in training, and few of them directly take the target 3D structure into account.
Although the assessment of protein ligand interactions by docking studies at the atomic level is extremely valuable for understanding the molecular mechanism of adverse therapeutic effects CITATION, CITATION, protein ligand docking on a large scale is hindered by the biased structural coverage of the human proteome CITATION and a lack of practical methodologies to accurately estimate the binding affinity CITATION.
Here we approach the problem from a different direction by postulating that proteins with similar binding sites are likely to bind to similar ligands CITATION.
In this study we test this postulate by predicting potential off-target binding sites for selective estrogen receptor modulators.
Several commercial drugs targeting estrogen receptor alpha have been developed to treat breast cancers and other diseases CITATION.
However, therapy from these drugs such as Tamoxifen is associated with undesirable side effects such as cardiac abnormalities CITATION, thromboembolic disorders CITATION, and ocular toxicity CITATION.
To identify off-targets of these SERMs and to attempt to elucidate the molecular mechanisms explaining their adverse effects, we searched for similar ligand binding sites across fold and functional space using a template for the known SERM binding site in ER.
The search used a robust and scalable functional site prediction and comparison algorithm developed recently in our laboratory 22; Xie and Bourne, submitted.
Consequently, a similar inhibitor site is detected for Sacroplasmic Reticulum Ca2 ion channel ATPase protein.
The prediction is further verified with detailed protein ligand docking and surface electrostatic potential analysis.
Our prediction correlates well with clinical and biochemical observations, providing molecular insight into reducing the adverse effect of SERMs.
The strategy used in this case study could be applied to discover off-targets for other commercially available pharmaceuticals and to repurpose existing drugs to treat different diseases Xie, Kinnings, and Bourne, in preparation.
The process could also be included in a drug discovery and development pipeline in an effort to optimize drug leads and reduce unwanted side effects.
The calcium/calmodulin-dependent protein kinase II plays a key role in the induction of long-term postsynaptic modifications following calcium entry.
Experiments suggest that these long-term synaptic changes are all-or-none switch-like events between discrete states.
The biochemical network involving CaMKII and its regulating protein signaling cascade has been hypothesized to durably maintain the evoked synaptic state in the form of a bistable switch.
However, it is still unclear whether experimental LTP/LTD protocols lead to corresponding transitions between the two states in realistic models of such a network.
We present a detailed biochemical model of the CaMKII autophosphorylation and the protein signaling cascade governing the CaMKII dephosphorylation.
As previously shown, two stable states of the CaMKII phosphorylation level exist at resting intracellular calcium concentration, and high calcium transients can switch the system from the weakly phosphorylated to the highly phosphorylated state of the CaMKII.
We show here that increased CaMKII dephosphorylation activity at intermediate Ca 2 concentrations can lead to switching from the UP to the DOWN state.
This can be achieved if protein phosphatase activity promoting CaMKII dephosphorylation activates at lower Ca 2 levels than kinase activity.
Finally, it is shown that the CaMKII system can qualitatively reproduce results of plasticity outcomes in response to spike-timing dependent plasticity and presynaptic stimulation protocols.
This shows that the CaMKII protein network can account for both induction, through LTP/LTD-like transitions, and storage, due to its bistability, of synaptic changes.
Synaptic plasticity is thought to underlie learning and memory, but the mechanisms by which changes in synaptic efficacy are induced and maintained over time are still unclear.
Numerous experiments have shown how synaptic efficacy can be increased or decreased by spike timing of presynaptic and postsynaptic neurons CITATION, CITATION, presynaptic firing rate CITATION, CITATION, or presynaptic firing paired with postsynaptic holding potential CITATION.
These experiments have led to phenomenological models that capture one or several of these aspects CITATION CITATION.
However, these models tell us nothing about the biochemical mechanisms of induction and maintenance of synaptic changes.
The question of the mechanisms at the biochemical level has been addressed by another line of research work originating from early work by Lisman CITATION.
Models at the biochemical level describe enzymatic reactions of proteins in the postsynaptic density CITATION CITATION.
These proteins form a network with positive feedback loops that can potentially provide a synapse with several stable states two, in the simplest case providing a means to maintain the evoked changes.
Hence, synapses in such models are similar to binary switches, exhibiting two stable states, an UP state with high efficacy, and a DOWN state with low efficacy.
The idea of binary synapses is supported by recent experiments on CA3-CA1 synapses CITATION CITATION .
One of the proposed positive feedback loops involves the calcium/calmodulin-dependent protein kinase II kinase-phosphatase system CITATION CITATION.
CaMKII activation is governed by Ca 2 /calmodulin binding and is prolonged beyond fast-decaying calcium transients by its autophosphorylation CITATION.
Autophosphorylation of CaMKII at the residue theronine-286 in the autoregulatory domain occurs after calcium/calmodulin binding and enables the enzyme to remain autonomously active after dissociation of calcium/calmodulin CITATION.
In turn, as long as CaMKII stays activated it is reversibly translocated to a postsynaptic density -bound state where it interacts with multiple LTP-related partners structurally organizing protein anchoring assemblies and therefore potentially delivering -amino-3-hydroxyl-5-methyl-4-isoxazole-propionate acid receptors to the cell surface CITATION, CITATION CITATION.
The direct phosphorylation of the AMPA receptor GluR1 subunit by active CaMKII enhances AMPA channel function CITATION, CITATION.
The network involving CaMKII is particularly appealing in terms of learning and memory maintenance since N-methyl-D-aspartate receptor -dependent LTP requires calcium/calmodulin activation of CaMKII, potentially expressed by the phosphorylation level or the number of AMPA receptors, or both CITATION, CITATION, CITATION, CITATION CITATION.
However, the role of CaMKII beyond LTP induction remains controversial CITATION CITATION.
Finally, there is experimental evidence for the involvement of proteins associated with CaMKII activity, and calcineurin in LTP and LTD CITATION CITATION.
We emphasize that multiple mechanisms supporting LTP/LTD induction and expression are likely to be present in synapses of different regions we focus here on synapses for which the above statements have been shown to apply, e.g., the CA3-CA1 Schaffer collateral synapse .
Modeling studies have shown that a system including CaMKII and associated pathways could be bistable in a range of calcium concentrations including the resting level a necessary requirement for the maintenance of long-term changes CITATION, CITATION, CITATION, CITATION.
In such models, the two states correspond to two stable phosphorylation levels of the CaMKII protein for a given calcium concentration, i.e., a weakly and a highly phosphorylated state.
A transition from the DOWN to the UP state which could underlie long-term potentiation can be induced by a sufficiently large and prolonged increase in calcium concentration.
However, the opposite transition which could underlie depotentiation or LTD only occurs under unrealistic conditions, for example decrease of calcium concentration below resting level.
Furthermore, it has not been considered how these biochemical network models behave in response to calcium transients evoked by experimental protocols that are known to induce synaptic plasticity such as STDP, which has been shown to rely on kinase and phosphatase activation CITATION.
Rubin et al. reproduce experimental results on STDP using a model detector system which qualitatively resembles the protein network influencing CaMKII, but this model does not exhibit bistability CITATION.
Other studies on biochemical signal transduction pathways including CaMKII showed that the AMPA receptor activity can reproduce bidirectional synaptic plasticity as a function of calcium CITATION, CITATION.
However, realistic stimulation protocols were not investigated in these models, and again they do not show bistability.
In this paper, we consider a realistic model of protein interactions associated with CaMKII autophosphorylation through calcium/calmodulin and dephosphorylation by protein phosphatase 1 in the PSD.
We first study the steady-state phosphorylation properties of CaMKII with respect to calcium and changing levels of PP1 activity.
Conditions are elaborated for which the system allows for LTP and LTD transitions in reasonable ranges of calcium concentrations.
We then demonstrate the ability of the CaMKII system to perform LTP- or LTD-like transitions in response to STDP stimulation protocols.
We expose the CaMKII system to calcium transients evoked by pairs of presynaptic and postsynaptic spikes with a given time lag and show that short positive time lags evoke transitions from the DOWN to the UP state and short negative time lags lead to transitions from the UP to the DOWN state.
We demonstrate furthermore that the CaMKII model qualitatively reproduces experimental plasticity outcomes for presynaptic stimulation protocols.
Finally, we consider the transition behavior in response to purely presynaptic or postsynaptic spike-pair stimulation protocols.
Selective attention is an important filter for complex environments where distractions compete with signals.
Attention increases both the gamma-band power of cortical local field potentials and the spike-field coherence within the receptive field of an attended object.
However, the mechanisms by which gamma-band activity enhances, if at all, the encoding of input signals are not well understood.
We propose that gamma oscillations induce binomial-like spike-count statistics across noisy neural populations.
Using simplified models of spiking neurons, we show how the discrimination of static signals based on the population spike-count response is improved with gamma induced binomial statistics.
These results give an important mechanistic link between the neural correlates of attention and the discrimination tasks where attention is known to enhance performance.
Further, they show how a rhythmicity of spike responses can enhance coding schemes that are not temporally sensitive.
Past work with both human and animal subjects has focused on neural correlates of attention.
Attention raises the firing rate and the input output gain of orientation-selective neurons in the visual cortex CITATION CITATION, and shifts response curves so that physiologically relevant stimuli fall in the high-gain region CITATION, CITATION.
Also, when attended stimuli overlap with a recorded receptive field, gamma-band frequency components of local field potentials and single-unit spike responses increase CITATION CITATION.
Gamma oscillations in the field potential likely reflect correlated network activity CITATION, CITATION, as supported by simulations of spiking neurons with inhibitory or recurrent excitatory inhibitory coupling CITATION.
Attention is thought to influence cholingergic neuromodulation CITATION, which presumably affects synchrony of interneuron networks involved in gamma oscillations CITATION, CITATION, CITATION.
It is well-known that correlated network discharge effectively drives postsynaptic cells CITATION, making gamma-band activity a signature of efficient signal propagation.
This would allow attended objects to increase downstream responses, as compared to nonattended objects.
In contrast, we assess the role of gamma oscillations in the signal coding of neural populations participating in gamma oscillatory dynamics.
Tasks where attention improves performance typically involve discrimination between different signals, such as visual cues with different colors, shapes, or orientations CITATION, CITATION CITATION.
Although there are a large number of studies exploring how gamma rhythms are generated in networks of spiking neurons, the mechanisms by which gamma oscillations modify signal discrimination are elusive in three aspects.
First, the relation between gain modulation and gamma oscillations, both of which are attention-dependent, is unclear.
Second, the temporal relation between a network gamma rhythm and the time course of a driving signal is often unclear.
Third, gamma-induced synchronous firing may be deleterious for coding due to increased variability of population activity CITATION .
A popular framework for neural coding is that the number of spikes produced by a single neuron or a population of neurons carries information about a driving signal.
However, in vivo spike trains often show a spike count Fano factor that is close to or even exceeds unity CITATION CITATION.
This trial-to-trial variability is deleterious to the code performance and degrades putative spike-count based signal-discrimination schemes.
In certain situations, Fano factors much less than 1 are observed in the visual cortex CITATION, CITATION, the auditory cortex CITATION, and the salamander retina CITATION.
In an extreme case, if a neuron fires with high probability in response to a relevant input signal and rarely fires otherwise CITATION, then the signal can be estimated from the spike count with small error.
In addition, spike-timing reliability, for which a neuron robustly emits just a single spike during a steep upstroke of the input and seldom fires elsewhere CITATION, CITATION, is also supportive of such binary spiking.
In this study we model the essence of a gamma frequency modulation as a simple rhythmic forcing of a population of uncoupled spiking neurons.
We show that gamma oscillations endow population spike counts with binomial-like statistics, which improve signal discrimination over a range of stimuli through reduced spike-count variability.
In this way, we propose a connection between gamma oscillations and enhanced task performance found in behavioral experiments.
Our results are both distinct and complementary to previously described influences of rhythmic network behavior in temporal coding schemes by improving spike precision CITATION, CITATION or by providing a clock for a phase-based code CITATION CITATION .
Phylogenetic profiling is based on the hypothesis that during evolution functionally or physically interacting genes are likely to be inherited or eliminated in a codependent manner.
Creating presence absence profiles of orthologous genes is now a common and powerful way of identifying functionally associated genes.
In this approach, correctly determining orthology, as a means of identifying functional equivalence between two genes, is a critical and nontrivial step and largely explains why previous work in this area has mainly focused on using presence absence profiles in prokaryotic species.
Here, we demonstrate that eukaryotic genomes have a high proportion of multigene families whose phylogenetic profile distributions are poor in presence absence information content.
This feature makes them prone to orthology mis-assignment and unsuited to standard profile-based prediction methods.
Using CATH structural domain assignments from the Gene3D database for 13 complete eukaryotic genomes, we have developed a novel modification of the phylogenetic profiling method that uses genome copy number of each domain superfamily to predict functional relationships.
In our approach, superfamilies are subclustered at ten levels of sequence identity from 30 percent to 100 percent and phylogenetic profiles built at each level.
All the profiles are compared using normalised Euclidean distances to identify those with correlated changes in their domain copy number.
We demonstrate that two protein families will auto-tune with strong co-evolutionary signals when their profiles are compared at the similarity levels that capture their functional relationship.
Our method finds functional relationships that are not detectable by the conventional presence absence profile comparisons, and it does not require a priori any fixed criteria to define orthologous genes.
Comparison of the phylogenetic profiles of orthologous proteins in different species is a well-known and powerful method for detecting functionally related proteins.
The approach assumes that two functionally related proteins will have been inherited or eliminated in a codependent fashion through speciation.
Therefore, by examining correlated presence absence patterns in different genomes, it is possible to infer protein co-evolution and a functional relationship.
After the original idea was published CITATION, the phylogenetic profile method was improved or reinterpreted in many different ways.
For example: through the application of more complex logical rules to associate and compare protein profiles CITATION ; the use of domain profiles instead of whole proteins CITATION ; refining the algorithm CITATION ; or integration of species phylogenetic information CITATION, CITATION .
Although the phylogenetic profile method can be improved by integrating new sources of information, in all cases the prediction quality of this method depends on two critical steps: the selection of the reference species sample and the determination of which proteins are orthologues.
Typically the latter is done using a Reciprocal Best Hits approach with similarity determined by the BLAST algorithm CITATION, CITATION CITATION and an E-value cutoff for potential orthologues.
In fact, these two steps have different impacts on the prediction quality.
The reference species problem can be avoided by simply increasing the sample size with new genomes until a certain number has been reached.
However, there are many problems, e.g., CITATION CITATION, in determining orthology, especially the separation of orthologues from paralogues.
Multigene families that exist within one genome can also exhibit functional overlap and substitutability between the members.
The fact that genes evolve at different rates, due to both uneven natural selection pressure on their functions and different species having different mutation rates e.g., rodents accumulate point mutations more rapidly than apes CITATION implies that the evolutionary rates of proteins may vary over several orders of magnitude in the different gene families CITATION.
This rate variation makes it difficult to choose a single similarity E-value cutoff that can be broadly applied to identify those orthologues most likely to have retained similar functionality.
The multigene family problem is particularly challenging in eukaryotic genomes wherein the percentage of genes present in multiple homologous copies is much higher than in prokaryotic genomes.
However, the higher percentage of multigene families is not the only problem that makes it more difficult to correctly assign orthologous relationships in eukaryotic species.
In contrast to prokaryotes, accurate identification of ORFs is complicated in eukaryotes by noise from domain rearrangements, more complex gene architectures, and a higher presence of noncoding regions.
Furthermore, in eukaryotes there is a weaker correlation between the number of ORFs and the phenotypic complexity of an organism.
This is probably due to a number of reasons, perhaps most significantly the greater use of RNA-based regulatory mechanisms CITATION .
We have developed a novel modification of the phylogenetic profile method that bypasses several of these problems, especially the orthology or functional equivalence as it can also be perceived detection problem, and can detect interacting multigene families.
This method is particularly applicable to identifying functional networks in eukaryotes, which have so far proven intractable.
Our approach is based around protein domains, since these are the most elemental units of protein function.
Furthermore, this allows us to bypass confusion caused by domain rearrangements.
For this study we have used the domain annotation from the Gene3D database, which stores CATH assignments for complete genomes.
The first key modification is that we do not consider the presence absence of domains but the number of copies of the domain.
The second key modification is that we subcluster all the domains at ten levels of sequence identity from 30 percent to 100 percent.
We then create profiles for every domain family and the subclusters within it, which enables the identification of distinct functional subgroups within domain families.
Although it is clear that there are always exceptions to any evolutionary model that can be proposed, the co-evolutionary hypothesis implicit in our model supposes that gene copy number in two functionally related protein clusters will vary in a related fashion.
In our approach, domain occurrence profiles are built at many identity levels, and therefore it is expected that two protein clusters will auto-tune with a significant correlation signal when their profiles are compared at the similarity levels that retain their functional relationship.
Therefore, domain occurrence profiles were compared all against all to identify correlations in domain copy number variation in all the different identity levels.
Our method found strong co-evolutionary signals amongst functionally related multigene domain families that could not have been predicted by the conventional presence absence comparison of profiles proposed by Pellegrini et al. CITATION .
This new approach has a number of features that make it especially useful for eukaryotic genome analysis.
Firstly, phylogenetic profiles based on protein domains can detect functional relationships that are not detectable using phylogenetic profiles of whole proteins, reducing the noise that protein domain rearrangements produce, particularly in eukaryotes CITATION.
Secondly, it uses domain occurrence profiles instead of presence absence profiles.
The latter are less effective in eukaryotic genomes as they do not account for the wide variation in gene copy number observed in eukaryotes.
And thirdly, the method applied does not require a priori any fixed E-value cutoff to define orthologous groups.
Because domain clusters are built at several discrete identity levels, the method takes into account much of the variation that uneven selection pressure produces on sequence and functional conservation.
microRNAs are important post-transcriptional regulators, but the extent of this regulation is uncertain, both with regard to the number of miRNA genes and their targets.
Using an algorithm based on intragenomic matching of potential miRNAs and their targets coupled with support vector machine classification of miRNA precursors, we explore the potential for regulation by miRNAs in three plant genomes: Arabidopsis thaliana, Populus trichocarpa, and Oryza sativa.
We find that the intragenomic matching in conjunction with a supervised learning approach contains enough information to allow reliable computational prediction of miRNA candidates without requiring conservation across species.
Using this method, we identify 1,200, 2,500, and 2,100 miRNA candidate genes capable of extensive base-pairing to potential target mRNAs in A. thaliana, P. trichocarpa, and O. sativa, respectively.
This is more than five times the number of currently annotated miRNAs in the plants.
Many of these candidates are derived from repeat regions, yet they seem to contain the features necessary for correct processing by the miRNA machinery.
Conservation analysis indicates that only a few of the candidates are conserved between the species.
We conclude that there is a large potential for miRNA-mediated regulatory interactions encoded in the genomes of the investigated plants.
We hypothesize that some of these interactions may be realized under special environmental conditions, while others can readily be recruited when organisms diverge and adapt to new niches.
Small RNAs are now accepted as major players in the control of eukaryotic gene expression.
Most well known are microRNAs and small interfering RNAs, both of which are derived from the processing of dsRNA molecules by members of the Drosha/Dicer family of endonucleases.
In plants, siRNA and miRNA are distinguished mainly by their biogenesis, not by their mechanism of action.
MiRNAs arise from stem-loop precursors encoded in the genome, and their major mechanism of action in plants is thought to be post-transcriptional regulation through near-complementary base-pairing to target mRNAs, leading to specific endonucleolytic cleavage and degradation of the target CITATION .
Most of the initially discovered miRNAs were so highly conserved in evolution that a defining characteristic of a miRNA was that it had to be conserved CITATION.
This attribute of those miRNAs discovered early has been used successfully by a number of groups to computationally predict new miRNA genes CITATION CITATION.
Basically, these methods scan the genome for inverted repeats with the potential to form miRNA precursors.
Such scans typically find on the order of hundreds of thousands to millions of hairpins, depending on genome size and search parameters CITATION.
This high number is then reduced by only keeping hairpins that are conserved in other species.
Another approach is to search only transcribed sequences in the form of expressed sequence tags CITATION, CITATION.
This method works for nonsequenced genomes and efficiently reduces the search space, probably leading to a lower number of false positives, but the method also misses candidates not covered by the expressed sequence tag libraries.
In miRBase version 8.2, Arabidopsis thaliana has 118 miRNA genes listed, most of which are conserved down to the monocot Oryza sativa.
However, studies of noncoding RNA have shown that lack of conservation does not necessarily mean lack of function CITATION.
Potentially, all it takes to evolve a miRNA is for one of the many inverted repeats in the genome to be transcribed and have the necessary structure and sequence features to be recognized and processed by Drosha/Dicer.
Indeed, large numbers of more narrowly conserved miRNAs also exist CITATION.
A recent bioinformatic study in human identified patterns associated with miRNA precursors and suggested that the number of miRNA precursors is larger than 25,000 CITATION.
In plants, a similar situation could exist.
A deep sequencing effort in Arabidopsis using the massively parallel signature sequence technique has revealed 75,000 distinct small RNA species CITATION mapping to a large variety of genomic contexts, including exons, introns, repetitive DNA, and intergenic regions.
This is perhaps not surprising considering other studies finding that unexpectedly large fractions of eukaryotic genomes are transcribed also outside and antisense to annotated protein-coding genes CITATION CITATION .
A necessary feature of any functional miRNA is that it must target at least one mRNA.
In plants, this means that the miRNA must be almost complementary to some part of the spliced mRNA transcript.
A set of rules allowing mismatches only in certain positions has been suggested based on experimental observations CITATION.
The requirement for a target has previously been used to predict plant miRNAs CITATION CITATION : instead of relying on phylogenetic conservation, these methods have successfully used intragenomic matches with potential target mRNAs to find the hairpins potentially capable of producing miRNAs that can regulate the target.
Such intragenomic matches will inherently arise from the structure and dynamics of the genome: retrotransposons, formation of pseudogenes, and other duplicative events provide sequences almost ready to regulate the originally copied gene CITATION ; likewise, the reverse strand of one gene is complementary to other paralogous genes.
By not relying on conservation between species, intragenomic matching is capable of more fully charting the potential for post-transcriptional regulation by miRNAs.
In an effort to reduce spurious predictions, earlier screens for new miRNAs have removed candidates overlapping existing annotation, such as repeats and protein-coding regions.
Although such filters probably increase the signal-to-noise ratio, they also introduce biases assuming that repeat-derived sequences are not functional and that each sequence segment can have only one function.
However, transposon-derived conventional miRNAs have been demonstrated in Arabidopsis CITATION, and recent work of several groups show that repeat-associated miRNAs are quite common in mammals CITATION CITATION.
Borchert et al. point to 50 human miRNAs that are associated with Alu repeats and polymerase III transcription CITATION.
Piriyapongsa et al. link 55 experimentally characterized human miRNAs to different types of transposable elements CITATION.
Of these, 18 are conserved in other vertebrate genomes, and the authors predict an additional 85 novel transposable element derived miRNAs.
These observations, along with the evidence of very complex and widespread transcriptional patterns in eukaryotes, including nested transcripts and antisense transcription CITATION, underlines the importance of enumerating all possible miRNA/target interactions in order to explore the full potential of miRNA-mediated regulation.
In this paper, we develop and apply the miMatcher pipeline to perform intragenomic matching followed by classification of miRNA candidates using support vector machines.
Using this method in the three plant genomes A. thaliana, O. sativa, and P. trichocarpa, we find species-specific miRNA-like hairpins with almost perfect complementarity to mRNA targets.
We present indications that many of these are active and hypothesize that the remainder forms a pool of regulators, which can easily be recruited by natural selection on the adapting organisms.
Finding functional DNA binding sites of transcription factors throughout the genome is a crucial step in understanding transcriptional regulation.
Unfortunately, these binding sites are typically short and degenerate, posing a significant statistical challenge: many more matches to known TF motifs occur in the genome than are actually functional.
However, information about chromatin structure may help to identify the functional sites.
In particular, it has been shown that active regulatory regions are usually depleted of nucleosomes, thereby enabling TFs to bind DNA in those regions.
Here, we describe a novel motif discovery algorithm that employs an informative prior over DNA sequence positions based on a discriminative view of nucleosome occupancy.
When a Gibbs sampling algorithm is applied to yeast sequence-sets identified by ChIP-chip, the correct motif is found in 52 percent more cases with our informative prior than with the commonly used uniform prior.
This is the first demonstration that nucleosome occupancy information can be used to improve motif discovery.
The improvement is dramatic, even though we are using only a statistical model to predict nucleosome occupancy; we expect our results to improve further as high-resolution genome-wide experimental nucleosome occupancy data becomes increasingly available.
Finding functional DNA binding sites of transcription factors throughout the genome is a necessary step in understanding transcriptional regulation.
However, despite an explosion of TF binding data from high-throughput technologies like ChIP-chip, DIP-chip CITATION, PBM CITATION, and gene expression arrays, finding functional occurrences of binding sites of TFs remains a difficult problem because the binding sites of most TFs are short, degenerate sequences that occur frequently in the genome by chance.
In particular, matches to known TF motifs in the genome often do not appear to be bound by the respective TFs in vivo.
One popular explanation for this is that when the DNA is in the form of chromatin, not all parts of the DNA are equally accessible to TFs.
In this state, DNA is wrapped around histone octamers, forming nucleosomes.
The positioning of these nucleosomes along the DNA is believed to provide a mechanism for differential access to TFs at potential binding sites.
Indeed, it has been shown that functional binding sites of TFs at regulatory regions are typically depleted of nucleosomes in vivo CITATION CITATION .
If we knew the precise positions of nucleosomes throughout the genome under various conditions, we could increase the specificity of motif finders by restricting the search for functional binding sites to nucleosome-free areas.
Here, we describe a method for incorporating nucleosome positioning information into motif discovery algorithms by constructing informative priors biased toward less-occupied promoter positions.
Our method should improve motif discovery most when it has access to high-resolution nucleosome occupancy data gathered under various in vivo conditions.
Unfortunately, this data is not currently available for any organism at a whole-genome scale, let alone under a variety of conditions.
Nevertheless, because our method is probabilistic, even noisy evidence regarding nucleosome positioning can be effectively exploited.
For example, Segal et al. CITATION recently published a computational model based on high-quality experimental nucleosome binding data that predicts the probability of each nucleotide position in the yeast genome being bound by a nucleosome; these predictions are intrinsic to the DNA sequence and thus independent of condition, but were purported to explain around half of nucleosome positions observed in vivo.
In addition, Lee et al. CITATION have used ChIP-chip to profile the average nucleosome occupancy of each yeast intergenic region.
We show that informative positional priors, whether learned from computational occupancy predictions or low-resolution average occupancy data, significantly outperform not only the commonly used uniform positional prior, but also state-of-the-art motif discovery programs.
It has become clear that noncoding RNAs play important roles in cells, and emerging studies indicate that there might be a large number of unknown ncRNAs in mammalian genomes.
There exist computational methods that can be used to search for ncRNAs by comparing sequences from different genomes.
One main problem with these methods is their computational complexity, and heuristics are therefore employed.
Two heuristics are currently very popular: pre-folding and pre-aligning.
However, these heuristics are not ideal, as pre-aligning is dependent on sequence similarity that may not be present and pre-folding ignores the comparative information.
Here, pruning of the dynamical programming matrix is presented as an alternative novel heuristic constraint.
All subalignments that do not exceed a length-dependent minimum score are discarded as the matrix is filled out, thus giving the advantage of providing the constraints dynamically.
This has been included in a new implementation of the FOLDALIGN algorithm for pairwise local or global structural alignment of RNA sequences.
It is shown that time and memory requirements are dramatically lowered while overall performance is maintained.
Furthermore, a new divide and conquer method is introduced to limit the memory requirement during global alignment and backtrack of local alignment.
All branch points in the computed RNA structure are found and used to divide the structure into smaller unbranched segments.
Each segment is then realigned and backtracked in a normal fashion.
Finally, the FOLDALIGN algorithm has also been updated with a better memory implementation and an improved energy model.
With these improvements in the algorithm, the FOLDALIGN software package provides the molecular biologist with an efficient and user-friendly tool for searching for new ncRNAs.
The software package is available for download at LINK.
Noncoding RNA genes and regulatory structures have been shown to be both highly abundant and highly diverse parts of the genome CITATION, CITATION.
One theory is that many of these ncRNAs are part of RNA-based regulatory systems CITATION.
Recently, several papers about large-scale searches for vertebrate RNA genes or motifs using comparative genomics have been published CITATION CITATION.
These large-scale searches indicate that there are potentially many unknown structures still hidden in the genomes.
It has been shown that alignment of ncRNAs requires information about secondary structure when the sequence similarity is below 60 percent CITATION.
The reason for this is that compensating mutations change the primary sequence without changing the structure of the molecule.
The Sankoff algorithm for simultaneously folding and aligning of RNA sequences can in principle be applied to cope with this CITATION.
However, the resource requirements of the algorithm are too high even for a few short sequences.
For two sequences of length L, the time complexity is O and the memory complexity is O. Heuristics are therefore needed before the algorithms for folding and aligning RNA sequences become fast enough to be useful.
FOLDALIGN 1.0 was the first simplified implementation of the Sankoff algorithm CITATION.
It contained a simple scoring scheme with separate substitution matrices for base-paired and single-stranded nucleotides.
It had three constraints: the length of the final alignment could not be longer than nucleotides; the maximum length difference between two subsequences being aligned was limited to nucleotides, and it could only align stem-loop structures.
The second version of the algorithm uses a combination of substitutions and a lightweight energy model to align two sequences CITATION.
FOLDALIGN 2.0 also uses the and constraints, but it can align branched structures.
This algorithm was used in one of the large-scale searches for vertebrate ncRNAs CITATION .
Variants of two types of heuristics are currently very popular, namely pre-aligning and pre-folding.
The pre-aligning methods use sequence similarity to limit the search space by requiring that the final alignment must contain the pre-aligned nucleotides.
The length of the pre-aligned subsequences varies from short stretches called anchors CITATION to full sequences CITATION CITATION.
Methods that require the sequences to be fully aligned before the structure is predicted are not strictly Sankoff-based methods, as they separate the alignment and folding steps completely.
Pre-folding uses single-sequence folding to limit the structures that can be found by the comparative algorithms.
A popular method of pre-folding is to use base-pairing probabilities found by the single-sequence folding to limit which base pairs can be included in the conserved structure CITATION, CITATION.
As for align-then-fold methods, methods using pre-folding can be taken to the extreme where the folding and alignment steps are completely separated.
One example of this is the combination of the RNAcast and the RNAforester methods CITATION, CITATION.
Some methods can use both pre-aligning and pre-folding heuristics CITATION CITATION .
The currently implemented Sankoff-based methods, for pairwise alignment and secondary-structure prediction of RNA sequences, can be split into two groups: the energy-based methods and the probabilistic methods.
The energy-based methods, FOLDALIGN, Dynalign CITATION, locaRNA CITATION, and SCARNA CITATION, are based on minimization of the free energy CITATION.
Free-energy minimization is based on a physical model of how the different elements of an RNA structure contribute to the free energy.
The parameters are partly found experimentally and partly estimated from multiple alignments.
The probabilistic models are usually based on Stochastic Context Free Grammars ; see CITATION for an introduction.
These methods include Consan CITATION and Stemloc CITATION.
The Stochastic Context Free Grammars parameters are estimated from multiple alignments.
Each of these methods uses different kinds of heuristics.
The previous version of FOLDALIGN CITATION uses banding, limits the alignment length for local alignments, and limits the number of ways a bifurcation can be calculated.
Dynalign CITATION uses banding based on pre-alignment using a hidden Markov model.
LocaRNA CITATION limits the number of potential base pairs by only using base pairs with a single-sequence base-pair probability above a given cutoff.
SCARNA CITATION uses a similar strategy, but further decouples the left and right sides of the base pairs.
Consan CITATION uses short stretches of normal sequence alignments to constrain the folding.
Stemloc CITATION uses the N 1 best single-sequence predicted structures and the N 2 best normal-sequence alignments to limit the final combined alignment and structure prediction.
In this paper, dynamical pruning of the dynamic programming matrix is introduced as a new heuristic in the FOLDALIGN algorithm CITATION.
In all its simplicity, the dynamic pruning discards any subalignment that does not have a score above a length-dependent threshold.
This is similar to one of the heuristics used in BLAST CITATION.
The advantage of the pruning method compared with the pre-aligning methods is that it can be used when there is not enough sequence similarity to make the necessary alignments.
The advantage compared with the pre-folding methods is that none of the comparative information is lost in a single-sequence folding step.
It is shown empirically that the pruning leads to a huge speed increase while the algorithm retains its good performance.
The speed increase makes studies like CITATION much more feasible.
The method of dynamical pruning is simple and general.
It should therefore be possible to use it in many of the other methods available for folding and aligning RNA sequences.
As pruning is a feature of the dynamic programming method, it may be used in any algorithm using dynamic programming.
In addition to the dynamical pruning, the FOLDALIGN software package has been significantly updated.
The constraint, which speeds up the algorithm by limiting the calculation of branch points, is now also used to lower the memory requirement during the local-alignment stage.
During the backtrack stage of the algorithm, more information is needed.
To try to limit the memory consumption during this stage, an extra pre-backtrack step is used.
The pre-backtrack step locates all branch points in the conserved structure, and these are then used to divide the structure into unbranched segments.
The unbranched structures are then realigned and backtracked separately.
As the unbranched structures usually are shorter than the full branched structure, the memory consumption is reduced.
The use of the divide and conquer method increases the run time of the algorithm, but not by much since the realignments of the segments are unbranched.
In addition to the algorithmic improvements, the energy model has been improved as well.
External single-strand nucleotides are scored in a consistent way.
Also, more insert base pairs are allowed.
These improvements lead to better structure predictions.
Acetylcholinesterase rapidly hydrolyzes acetylcholine in the neuromuscular junctions and other cholinergic synapses to terminate the neuronal signal.
In physiological conditions, AChE exists as tetramers associated with the proline-rich attachment domain of either collagen-like Q subunit or proline-rich membrane-anchoring protein.
Crystallographic studies have revealed that different tetramer forms may be present, and it is not clear whether one or both are relevant under physiological conditions.
Recently, the crystal structure of the tryptophan amphiphilic tetramerization domain of AChE associated with PRAD, which mimics the interface between ColQ and AChE tetramer, became available.
In this study we built a complete tetrameric mouse AChE T 4 ColQ atomic structure model, based on the crystal structure of the WAT 4PRAD complex.
The structure was optimized using energy minimization.
Block normal mode analysis was done to investigate the low-frequency motions of the complex and to correlate the structure model with the two known crystal structures of AChE tetramer.
Significant low-frequency motions among the catalytic domains of the four AChE subunits were observed, while the WAT 4PRAD part held the complex together.
Normal mode involvement analysis revealed that the two lowest frequency modes were primarily involved in the conformational changes leading to the two crystal structures.
The first 30 normal modes can account for more than 75 percent of the conformational changes in both cases.
The evidence further supports the idea of a flexible tetramer model for AChE.
This model can be used to study the implications of the association of AChE with ColQ.
Acetylcholinesterase rapidly hydrolyzes acetylcholine to terminate neurotransmissions at cholinergic synapses CITATION, CITATION.
The reaction is very fast, approaching the diffusion limit.
AChE has three different molecular forms due to an alternate splicing scheme at the C-terminus CITATION.
The T-subtype with a 40-residue C-terminal t-peptide is the only form expressed in the brain and adult muscles of normal adult mammals CITATION.
In vertebrate cholinergic synapses, tetramers of AChE T are associated with either collagen-like Q subunit or transmembrane proline-rich membrane-anchoring protein CITATION, CITATION.
ColQ is a structural protein that anchors AChE T to the synaptic basal lamina CITATION, CITATION, and PRiMA is a membrane protein that anchors AChE T to the membrane of neuronal synapses in the brain CITATION.
They both contain a proline-rich attachment domain near the N-terminus, which is the site for interacting with the t-peptide of AChE.
The PRAD has three and five consecutive proline residues, and it has been shown that synthetic polyproline could replace PRAD in its association with AChE T tetramers CITATION.
In AChE T the t-peptide is absolutely required in its association with PRAD CITATION.
The sequence of the t-peptide is highly conserved throughout vertebrates, with a cysteine at 4 position from the C-terminus and a series of seven aromatic residues, including four equally spaced tryptophans.
Because the t-peptide constitutes an autonomous interacting domain, it has been named the tryptophan amphiphilic tetramerization domain.
In this notation, AChE T is equivalent to AChE WAT CITATION .
Recently the crystal structure of PRAD/WAT complex was solved at 2.35 resolution CITATION.
The complex has the expected WAT 4PRAD stoichiometry.
Four parallel -helical WAT chains wrap around a single antiparallel PRAD helix, which itself has a left-handed polyproline II conformation.
Each WAT helix assumes a coiled-coil conformation, and all four of them form a left-handed supercoil around the PRAD.
The WWW motif in the WAT makes repetitive hydrophobic stacking and hydrogen bond interactions with the PRAD.
The four WAT chains are related by a 4-fold screw axis around the PRAD.
The strength of PRAD WAT interaction is very tight, with no monomer of WAT detected in the range of 10 10 to 10 12 M CITATION .
It remains unknown how the four AChE subunits are arranged in the tetramer associated with the PRAD.
Low-resolution crystallographic studies revealed two distinct three-dimensional structures of AChE tetramer CITATION.
Both crystal structures show a dimer of dimers, i.e., there is no 4-fold symmetry to relate all four subunits.
In one structure two AChE dimers are close with all four C-terminal sequences aligning to the same direction, and in the other structure the space between the two dimers is large and the four C-terminal sequences are aligned antiparallel to the middle.
The crystals were grown from trypsin-digested, collagen-tailed AChE and should both have WAT and PRAD preserved; although electronic densities were seen, it was not possible to resolve them.
It was suggested that the flexibility of AChE tetramers might be related to the regulation of catalysis CITATION, CITATION.
To test this, reaction-rate calculations were conducted using these two tetramer structures and a morphed intermediate structure.
The results showed that the rate per active site was reduced due to active site occlusion and sink sink competition compared to the monomeric form, but could be partly compensated by electrostatic enhancements in the tetramers CITATION.
The rate reduction due to active site occlusion was particularly notable for the compact tetramer CITATION .
Efforts to combine the PRAD/WAT structure and the two AChE tetramer structures were not very successful.
The problem was that in both tetramer structures the four AChE subunits lacked 4-fold symmetry as seen in the WAT 4PRAD complex crystal structure CITATION.
Since the four WAT chains are staggered in the structure, it is impossible, without substantial distortion of the WAT 4PRAD complex and/or the AChE tetramer structures, to dock the WAT 4PRAD complex along the 2-fold axis between the pairs of AChE dimers.
In fact, the superhelical axis of the WAT 4PRAD complex structure is at an angle of about 30 to this 2-fold axis CITATION .
Considering the tight association between PRAD and WAT, the weak affinity of the AChE dimer, and the limited contact between the two dimers in the tetramer, it is reasonable to assume that the PRAD WAT interaction dominates over the inter-subunit interaction in the association of AChE tetramer with ColQ.
In fact, AChE only exists as a soluble monomer if the WAT sequence is deleted CITATION, indicating that the dimerization forces are weak.
In a previous study of subunit association in cholinesterases, this interaction was identified as the weak hydrophobic interaction, compared with the strong interaction seen between WAT and PRAD CITATION.
It has also been shown that if two proteins, for example, AChE and alkaline phosphatase both with WAT sequences at their C-termini are mixed with PRAD at various ratios, tetramers containing three subunits of one protein and one of the other are underrepresented CITATION.
This points to a symmetric AChE T 4 ColQ complex form in physiological conditions.
Since the two crystal structures of AChE tetramer were generated experimentally, they have to be accessible from the physiological model through conformational changes.
In this study, we built an AChE T 4 ColQ complex model strictly following the PRAD WAT interaction.
Although molecular dynamics is a useful tool to observe dynamics in conformational changes, the size of the system poses too much a challenge in this case CITATION.
Block normal mode analysis uses coarse-grained representation for the protein to reduce the computational cost and has been shown to be a useful tool in predicting low-frequency motions seen in large protein assemblies, for example, the swelling of a virus capsid CITATION CITATION, the ratchet motion of the ribosome CITATION, the collective motions in HIV-1 transcriptase CITATION, myosin CITATION, CITATION, ATPase CITATION, CITATION, and chaperonin GroEL CITATION, etc. These low-frequency motions are often related to the conformational changes required by the biological functions of the protein assemblies.
Here we applied BNMA to the AChE T 4 ColQ complex model and calculated the 100 lowest normal modes.
By projecting the conformational changes onto these normal modes, it was found that the two AChE tetramer crystal structures could be rationalized by using these low-frequency normal modes.
Network analysis transcends conventional pairwise approaches to data analysis as the context of components in a network graph can be taken into account.
Such approaches are increasingly being applied to genomics data, where functional linkages are used to connect genes or proteins.
However, while microarray gene expression datasets are now abundant and of high quality, few approaches have been developed for analysis of such data in a network context.
We present a novel approach for 3-D visualisation and analysis of transcriptional networks generated from microarray data.
These networks consist of nodes representing transcripts connected by virtue of their expression profile similarity across multiple conditions.
Analysing genome-wide gene transcription across 61 mouse tissues, we describe the unusual topography of the large and highly structured networks produced, and demonstrate how they can be used to visualise, cluster, and mine large datasets.
This approach is fast, intuitive, and versatile, and allows the identification of biological relationships that may be missed by conventional analysis techniques.
This work has been implemented in a freely available open-source application named BioLayout Express 3D.
Complete genome sequencing of hundreds of pathogenic and model organisms has provided the parts list required for large-scale studies of gene function CITATION.
Enormous amounts of data pertaining to the nature of genes and proteins and their interactions in the cell have now been generated by techniques including, but not limited to: gene coexpression analysis, yeast two-hybrid assays, mass spectrometry, and RNA interference CITATION.
Such functional genomics and proteomics approaches, when combined with computational biology and the emerging discipline of systems biology, finally allow us to begin comprehensive mapping of cellular and molecular networks and pathways CITATION, CITATION.
One of the main difficulties we currently face is how best to integrate these disparate data sources and use them to better understand biological systems in health and disease CITATION .
Visualisation and analysis of biological data as networks is becoming an increasingly important approach to explore a wide variety of biological relationships.
Such approaches have already been used successfully in the study of sequence similarity, protein structure, protein interactions, and evolution CITATION CITATION.
Shifting biological data into a graph/network paradigm allows one to use algorithms, techniques, ideas, and statistics previously developed in graph theory, engineering, computer science, and computational systems biology.
In classical graph theory, a graph or network consists of nodes connected by edges.
For biological networks, nodes are usually genes, transcripts, or proteins, while edges tend to represent experimentally determined similarities or functional linkages between them CITATION .
Conventional analysis techniques are generally pairwise, where an individual relationship between two biological entities is studied without considering higher-order interactions with their neighbours.
Graph and network analysis techniques allow the exploration of the position of a biological entity in the context of its local neighbourhood in the graph, and the network as a whole CITATION.
Another important advantage of such techniques is that for noisy datasets, spurious edges tend not to form structure in the resultant graph, but instead randomly link nodes; although this may not be the case for data generated by techniques with inherent technical biases.
Because many network analysis techniques exploit local structure in networks between biologically related nodes, they are far less troubled by inherent noise, which may confound conventional pairwise approaches CITATION .
One example of network analysis is the clustering of protein protein similarity and interaction networks.
These techniques illustrate that graph clustering performs extremely well and allows the discovery of novel aspects of biological function CITATION.
Such techniques can hence provide insight into both local features of networks and also global features of the network .
Although network analysis of biological data has shown great promise, little attention has been paid to microarray gene expression data.
These data are now abundant, generally of high quality, and consist of the type of high-dimensional data for which such approaches are well-suited.
In principle, transformation of gene expression data into a network graph holds few challenges.
The similarity between individual expression profiles may be determined by one of a number of possible statistical techniques, e.g., the Pearson and Spearman correlation coefficients CITATION.
Networks can be constructed by connecting transcripts by edges that infer varying degrees of coexpression based on an arbitrary correlation threshold CITATION.
Indeed, a number of groups have previously sought to apply the network paradigm to microarray data, establishing relationships between genes based on correlation of expression CITATION CITATION.
While these studies have suggested the power of this approach, limitations in the functionality and visualisation capabilities of the tools supporting their attempts have severely limited their approaches for general application.
In this manuscript, we describe the development and application of a new network analysis tool, BioLayout Express 3D, that facilitates the construction, visualisation, clustering, and analysis of microarray gene expression data.
Specifically, we chose to analyse the Genomics Institute of the Novartis Research Foundation mouse tissue gene expression atlas to demonstrate the efficacy of this approach CITATION.
The GNF data was generated so as to provide a genome-wide analysis of transcript abundance across a wide range of normal tissue and cell types.
This dataset represents one of the most complete systematic studies of tissue-specific expression in the mammalian transcriptome to date.
However, in common with other large datasets, analysis of these data presents significant challenges.
Certain genes are known to only be expressed by a single cell type, at specific times during development, or in response to explicit stimuli.
Others are thought to be expressed by all cells simultaneously at about the same level.
Between these two extremes, there are many other genes that are expressed in most or a number of cell types, but whose transcription may be regulated to give a specific temporal and spatial pattern of expression.
It is also known that genes that play distinct roles in a common pathway or biological process are often expressed in a similar manner; i.e., they are coexpressed CITATION.
Hence, when genes are found to have analogous expression profiles, this may indicate that the genes have linked functional activities.
To better understand aspects of gene regulation and the functional role of the encoded proteins, we chose to explore the utility of network analysis to explore the innate structure of this dataset.
We demonstrate that this approach can accurately locate clusters of genes sharing similar network connectivity, the relationships between these clusters, and statistical analysis of functional annotations.
In many biological systems, the interactions that describe the coupling between different units in a genetic network are nonlinear and stochastic.
We study the interplay between stochasticity and nonlinearity using the responses of Chinese hamster ovary mammalian cells to different temperature shocks.
The experimental data show that the mean value response of a cell population can be described by a mathematical expression which is valid for a large range of heat shock conditions.
A nonlinear stochastic theoretical model was developed that explains the empirical law for the mean response.
Moreover, the theoretical model predicts a specific biological probability distribution of responses for a cell population.
The prediction was experimentally confirmed by measurements at the single-cell level.
The computational approach can be used to study other nonlinear stochastic biological phenomena.
Complex biological systems are built out of a huge number of components.
These components are diverse: DNA sequence elements, mRNA, transcription factors, etc. The concentration of each component changes over time.
One way to understand the functions of a complex biological system is to construct a quantitative model of the interactions present in the system.
These interactions are usually nonlinear in terms of the concentrations of the components that participate in the interaction process.
For example, the concentration of a dimer is proportional to the product of the concentrations of the molecules that dimerise.
Besides being nonlinear, the interactions are also stochastic.
The production process of a molecule is not deterministic, and it is governed by a probability rate of production.
In what follows, a nonlinear stochastic model for the response to heat shocks in CHO mammalian cells will be developed.
Heat stress is just one example of the many ways a molecular system can be perturbed.
From a general perspective, the structure of a molecular system is uncovered by imposing different perturbations on the system under study, and then the responses of the system are measured.
From the experimental collection of pairs of input output signals, laws that describe the system can be uncovered.
This is the fundamental idea in Systems and Synthetic Biology CITATION CITATION and has long proved to be successful in the field of electronics.
The input signals are applied through the use of signal generators CITATION CITATION.
An input signal that is easy to manipulate is a heat pulse, the parameters to change being the pulse temperature and its duration.
Members of the stress protein family such as the heat shock protein 70 are highly responsive to temperature variations.
This protein is a molecular chaperone and is a critical component of a complex genetic network that enables the organism to respond to deleterious effects of stress CITATION CITATION.
Since Hsp70 is thus an important regulator in a complex system, our goal was to find if it is possible to develop a mathematical model of the regulation of its expression in mammalian cells exposed to heat shock.
Our specific objectives were determine an equation representing the average expression of Hsp70 over time in a cell population after an initial heat shock, determine how the physical parameters of heat shock influence the parameters of this equation, and determine the mathematical model that describes the expression of Hsp70 at the single-cell level.
We first describe the process of inferring the mathematical model from the experimental data.
Then a mathematical study of the model will follow.
We study how functional constraints bound and shape evolution through an analysis of mammalian voltage-gated sodium channels.
The primary function of sodium channels is to allow the propagation of action potentials.
Since Hodgkin and Huxley, mathematical models have suggested that sodium channel properties need to be tightly constrained for an action potential to propagate.
There are nine mammalian genes encoding voltage-gated sodium channels, many of which are more than 90 percent identical by sequence.
This sequence similarity presumably corresponds to similarity of function, consistent with the idea that these properties must be tightly constrained.
However, the multiplicity of genes encoding sodium channels raises the question: why are there so many?
We demonstrate that the simplest theoretical constraints bounding sodium channel diversity the requirements of membrane excitability and the uniqueness of the resting potential act directly on constraining sodium channel properties.
We compare the predicted constraints with functional data on mammalian sodium channel properties collected from the literature, including 172 different sets of measurements from 40 publications, wild-type and mutant, under a variety of conditions.
The data from all channel types, including mutants, obeys the excitability constraint; on the other hand, channels expressed in muscle tend to obey the constraint of a unique resting potential, while channels expressed in neuronal tissue do not.
The excitability properties alone distinguish the nine sodium channels into four different groups that are consistent with phylogenetic analysis.
Our calculations suggest interpretations for the functional differences between these groups.
Despite the relatively small number of genes in the human genome, there are many examples of groups of nearly identical genes that perform similar functions.
Such diversity could either reflect redundancy or evolutionary specialization CITATION, CITATION.
Specialization could result from tuning to different functional environments, or nearly identical genes might play very different functional roles CITATION .
Here we explore how functional constraints bound and shape evolution through an analysis of mammalian voltage-gated sodium channels.
The primary function of voltage-gated sodium channels is to allow the propagation of action potentials CITATION.
Since Hodgkin and Huxley CITATION, mathematical models have suggested that sodium channel properties need to be tightly constrained for an action potential to propagate.
In mammals, there are nine different genes encoding voltage-gated sodium channels CITATION, many of which are more than 90 percent identical by sequence CITATION.
On one hand, the sequence similarity of the channels presumably corresponds to similarity of their functional properties; this is consistent with the idea that these properties must be tightly constrained.
On the other hand, the multiplicity of genes encoding sodium channels raises the question: why are so many different mechanisms for generating an action potential necessary?
Sodium channels are predominantly found in specific anatomical regions, suggesting that they might be tuned for specific functions.
For example, the channels Na v1.1, Na v1.2, Na v1.3, Na v1.6, and Na v1.7 are predominantly localized in the central and peripheral nervous systems; Na v1.8 and Na v1.9 primarily in the dorsal root ganglion; Na v1.4 primarily at skeletal muscular junctions; Na v1.5 primarily in cardiac tissue CITATION .
In this paper, we address the questions of whether and how sodium channel diversity is bounded by the simplest theoretical constraints on action potential propagation: the sodium channel properties must be tuned to allow the membrane to be excitable, i.e., there must exist a voltage threshold above which an action potential can be produced, and the constraint of a unique resting potential.
Through a theoretical analysis of macroscopic sodium currents, we demonstrate that these two requirements depend only on sodium channel properties, directly constraining the activation and inactivation curves of sodium channels, which are routinely directly measured in experiments.
We then compare the constraints with measurements of mammalian sodium channels reported in the literature.
Our dataset uses 172 different measurements from 40 distinct publications, including both wild-type and mutant Na v1.1 1.9, in human, mouse, and rat, under a range of different conditions including with and without different types of subunits, and with chemicals CITATION CITATION.
The mutant channels tend to be associated with a disease state and hence presumably differ in a physiologically significant way from the wild-type.
Our analysis demonstrates that excitability properties alone distinguish the nine sodium channels into four different groups.
Within each group there is a strong positive correlation between the voltage dependence of activation and inactivation.
The members of each of the four groups are close according to phylogenetic analysis CITATION, CITATION CITATION.
What are the functional differences between these groups?
Two groups correspond to channels expressed in nerve and muscle tissue, respectively.
Another group has the potential for a voltage threshold substantially higher than the other channels.
The final group can only produce action potentials in a narrow conductance range and even then has a maximum voltage threshold which is less than thermal fluctuations.
The separation of the channels into functionally distinct groups suggests that they have evolved to perform specialized tasks.
Lightness illusions are fundamental to human perception, and yet why we see them is still the focus of much research.
Here we address the question by modelling not human physiology or perception directly as is typically the case but our natural visual world and the need for robust behaviour.
Artificial neural networks were trained to predict the reflectance of surfaces in a synthetic ecology consisting of 3-D dead-leaves scenes under non-uniform illumination.
The networks learned to solve this task accurately and robustly given only ambiguous sense data.
In addition and as a direct consequence of their experience the networks also made systematic errors in their behaviour commensurate with human illusions, which includes brightness contrast and assimilation although assimilation only emerged when the virtual ecology included 3-D, as opposed to 2-D scenes.
Subtle variations in these illusions, also found in human perception, were observed, such as the asymmetry of brightness contrast.
These data suggest that illusions arise in humans because natural stimuli are ambiguous, and this ambiguity is resolved empirically by encoding the statistical relationship between images and scenes in past visual experience.
Since resolving stimulus ambiguity is a challenge faced by all visual systems, a corollary of these findings is that human illusions must be experienced by all visual animals regardless of their particular neural machinery.
The data also provide a more formal definition of illusion: the condition in which the true source of a stimulus differs from what is its most likely source.
As such, illusions are not fundamentally different from non-illusory percepts, all being direct manifestations of the statistical relationship between images and scenes.
Understanding how we generate accurate perceptions of surfaces is often best informed by understanding why we sometimes do not.
Thus, illusions of lightness are essential tools to vision research.
In many natural environments, light levels vary across space and over time.
It is important to be able to perceive surfaces independently of this varying light intensity in order to forage or predate successfully, for example.
A number of models of lightness perception have been proposed, but most of these fail to deal with complex stimuli or only demonstrate a narrow range of behaviours.
For instance, one well-known heuristic model predicts human lightness perceptions by first subdividing stimuli into multiple local frameworks based on, for instance, junction analysis, and co-planarity as well as other classic gestalt factors.
Then, within each framework, the ratio of a patch's intensity and the maximum intensity in that patch's local framework is used to predict the reflectance, combining a bright is white and a large is white area rule CITATION.
These rules are well-defined and effective for simple stimuli, but the application of the rule has not been studied for more complex images CITATION.
Indeed, it is hard to see how such a model could be applied to even moderately complex stimuli, much less natural scenes under spatially heterogeneous illumination, without extremely complex edge-classification rules that are as yet undefined.
Furthermore, such human-based heuristics provide little insight into the physiological and/or computational principles of vision that are relevant to all visual animals.
More computational approaches, on the other hand, are less descriptive, more quantitative, and make fewer assumptions.
For example, artificial neural networks have been trained to extract scene information, such as object shape and movement, from simple synthetic images CITATION, CITATION ; and a statistical approach using Gibbs sampling and Markov random fields has been used to separate reflectance and illumination from simple images CITATION.
Most such models, however, are unable to explain brightness contrast and assimilation simultaneously without recourse to one or more adjustable weighting factors.
One approach that can is the Blakeslee and McCourt filter model CITATION.
By applying a set of filters, the model produces results that correspond closely to psychophysical results on a wide range of illusory stimuli.
The same model, however, fails to predict the asymmetry of brightness contrast, where darker surrounds cause larger illusions than equally lighter surrounds, as we discuss later.
While these asymmetries are not captured by the ODOG model as it is presently implemented, permitting different gain parameters to be applied to the outputs of independent on-channels and off-channels would constitute a logical first step toward accommodating these differences CITATION.
It is also important to stress that the model does not attempt to predict the reflectance of surfaces, only the perceived brightness of a stimulus, and therefore is unable to explain lightness constancy in more natural scenes under spatially heterogeneous illumination.
Related machine vision work includes the separation of luminance changes into those caused by shading, and those caused by paint on the surface, using filters and a mixture of Gaussians CITATION ; and a localised mixture of experts and a set of multiscale filters has been used to extract the intrinsic components of an image, including de-noising it CITATION.
However, these studies do not attempt to explain the human perception of lightness or illusions.
Thus, explanations as to why and how we see lightness illusions remain incomplete.
Here we take a different approach to rationalising human illusions and, by extension, lightness perception generally.
Rather than modelling human perception or known primate physiology as is typical of most models we instead model the empirical process by which vision resolves the most fundamental challenge of visual ecology: the inherent ambiguity of visual stimuli.
We make no assumptions about particular physiology or cognition, but instead model the process of development/learning from stimuli with feedback from the environment.
This is analogous to the experiential learning of any animal whose behaviour is guided visually, and which must learn to resolve perceptual ambiguity in order to survive.
The major DNA constituent of primate centromeres is alpha satellite DNA.
As much as 2 percent 5 percent of sequence generated as part of primate genome sequencing projects consists of this material, which is fragmented or not assembled as part of published genome sequences due to its highly repetitive nature.
Here, we develop computational methods to rapidly recover and categorize alpha-satellite sequences from previously uncharacterized whole-genome shotgun sequence data.
We present an algorithm to computationally predict potential higher-order array structure based on paired-end sequence data and then experimentally validate its organization and distribution by experimental analyses.
Using whole-genome shotgun data from the human, chimpanzee, and macaque genomes, we examine the phylogenetic relationship of these sequences and provide further support for a model for their evolution and mutation over the last 25 million years.
Our results confirm fundamental differences in the dispersal and evolution of centromeric satellites in the Old World monkey and ape lineages of evolution.
Alpha-satellite is the only functional DNA sequence associated with all naturally occurring human centromeres.
Alpha satellite consists of tandem repetitions of a 171-bp AT-rich sequence motif.
In humans, two distinct forms of alpha-satellite are recognized based on their organization and sequence properties.
In humans, a large fraction is arranged into higher-order repeat arrays where alpha-satellite monomers are organized as multimeric repeat units ranging in size from 3 5 Mb CITATION.
While individual human alpha satellite monomer units show 20 percent 40 percent single-nucleotide variation, the sequence divergence between higher-order repeat units is typically less than 2 percent CITATION, CITATION.
The number of multimeric repeats within any centromere varies between different human individuals and, as such, is a source of considerable chromosome length polymorphism.
Unequal crossover of satellite DNA between sister chromatid pairs or between homologous chromosomes during meiosis is largely responsible for copy-number differences and is thought to be fundamental in the evolution of these HOR arrays.
The organization and unit of periodicity of these arrays are specific to each human chromosome CITATION, CITATION, with the individual monomer units classified into one of five different suprafamilies based on their sequence properties CITATION, CITATION.
Interestingly, studies of closely related primates, such as the chimpanzee and orangutan CITATION, CITATION indicate that these particular associations do not persist among the centromeres of homologous chromosome, implying that the structure and content of centromeric DNA changes very quickly over relatively short periods of evolutionary time.
In addition to higher-order arrays, large tracts of alpha-satellite DNA have more recently been described that are devoid of any HOR structure CITATION, CITATION CITATION.
The individual repeats within these segments show extensive sequence divergence and have been classified as monomeric alpha-satellite DNA.
Such monomeric tracts are frequently located at the periphery of centromeric DNA CITATION, CITATION, CITATION.
Consequently, unlike higher-order arrays, some of these regions have been accurately sequenced and assembled because they localize in the transition regions between euchromatin and heterochromatin.
Phylogenetic and probabilistic analyses suggest that the higher-order alpha-satellite DNA emerged more recently and displaced existing monomeric repeat sequence as opposed to having arisen by unequal crossing-over of local monomeric DNA CITATION .
Centromeres and pericentromeric regions are frequently poorly assembled in primate whole-genome sequence assemblies CITATION CITATION.
These regions are generally regarded as too difficult to accurately sequence and assemble strictly from whole-genome shotgun sequence.
However, most WGS sequencing efforts include substantial amounts of alpha-satellite repeat sequence.
Indeed, as much as 2 percent 5 percent of the sequence generated from the underlying WGS consists of centromeric satellite sequences such data most often remain as unassembled in public database repositories.
In this study, we develop computational methods to systematically identify and classify alpha-satellite sequences from primate WGS sequence.
We predict novel HOR structures from uncharacterized primate genomes and define the phylogenetic relationship of these sequences within the context of known human HOR satellite sequences.
Finally, we take advantage of publicly available cloned resources to experimentally validate the dispersal of these newly described alpha-satellite sequences within various primate genomes.
The data provide the first genome-wide sequence analysis of alpha-satellite DNA among primates from WGS data and a framework to identify and characterize more repeat-rich, complex regions of genomes as part of genome sequencing projects.
A fundamental problem in neuroscience is understanding how working memory the ability to store information at intermediate timescales, like tens of seconds is implemented in realistic neuronal networks.
The most likely candidate mechanism is the attractor network, and a great deal of effort has gone toward investigating it theoretically.
Yet, despite almost a quarter century of intense work, attractor networks are not fully understood.
In particular, there are still two unanswered questions.
First, how is it that attractor networks exhibit irregular firing, as is observed experimentally during working memory tasks?
And second, how many memories can be stored under biologically realistic conditions?
Here we answer both questions by studying an attractor neural network in which inhibition and excitation balance each other.
Using mean-field analysis, we derive a three-variable description of attractor networks.
From this description it follows that irregular firing can exist only if the number of neurons involved in a memory is large.
The same mean-field analysis also shows that the number of memories that can be stored in a network scales with the number of excitatory connections, a result that has been suggested for simple models but never shown for realistic ones.
Both of these predictions are verified using simulations with large networks of spiking neurons.
A critical component of any cognitive system is working memory a mechanism for storing information about past events, and for accessing that information at later times.
Without such a mechanism, even simple tasks, such as deciding whether to wear a heavy jacket or a light sweater after hearing the weather report, would be impossible.
Although it is not known exactly how storage and retrieval of information is implemented in neural systems, a very natural way is through attractor networks.
In such networks, transient events in the world trigger stable patterns of activity in the brain, so by looking at the pattern of activity at the current time, other areas in the brain can know something about what happened in the past.
There is now considerable experimental evidence for attractor networks in areas such as inferior temporal cortex CITATION CITATION, prefrontal cortex CITATION CITATION, and hippocampus CITATION, CITATION.
And from a theoretical standpoint, it is well understood how attractor networks could be implemented in neuronal networks, at least in principle.
Essentially, all that is needed is an increase in the connection strength among subpopulations of neurons.
If the increase is sufficiently large, then each subpopulation can be active without input, and thus remember events that happened in the past.
While the basic theory of attractor networks has been known for some time CITATION CITATION, moving past the in principle qualifier, and understanding how attractors could be implemented in realistic, spiking networks, has been difficult.
This is because the original Hopfield model violated several important principles: neurons did not obey Dale's law; when a memory was activated, neurons fired near saturation, much higher than is observed experimentally in working memory tasks CITATION, CITATION ; and there was no null background state no state in which all neurons fired at low rates.
Most of these problems have been solved.
The first, that Dale's law was violated, was solved by clipping synaptic weights; that is, by using the Hopfield prescription CITATION, assigning neurons to be either excitatory or inhibitory, and then setting any weights of the wrong sign to zero CITATION, CITATION.
The second, building a Hopfield-type network with low firing rate, was solved by adding appropriate inhibition CITATION CITATION.
The third problem, no null background, was solved either by making the units sufficiently stochastic CITATION CITATION or adding external input CITATION, CITATION CITATION .
In spite of these advancements, there are still two fundamental open questions.
One is: how can we understand the highly irregular firing that is observed experimentally in working memory tasks CITATION ? Answering this question is important because irregular firing is thought to play a critical role both in how fast computations are carried out CITATION and in the ability of networks to perform statistical inference CITATION.
Answering it is hard, though, because, as pointed out in CITATION, with naive scaling the net synaptic drive to the foreground neurons is proportional to the number of connections per neuron.
Consequently, because of the high connectivity observed in cortex, the mean synaptic drive is much larger than the fluctuations, which implies that the foreground neurons should fire regularly.
Moreover, as pointed out by Renart et al. CITATION, even for models that move beyond the naive scaling and produce irregularly firing neurons, the foreground neurons still tend to fire more regularly than the background neurons, something that is inconsistent with experiments CITATION .
Several studies have attempted to get around this problem, either directly or indirectly CITATION, CITATION CITATION.
Most of them, however, did not investigate the scaling of the network parameters with its size.
So, although parameters were found which led to irregular activity, it was not clear how those parameters should scale as the size of the network increased to realistic values.
In the two that did investigate scaling CITATION, CITATION, irregular firing was possible only if a small fraction of neurons was involved in each memory; i.e., only if the coding level was very small.
Although there have been no direct measurements of the coding level during persistent activity, at least to our knowledge, experiments in superior temporal sulcus CITATION suggest that it is much larger than the one used in these models.
We should point out, though, that the model of Renart et al. CITATION is the only one in which the foreground neurons are at least as regular as the background neurons.
The second open question is: what is the storage capacity of realistic attractor networks?
That is, how many different memories can be stored in a single network?
Answering this is critical for understanding the highly flexible and seemingly unbounded memory capacity observed in animals.
For simple, albeit unrealistic, models the answer is known: as shown in the seminal work of Amit, Gutfreund, and Sompolinsky CITATION, the number of memories that can be stored in a classical Hopfield network CITATION is about 0.14 times the number of neurons.
For slightly more realistic networks the answer is also known CITATION, CITATION, CITATION, CITATION, CITATION CITATION.
However, even these more realistic studies lacked biological plausibility in at least one way: connectivity was all all rather than sparse CITATION, CITATION, CITATION, CITATION, the neurons were binary CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, there was no null background CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, the firing rate in the foreground state was higher than is observed experimentally CITATION, CITATION, CITATION, CITATION, CITATION, CITATION, or the coding level was very small CITATION, CITATION .
Here we answer both questions: we show, for realistic networks of spiking neurons, how irregular firing can be achieved, and we compute the storage capacity.
Our analysis uses relatively standard mean-field techniques, and requires only one assumption: neurons in the network fire asynchronously.
Given this assumption, we first show that neurons fire irregularly only if the coding level is above some threshold, although a feature of our model is that the foreground neurons are slightly more regular than the background neurons.
We then show that the maximum number of memories in our network the capacity is proportional to the number of connections per neuron, a result that is consistent with the simplified models discussed above.
These predictions are verified with simulations of biologically plausible networks of spiking neurons.
Bacterial species, and even strains within species, can vary greatly in their gene contents and metabolic capabilities.
We examine the evolution of this diversity by assessing the distribution and ancestry of each gene in 13 sequenced isolates of Escherichia coli and Shigella.
We focus on the emergence and demise of two specific classes of genes, ORFans and HOPs, since these genes, in contrast to most conserved ancestral sequences, are known to be a major source of the novel features in each strain.
We find that the rates of gain and loss of these genes vary greatly among strains as well as through time, and that ORFans and HOPs show very different behavior with respect to their emergence and demise.
Although HOPs, which mostly represent gene acquisitions from other bacteria, originate more frequently, ORFans are much more likely to persist.
This difference suggests that many adaptive traits are conferred by completely novel genes that do not originate in other bacterial genomes.
With respect to the demise of these acquired genes, we find that strains of Shigella lose genes, both by disruption events and by complete removal, at accelerated rates.
The wide variation in bacterial genome sizes was originally detected in the 1960s by DNA reassociation analyses CITATION.
And because bacteria have gene dense chromosomes, the differences in genome sizes implied that there were likely to be vast differences in the gene contents of bacterial species.
With the current availability of hundreds of complete genome sequences, it is now possible to establish exactly which genes are present in, as well as those that are absent from, a genome.
Among sequenced bacterial genomes, gene sets vary over 40-fold, ranging from 182 genes in the gammaproteobacterial symbiont Carsonella ruddii CITATION to almost 8000 genes in the soil-dwelling acidobacterium Solibacter usitatus .
The wide variation in genome sizes and gene contents can also be observed between strains within individual bacterial genera or species.
For example, isolates of Frankia that are more than 97 percent identical in their rRNA sequences the conventional cutoff value for a bacterial species can differ by as many as 3500 genes, which represents nearly half of their 7.5 Mb genome CITATION.
Even among bacterial strains of similar genome sizes, there can be substantial differences in gene repertoires CITATION.
Unlike mammals, in which only about 1 percent of the genes in a genome are unique to a taxonomic order, the gene contents of bacterial genomes can change rapidly over relatively short evolutionary distances.
The generation of novel gene repertoires is a consequence of the ongoing processes of gene acquisition and gene loss CITATION CITATION.
Although several mechanisms can generate new genes CITATION, CITATION, CITATION, the novel gene sets observed in closely related bacterial strains result largely from gene transfer from distant sources, as duplications and gene rearrangement only rarely produce entirely unique genes in the short timescales in which bacterial gene sets evolve.
Although homolog searches indicate that many genes arise from lateral transfer from other bacteria, most bacteria also contain genome-specific sets of genes that lack any homologs in the known databases CITATION, CITATION, CITATION.
Counteracting the augmentation of bacterial genomes by gene acquisition, gene loss occurs both through large-scale deletions CITATION as well as by smaller changes that erode and inactivate individual genes CITATION, CITATION, CITATION.
As observed for acquired sequences, prokaryotes also contain genome-specific sets of inactivated genes, which can comprise up to 41 percent of their annotated genes CITATION .
Taken together, these lineage-specific gene repertoires indicate the need to monitor bacterial genome dynamics i.e., the manner in which genes are gained and lost over short evolutionary timescales.
To this end, comparisons of closely related strains of Bacillus CITATION, Staphylococcus aureus CITATION and E. coli CITATION, CITATION have shown that gene acquisitions are prevalent at the tips of the phylogeny and that recently acquired genes seem to evolve more quickly.
However, few studies have examined the fate of these genes within a bacterial lineage or have asked how many or which classes of genes, once acquired, are maintained, disrupted or removed from a genome.
We address these questions by assessing the differences in gene repertoires among 13 sequenced strains of E. coli/Shigella clade.
These strains are closely related, yet display substantial differences in genome size and gene content CITATION, CITATION, allowing us to pinpoint the introduction and persistence of genes in the lineages leading to these genomes.
Recently, a number of advanced screening technologies have allowed for the comprehensive quantification of aggravating and alleviating genetic interactions among gene pairs.
In parallel, TAP-MS studies have been successful at identifying physical protein interactions that can indicate proteins participating in the same molecular complex.
Here, we propose a method for the joint learning of protein complexes and their functional relationships by integration of quantitative genetic interactions and TAP-MS data.
Using 3 independent benchmark datasets, we demonstrate that this method is 50 percent more accurate at identifying functionally related protein pairs than previous approaches.
Application to genes involved in yeast chromosome organization identifies a functional map of 91 multimeric complexes, a number of which are novel or have been substantially expanded by addition of new subunits.
Interestingly, we find that complexes that are enriched for aggravating genetic interactions are more likely to contain essential genes, linking each of these interactions to an underlying mechanism.
These results demonstrate the importance of both large-scale genetic and physical interaction data in mapping pathway architecture and function.
Genetic interactions are logical relationships between genes that occur when mutating two or more genes in combination produces an unexpected phenotype CITATION CITATION.
Recently, rapid screening of genetic interactions has become feasible using Synthetic Genetic Arrays or diploid Synthetic Lethality Analysis by Microarray CITATION, CITATION.
SGA pairs a gene deletion of interest against a deletion to every other gene in the genome.
The growth/no growth phenotype measured over all pairings defines a genetic interaction profile for that gene, with no growth indicating a synthetic-lethal genetic interaction.
Alternatively, all combinations of double deletions can be analyzed among a functionally-related group of genes CITATION CITATION.
A recent variant of SGA termed E-MAP CITATION has made it possible to measure continuous rates of growth with varying degrees of epistasis.
Aggravating interactions are indicated if the growth rate of the double gene deletion is slower than expected, while for alleviating interactions the opposite is true CITATION, CITATION .
One popular method to analyze genetic interaction data has been to hierarchically cluster genes using the distance between their genetic interaction profiles.
Clusters of genes with similar profiles are manually searched to identify the known pathways and complexes they contain as well as any genetic interactions between these complexes.
This approach has been applied to several large-scale genetic interaction screens in yeast including genes involved in the secretory pathway CITATION and chromosome organization CITATION.
Segr et al. CITATION extended basic hierarchical clustering with the concept of monochromaticity, in which genes were merged into the same cluster based on minimizing the number of interactions with other clusters that do not share the same classification .
Another set of methods has sought to interpret genetic relationships using physical protein-protein interactions CITATION.
Among these, Kelley and Ideker CITATION used physical interactions to identify both within-module and between-module explanations for genetic interactions.
In both cases, modules were detected as clusters of proteins that physically interact with each other more often than expected by chance.
The within-module model predicts that these clusters directly overlap with clusters of genetic interactions.
The between-module model predicts that genetic interactions run between two physical clusters that are functionally related.
This approach was improved by Ulitsky et al. CITATION using a relaxed definition of physical modules.
In related work, Zhang et al. CITATION screened known complexes annotated by the Munich Information Center for Protein Sequences to identify pairs of complexes with dense genetic interactions between them.
One concern with the above approaches, and the works by Kelley and Ulitsky in particular, is that they make assumptions about the density of interactions within and between modules which have not been justified biologically.
Ideally, such parameters should be learned directly from the data.
Second, between-module relationships are identified by separate, independent searches of the network seeded from each genetic interaction.
This local search strategy can lead to a set of modules that are highly overlapping or even completely redundant with one another.
Finally, genetic interactions are assumed to be binary growth/no growth events while E-MAP technology has now made it possible to measure continuous values of genetic interaction with varying degrees of epistasis.
Here, we present a new approach for integrating quantitative genetic and physical interaction data which addresses several of these shortcomings.
Interactions are analyzed to infer a set of modules and a set of inter-module links, in which a module represents a protein complex with a coherent cellular function, and inter-module links capture functional relationships between modules which can vary quantitatively in strength and sign.
Our approach is supervised, in that the appropriate pattern of physical and genetic interactions is not predetermined but learned from examples of known complexes.
Rather than identify each module in independent searches, all modules are identified simultaneously within a single unified map of modules and inter-module functional relationships.
We show that this method outperforms a number of alternative approaches and that, when applied to analyze a recent EMAP study of yeast chromosome function, it identifies numerous new protein complexes and protein functional relationships.
Quasispecies are clouds of genotypes that appear in a population at mutation selection balance.
This concept has recently attracted the attention of virologists, because many RNA viruses appear to generate high levels of genetic variation that may enhance the evolution of drug resistance and immune escape.
The literature on these important evolutionary processes is, however, quite challenging.
Here we use simple models to link mutation selection balance theory to the most novel property of quasispecies: the error threshold a mutation rate below which populations equilibrate in a traditional mutation selection balance and above which the population experiences an error catastrophe, that is, the loss of the favored genotype through frequent deleterious mutations.
These models show that a single fitness landscape may contain multiple, hierarchically organized error thresholds and that an error threshold is affected by the extent of back mutation and redundancy in the genotype-to-phenotype map.
Importantly, an error threshold is distinct from an extinction threshold, which is the complete loss of the population through lethal mutations.
Based on this framework, we argue that the lethal mutagenesis of a viral infection by mutation-inducing drugs is not a true error catastophe, but is an extinction catastrophe.
The concept of a mutation selection balance is one of the oldest and most fundamental pillars of population genetics: natural selection increases the frequency of fit variants while mutations introduce unfit variants, giving rise to an equilibrium distribution balanced between these two effects.
Mutation selection balance has been invoked to explain the persistence of undesirable genes, for example, those underlying inbreeding depression, genetic diseases, and even senescence.
Despite the long history of the concept, some of its consequences were only realized in 1971, when Manfred Eigen studied mutation selection dynamics in long genomes CITATION.
He found that populations do not necessarily attain classic mutation selection balances in which the wild-type allele is most common, but rather attain an equilibrium with an abundant assemblage of mutant genotypes and a rare wild-type.
He and Peter Schuster later called this collection of genotypes at equilibrium a quasispecies CITATION.
This concept offered not only an intuitive extension of the mutation selection theory based on simple one- or two-locus systems, but also a novel insight into the impact of mutation rate on evolutionary dynamics.
In particular, Eigen found that there are states in which a trivial boost in the mutation rate can lead to a fundamental change in the composition of genotypes in the population.
This change, a phase transition in physics terms, is called the error catastrophe.
The error catastrophe has been applied liberally as a metaphor for complications of high mutation rates, as likely plagued primordial life CITATION and currently challenges extant viruses with RNA genomes CITATION.
The error-catastrophe model inspired treatments to extirpate viral populations by mutation enhancement CITATION, CITATION, and the model has been generalized to explain the attraction of populations to mutationally robust regions of fitness landscapes CITATION.
The error catastrophe has imparted a mystique to the quasispecies concept, and much of the literature on RNA virus evolution now uses quasispecies as an enriched synonym for a high mutation rate.
An excellent and short review of the topic and its relationship to population genetics theory is provided by Wilke CITATION .
Eigen's insights were developed in the context of genomes with many loci, each of which suffered mutation.
Appropriately, the quasispecies has since been considered in this large-genome context.
Yet many of its concepts are easily illustrated in the much simpler case of few genotypes, which is our approach here.
Our results are not new, per se, but our models should convey quasispecies and error-catastrophe concepts to a broad audience and correct some common misunderstandings.
Paired-end sequencing is emerging as a key technique for assessing genome rearrangements and structural variation on a genome-wide scale.
This technique is particularly useful for detecting copy-neutral rearrangements, such as inversions and translocations, which are common in cancer and can produce novel fusion genes.
We address the question of how much sequencing is required to detect rearrangement breakpoints and to localize them precisely using both theoretical models and simulation.
We derive a formula for the probability that a fusion gene exists in a cancer genome given a collection of paired-end sequences from this genome.
We use this formula to compute fusion gene probabilities in several breast cancer samples, and we find that we are able to accurately predict fusion genes in these samples with a relatively small number of fragments of large size.
We further demonstrate how the ability to detect fusion genes depends on the distribution of gene lengths, and we evaluate how different parameters of a sequencing strategy impact breakpoint detection, breakpoint localization, and fusion gene detection, even in the presence of errors that suggest false rearrangements.
These results will be useful in calibrating future cancer sequencing efforts, particularly large-scale studies of many cancer genomes that are enabled by next-generation sequencing technologies.
Cancer is a disease driven by selection for somatic mutations.
These mutations range from single nucleotide changes to large-scale chromosomal aberrations such as deletion, duplications, inversions and translocations.
While many such mutations have been cataloged in cancer cells via cytogenetics, gene resequencing, and array-based techniques there is now great interest in using genome sequencing to provide a comprehensive understanding of mutations in cancer genomes.
The Cancer Genome Atlas is one such sequencing initiative that focuses sequencing efforts in the pilot phase on point mutations in coding regions.
This approach largely ignores copy neutral genome rearrangements including translocations and inversions.
Such rearrangements can create novel fusion genes, as observed in leukemias, lymphomas, and sarcomas CITATION CITATION.
The canonical example of a fusion gene is BCR-ABL, which results from a characteristic translocation in many patients with chronic myelogenous leukemia CITATION.
The advent of Gleevec, a drug targeted to the BCR-ABL fusion gene, has proven successful in treatment of CML patients CITATION, invigorating the search for other fusion genes that might provide tumor-specific biomarkers or drug targets.
Until recently, it is was generally believed that recurrent translocations and their resulting fusion genes occurred only in hematological disorders and sarcomas, with few suggesting that such recurrent events were prevalent across all tumor types including solid tumors CITATION, CITATION.
This view has been challenged by the discovery of a fusion between the TMPRSS2 gene and several members of the ERG protein family in prostate cancer CITATION and the EML4-ALK fusion in lung cancer CITATION .
These studies raise the question of what other recurrent rearrangements remain to be discovered.
One strategy for genome-wide high-resolution identification of fusion genes and other large scale rearrangements is paired-end sequencing of clones, or other fragments of genomic DNA, from tumor samples.
The resulting end-sequence pairs, or paired reads, are mapped back to the reference human genome sequence.
If the mapped locations of the ends of a clone are invalid then a genomic rearrangement is suggested.
This strategy was initially described in the End Sequence Profiling approach CITATION and later used to assess genetic structural variation CITATION, CITATION.
An innovative approach utilizing SAGE-like sequencing of concatenated short paired-end tags successfully identified fusion transcripts in cDNA libraries CITATION.
Present and forthcoming next-generation DNA sequencers hold promise for extremely high-throughput sequencing of paired-end reads.
For example, the Illumina Genome Analyzer will soon be able to produce millions of paired reads of approximately 30 bp from fragments of length 500 1000 bp CITATION, while the SOLiD system from Applied Biosystems promises 25 bp reads from each end of size selected DNA fragments of many sizes CITATION.
Similar strategies coupling the generation of paired-end tags with 454 sequencing have also been described CITATION, CITATION .
Whole genome paired-end sequencing approaches allow for a genome-wide survey of all potential fusion genes and other rearrangements in a tumor.
This approach holds several advantages over transcript or protein profiling in cancer studies.
First, discovery of fusion genes using mRNA expression CITATION, cDNA sequencing, or mass spectrometry CITATION depends on the fusion genes being transcribed under the specific cellular conditions present in the sample at the time of the assay.
These conditions might be different than those experienced by the cells during tumor development.
Second, measurement of fusions at the DNA sequence level focuses on gene fusions due to genomic rearrangements and thus is less impeded by splicing artifacts or trans splicing CITATION.
Finally, genome sequencing can identify more subtle regulatory fusions that result when the promoter of one gene is fused to the coding region of another gene, as in the case with with the c-Myc oncogene fusion with the immunoglobin gene promoter in Burkitt's lymphoma CITATION .
In this paper, we address a number of theoretical and practical considerations for assessing cancer genome organization using paired-end sequencing approaches.
We are largely concerned with detecting a rearrangement breakpoint, where a pair of non-adjacent coordinates in the reference genome is adjacent in the cancer genome.
In particular, we extend this idea of a breakpoint to examine the ability to detect fusion genes.
Specifically, if a clone with end sequences mapping to distant locations identifies a rearrangement in the cancer genome, does this rearrangement lead to formation of a fusion gene?
Obviously, sequencing the clone will answer this question, but this requires additional effort/cost and may be problematic; e.g. most next-generation sequencing technologies do not archive the genome in a clone library for later analysis.
We derive a formula for the probability of fusion between a pair of genomic regions given the set of all mapped clones and the empirical distribution of clone lengths.
These probabilities are useful for prioritizing follow-up experiments to validate fusion genes.
In a test experiment on the MCF7 breast cancer cell-line, 3,201 pairs of genes were found near clones with aberrantly mapping end-sequences.
However, our analysis revealed only 18 pairs of genes with a high probability of fusion, of which six were tested and five experimentally confirmed .
The advent of high throughput sequencing strategies raises important experimental design questions in using these technologies to understand cancer genome organization.
Obviously, sequencing more clones improves the probability of detecting fusion genes and breakpoints.
However, even with the latest sequencing technologies, it would be neither practical nor cost effective to shotgun sequence and assemble the genomes of thousands of tumor samples.
Thus, it is important to maximize the probability of detecting fusion genes with the least amount of sequencing.
This probability depends on multiple factors including the number and length of end-sequenced clones, the length of genes that are fused, and possible errors in breakpoint localization.
Here, we derive several formulae that elucidate the trade-offs in experimental design of both current and next-generation sequencing technologies.
Our probability calculations and simulations demonstrate that even with current paired-end technology we can obtain an extremely high probability of breakpoint detection with a very low number of reads.
For example, more than 90 percent of all breakpoints can be detected with paired-end sequencing of less than 100,000 clones.
Additionally, next-generation sequencers can potentially detect rearrangements with a greater than 99 percent probability and localize the breakpoints of these rearrangements to intervals of less than 300 bp in a single run of the machine .
The merging of network theory and microarray data analysis techniques has spawned a new field: gene coexpression network analysis.
While network methods are increasingly used in biology, the network vocabulary of computational biologists tends to be far more limited than that of, say, social network theorists.
Here we review and propose several potentially useful network concepts.
We take advantage of the relationship between network theory and the field of microarray data analysis to clarify the meaning of and the relationship among network concepts in gene coexpression networks.
Network theory offers a wealth of intuitive concepts for describing the pairwise relationships among genes, which are depicted in cluster trees and heat maps.
Conversely, microarray data analysis techniques can also be used to address difficult problems in network theory.
We describe conditions when a close relationship exists between network analysis and microarray data analysis techniques, and provide a rough dictionary for translating between the two fields.
Using the angular interpretation of correlations, we provide a geometric interpretation of network theoretic concepts and derive unexpected relationships among them.
We use the singular value decomposition of module expression data to characterize approximately factorizable gene coexpression networks, i.e., adjacency matrices that factor into node specific contributions.
High and low level views of coexpression networks allow us to study the relationships among modules and among module genes, respectively.
We characterize coexpression networks where hub genes are significant with respect to a microarray sample trait and show that the network concept of intramodular connectivity can be interpreted as a fuzzy measure of module membership.
We illustrate our results using human, mouse, and yeast microarray gene expression data.
The unification of coexpression network methods with traditional data mining methods can inform the application and development of systems biologic methods.
Many biological networks share topological properties.
Common global properties include modular organization CITATION, CITATION, the presence of highly connected hub nodes, and approximate scale free topology CITATION, CITATION.
Common local topological properties include the presence of recurring patterns of interconnections in regulation networks CITATION CITATION .
One goal of this article is to describe existing and novel network concepts that can be used to describe local and global network properties.
For example, the clustering coefficient CITATION is a network concept, which measures the cohesiveness of the neighborhood of a node.
We are particularly interested in network concepts that are defined with regard to a gene significance measure.
Gene significance measures are of great practical importance since they allow one to incorporate external gene information into the network analysis.
In functional enrichment analysis, a gene significance measure could indicate pathway membership.
In gene knock-out experiments, gene significance could indicate knock-out essentiality.
We study gene significance measures since a microarray sample trait gives rise to a statistical measure of gene significance.
For example, the Student t-test of differential expression leads to a gene significance measure.
Many traditional microarray data analysis methods focus on the relationship between the microarray sample trait and the gene expression data.
For example, gene filtering methods aim to find a list of genes that are significantly associated with the microarray sample trait; another example are microarray-based prediction methods that aim to accurately predict the sample trait on the basis of the gene expression data.
Gene expression profiles across microarray samples can be highly correlated and it is natural to describe their pairwise relations using network language.
Genes with similar expression patterns may form complexes, pathways, or participate in regulatory and signaling circuits CITATION CITATION.
Gene coexpression networks have been used to describe the transcriptome in many organisms, e.g., yeast, flies, worms, plants, mice, and humans CITATION CITATION.
Gene coexpression network methods have also been used for typical microarray data analysis tasks such as gene filtering CITATION, CITATION CITATION and outcome prediction CITATION, CITATION .
While the utility of network methods for analyzing microarray data has been demonstrated in numerous publications, the utility of microarray data analysis techniques for solving network theoretic problems has not yet been fully appreciated.
One goal of this article is to show that simple geometric arguments can be used to derive network theoretic results if the networks are defined on the basis of a correlation matrix.
Although many of our network concepts will be useful for general networks, we are particularly interested in gene coexpression networks.
Gene coexpression networks are built on the basis of a gene coexpression measure.
The network nodes correspond to genes or more precisely to gene expression profiles.
The ith gene expression profile x i is a vector whose components report the gene expression values across m microarrays.
We define the coexpression similarity s ij between genes i and j as the absolute value of the correlation coefficient between their expression profiles:FORMULA
Using a thresholding procedure, this coexpression similarity is transformed into a measure of connection strength.
An unweighted network adjacency a ij between gene expression profiles x i and x j can be defined by hard thresholding the coexpression similarity s ij as followsFORMULAwhere is the hard threshold parameter.
Thus, two genes are linked if the absolute correlation between their expression profiles exceeds the threshold.
Hard thresholding of the correlation leads to simple network concepts but it may lead to a loss of information: if has been set to 0.8, there will be no link between two genes if their correlation equals 0.799.
To preserve the continuous nature of the coexpression information, one could simply define a weighted adjacency matrix as the absolute value of the gene expression correlation matrix, i.e., a ij s ij.
However, since microarray data can be noisy and the number of samples is often small, we and others have found it useful to emphasize strong correlations and to punish weak correlations.
It is natural to define the adjacency between two genes as a power of the absolute value of the correlation coefficient CITATION, CITATION :FORMULAwith 1.
This soft thresholding approach leads to a weighted gene coexpression network.
We present empirical results for weighted and unweighted networks in the main text, Text S1, Text S2, and Text S3.
Signal output from receptor G-protein effector modules is a dynamic function of the nucleotide exchange activity of the receptor, the GTPase-accelerating activity of GTPase-activating proteins, and their interactions.
GAPs may inhibit steady-state signaling but may also accelerate deactivation upon removal of stimulus without significantly inhibiting output when the receptor is active.
Further, some effectors are themselves GAPs, and it is unclear how such effectors can be stimulated by G proteins at the same time as they accelerate G protein deactivation.
The multiple combinations of protein protein associations and interacting regulatory effects that allow such complex behaviors in this system do not permit the usual simplifying assumptions of traditional enzyme kinetics and are uniquely subject to systems-level analysis.
We developed a kinetic model for G protein signaling that permits analysis of both interactive and independent G protein binding and regulation by receptor and GAP.
We evaluated parameters of the model by global least-squares fitting to a diverse set of steady-state GTPase measurements in an m1 muscarinic receptor G q phospholipase C- 1 module in which GTPase activities were varied by 10 4-fold.
We provide multiple tests to validate the fitted parameter set, which is consistent with results from the few previous pre-steady-state kinetic measurements.
Results indicate that GAP potentiates the GDP/GTP exchange activity of the receptor, an activity never before reported; exchange activity of the receptor is biased toward replacement of GDP by GTP; receptor and GAP bind G protein with negative cooperativity when G protein is bound to either GTP or GDP, promoting rapid GAP binding and dissociation; GAP indirectly stabilizes the continuous binding of receptor to G protein during steady-state GTPase hydrolysis, thus further enhancing receptor activity; and receptor accelerates GDP/GTP exchange primarily by opening an otherwise closed nucleotide binding site on the G protein but has minimal effect on affinity of G protein for nucleotide.
Model-based simulation explains how GAP activity can accelerate deactivation 10-fold upon removal of agonist but still allow high signal output while the receptor is active.
Analysis of GTPase flux through distinct reaction pathways and consequent accumulation of specific GTPase cycle intermediates indicate that, in the presence of a GAP, the receptor remains bound to G protein throughout the GTPase cycle and that GAP binds primarily during the GTP-bound phase.
The analysis explains these behaviors and relates them to the specific regulatory phenomena described above.
The work also demonstrates the applicability of appropriately data-constrained system-level analysis to signaling networks of this scale.
G protein-mediated signaling modules display a variety of dynamic input-output behaviors despite their use of a single, relatively simple biochemical mechanism.
Signal amplification, the ratio of effector proteins activated to agonist-bound receptors, can vary from unity to hundreds.
Activating ligands may bind receptors with affinities ranging from picomolar through millimolar.
GAPs, which can accelerate hydrolysis of bound GTP over 2000-fold, can accelerate both activation and deactivation in cells with variable inhibitory effect CITATION.
Activation and deactivation rates upon addition and removal of agonist can thus range from 10 ms to minutes.
Heterotrimeric G proteins convey signals by traversing a cycle of GTP binding and hydrolysis: the GTP bound state of the G subunit is active and deactivation is caused by hydrolysis of bound GTP to GDP CITATION.
The rates of activation and deactivation, and consequent effects on signal output at steady state, are regulated by interactions of the G subunit with receptors CITATION, G subunits CITATION, GTPase-activating proteins CITATION and multiple other proteins CITATION.
The net effect of these inputs depends on the identities of the individual proteins, their concentrations and their own regulatory controls.
Regulatory inputs to G protein modules are interactive, and it has been difficult to establish quantitative understanding of how they cooperate to control signal output.
While some signals, particularly G protein-gated channels, can be monitored accurately in cells in real time, it has been harder to quantitate the intermediary reactions of the GTPase cycle and protein protein binding or dissociation.
Recently developed optical sensors are promising CITATION CITATION but still do not provide complete or simultaneous coverage of multiple events and often do not provide absolute data.
Conversely, quantitative biochemical assays using in vitro reconstituted systems have provided absolute biochemical data CITATION, CITATION but have not adequately described the simultaneous regulatory interactions that are so important.
Consequently, quantitative understanding of the dynamic behavior of an intact G protein module remains elusive.
Computational modeling is used frequently to clarify mechanistic thinking about complex biochemical systems, including G protein signaling.
Quantitative models can potentially combine information on individual reactions to simulate the behavior of a complex system, or use system-level data to test the validity of a proposed mechanism.
The work of Linderman and colleagues, for example, has provided consistent examples of these approaches to G protein signaling CITATION CITATION.
The G protein-mediated yeast pheromone response has also been the focus of significant modeling efforts because of its presumed paucity of off-pathway inputs CITATION CITATION.
In at least one case, the failure of a simple model of this pathway motivated discovery of a novel mechanism for feedback regulation and subsequent refinement of the model CITATION.
However, modeling of G protein modules has often been descriptive, with parameters arbitrarily chosen for a few reactions such that model output mimics an experimental result.
Alternatively, the inner workings of the G protein module itself have been condensed into an arbitrary function of agonist concentration and receptor regulation to allow analysis of a downstream event such as Ca 2 release or protein phosphorylation or, even more distal, transcription.
A major problem in developing quantitative models of G protein modules has been accurate assignment of parameters to the many processes that are known to occur.
These include both the GTPase cycle reactions and the multiple protein-protein interactions that govern these reactions.
This problem is significant because local protein concentrations at the plasma membrane and the regulated association of these proteins are both unknown, either for resting cells or during dynamic signaling.
In this study, we have used purified proteins, heterotrimeric G q, m1 muscarinic acetylcholine receptors and phospholipase C- 1, reconstituted at uniform and controllable concentrations into unilamellar phospholipid vesicles, to overcome this first limitation.
We estimated formation of multi-protein complexes according to their individual activities.
The second major problem in modeling signaling through G protein modules is the difficulty in assigning correct, or even plausible, values of rate or equilibrium constants for the reactions included in the model.
Despite their apparently small size, an informative model of a single G protein module will contain multiple parameters that are not readily accessible from individual measurements.
These parameters may vary widely among different modules, which prohibits most literature-mining approaches.
If all or most of the relevant parameters are not individually available for the module of interest, then an adequately large and diverse dataset must be produced to allow parameters to be fit to the data.
Last, even with a presumably adequate dataset, the numerical fitting process that extracts values for the parameters and subsequent validation of the fit are both central problems in modeling signaling systems.
We have adapted and extended several approaches to deal with the difficulty of fitting a model with a fairly large number of parameters using a modest amount of data.
We present a modestly complex model of signal output in a G protein model that contains many of the salient regulatory interactions that characterize G protein signaling.
We used steady-state GTPase data to support a Metropolis-Monte Carlo fitting strategy, and argue that most parameters are reasonably assigned, with statistical data to help qualify fits for individual parameters.
The resultant parameter set shows that receptor accelerates both GDP dissociation and GTP binding, and that GAPs potentiate the receptor's nucleotide exchange catalyst activity.
Further, the model argues strongly that GAP activity indirectly favors continued binding of receptor to G protein throughout the GTPase cycle, thus further potentiating the receptor's activity.
Such indirect stabilization of receptor-G protein binding, referred to as kinetic scaffolding to distinguish it from direct interaction, was suggested as a mechanism for how a GAP can accelerate deactivation upon removal of agonist without substantially inhibiting signaling CITATION, CITATION, CITATION, CITATION.
Model-based simulation of signal output describes how GAPs combine these mechanisms to independently control signal amplitude and kinetics.
Patterns of spontaneous activity in the developing retina, LGN, and cortex are necessary for the proper development of visual cortex.
With these patterns intact, the primary visual cortices of many newborn animals develop properties similar to those of the adult cortex but without the training benefit of visual experience.
Previous models have demonstrated how V1 responses can be initialized through mechanisms specific to development and prior to visual experience, such as using axonal guidance cues or relying on simple, pairwise correlations on spontaneous activity with additional developmental constraints.
We argue that these spontaneous patterns may be better understood as part of an innate learning strategy, which learns similarly on activity both before and during visual experience.
With an abstraction of spontaneous activity models, we show how the visual system may be able to bootstrap an efficient code for its natural environment prior to external visual experience, and we continue the same refinement strategy upon natural experience.
The patterns are generated through simple, local interactions and contain the same relevant statistical properties of retinal waves and hypothesized waves in the LGN and V1.
An efficient encoding of these patterns resembles a sparse coding of natural images by producing neurons with localized, oriented, bandpass structure the same code found in early visual cortical cells.
We address the relevance of higher-order statistical properties of spontaneous activity, how this relates to a system that may adapt similarly on activity prior to and during natural experience, and how these concepts ultimately relate to an efficient coding of our natural world.
The classic debates of nature vs. nurture, or innate vs. learned, are pervasive in the literature of early visual development.
A variety of studies have shown that the visual system requires external experience to mature.
On the other hand, many animals are able to see at birth, and have a functioning primary visual cortex even before eye opening.
It might seem straightforward to assign the properties found at birth to be innate and the properties dependent on visual experience to be learned.
However, a strict dichotomy may unnecessarily limit our integrated understanding of visual development.
In particular, we wish to focus on the issue of a form of learning that occurs before birth on patterns of activity that are generated internally.
It is well known that spontaneous endogenous activity is necessary, or permissive, for the proper development of the visual system.
The point of this paper is to discuss the statistical aspects of this activity that may be sufficient, or instructive, to guide development in much the same way that visual experience refines the mature visual system.
Essentially we propose an innate learning approach which prepares the system for later experienced-based refinement a diplomatic balance between nature and nurture.
Several Studies have shown that in the early stages of rat visual development, retinal neurons are spontaneously active and correlated in their bursting patterns of activity CITATION, CITATION.
Later, these retinal wave patterns were recorded from many animals by calcium imaging in the developing retina CITATION, CITATION, with one example shown in figure 1A.
Experiments since then have manipulated these waves by abolishing them, over-stimulating them, or otherwise altering their properties and have shown how they are necessary for proper development CITATION CITATION.
Several models have been proposed for the production of these waves CITATION CITATION.
For the two most recent models, cholinergic amacrine cells mediate this activity with general agreement about the mechanism.
Neurons begin bursting spontaneously, while neighboring cells can be recruited if enough cells in the local area are also bursting.
With such rules for wave formation and propagation, biologically plausible models of retinal wave formation have been able to create complex images, such as those in figure 1B.
Although retinal spontaneous activity has been well studied, many areas beyond the retina exhibit patterned, spontaneous neural activity.
In the visual system, both the LGN CITATION and V1 CITATION CITATION have patterned, spontaneous activity during development.
The effects on LGN and V1 connectivity have been analyzed functionally by layer segregation and orientation column formation CITATION, CITATION.
Patterned, spontaneous activity is also known to occur in the developing auditory system and is necessary for proper development CITATION, CITATION.
Similar developmental mechanisms are also found in hippocampus CITATION CITATION and spinal cord CITATION, CITATION.
From a biophysical perspective it has been shown that spontaneous neural activity is necessary to mediate many mechanistic effects such as axon branching CITATION, dendritic patterning CITATION, and synaptic pruning CITATION, CITATION.
With the ubiquity of spontaneous activity in development and its ability to affect various aspects of neural connectivity, understanding the general role of spontaneous activity in early visual development is likely to have implications beyond vision.
In adult primary visual cortex, it has been known for nearly half a century that V1 cells respond strongly to bars and edges CITATION with later experiments demonstrating that simple cells in V1 have a characteristic filter description much like a 2D gabor function CITATION, CITATION as shown in figure 2A.
The V1 cell has specific elongated subregions of visual space where relatively bright or dark parts in the visual image will stimulate the cell.
Note that this characterization is purely descriptive as a stimulus-response paradigm by answering what the neuron responds to instead of why the filters have that appearance.
According to the efficient coding hypothesis, the role of the early visual system is to remove statistical redundancy in the visual code CITATION, CITATION.
From this hypothesis, one way to understand the visual system is to develop and analyze a visual encoding scheme to remove the redundancy in images of natural scenes.
This was done using sparse coding CITATION and independent components analysis CITATION on a set of natural images pictures of rocks, trees, forest scenes, etc...
The derived filters resemble the 2D gabor filters found in V1 simple cells see figure 2B-C.
One can conclude from these results that V1 is developed and tuned to efficiently encode the visual world.
In this paper, we make the claim that there is a parsimonious computational reason for the existence of spontaneous patterns - a functional strategy that the early visual system can employ to guide this development both prior to and throughout experience.
In addition to molecular guidance cues we believe the visual code is refined from training on patterns of spontaneous activity during development in a similar manner to how the juvenile animals refine the visual code on statistical patterns found in natural images.
Many statistical structure models rely on the pairwise correlations between neighboring units an implicit assumption in other functional descriptions of spontaneous activity CITATION CITATION.
However, many efficient coding models applied to natural images, such as sparse coding and ICA, rely on statistics beyond pairwise correlations.
In fact, often as a first step these correlations are removed in a process known as decorrelation or whitening ; a process that at least in part is considered a function of retinal ganglion cells CITATION.
Although the developmental activity patterns are known to have relevant pairwise correlations, we argue receptive field refinement may also rely on higher-order statistics thus bridging the gap between models of sparse, efficient coding and spontaneous activity.
We will demonstrate that simple patterns of activity can be used as training images for refining the visual code.
The patterns we use resemble the only 2D imaged spontaneous activity available retinal waves; this is demonstrated in figure 1, with specific examples of our generated patterns in figure 1D.
Beyond a visual resemblance, our pattern generation technique also abstracts from the general properties and parameters of the current retinal wave models.
We strongly note, however, that this is strictly not a retinal wave model but an abstraction of what we believe are the essential features of the relevant endogenous activity.
We are more concerned in this paper with the statistical nature of the produced activity than its precise localization including whether the activity originates in one particular area or is part of a larger, dynamical system.
For example, in comparison to retinal waves, LGN/V1 spontaneous activity has a more direct influence on cortical receptive field formation.
In ferrets the LGN remains spontaneously active at the beginning of V1 activation, while V1 activity and retinal wave production do not significantly overlap in time CITATION.
LGN and V1 activity have been experimentally characterized CITATION, CITATION, but are far less understood than retinal activity, thus prompting our analogies to retinal waves in this paper.
Our patterns are generated using a variant of traditional site percolation models CITATION - the analogy to retinal wave propagation and its relation to physiological models is detailed in the discussion section.
Models common to the study of critical phenomena in physics, such as percolation models or the Ising model, have been used in artificial neural networks and understanding adult retinal neurons and can be equally useful in understanding models of development.
Ising models, for example, have been adapted as artificial neural networks since Hopfield's network CITATION.
Recent work has also shown that Ising models are apt analogies for the maximal entropy and high-predictability neural firing in the retina upon natural stimulation CITATION.
Although the pattern generation technique we use is quite abstract, similar networks have been shown to be relevant biologically and demonstrate desirable statistical properties.
The main goal of this paper is to show how the same adaptive, efficient algorithm can be applied for both natural inputs as well as spontaneous activity.
We show that certain wavefront-containing patterns possess the relevant statistics and a percolation network provides a useful abstraction for demonstrating this property.
These patterns, independent of how they were generated, can simply be used as an existence proof for the possible training role of spontaneous activity.
First, we will show our generated patterns qualitatively resemble known patterns of spontaneous activity.
We will then compare various methods of learning V1 receptive fields showing how both natural images and spontaneous activity patterns can be used to produce V1-like gabor filters.
We will also demonstrate how significant variations of receptive field properties can occur even at the threshold for scale invariance showing flexibility of learning even for this simplified model.
Finally, one of the main points of this paper, as expressed in the final figure, is that the relevant statistics for sensory coding go beyond simple correlations.
There are higher-order statistics which are still present after decorrelation.
Sparse and independent efficient coding algorithms rely on these statistics, which are found in natural scenes and are also present in the particular amorphous, wavefront-containing structure of spontaneous activity patterns.
We will present how this fact points to the conclusion that the same adaptive coding strategy may then be present both before and during visual experience.
Amplification, deletion, and loss of heterozygosity of genomic DNA are hallmarks of cancer.
In recent years a variety of studies have emerged measuring total chromosomal copy number at increasingly high resolution.
Similarly, loss-of-heterozygosity events have been finely mapped using high-throughput genotyping technologies.
We have developed a probe-level allele-specific quantitation procedure that extracts both copy number and allelotype information from single nucleotide polymorphism array data to arrive at allele-specific copy number across the genome.
Our approach applies an expectation-maximization algorithm to a model derived from a novel classification of SNP array probes.
This method is the first to our knowledge that is able to determine the generalized genotype of aberrant samples at each SNP site, and infer the copy number of each parental chromosome across the genome.
With this method, we are able to determine not just where amplifications and deletions occur, but also the haplotype of the region being amplified or deleted.
The merit of our model and general approach is demonstrated by very precise genotyping of normal samples, and our allele-specific copy number inferences are validated using PCR experiments.
Applying our method to a collection of lung cancer samples, we are able to conclude that amplification is essentially monoallelic, as would be expected under the mechanisms currently believed responsible for gene amplification.
This suggests that a specific parental chromosome may be targeted for amplification, whether because of germ line or somatic variation.
An R software package containing the methods described in this paper is freely available at LINK.
Genomic alterations are believed to be the major underlying cause of cancer CITATION CITATION.
These alterations include various types of mutations, translocations, and copy number alterations.
The last category involves chromosomal regions with either more than two copies, one copy, or zero copies in the cell.
Genes contained in amplified regions are natural candidates for cancer-causing oncogenes CITATION, while those in regions of deletion are potential tumor-suppressor genes CITATION.
Thus, the localization of these alterations in cell lines and tumor samples is a central aim of cancer research.
In recent years, a variety of array-based technologies have been developed to identify and classify genomic alterations CITATION CITATION.
Studies using these technologies typically analyze the raw data to produce estimates of total copy number across the genome CITATION CITATION.
However, these studies ignore the individual contributions to copy number from each chromosome.
Thus, for example, if a region containing a heterozygous locus undergoes amplification, the question of which allele is being amplified generally remains unanswered.
The amplified allele is of interest because it may have been selected for amplification because of its oncogenic effect.
Data from array-based platforms have also been employed to identify loss-of-heterozygosity events CITATION, CITATION.
In these studies LOH is typically inferred to have occurred where there is an allelic imbalance in a tumor sample at the same site at which the matched normal sample is heterozygous.
A complicating issue is that the imbalance may be due to the amplification of one of the alleles rather than the deletion of the other, and thus LOH may not in fact be present.
Copy number analysis and LOH detection can both be improved by combining copy number measurement with allelotype data.
In this paper, we present a probe-level allele-specific quantitation procedure that infers allele-specific copy numbers from 100K single nucleotide polymorphism array CITATION data.
Our algorithm yields highly accurate genotypes at the over 100,000 SNP sites.
We are also able to infer parent-specific copy numbers across the genome, making use of the fact that PSCN is locally constant on each chromosome.
Our results also allow the distinction to be made between true LOH and apparent LOH due to the amplification of a portion of only one of the chromosomes.
The PSCNs of 12 lung cancer samples that we initially analyzed reveal almost exclusively monoallelic amplification of genomic DNA, a result that we subsequently confirm in 89 other lung cell lines and tumors.
Monoallelic amplification has previously been noted in the literature on the single gene level CITATION CITATION, wherein mutant forms of known oncogenes are amplified, while their wild-type counterparts are left unaltered.
To our knowledge, this phenomenon has not previously been described on a genome-wide scale, though proposed mechanisms of amplification such as unequal sister chromatid exchange CITATION would suggest monoallelic amplification as the expected result.
In addition, our ASCNs identify the SNP haplotypes being amplified.
These haplotypes could conceivably serve as markers for deleterious germ line mutations via linkage disequilibrium.
Indeed, the presence of monoallelic amplification makes such linkage studies statistically tractable .
The centrality-lethality rule, which notes that high-degree nodes in a protein interaction network tend to correspond to proteins that are essential, suggests that the topological prominence of a protein in a protein interaction network may be a good predictor of its biological importance.
Even though the correlation between degree and essentiality was confirmed by many independent studies, the reason for this correlation remains illusive.
Several hypotheses about putative connections between essentiality of hubs and the topology of protein protein interaction networks have been proposed, but as we demonstrate, these explanations are not supported by the properties of protein interaction networks.
To identify the main topological determinant of essentiality and to provide a biological explanation for the connection between the network topology and essentiality, we performed a rigorous analysis of six variants of the genomewide protein interaction network for Saccharomyces cerevisiae obtained using different techniques.
We demonstrated that the majority of hubs are essential due to their involvement in Essential Complex Biological Modules, a group of densely connected proteins with shared biological function that are enriched in essential proteins.
Moreover, we rejected two previously proposed explanations for the centrality-lethality rule, one relating the essentiality of hubs to their role in the overall network connectivity and another relying on the recently published essential protein interactions model.
An intriguing question in the analysis of biological networks is whether biological characteristics of a protein, such as essentiality, can be explained by its placement in the network, i.e., whether topological prominence implies biological importance.
One of the first connections between the two in the context of a protein interaction network, the so-called centrality-lethality rule, was observed by Jeong and colleagues CITATION, who demonstrated that high-degree nodes or hubs in a protein interaction network of Saccharomyces cerevisiae contain more essential proteins than would be expected by chance.
Since then the correlation between degree and essentiality was confirmed by other studies CITATION CITATION, but until recently there was no systematic attempt to examine the reasons for this correlation.
In particular, what is the main topological determinant of essentiality?
Is it the number of immediate neighbors or some other, more global topological property that essential proteins may have in a protein interaction network?
Jeong and colleagues CITATION suggested that overrepresentation of essential proteins among high-degree nodes can be attributed to the central role that hubs play in mediating interactions among numerous, less connected proteins.
Indeed, the removal of hubs disrupts the connectivity of the network, as measured by the network diameter or the size of the largest connected component, more than the removal of an equivalent number of random nodes CITATION, CITATION.
Therefore, under the assumption that an organism's function depends on the connectivity among various parts of its interactome, hubs would be predominantly essential because they play a central role in maintaining this connectivity.
Recently, He and colleagues challenged the hypothesis of essentiality being a function of a global network structure and proposed that the majority of proteins are essential due to their involvement in one or more essential protein protein interactions that are distributed uniformly at random along the network edges CITATION.
Under this hypothesis, hubs are proposed to be predominantly essential because they are involved in more interactions and thus are more likely to be involved in one which is essential.
In this work we carefully evaluate each of the proposed explanations for the centrality-lethality rule.
Recently several hypotheses that linked structural properties of protein interaction networks to biological phenomena have come under scrutiny, with the main concern being that the observed properties are due to experimental artifacts and/or other biases present in the networks and as such lack any biological implication.
To limit the impact of such biases on the results reported in our study we use six variants of the genomewide protein interaction network for Saccharomyces cerevisiae compiled from diverse sources of interaction evidence CITATION CITATION .
To assess whether the essentiality of hubs is related to their role in maintaining network connectivity we performed two tests.
First, if this were the case, then we would expect essential hubs to be more important for maintaining network connectivity than nonessential hubs.
We found that this is not the case.
Next, in addition to node degree, we consider several other measures of topological prominence, and we demonstrate that some of them are better predictors of the role that a node plays in network connectivity than node degree.
Thus, if essentiality were related to maintaining network connectivity, then one would expect essentiality to be better correlated with these centrality measures than with the node degree.
However, we found that node degree is a better predictor of essentiality than any other measure tested.
To reject the essential protein interaction model CITATION, we used a hypothesis testing approach.
Namely, we observed that this model implies that the probability that a protein is essential is independent of the probability that another noninteracting protein is essential.
However, in the tested networks the essentiality of noninteracting proteins that share interaction partners is correlated.
Thus, we reject the independence assumption and, as a result, the essential protein interaction model with high confidence.
Motivated by our findings we propose an alternative explanation for the centrality-lethality rule.
Our explanation draws on a growing realization that phenotypic effect of gene-knockout experiments is a function of a group of functionally related genes, such as genes whose gene products are members of the same multiprotein complex CITATION.
It is well known that densely connected subnetworks are enriched in proteins that share biological function.
Therefore, one would expect that dense subnetworks of protein interaction networks should be either enriched or depleted in essential proteins.
Indeed, Hart and colleagues observed that essential proteins are not distributed evenly among the set of automatically indentified multiprotein complexes CITATION.
In this work we observe that the same phenomenon holds for potentially larger groups of densely connected and functionally related proteins, which we call COmplex BIological Modules.
We demonstrate that due to the uneven distribution of essential proteins among COBIMs the majority of the essential proteins lie in those COBIMs that are enriched in essential proteins, which we call Essential COmplex BIological Modules .
By the very definition, ECOBIMs contain, relative to their size, more essential nodes than a random group of proteins of the same size.
But what fraction of all essential hubs are members of such ECOBIMs?
How does this number relate to what is expected by chance?
In fact, how does the enrichment of hubs that are members/nonmembers of ECOBIMs in essential proteins relate to the enrichment values expected by chance under a suitable randomization protocol?
We propose that membership in ECOBIMs largely accounts for the enrichment of hubs in essential proteins.
In support of this hypothesis, we found that the fraction of essential proteins among non-ECOBIM hubs is, depending on the network, only 13 35 percent, which is almost as low as the network average.
Furthermore the essentiality of nodes that are not members of ECOBIMs is only weakly correlated with their degree.
Finally, using a randomization experiment we demonstrated that these properties are characteristic of the protein interaction network and are unlikely in a corresponding randomized network.
Long after a new language has been learned and forgotten, relearning a few words seems to trigger the recall of other words.
This free-lunch learning effect has been demonstrated both in humans and in neural network models.
Specifically, previous work proved that linear networks that learn a set of associations, then partially forget them all, and finally relearn some of the associations, show improved performance on the remaining associations.
Here, we prove that relearning forgotten associations decreases performance on nonrelearned associations; an effect we call negative free-lunch learning.
The difference between free-lunch learning and the negative free-lunch learning presented here is due to the particular method used to induce forgetting.
Specifically, if forgetting is induced by isotropic drifting of weight vectors, then free-lunch learning is observed.
However, as proved here, if forgetting is induced by weight values that simply decay or fall towards zero, then negative free-lunch learning is observed.
From a biological perspective, and assuming that nervous systems are analogous to the networks used here, this suggests that evolution may have selected physiological mechanisms that involve forgetting using a form of synaptic drift rather than synaptic decay, because synaptic drift, but not synaptic decay, yields free-lunch learning.
The idea that structural changes underpin the formation of new memories can be traced to the 19th century CITATION.
More recently, Hebb proposed that When an axon of cell A is near enough to excite B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased CITATION.
It is now widely accepted that learning involves some form of Hebbian adaptation, and a growing body of evidence suggests that Hebbian adaptation is associated with the long-term potentiation observed in neuronal systems CITATION.
LTP is an increase in synaptic efficacy which occurs in the presence of pre-synaptic and post-synaptic activity, and can be specific to a single synapse.
One consequence of Hebbian adaptation is that information regarding a specific association is distributed amongst many synaptic connections, and therefore gives rise to a distributed representation of each association.
In CITATION, participants learned the layout of letters on a scrambled keyboard.
After a period of forgetting, participants relearned a subset of letter positions.
Crucially, this improved performance on the remaining letter positions.
However, whereas relearning some associations shows evidence of FLL in some studies CITATION CITATION, this is not found in not all studies CITATION.
This discrepancy may be because the many studies performed to investigate this general phenomenon use a wide variety of different materials and procedures, with some measuring recall and others measuring recognition performance, for example.
However, within the realms of psychology, one relevant effect is known as part-set cueing inhibition.
Part-set cueing inhibition CITATION occurs when a subject is exposed to part of a set of previously learned items, which is found to reduce recall of nonrelearned items.
However, CITATION showed that a learned row of words was better recalled if the cues consisted of a subset of words placed in their learned positions than if cue words were placed in other positions.
In this case, part-set cueing seems to improve performance, but only if each part appears in the spatial position in which it was originally learned.
This position-specificity is consistent with the FLL effect reported using the scrambled keyboard procedure in CITATION but has no obvious concomitant in network models .
If the brain stores information as distributed representations, then each neuron contributes to the storage of many associations.
Therefore, relearning some old and partially forgotten associations should affect the integrity of other associations learned at about the same time.
As noted above, previous work has shown that relearning some forgotten associations does not disrupt other associations, but partially restores them.
This FLL effect has also been demonstrated in neural network models, where it can accelerate evolution of adaptive behaviors CITATION.
Crucially, in CITATION, the proof that relearning some associations partially restores other associations assumes that forgetting is caused by the addition of isotropic noise to connection weights, which could result from the cumulative effect of small random changes in connection weights.
In contrast, here we prove that if forgetting is induced by shrinking weights towards zero, so that weights fall towards the origin, then relearning some associations disrupts other associations.
The protocol used to examine FLL here is the same as that used in CITATION and CITATION and is as follows.
First, learn a set of n 1 n 2 associations A A 1 A 2 consisting of two subsets A 1 and A 2 of n 1 and n 2 associations, respectively.
After all learned associations A have been partially forgotten, measure performance error on subset A 1.
Finally, relearn only subset A 2 and then remeasure performance on subset A 1.
FLL occurs if relearning subset A 2 improves performance on A 1.
In order to preclude a common misunderstanding, we emphasize that, for a network with n connection weights, it is assumed that n n 1 n 2 ; that is, the number of connection weights on each output unit is not less than the number n 1 n 2 of learned associations.
Using the class of linear network models described below, up to n associations can be learned perfectly .
The proofs below refer to a network with one output unit.
However, these proofs apply to networks with multiple output units, because the n connections to each output unit can be considered as a distinct network, in which case our results can be applied to the network associated with each output unit.
Each association consists of an input vector x and a corresponding target value d. For a network with weight vector w, the response to an input vector x is y w x. We define the performance error for input vectors x 1, ,x k and desired outputs d 1, ,d k to beFORMULAwhere y i w x i is the output response to the input vector x i. By putting X T, d T andFORMULAwe can write Equation 1 succinctly asFORMULA
The two subsets A 1 and A 2 consist of n 1 and n 2 associations, respectively.
Let w 0 be the network weight vector after A 1 and A 2 are learned.
When A 1 and A 2 are forgotten, the network weight vector changes to w 1, say, and the performance error on A 1 becomes E pre E. Finally, relearning A 2 yields a new weight vector, w 2, say, and the performance error on A 1 is E post E. Free-lunch learning has occurred if performance error on A 1 is less after relearning A 2 than it was before relearning A 2 .
Given weight vectors w 1 and w 2, a matrix X of input vectors, and a vector d of desired outputs, defineFORMULAwhich we shall also refer to simply as .
In previous work CITATION, we assumed that the forgetting vector v has an isotropic distribution.
Here we shall assume instead that the post-forgetting weight vector w 1 is given byFORMULAfor some scalar r, so thatFORMULAand thereforeFORMULAThe interpretation of Equation 6 is that forgetting consists of making the optimal weight vector w 0 fall towards the origin by a falling factor 1 r.
The two main agents of human malaria, Plasmodium vivax and Plasmodium falciparum, can induce severe anemia and provoke strong, complex immune reactions.
Which dynamical behaviors of host immune and erythropoietic responses would foster control of infection, and which would lead to runaway parasitemia and/or severe anemia?
To answer these questions, we developed differential equation models of interacting parasite and red blood cell populations modulated by host immune and erythropoietic responses.
The model immune responses incorporate both a rapidly responding innate component and a slower-responding, long-term antibody component, with several parasite developmental stages considered as targets for each type of immune response.
We found that simulated infections with the highest parasitemia tended to be those with ineffective innate immunity even if antibodies were present.
We also compared infections with dyserythropoiesis to those with compensatory erythropoiesis or a fixed basal RBC production rate.
Dyserythropoiesis tended to reduce parasitemia slightly but at a cost to the host of aggravating anemia.
On the other hand, compensatory erythropoiesis tended to reduce the severity of anemia but with enhanced parasitemia if the innate response was ineffective.
For both parasite species, sharp transitions between the schizont and the merozoite stages of development were associated with lower parasitemia and less severe anemia.
Thus tight synchronization in asexual parasite development might help control parasitemia.
Finally, our simulations suggest that P. vivax can induce severe anemia as readily as P. falciparum for the same type of immune response, though P. vivax attacks a much smaller subset of RBCs.
Since most P. vivax infections are nonlethal clinically, this suggests that P. falciparum adaptations for countering or evading immune responses are more effective than those of P. vivax.
The parasites that cause human malaria are inoculated by an Anopheles mosquito and initially multiply in the liver.
After about a week, a primary wave of merozoite forms enters the bloodstream, invades RBCs, and continues the asexual cycle of multiplication, developing into the schizont forms that burst and release more merozoites.
The pathology of malaria is due to this asexual blood- stage cycle.
The sexual forms transmissible to mosquitoes appear over time, but in much smaller numbers than the asexual forms.
The two parasite species that cause the vast majority of human cases, Plasmodium vivax and Plasmodium falciparum, can induce severe anemia; with P. vivax especially, the anemia can appear far out of proportion to the percentage of RBCs infected CITATION .
Both the innate and adaptive arms of the human immune system mount responses to infections with both species.
High fevers are a classic feature of infections.
During P. vivax infections near-periodic episodes of fever are associated with high levels of tumor necrosis factor and other cytokines associated with innate immunity CITATION CITATION.
Strong cytokine responses also occur in P. falciparum infection, though the timing of paroxysms tends to be irregular CITATION CITATION.
These fever paroxysms are associated with the synchronized release of merozoites from bursting schizonts.
This synchronization has been the subject of considerable experimental and theoretical work.
It is possible that febrile temperatures induce synchronization by differentially influencing development rates of different parasite stages CITATION and that immune responses CITATION, CITATION as well as the host's melatonin release cycle CITATION CITATION contribute to this phenomenon.
But it is not yet clear whether synchronization helps parasites, perhaps in the way a sudden overwhelming abundance of prey may overwhelm a predator's capacity, or hinders them.
Malaria parasites have certainly evolved mechanisms of immune evasion; however, P. falciparum exhibits antigenic variation CITATION, adheres to vascular endothelium in response to fever CITATION, and produces prostaglandins which probably modulate host TNF- production CITATION.
This species also manages to keep the membrane of infected RBCs deformable during its ring stage, apparently reducing exposure of ring-stage parasites to clearance by the spleen CITATION.
P. vivax can also evade spleen clearance and suppress immune responses directed against its liver stage CITATION .
Clinical investigations suggest that the malaria parasite and host immune response interact with the host erythropoietic system in a complex, dynamic manner.
Increased production of TNF- by the host apparently induces anemia CITATION ; experimental evidence suggests that hemozoin produced by P. falciparum suppresses RBC production CITATION.
Abnormalities observed in P. falciparum-infected patients include suppression of erythroid progenitor cells in bone marrow CITATION, sequestration of parasites in the marrow CITATION, decreased iron absorption and hemoglobin synthesis CITATION, and decreased RBC survival time CITATION.
Leucocyte infiltration of the marrow and erythroblast degradation by macrophages have been observed in P. vivax infections CITATION.
Phagocytosis of uninfected RBCs has been observed in vitro and in P. berghei-infected mice CITATION, and is suspected in human malaria CITATION, CITATION.
P. chabaudi-infected mice show enhanced erythropoiesis that can compensate for RBC loss CITATION ; in humans, elevated levels of erythropoietin are produced in response to P. falciparum infection CITATION.
Overall, the evidence suggests that RBC destruction or ineffective erythropoiesis may thwart erythropoietin-initiated processes that might otherwise compensate for RBC loss, although erythropoietin may have other protective effects CITATION .
Most fatal malaria infections are due to P. falciparum, which can induce cerebral complications as well as severe anemia.
P. vivax infections are characterized by lower levels of parasitemia, and, though often debilitating, are rarely fatal.
P. falciparum attacks RBCs of all ages, while P. vivax mainly attacks reticulocytes CITATION CITATION, and possibly RBCs up to two weeks old CITATION.
In a previous report, we argued that targeted depletion of the youngest RBCs makes P. vivax infection potentially much more dangerous than is commonly appreciated: unchecked growth of a P. vivax population would eventually prevent the replacement of older, uninfected RBCs as these senesce and are culled from the circulation CITATION.
Thus one might expect a strong immune response to P. vivax, despite its seemingly lower threat relative to P. falciparum.
Furthermore, in that model which did not incorporate an immune response compensatory boosting of RBC production tended to increase parasitemia, while dyserythropoietic response had the opposite effect.
Here we consider compartmentalized ordinary differential equations representing P. vivax and P. falciparum infections which incorporate a quick-acting innate response and a longer-term acquired antibody response as well as a dynamic erythropoietic system.
Figure 1 shows the basic scheme; the details are presented below in the Model section.
The innate response emulates aspects of the fever paroxysm response which is the hallmark symptom of malaria.
We analyze how these components jointly affect parasite and RBC dynamics.
We do not attempt to model the full complexity of the immune response in malaria infections: our aim is to assess potential trade-offs between host and parasite for given characteristics of immune and erythropoietic responses.
However, we do consider several choices of targets for both the innate and antibody responses in the model.
We assume that bursting schizonts activate the innate response, and that the stage that triggers the antibody response is the same as the stage it targets.
We also consider different values for the time constant of decay for the effective clearance rate of parasites by the antibody response.
This time constant is not necessarily the biochemical decay constant of actual antibodies as it also incorporates the possibility of a long lived population of B-cells producing the antibodies.
The models also incorporate a nonzero standard deviation in the time parasites develop within RBCs.
We show that host erythopoietic response can affect infection outcome even in the presence of sustained immunological action.
We show how infection outcome varies with the life stages of parasite targeted by the model immune responses.
Furthermore, we show that a tight synchronization of merozoite release does not necessarily help parasite populations evade host immune responses.
Most of our simulated infections assume that the host has no pre-existing antibodies or memory B cells for the parasite, but for some examples we examine the effects of pre-existing antibodies.
RNA molecules will tend to adopt a folded conformation through the pairing of bases on a single strand; the resulting so-called secondary structure is critical to the function of many types of RNA.
The secondary structure of a particular substring of functional RNA may depend on its surrounding sequence.
Yet, some RNAs such as microRNAs retain their specific structures during biogenesis, which involves extraction of the substructure from a larger structural context, while other functional RNAs may be composed of a fusion of independent substructures.
Such observations raise the question of whether particular functional RNA substructures may be selected for invariance of secondary structure to their surrounding nucleotide context.
We define the property of self containment to be the tendency for an RNA sequence to robustly adopt the same optimal secondary structure regardless of whether it exists in isolation or is a substring of a longer sequence of arbitrary nucleotide content.
We measured degree of self containment using a scoring method we call the self-containment index and found that miRNA stem loops exhibit high self containment, consistent with the requirement for structural invariance imposed by the miRNA biogenesis pathway, while most other structured RNAs do not.
Further analysis revealed a trend toward higher self containment among clustered and conserved miRNAs, suggesting that high self containment may be a characteristic of novel miRNAs acquiring new genomic contexts.
We found that miRNAs display significantly enhanced self containment compared to other functional RNAs, but we also found a trend toward natural selection for self containment in most functional RNA classes.
We suggest that self containment arises out of selection for robustness against perturbations, invariance during biogenesis, and modular composition of structural function.
Analysis of self containment will be important for both annotation and design of functional RNAs.
A Python implementation and Web interface to calculate the self-containment index are available at LINK.
Our understanding of the significance of noncoding RNAs has increased dramatically over the last decade, notably marked by the discovery of the endogenously coded microRNAs CITATION CITATION.
Along with the increased awareness of the diversity of ncRNAs has come a corresponding heightened attention to RNA sequence and structural measures with which to characterize known and novel RNAs.
The secondary structure of an RNA, consisting of the energy-minimizing base interactions along the length of the molecule, has a direct effect on its function CITATION, a fact that has been well-characterized for a variety of RNA classes.
Ribosomal RNAs are among the largest examples that illustrate the functional importance of RNA structure several rRNAs along with associated proteins assemble into the large and small subunits of the ribosome, with the structural specificity to direct protein translation CITATION.
The cloverleaf transfer RNA structure allows it to associate with the ribosome and properly orient its bound amino acid during aminoacylation CITATION.
Various small nuclear RNAs and small nucleolar RNAs are involved in RNA editing and splicing on the basis of their shape specificity CITATION, CITATION, while the stem-loop structure of precursor miRNAs allows them to be recognized by the ribonuclease Dicer during the miRNA maturation process CITATION.
Structure-derived functionality is not limited to nonprotein coding RNAs; however, some messenger RNAs contain structural regulatory motifs, such as the hairpin selenocysteine insertion sequence that occurs predominantly in the 3 untranslated regions of mRNAs coding for selenoproteins CITATION and the internal ribosome entry site in viral 5 UTRs that promotes translation initiation in the middle of the mRNA CITATION.
Additionally, recognition of specific mRNAs by RNA binding proteins as well as pre-mRNA splicing all involve molecular interactions of the folded RNA structure CITATION, CITATION .
The importance of structural specificity is not limited to the end product sequence and structural specificity during various stages of RNA biogenesis are also critical.
Eukaryotic tRNAs, for example, are transcribed as longer precursor transcripts, which are recognized and cleaved on both the 5 and 3 ends by RNaseP and an uncharacterized endonuclease, respectively CITATION, CITATION ; some tRNAs also contain introns, which disrupt the canonical cloverleaf structure and are spliced out before the mature tRNA is exported out of the nucleus CITATION, CITATION.
The eukaryotic 18S, 5.8S, and 28S rRNAs are transcribed as a single unit and subsequently cleaved apart CITATION, CITATION.
The hammerhead ribozyme is an example of a self-splicing RNA, such that its three helices mediate cleavage of a motif that occurs on the same RNA molecule CITATION .
In the case of miRNAs, biogenesis begins with the transcription of long primary transcripts, which fold into large structures that serve as substrates for the endonuclease Drosha CITATION.
Drosha, in complex with Pasha to form the Microprocessor complex, recognizes specific hairpin substructures in the pri-miRNA and cleaves them at the base of the helical stem region, yielding the pre-miRNA hairpins CITATION, CITATION.
These range in size from 60 120 nucleotides and are subsequently processed by Dicer, which targets the pre-miRNAs on the basis of their hairpin shape CITATION, CITATION.
miRNAs are notable in that the sequence of the pre-miRNA hairpin remains a robust structure through these biogenesis steps, regardless of the sequence context: when embedded in the larger primary sequence, the pre-miRNA subsequence folds into a hairpin, and when it is cleaved off to form an independent unit, the sequence folds into the same hairpin CITATION .
The need for context-independent structural conservation, as exemplified by the miRNA biogenesis pathway, is a hallmark of a broader phenomenon of modular composability i.e., the generation of biopolymers through combinatorial composition of structural motifs.
It is now well recognized that novel proteins can arise from shuffling of structural domains, the most vivid example being circularly permuted proteins CITATION, CITATION.
Given the critical role of structural features in RNA function and the already recognized motifs as compiled in databases such as RFAM CITATION, it is conceivable that many RNAs might also have arisen from evolutionary steps of domain shuffling and domain fusions.
Such a process would require that the novel molecule reach a folded state that is a composition of the structural features of its parts i.e., the structural features of the combinatorial pieces need to be invariant to composition with other sequences.
On the one hand, structural context robustness may be a product of the specific relationship between each sequence and its genomic context, a property that has been exploited in computational miRNA finders such as in CITATION.
On the other hand, certain subsequences may have some intrinsic tendency to be structurally indifferent to their surrounding sequence, irrespective of the particular identity of that surrounding sequence e.g., a pre-miRNA would still be structurally robust if it were inserted into a different context.
We call this property of intrinsic structural invariance self containment.
A self-contained structural RNA has the potential to be a modular building block in a larger structure, carry out consistent function through biochemical modifications of surrounding sequences, and potentially maintain function when inserted into novel contexts, as might occur with viral elements.
Previously, while studying the general mutational robustness of 170 structural elements of RNA viral genomes, Wagner and Stadler found that there was a trend toward higher structural robustness in conserved elements than in nonconserved elements when placed in short nongenomic contexts CITATION.
Using a similar approach, Ancel and Fontana studied the intrinsic context insensitivity of a set of canalized artificial RNAs, selected to have reduced environmental plasticity, and found a positive relationship between environmental canalization and modularity CITATION.
Other work in RNA and proteins suggests that there is an intimate relationship between mutational robustness and domain modularity with folding kinetics, thermodynamic stability, as well as other biogenerative processes.
In this work, we analyze self containment over a broad range of biological RNAs using an intuitive scoring method to quantify different degrees of context robustness.
We show that in fact pre-miRNAs do exhibit a high degree of intrinsic self containment, while most other biologically relevant RNAs tend not to show such self containment.
We relate self containment to other sequence and structural features of RNA and find that no simple combination of features can completely explain self containment.
Finally, we show that variation among miRNAs in degree of self containment is correlated with genomic location and miRNA-family membership, as well as their biogenerative process, as illustrated by miRNAs produced by the alternate mirtron pathway.
We propose that high self containment is an intrinsic property of particular RNA sequences and may be an evolutionarily selected characteristic in molecules that need to maintain structural robustness for some aspect of their function in the face of genetic perturbations, generative perturbations, and modular composition in combinatorial contexts.
Combination therapies are often needed for effective clinical outcomes in the management of complex diseases, but presently they are generally based on empirical clinical experience.
Here we suggest a novel application of search algorithms originally developed for digital communication modified to optimize combinations of therapeutic interventions.
In biological experiments measuring the restoration of the decline with age in heart function and exercise capacity in Drosophila melanogaster, we found that search algorithms correctly identified optimal combinations of four drugs using only one-third of the tests performed in a fully factorial search.
In experiments identifying combinations of three doses of up to six drugs for selective killing of human cancer cells, search algorithms resulted in a highly significant enrichment of selective combinations compared with random searches.
In simulations using a network model of cell death, we found that the search algorithms identified the optimal combinations of 6 9 interventions in 80 90 percent of tests, compared with 15 30 percent for an equivalent random search.
These findings suggest that modified search algorithms from information theory have the potential to enhance the discovery of novel therapeutic drug combinations.
This report also helps to frame a biomedical problem that will benefit from an interdisciplinary effort and suggests a general strategy for its solution.
The problem of combination therapy has medical and algorithmic aspects.
Medically, we are still not able to provide effective cures for most chronic, complex diseases that are the main causes of death and disability, nor are we able to address the progressive age-related decline in human functional capacity.
Algorithmically, when biological dysfunction involves complex biological networks, therapeutic interventions on multiple targets are likely to be required.
Because the effect of drugs depends on the dose, several doses need to be studied, and the number of possible combinations rises quickly.
For example, many cancer chemotherapy regimens are composed of 6 or more drugs from a pool of more than 100 clinically used anticancer drugs and exploring even larger combinations might be justified CITATION.
If we were to study all combinations of 6 out of 100 compounds at 3 different doses we would have 8.9 10 11 possibilities.
This example shows that the problem requires a qualitatively new approach rather than simply more efficient screening technology.
Combined drug interventions are a common therapeutic strategy for complex diseases such as hypertension and cancer.
As pointed out recently for cancer therapy CITATION, most therapies were initially developed as effective single agents and only later combined clinically.
A possible approach to the exploration of new therapeutic activities not present in individual drugs is based on the exhaustive study of all possible combinations of pairs of compounds CITATION.
This brute force approach has detected many interesting novel pairs of compounds CITATION, but the resulting exponential expansion in the number of possibilities precludes the comprehensive exploration of larger combinations.
Several authors CITATION, CITATION have recently argued that the future of combination therapy lies in the development of accurate quantitative network models that capture the mechanistic interactions of cellular and organism physiology.
Fitzgerald et al. CITATION acknowledge that we do not yet know what these models will look like, and that systems biology research is still data-limited for this purpose.
Indeed their recent review does not report any successful application of this approach to combination therapies.
Here we suggest a novel solution to the problem of combination drug therapy, making use of search algorithms originally developed for digital communication.
When modified in several key aspects, these search strategies can be used to find more effective combined therapeutic interventions without the need for a fully factorial experimental design.
These algorithms may also provide a framework upon which information from system-wide molecular data and from mechanistic computational networks models can be superimposed.
To understand the motivation for our work it is important to consider that, even if simulations might play a role, the intended use of the algorithms is not entirely in silico, but partially in vivo or in vitro, using high-throughput biological measurements in organisms or isolated cells, respectively.
This approach becomes increasingly relevant because high-throughput measurement technology, initially developed by drug companies for the screening of large libraries of compounds in multi-well plate formats, is now more and more available to the scientific community.
It is useful to regard the information processing by our experimental systems as parallel biological computations, since the algorithms we are using are indeed derived from algorithms that were implemented in silico in other scientific fields.
Parallel measurements are suitable for multi-well high-throughput technology.
There are requirements regarding the computational complexity of the algorithms that limit the choice of suitable approaches.
These requirements are discussed in more detail in the Results.
Both the number of operations and computational costs unique to in vivo/in vitro algorithms should be considered.
Algorithm design requires the application of an appropriate structure to the data.
Although there are many options to represent the space of possible drug combinations, we used a tree representation with drug combinations as nodes linking to all possible additions of one drug in the next level.
Individual drugs form the base of the tree and combinations of maximum size are at the top.
When exploring the drug combination tree going from smaller to larger combinations, as in the algorithms we suggest, we are giving more weight to lower-order drug interactions.
This is consistent with data available on adverse drug interactions, which are reported mostly for two-drug combinations CITATION, CITATION.
Estimating the optimal size of a combination is a different problem, examined in detail in the Discussion.
The beneficial effect of a combination is also due to additive components and to multiple higher-order effects.
The search algorithms we suggest are derived from sequential decoding algorithms.
These were chosen in part because of similarities among the data trees to be searched in the biological and decoding applications.
Sequential decoding algorithms are used for convolutional codes, in which nearby nodes in the data tree are related, similarly to different but partially overlapping combinations of drugs.
Another feature of sequential algorithms that fit our purposes is the use of a list-based memory of the path taken to reach each node.
We provide in the Discussion a detailed argument suggesting that a suitable algorithm should be able to integrate all available information on the state of the system with that obtained by iterative measurements.
The integration should take place at every iteration within the algorithm, rather than being a weighted average of different methods applied separately.
The presence of the updated list as a guide for each iteration provides our algorithms with a natural mean of information integration.
Both the fully factorial dataset we show in Figure 1 and the complex structure of the biological networks that are being reconstructed in systems biology supports this expectation of frequent non-linearities in phenotype measurements along the data tree.
Therefore we are interested in algorithms that can search within a solution space presenting substantial non-linearities.
If the relation among drugs in a combination were linear, the best algorithm would simply determine the best dose in single drug measurements and use these to obtain the best combination.
If, on the contrary, non-linearities were extreme, the use of stochastic algorithms might be preferable.
Stochastic algorithms can cope with multiple local minima in the solution space, but they do so by incorporating a random element.
This requires a price in terms of computational cost, and the performance of stochastic algorithms is therefore often not as good as that of more tailored algorithms CITATION, CITATION.
The algorithms we suggest can cope with moderate and variable non-linearities by going back to previous nodes in the tree.
Starting with the stack sequential algorithm, which was developed to search for optimal decoding in the field of digital communications CITATION, we describe and test algorithms that can be used to search for an optimal combination of a sizeable number of drugs, by testing only a small subset of all possible combinations.
The algorithms are useful for large combinations, where collecting fully factorial datasets is not feasible.
We present results obtained from simulations in a computational model of cell death and from experiments using two models with complementary biological properties: restoring the decline with age in heart function and exercise capacity in Drosophila melanogaster;and selective killing of human cancer cells.
The first in vivo experimental model has the advantage of including the complexity of whole organism interventions, while the second in vitro model has the potential for markedly higher throughput testing.
These models are also representative of two different general types of multi-drug interventions: one type aims at improving function, while the other is based on the induction of cell death, a selective disruption of network function.
Results suggest that optimal or near-optimal combinations of compounds can be found in these systems with only a small fraction of the number of tests as a fully factorial design, and with significantly higher efficacy than random searching.
In summary the contributions of this work are:
Constructing a novel problem statement for the search of drug combinations, using a novel approach to systems biology .
Collecting exhaustive experimental measurements sufficient to solve the problem conclusively.
Constructing a computational method to solve the problem approximately with fewer experimental measurements.
The suggested algorithms are modeled on algorithms already used in other fields, while our main original contribution is in their novel application.
Providing additional experiments to support the generality of the approach.
Dendrite morphology, a neuron's anatomical fingerprint, is a neuroscientist's asset in unveiling organizational principles in the brain.
However, the genetic program encoding the morphological identity of a single dendrite remains a mystery.
In order to obtain a formal understanding of dendritic branching, we studied distributions of morphological parameters in a group of four individually identifiable neurons of the fly visual system.
We found that parameters relating to the branching topology were similar throughout all cells.
Only parameters relating to the area covered by the dendrite were cell type specific.
With these areas, artificial dendrites were grown based on optimization principles minimizing the amount of wiring and maximizing synaptic democracy.
Although the same branching rule was used for all cells, this yielded dendritic structures virtually indistinguishable from their real counterparts.
From these principles we derived a fully-automated model-based neuron reconstruction procedure validating the artificial branching rule.
In conclusion, we suggest that the genetic program implementing neuronal branching could be constant in all cells whereas the one responsible for the dendrite spanning field should be cell specific.
Dendrite morphology is the most prominent feature of nerve cells, typically used by neuroanatomists to discriminate and classify them CITATION.
These tree-like ramifications represent the input region of the neurons and fulfil the role of a complex computational unit CITATION CITATION.
Typically, dendritic arborizations are analyzed in a descriptive way, e.g. by enumerating local and global branching parameters CITATION CITATION.
Very little is known about the general rule leading to their distinct appearance partly due to the wide variety among different neurons.
In insects, same neurons across individuals are rather invariant in their anatomy and constant in their function.
Lobula plate tangential cells of the fly visual system CITATION are uniquely identifiable and are therefore ideal subjects for investigating the basic rule constraining dendrite formation.
They integrate local motion information over an array of retinotopically arranged columnar elements CITATION.
Accordingly, their planar dendritic trees cover the area corresponding to their distinct primary receptive fields.
In this paper we isolate potential fundamental principles which may lead to the morphological identity of individual LPTCs.
Perception involves two types of decisions about the sensory world: identification of stimulus features as analog quantities, or discrimination of the same stimulus features among a set of discrete alternatives.
Veridical judgment and categorical discrimination have traditionally been conceptualized as two distinct computational problems.
Here, we found that these two types of decision making can be subserved by a shared cortical circuit mechanism.
We used a continuous recurrent network model to simulate two monkey experiments in which subjects were required to make either a two-alternative forced choice or a veridical judgment about the direction of random-dot motion.
The model network is endowed with a continuum of bell-shaped population activity patterns, each representing a possible motion direction.
Slow recurrent excitation underlies accumulation of sensory evidence, and its interplay with strong recurrent inhibition leads to decision behaviors.
The model reproduced the monkey's performance as well as single-neuron activity in the categorical discrimination task.
Furthermore, we examined how direction identification is determined by a combination of sensory stimulation and microstimulation.
Using a population-vector measure, we found that direction judgments instantiate winner-take-all when two stimuli are far apart, or vector averaging when two stimuli are close to each other.
Interestingly, for a broad range of intermediate angular distances between the two stimuli, the network displays a mixed strategy in the sense that direction estimates are stochastically produced by winner-take-all on some trials and by vector averaging on the other trials, a model prediction that is experimentally testable.
This work thus lends support to a common neurodynamic framework for both veridical judgment and categorical discrimination in perceptual decision making.
Perceptual judgments involve detection, identification and discrimination of objects in a sensory scene CITATION, CITATION.
Given an ambiguous visual motion pattern, for instance, a subject may be asked to detect whether a net motion direction is present or absent CITATION, to identify the motion direction as an analog quantity CITATION, or to discriminate the motion direction between two options CITATION.
Using the strategy of single-unit recording from behaving monkeys, neurophysiologists have begun to uncover neuronal activity that is linked to such perceptual judgments.
In monkey experiments using perceptual discrimination tasks, neural correlates of decision making have been observed in the parietal CITATION, CITATION, premotor CITATION CITATION and prefrontal CITATION, CITATION cortical areas.
Experimental observations have inspired the advance of neural circuit models which suggest that recurrent network dynamics can underlie temporal integration of sensory information and decision formation CITATION CITATION .
Focusing on categorical discrimination, those neural circuit models as well as abstract ramp-to-threshold models CITATION CITATION are typically endowed with a simple architecture consisting of discrete neural pools, selective for categorical alternatives.
Therefore, they are inadequate for exploring perceptual identification that requires neural representation of analog quantities, such as motion direction that can be an arbitrary angle between 0 and 360.
On the other hand, probabilistic estimation of an analog stimulus feature has been studied from the perspective of optimal population coding CITATION, CITATION, CITATION.
These studies centered on optimal algorithms for reading out a stimulus feature from sensory neural populations, such as inferring the orientation of a visual stimulus from neural activity in the primary visual cortex CITATION and the direction of a motion stimulus from activity profiles across the middle temporal visual area CITATION.
However, such probabilistic inference is believed to occur in higher-order cortical areas downstream from primary sensory areas, and the underlying circuit mechanism remains unclear.
In particular, it is unknown whether probabilistic estimation and categorical discrimination engage distinct decision processes or can be realized by a shared neural circuit mechanism.
In the present work, we investigated this outstanding question using a continuous recurrent network model of spiking neurons, which was initially proposed for spatial working memory CITATION.
We applied this model to the simulation of two monkey experiments using random-dot visual motion stimuli.
In a two-alternative forced-choice direction discrimination task, the monkey was trained to discriminate the motion direction by making a saccadic eye movement to one of two peripheral choice targets CITATION, CITATION, CITATION.
It was found that ramp-like spiking activity of neurons in the lateral intraparietal cortex is correlated with the monkey's choice.
By contrast, in a direction identification task, the monkey was required to report veridically its perceived direction of motion in the visual stimulus CITATION.
On some trials, electrical stimulation was applied simultaneously to MT neurons when the monkey viewed the random-dot display.
Microstimulation could bias the monkey's judgments toward the preferred direction of MT neurons at the microstimulation site CITATION, CITATION.
It was argued that both vector-averaging and winner-take-all algorithms might contribute to the interpretation of activity profiles of MT neurons.
But CITATION collected only behavioral data and did not record neural activity in MT or downstream cortical areas.
Thus, the neural mechanism for veridical judgments about the motion direction remains unknown.
Here we show that the continuous recurrent network model is capable of reproducing salient observations from both experiments.
Our results suggest that both categorical discrimination and veridical judgment can be subserved by a common cortical circuit endowed with reverberatory dynamics.
Identification of single nucleotide polymorphisms and mutations is important for the discovery of genetic predisposition to complex diseases.
PCR resequencing is the method of choice for de novo SNP discovery.
However, manual curation of putative SNPs has been a major bottleneck in the application of this method to high-throughput screening.
Therefore it is critical to develop a more sensitive and accurate computational method for automated SNP detection.
We developed a software tool, SNPdetector, for automated identification of SNPs and mutations in fluorescence-based resequencing reads.
SNPdetector was designed to model the process of human visual inspection and has a very low false positive and false negative rate.
We demonstrate the superior performance of SNPdetector in SNP and mutation analysis by comparing its results with those derived by human inspection, PolyPhred, and independent genotype assays in three large-scale investigations.
The first study identified and validated inter- and intra-subspecies variations in 4,650 traces of 25 inbred mouse strains that belong to either the Mus musculus species or the M. spretus species.
Unexpected heterozgyosity in CAST/Ei strain was observed in two out of 1,167 mouse SNPs.
The second study identified 11,241 candidate SNPs in five ENCODE regions of the human genome covering 2.5 Mb of genomic sequence.
Approximately 50 percent of the candidate SNPs were selected for experimental genotyping; the validation rate exceeded 95 percent.
The third study detected ENU-induced mutations in 64,896 traces of 1,236 zebra fish.
Our analysis of three large and diverse test datasets demonstrated that SNPdetector is an effective tool for genome-scale research and for large-sample clinical studies.
SNPdetector runs on Unix/Linux platform and is available publicly .
Identification of genetic variations and mutations is important for the discovery of genetic predisposition to complex diseases.
Although a wide variety of methods are available for de novo single nucleotide polymorphism discovery CITATION, DNA sequencing is the method of choice for high-throughput screening studies.
DNA sequencing may follow either a random shotgun strategy CITATION CITATION or a directed strategy using PCR amplification of specific target regions of interest CITATION.
As the high-density haplotype map of the human genome CITATION nears completion, the demand for large-scale SNP surveys seeking genetic mutations linked to or causative of a wide variety of human diseases is expected to greatly increase CITATION .
Direct sequencing of PCR-amplified genomic fragments from diploid samples results in mixed sequencing templates.
Therefore, one of the most challenging issues in SNP discovery by this method is to distinguish bona fide heterozygous allelic variations from sequencing artifacts, which can give rise to two overlapping fluorescence peaks similar to true heterozygotes.
Currently, PolyPhred CITATION is the most widely used SNP discovery software for such an analysis.
It reports a heterozygous allele only when the site shows a decrease of about 50 percent in peak height compared to the average height for homozygous individuals.
However, inspection of the computational results by human analysts is often required to ensure a low false positive rate, a labor-intensive process.
To provide a sensitive and accurate method for SNP detection in fluorescence-based resequencing, we developed a new software tool, SNPdetector, aiming to computerize the manual review process.
We report SNPdetector's application in three large-scale genetic variation studies and compare its results with those obtained by human inspection, by PolyPhred, and by experimental validation.
In the first study, resequencing was used to validate mouse SNPs discovered by whole-genome shotgun sequencing.
The second study identifies novel SNPs in the ENCODE regions of the human genome CITATION, and the third study aims to discover mutations induced by ENU in 1,236 zebra fish.
Pavlovian predictions of future aversive outcomes lead to behavioral inhibition, suppression, and withdrawal.
There is considerable evidence for the involvement of serotonin in both the learning of these predictions and the inhibitory consequences that ensue, although less for a causal relationship between the two.
In the context of a highly simplified model of chains of affectively charged thoughts, we interpret the combined effects of serotonin in terms of pruning a tree of possible decisions,.
We show how a drop in behavioral inhibition, putatively resulting from an experimentally or psychiatrically influenced drop in serotonin, could result in unexpectedly large negative prediction errors and a significant aversive shift in reinforcement statistics.
We suggest an interpretation of this finding that helps dissolve the apparent contradiction between the fact that inhibition of serotonin reuptake is the first-line treatment of depression, although serotonin itself is most strongly linked with aversive rather than appetitive outcomes and predictions.
Serotonin is a neuromodulator that appears to play a critical role in a wealth of psychiatric conditions, including depression, anxiety, panic, and obsessive compulsions.
However, despite the importance of serotonergic pharmacotherapies, notably selective serotonin reuptake inhibitors, the roles that serotonin plays in normal and abnormal function are still mysterious.
We start from three particular findings.
First, 5-HT is involved in the prediction of aversive events, possibly as a form of opponent CITATION CITATION to dopamine CITATION CITATION.
Second, 5-HT is involved in behavioral inhibition CITATION CITATION, preventing or curtailing ongoing actions in light of predictions of aversive outcomes.
The third finding is the collection of psychopharmacological data implicating 5-HT in animal models of depression and anxiety CITATION CITATION, together with the fact that depleting 5-HT in human subjects who have recovered from depression, can reinstate an acute, at times fulminant, re-experience of subjective symptoms of the disease, as assessed by various rating scales CITATION CITATION.
Furthermore, while SSRIs are used in the treatment of depression, genetically induced, constitutive decreases in the efficiency of 5-HT reuptake are a risk factor for depression CITATION CITATION.
These findings are hard to connect: the second fact seems orthogonal to the first and third, which are themselves in apparent contradiction.
If 5-HT is really involved in predicting aversive outcomes, then depleting it should surely have positive rather than negative affective consequences.
We suggest that the missing link comes from considering the interactions between Pavlovian predictions and ongoing action selection.
The interaction is seen in conditioned suppression CITATION, a standard workhorse test for aversive predictions.
Animals are trained to emit appetitive instrumental actions, and to associate a light with a shock.
Presentation of the light during instrumental performance reduces the rate at which animals emit those responses.
Neither the theoretical nor the neurobiological status of this interaction is completely resolved, though there is some evidence of the involvement of 5-HT in the nucleus accumbens in its realization CITATION CITATION .
Here, we treat a subset of the inhibitory processes associated with Gray's behavioral inhibition system CITATION, CITATION, CITATION, CITATION in terms of what might be called a preparatory Pavlovian response.
Consummatory Pavlovian responses are pre-programmed reactions to the presence of affectively significant outcomes such as food, water, or threats.
Preparatory Pavlovian responses are similarly pre-programmed responses to predictions of those outcomes.
Even though the predictions are learned, the responses are not, and may therefore be behaviorally inappropriate in certain circumstances CITATION, CITATION.
For our purposes, and as long noted by Deakin and Graeff CITATION, the most important preparatory Pavlovian response to a prediction of a threat CITATION is inhibition, in the form of withdrawal or disengagement.
This explicitly links the first two findings discussed above, as the inhibition is directly associated with aversive predictions.
To explore the consequences of reflexive, direct inhibition of action for learning in affective settings, together with the repercussions when 5-HT is compromised, we built a highly simplified model that sought to isolate these effects from more general learning effects.
More specifically, we built a model of trains of thoughts.
In our treatment, we considered thoughts as actions that lead from one belief state to the next.
Trains of thought gained value through their connections with a group of terminal states that were preassigned either positive or negative affective values.
5-HT directly inhibited chains of thought predicted to lead toward negative terminal states.
Our model can be seen in terms of 5-HT's pruning of a decision tree of outcome states and choices CITATION, CITATION .
We argue that the results on tryptophan depletion above now emerge when considering the consequences of this reflexive behavioral inhibition on ongoing learning about the world, and on subsequent action choice and predictions.
The most notable effect in the model is a critical bias toward optimistic valuation.
That is, states and actions with potentially negative consequences are under-explored and incorrectly -valued because of the reflexive inhibition.
When inhibition fails, though, which is the last of the three issues mentioned above, there are two adverse consequences.
First, the inhibition is no longer a crutch for instrumental action choice, so subjects have to learn to avoid potentially bad situations rather than being able to rely on this reflexive mechanism.
Second, due to a mismatch between policy and value function, characteristic inconsistencies between the predicted and actual values arise, with the actual values encountered being more negative than predicted, though also actually more realistic.
This mismatch between policy and value function also leads to an overall reduction in rewards obtained.
Boosting 5-HT in the model again restores the status quo.
Of course, this highly simplified model cannot possibly, by itself, accommodate all the diverse and confusing roles of 5-HT.
Nevertheless, it replicates some prominent behavioral and pharmacological facets of depression and anxiety in humans and animal models, which we return to in the Discussion.
The next section defines the model of trains of thought more formally.
The Results section considers normal learning, and the consequences of impairments to 5-HT processing.
We save for the Discussion a broader discussion of data and theories pertaining to 5-HT.
Numerous studies have noted that the evolution of new enzymatic specificities is accompanied by loss of the protein's thermodynamic stability, thus suggesting a tradeoff between the acquisition of new enzymatic functions and stability.
However, since most mutations are destabilizing, one should ask how destabilizing mutations that confer new or altered enzymatic functions relative to all other mutations are.
We applied G computations by FoldX to analyze the effects of 548 mutations that arose from the directed evolution of 22 different enzymes.
The stability effects, location, and type of function-altering mutations were compared to G changes arising from all possible point mutations in the same enzymes.
We found that mutations that modulate enzymatic functions are mostly destabilizing, and are almost as destabilizing as the average mutation in these enzymes.
Although their stability effects are not as dramatic as in key catalytic residues, mutations that modify the substrate binding pockets, and thus mediate new enzymatic specificities, place a larger stability burden than surface mutations that underline neutral, non-adaptive evolutionary changes.
How are the destabilizing effects of functional mutations balanced to enable adaptation?
Our analysis also indicated that many mutations that appear in directed evolution variants with no obvious role in the new function exert stabilizing effects that may compensate for the destabilizing effects of the crucial function-altering mutations.
Thus, the evolution of new enzymatic activities, both in nature and in the laboratory, is dependent on the compensatory, stabilizing effect of apparently silent mutations in regions of the protein that are irrelevant to its function.
With the exception of unstructured protein domains, the integrity of a protein's structure and function is largely dependent on its thermodynamic stability.
Evolutionary processes, be they neutral, or adaptive, involve the acquisition of mutations that may affect protein function and/or stability.
For example, a mutation that endows a desirable new function, but severely undermines stability, will not become fixed.
The relationship between mutational effects, function and stability is therefore crucial to our understanding not only of the evolutionary dynamics of proteins CITATION CITATION, but also in engineering, designing, and evolving, novel enzymes in the laboratory CITATION CITATION .
Stability-function tradeoffs became originally evident in enzymes, particularly in the structural tension created by the arrangement of catalytic residues in active sites.
From the point of view of overall protein stability, active site organization is inherently unfavorable for a number of reasons.
Functional residues, which are generally polar or charged, are embedded in hydrophobic clefts CITATION, sometimes with proximal like charges.
Key catalytic residues often possess unfavorable backbone angles CITATION, CITATION.
Consequently, the substitution of an enzyme's key catalytic side chains can dramatically increase stability whilst obviously sacrificing activity CITATION CITATION .
Such observations led to the generally accepted principle of stability-function tradeoffs CITATION, CITATION that was later extended to tradeoffs between new functions and stability CITATION.
However, as discussed below, we surmise that there exists a fundamental difference between mutations in key catalytic residues that relate to the well established stability-function tradeoff, and mutations that mediate the evolutionary divergence of new functions.
Enzymes evolve new functions via mutations that alter substrate specificity, typically by increasing the affinity and rates for weak promiscuous substrates.
These changes involve mutational adjustments of the active site, its periphery, or even the second and third shell of residues that surround it, while maintaining the key catalytic residues intact.
As shown below, in oppose to mutations in key catalytic residues that typically involve an exchange into alanine of a charged/polar residue within a hydrophobic surroundings, the type and location of new function mutations is far more diverse.
As initially observed by Wang et al. CITATION, CITATION, most mutations that confer new functions have been proven to be destabilizing.
However, the generality of stability-function tradeoffs with regard to new functions should be addressed in view of the fact that, regardless of their relevance to function, most mutations are destabilizing CITATION, CITATION CITATION.
Indeed, derivation of the G distributions of all possible mutations in a series of globular proteins using the experimentally validated FoldX algorithm CITATION, CITATION indicated that about 70 percent of mutations are destabilizing, and 20 percent are significantly destabilizing CITATION.
On the other hand, mutations that characterize neutral, non-adaptive changes occur primarily on the surface, certainly at the first steps of sequence divergence CITATION, and this subgroup of mutations is much less destabilizing average G 0.6 kcal/mol CITATION.
Thus, better understanding of how the emergence of new functions trades-off with protein stability requires a comparison of mutations that confer new protein functions to all other possible mutations in a protein, as well as to mutations that characterize neutral, non-adaptive changes.
With this in mind, we investigated a large set of mutations that were found in enzymes that acquired new substrate specificities in directed evolution experiments and clinical isolates.
We applied FoldX to compute the G values of these mutations, and compared the type, location, and G values of these mutations with all possible point mutations in the same proteins.
While realizing that the FoldX values are a prediction of limited accuracy, they do enable the examination the distributions of G values for a large set of proteins and mutations, and on the whole, these predictions show reasonable correlation with experimental data CITATION.
Thus, whilst the values for individual mutations can considerably deviate from the experimental values, the trends we observed are likely to be relevant CITATION .
Cellular signaling networks are subject to transcriptional and proteolytic regulation under both physiological and pathological conditions.
For example, the expression of proteins subject to covalent modification by phosphorylation is known to be altered upon cellular differentiation or during carcinogenesis.
However, it is unclear how moderate alterations in protein expression can bring about large changes in signal transmission as, for example, observed in the case of haploinsufficiency, where halving the expression of signaling proteins abrogates cellular function.
By modeling a fundamental motif of signal transduction, the phosphorylation dephosphorylation cycle, we show that minor alterations in the concentration of the protein subject to phosphorylation can affect signal transmission in a highly ultrasensitive fashion.
This ultrasensitization is strongly favored by substrate sequestration on the catalyzing enzymes, and can be observed with experimentally measured enzymatic rate constants.
Furthermore, we show that coordinated transcription of multiple proteins within a protein kinase cascade results in even more pronounced all-or-none behavior with respect to signal transmission.
Finally, we demonstrate that ultrasensitization can account for specificity and modularity in the regulation of cellular signal transduction.
Ultrasensitization can result in all-or-none cell-fate decisions and in highly specific cellular regulation.
Additionally, switch-like phenomena such as ultrasensitization are known to contribute to bistability, oscillations, noise reduction, and cellular heterogeneity.
Cellular signal transduction exhibits two layers of regulation: upstream stimuli such as extracellular peptide hormones activate intracellular signaling intermediates, which in turn induce intracellular responses as indicated in Figure 1A.
This type of regulation, which usually operates on the time scale of minutes, will be termed fast regulation in this paper.
Fast regulation involves the posttranslational modification of pre-existing protein pools in order to transduce signals to the nucleus.
Responses induced by fast regulation, such as activated transcription factors, often in turn alter the total abundance of their own activators or that of intermediates in heterologous cascades, e.g., owing to induced mRNA/protein synthesis or to degradation.
For example, signal transduction pathways in the immune system alter the concentration of their own constituents by transcriptional positive or negative feedback to bring about sensitization CITATION or desensitization CITATION.
Likewise, cyclic guanosine monophosphate signaling affects the heterologous mitogenic cascades via transcriptional crosstalk by inducing the MKP-1 phosphatase and cyclin-dependent kinase inhibitors CITATION.
Owing to the long half-lives of most mRNAs CITATION and proteins CITATION, this type of regulation operates on the time scale of hours in most cases and thus will be referred to as slow regulation here.
Figure 1A shows a simplified view of signal transduction: upstream stimuli such as hormones induce gene expression, which in turn alters cellular signal processing.
Previous theoretical and experimental studies have mainly analyzed how fast regulation influences slow regulation, while downstream effects, i.e., the impact of slow regulation on fast regulation, are less well investigated.
However, slow regulation is probably equally important, since the expression of phosphoproteins is altered during a variety of physiological processes such as differentiation CITATION, development CITATION, apoptosis CITATION, long-term potentiation CITATION, the cell cycle CITATION, and the circadian rhythm CITATION.
Furthermore, the deregulated expression of wild-type phosphoproteins has been shown to be correlated with diseases such as diabetes CITATION and cancer CITATION .
At a first glance, one may expect that altered expression of Intermediate 1 affects steady-state signal transmission via Intermediate 1 in a linear fashion.
However, recent research suggests that strong nonlinearity is observed at least in some cases: a variety of tumor-suppressor genes involved in cellular signal transduction do not follow Knudson's two-hit hypothesis, i.e., they do not require a homozygotic loss of both alleles to support tumor progression.
Instead, loss of a single copy, i.e., halving protein expression, is sufficient to abrogate tumor-suppressor function and this phenomenon has been termed haploinsufficiency CITATION, CITATION.
This suggests that the expression of signaling proteins affects signal transmission in a highly switch-like fashion.
As increased signal transmission elicited by transcriptional induction has been referred to as sensitization CITATION, such ultrasensitive regulation will be referred to here as ultrasensitization.
Available experimental data suggest that ultrasensitization is physiologically advantageous: receptor-tyrosine kinases, which elicit different cellular responses, are known to induce broadly overlapping sets of immediate early genes, although the amplitude of the immediate early gene-induction is receptor-specific CITATION, CITATION.
Ultrasensitization allows cells to discriminate such minor differences in immediate early gene expression, and thus confers specificity to receptor-tyrosine kinase signaling.
In addition, even saturating hormone concentrations induce/repress the vast majority of target mRNAs less than 10-fold.
Thus, the stimulus-response of hormone-induced transcription exhibits significant basal activation, so that mRNA induction is relatively insensitive towards extracellular hormone concentrations CITATION.
In such cases, ultrasensitization can dramatically increase the cellular effects of extracellular hormone administration and may help to establish all-or-none cell-fate decisions.
Finally, strong nonlinearities such as ultrasensitization are known to contribute to bistability, oscillations, noise reduction, and cellular heterogeneity .
Phosphorylation is the most common mode of eukaryotic information transfer, and it has been estimated that one third of all cellular proteins are phosphorylated CITATION.
As several haploinsufficient tumor suppressors encode phosphoproteins, or phosphatases, we were interested in whether ultrasensitization can occur in a simple phosphorylation dephosphorylation cycle.
We show here that pronounced ultrasensitization is possible with experimentally measured kinetic constants, particularly in the parameter range where the substrate enzyme ratio is such that the majority of the substrate subject to phosphorylation is sequestered by the catalyzing enzymes.
Furthermore, we show that coordinated transcription of multiple proteins within a kinase cascade can result in even more pronounced ultrasensitization.
Finally, we demonstrate that ultrasensitization can account for specificity and modularity in the regulation of cellular signal transduction.
The transcription factor Myc plays a central role in regulating cell-fate decisions, including proliferation, growth, and apoptosis.
To maintain a normal cell physiology, it is critical that the control of Myc dynamics is precisely orchestrated.
Recent studies suggest that such control of Myc can be achieved at the post-translational level via protein stability modulation.
Myc is regulated by two Ras effector pathways: the extracellular signal-regulated kinase and phosphatidylinositol 3-kinase pathways.
To gain quantitative insight into Myc dynamics, we have developed a mathematical model to analyze post-translational regulation of Myc via sequential phosphorylation by Erk and PI3K.
Our results suggest that Myc integrates Erk and PI3K signals to result in various cellular responses by differential stability control of Myc protein isoforms.
Such signal integration confers a flexible dynamic range for the system output, governed by stability change.
In addition, signal integration may require saturation of the input signals, leading to sensitive signal integration to the temporal features of the input signals, insensitive response to their amplitudes, and resistance to input fluctuations.
We further propose that these characteristics of the protein stability control module in Myc may be commonly utilized in various cell types and classes of proteins.
The proto-oncogene protein Myc is a transcription factor that regulates numerous signaling pathways involved in cell-fate decisions CITATION CITATION.
Sufficient accumulation of Myc leads to the activation of Cyclin D and cyclin dependent kinases, which subsequently phosphorylate Rb and release E2F.
This results in the initiation of DNA replication and cell cycle entry CITATION.
Excessive accumulation of Myc, however, induces apoptosis CITATION, CITATION when cells are under stress or deprived of growth factors.
Finally, Myc also drives cell growth by activating genes that encode cellular metabolic activities, including translational factors, ribosomal proteins and RNAs CITATION .
Given its importance, Myc activity must be properly controlled in response to different environmental cues.
Past studies have suggested that Myc is regulated at multiple levels, including auto-regulation of Myc transcription CITATION and post-transcriptional regulation CITATION, CITATION.
More recent discoveries indicate that Myc is also dynamically regulated at the protein level by the Ras effector pathways CITATION CITATION.
These discoveries suggest that Myc protein undergoes a series of modifications that are sequential and irreversible CITATION CITATION, CITATION, CITATION.
More specifically, when Myc is newly synthesized, it is highly unstable and quickly undergoes ubiquitination and degradation CITATION.
It can be substantially stabilized when phosphorylated at serine 62 by Ras-activated Erk activity.
Subsequent phosphorylation of Myc at threonine 58 by Gsk3, however, initiates a destabilization process in a sequential manner.
This is achieved by a dephosphorylation mechanism by a prolyl isomerase Pin1 and a protein phosphatase PP2A.
Once Myc is phosphorylated at Thr58, Pin1 induces it to undergo conformation changes, which are required for PP2A to dephosphorylate the Ser62 residue CITATION.
To date, this is the only dephosphorylation mechanism identified in the Myc stabilization processes.
Destabilization of Myc by Gsk3 can be blocked by the Ras-activated PI3K pathway .
The unique control of Myc dynamics by sequential phosphorylation allows Myc to integrate upstream signals from Erk and PI3K, which play critical roles in controlling diverse cell fates CITATION, CITATION, CITATION.
Erk often exhibits an early, transient peak of activation upon growth stimulation.
The peak is followed by varying residual activities, which depend on cell lines and growth factors.
This residual Erk is critical in downstream signal encoding.
For example, in PC12 cells, a small residual Erk activity, as a result of epidermal growth factor stimulation, leads to proliferation.
In contrast, a high residual Erk activity as a result of nerve growth factor stimulation in the same cell line leads to differentiation CITATION, CITATION.
The residual Erk level has also been observed to be critical in regulating c-Fos level in fibroblasts CITATION .
The PI3K activation pattern depends on cell lines and stimulants, as detailed in Table S2.
It is bimodal in various cell lines including WI38, NIH 3T3, or HepG2 when stimulated by platelet-derived growth factors or fetal bovine serum CITATION, CITATION, CITATION.
In contrast, PI3K appears to have only an early, transient single peak in the U-2OS or PVSM cell lines stimulated with other growth stimulants CITATION, CITATION.
The bimodal activation of PI3K has been shown to be important for cell cycle regulation CITATION, CITATION, CITATION.
In particular, the second peak has been found sufficient and critical to drive the G1/S transition during cell cycle CITATION, CITATION .
The temporal pattern of Myc activation closely correlates with those of Erk and PI3K.
Myc protein reaches its peak at 2 hours after growth stimulation and decreases to and remains at an intermediate value, or hump, for over 6 hours before reducing to its basal level CITATION, CITATION.
The peak and the hump of Myc coincide with the Erk peak and the 2 nd PI3K pulse, respectively.
These observations suggest that Myc may sense and integrate signals from its two regulators .
To gain insight into this control mechanism, we have constructed a mathematical model to analyze dynamics of Myc accumulation controlled by sequential phosphorylation.
Using this model, we aimed to investigate how signaling patterns of Erk and PI3K regulate Myc dynamics at the post-translational level.
Also, how robust is Myc dynamics with respect to network parameters, such as phosphorylation and dephosphorylation rate constants?
What is unique about this strategy of controlling Myc accumulation by sequentially modulating protein stability?
Is this a common strategy by which cells achieve reliable temporal control of key regulatory proteins?
By exploring these questions, our work may provide insights into design features of cell signaling networks and guidance for experimental intervention.
Conceptually, our model defines a unique module that connects with other models that deal with upstream signaling dynamics leading to the activation of Erk CITATION or PI3K CITATION, CITATION, as well as downstream dynamics leading to mammalian cell fate decisions CITATION CITATION We further propose that post-translation regulation of Myc represents an example of a generic dual-kinase motif.
With appropriate parameters, this motif will enable precise temporal sensing of input signals.
Protein identification using mass spectrometry is an indispensable computational tool in the life sciences.
A dramatic increase in the use of proteomic strategies to understand the biology of living systems generates an ongoing need for more effective, efficient, and accurate computational methods for protein identification.
A wide range of computational methods, each with various implementations, are available to complement different proteomic approaches.
A solid knowledge of the range of algorithms available and, more critically, the accuracy and effectiveness of these techniques is essential to ensure as many of the proteins as possible, within any particular experiment, are correctly identified.
Here, we undertake a systematic review of the currently available methods and algorithms for interpreting, managing, and analyzing biological data associated with protein identification.
We summarize the advances in computational solutions as they have responded to corresponding advances in mass spectrometry hardware.
The evolution of scoring algorithms and metrics for automated protein identification are also discussed with a focus on the relative performance of different techniques.
We also consider the relative advantages and limitations of different techniques in particular biological contexts.
Finally, we present our perspective on future developments in the area of computational protein identification by considering the most recent literature on new and promising approaches to the problem as well as identifying areas yet to be explored and the potential application of methods from other areas of computational biology.
Proteomics is a relatively new but rapidly maturing discipline within life science research for understanding the biology of an organism via the large-scale study of the proteins expressed by the organism.
There is already a vast body of literature applying proteomics in many different areas of clinical and biochemical interest and in the study of the pathogenesis, development, prevention, and treatment of a wide range of diseases CITATION .
Protein identification is a key and essential step in the field of proteomics.
The examination of patterns of protein expression alone can, of course, lead to important discoveries, including, for example, classification of samples on the basis of a particular pattern.
However, without identifying the proteins known to be critically involved in the system under investigation, it is not possible to delve into the biological explanation for these patterns or to develop hypotheses as to the underlying biology of the system of interest.
Thus, while protein identification may often be overlooked or taken for granted, it remains the key initial step in elucidating the biology of an organism by studying its protein expression.
Our ability to maximize the benefit of proteomics to life science research is often dependent on our ability to accurately, quickly, and completely identify the full complement of proteins found in our samples of interest.
The exponential growth in DNA sequence and protein databases, coupled with a similar growth in machine throughput, and the critical nature of protein identification to the proteomics process, has seen an explosion in interest in protein identification.
For example, both the number and proportion of National Center for Biotechnology articles containing the phrase protein identification has seen exponential growth in the past decade.
Mass spectrometry has emerged as the primary tool for protein identification and is the cornerstone of proteomics.
While it was first used almost a century ago CITATION, the use of mass spectrometry for biological applications dates from the 1950s CITATION, and its use in peptide identification dates from the 1960s CITATION.
Accuracy, speed, and sample weight range have seen improvements spanning many orders of magnitude in recent decades CITATION, making mass spectrometry one of the greatest scientific success stories of the twentieth century.
No fewer than five Nobel laureates have been awarded the distinction for their pioneering work in mass spectrometry.
The speed and accuracy of these machines make them amenable to the high-throughput applications required not just in proteomics, but also in many other areas of the life sciences, resulting in rapid developments in hardware, software, and data management in the last decade.
When we consider the use of mass spectrometers for protein identification, these rapid developments have lead to a bewildering number of instrument configurations, analysis algorithms, and data formats.
Mass spectrometry is often critically important in a number of research pipelines.
As such, biologists, and computational biologists especially, are often expected to meaningfully manage and interpret mass spectrometry data, requiring an understanding of the most up-to-date methods available to maximize true protein identifications and minimize false identifications for their particular application.
This insight into protein identification algorithms is important because often the results may be ambiguous, and the biases chosen to make the problem computationally tractable can radically affect the result.
Despite the improvements in mass spectrometry hardware and the reliability of modern protein identification software, several studies involving a range of mass spectrometers, datasets, and identification algorithms have shown in each case that fewer than half of the proteins in a complex proteomic sample can be identified CITATION CITATION.
Given the critical role of protein identification in proteomic analysis, this review aims to explore this apparent upper limit on the effectiveness of current protein identification algorithms and to give relevant background information and practical suggestions to computational biologists and life scientists so the best possible protein identifications can be realized.
Previous model-based analysis of the metabolic network of Geobacter sulfurreducens suggested the existence of several redundant pathways.
Here, we identified eight sets of redundant pathways that included redundancy for the assimilation of acetate, and for the conversion of pyruvate into acetyl-CoA.
These equivalent pathways and two other sub-optimal pathways were studied using 5 single-gene deletion mutants in those pathways for the evaluation of the predictive capacity of the model.
The growth phenotypes of these mutants were studied under 12 different conditions of electron donor and acceptor availability.
The comparison of the model predictions with the resulting experimental phenotypes indicated that pyruvate ferredoxin oxidoreductase is the only activity able to convert pyruvate into acetyl-CoA.
However, the results and the modeling showed that the two acetate activation pathways present are not only active, but needed due to the additional role of the acetyl-CoA transferase in the TCA cycle, probably reflecting the adaptation of these bacteria to acetate utilization.
In other cases, the data reconciliation suggested additional capacity constraints that were confirmed with biochemical assays.
The results demonstrate the need to experimentally verify the activity of key enzymes when developing in silico models of microbial physiology based on sequence-based reconstruction of metabolic networks.
Geobacter species are of interest because of their natural role in carbon and mineral cycling, their ability to remediate organic and metal contaminants in the subsurface, and their capacity to harvest electricity from waste organic matter CITATION CITATION.
Geobacter sulfurreducens CITATION is the most commonly investigated species of this genus because a genetic system CITATION, the complete genome sequence CITATION, whole genome microarrays CITATION and genome-scale proteomics CITATION are available.
Furthermore, functional genomics studies have provided insight into the mechanisms of extracellular electron transport onto important electron acceptors such as Fe oxides and electrodes CITATION CITATION .
G.
sulfurreducens can use either acetate or hydrogen as the sole electron donors for Fe reduction, and fumarate or malate can also be used as terminal electron acceptors CITATION.
An understanding of acetate metabolism in Geobacter species is required because acetate, secreted by fermenting organisms, is the dominant electron donor for Geobacteraceae in soils and sediments CITATION, and because recent studies have shown that the addition of acetate to uranium-contaminated aquifers can stimulate in situ bioremediation of uranium contamination by Geobacter species CITATION, CITATION.
Previous studies have demonstrated that Geobacter species, and the closely related Desulfuromonas acetoxidans, oxidize acetate via the TCA cycle CITATION CITATION.
However, many other aspects of acetate metabolism, and central metabolism in general, are still poorly understood.
To better understand the physiology of G. sulfurreducens, a constraint-based genome-scale metabolic model was constructed and used to investigate the unique physiology associated with the reduction of extracellular electron acceptors, such as Fe CITATION.
The genome-scale model enabled the assessment of the impact of global proton balance during Fe reduction on biomass and energy yields, and successfully predicted the lower biomass yields observed during the growth of a mutant in which the fumarate reductase had been deleted CITATION .
Furthermore, the network reconstruction revealed the existence of a number of redundant or alternate pathways in the central metabolism of G. sulfurreducens CITATION.
Recent genetic and in silico studies have shown that the presence of such redundant metabolic pathways, as well as isozymes, can enable metabolic networks to withstand genetic perturbations CITATION CITATION.
Experimental evidence for alternate optimal pathways have been observed in E. coli, where four metabolic gene deletion mutants had significantly different metabolic flux distributions, but similar overall growth rates CITATION.
Hence, the systematic investigation of the role of redundant pathways using in silico models can provide key insights into the properties of the metabolic networks.
Here we report on a coupled computational and experimental evaluation of potential redundant pathways during acetate metabolism in G. sulfurreducens.
We demonstrate the need for redundancy in the acetate assimilation pathways, due to a coupling between the TCA cycle and acetate activation to acetyl-CoA, and also the inactivity of some of the predicted alternatives for pyruvate oxidation to acetyl-CoA.
We also show that by using this information to constrain the model, its predictive capacity can be improved.
Both the excitability of a neuron's membrane, driven by active ion channels, and dendritic morphology contribute to neuronal firing dynamics, but the relative importance and interactions between these features remain poorly understood.
Recent modeling studies have shown that different combinations of active conductances can evoke similar firing patterns, but have neglected how morphology might contribute to homeostasis.
Parameterizing the morphology of a cylindrical dendrite, we introduce a novel application of mathematical sensitivity analysis that quantifies how dendritic length, diameter, and surface area influence neuronal firing, and compares these effects directly against those of active parameters.
The method was applied to a model of neurons from goldfish Area II.
These neurons exhibit, and likely contribute to, persistent activity in eye velocity storage, a simple model of working memory.
We introduce sensitivity landscapes, defined by local sensitivity analyses of firing rate and gain to each parameter, performed globally across the parameter space.
Principal directions over which sensitivity to all parameters varied most revealed intrinsic currents that most controlled model output.
We found domains where different groups of parameters had the highest sensitivities, suggesting that interactions within each group shaped firing behaviors within each specific domain.
Application of our method, and its characterization of which models were sensitive to general morphologic features, will lead to advances in understanding how realistic morphology participates in functional homeostasis.
Significantly, we can predict which active conductances, and how many of them, will compensate for a given age- or development-related structural change, or will offset a morphologic perturbation resulting from trauma or neurodegenerative disorder, to restore normal function.
Our method can be adapted to analyze any computational model.
Thus, sensitivity landscapes, and the quantitative predictions they provide, can give new insight into mechanisms of homeostasis in any biological system.
Recent studies have demonstrated that neurons and neuronal networks are capable of functional homeostasis, maintaining specific levels of neural activity over long time scales.
Although the combination of currents within individual neurons of the same class varies widely, the output of these neurons and even the network as a whole remains remarkably stable CITATION CITATION.
Interestingly, in some circumstances function is maintained despite changes in neuronal morphology CITATION.
Computational models have been used to explore processes underlying neuronal homeostasis CITATION, and have demonstrated that many combinations of conductance parameters across the parameter space can evoke similar firing patterns CITATION, CITATION, CITATION.
Neuronal morphology also seems to be under homeostatic control: global features, including the distribution of dendritic mass, are conserved across different classes of neurons, making it unlikely that local morphologic features, such as dendritic length and numbers of branches, are regulated independently CITATION, CITATION.
Nonetheless, computational models have not yet explored how morphology might contribute to functional homeostasis.
Dendritic morphology is a critical determinant of neuronal firing dynamics and signal processing CITATION CITATION.
The influence of morphology on neuronal processing is further enhanced by active ion channels distributed throughout the dendrites CITATION, CITATION CITATION.
Previous computational studies have identified conductance CITATION, CITATION, CITATION, CITATION and morphologic CITATION, CITATION, CITATION parameters that drive general firing patterns, but have not quantified how these different parameters influence individual models across parameter space.
Moreover, few studies have analyzed the contributions of intracellular calcium dynamics to firing patterns CITATION, despite the crucial role of intracellular Ca 2 in shaping membrane potential, synaptic transmission, and signaling cascades CITATION .
Working memory, which maintains a brief mental representation of a recent event necessary for future task performance CITATION, CITATION, is one function thought to exploit the computational capacity of dendrites CITATION, CITATION.
Persistent neural activity, a hallmark of working memory, has been observed throughout the brain.
Neurons from the pre-cerebellar nucleus Area II of goldfish exhibit, and likely contribute to, eye velocity storage CITATION CITATION, a mechanism that displays persistent activity long after termination of its eliciting stimulus.
Experiments suggest that intrinsic properties of individual cells contribute to persistent activity.
We hypothesize that morphologic properties of Area II neurons partially compensate for intrinsic differences in active and passive conductances to maintain similar firing patterns throughout the nucleus CITATION, CITATION .
To begin to test hypotheses like this, we introduce a novel application of mathematical sensitivity analysis CITATION in which we quantify the effects of different classes of parameters, like active conductances and morphology, on model output.
This quantification allows us to compare the effects of each of these parameters on models across parameter space that produce similar output, identifying mechanisms of homeostasis.
Sensitivity analysis has been used widely in physics, chemical engineering, and biological signaling models to understand the relationship between model input and output CITATION CITATION, but its use within computational neuroscience has been limited CITATION, CITATION.
Traditionally, sensitivity analysis is performed locally around a single optimal model, or globally reporting a general sensitivity throughout the parameter space, to quantify how each parameter contributes to model output.
The large number of parameters and their nonlinear interactions render such techniques insufficient over the broad parameter space of neuronal compartment models.
To synthesize these approaches, we explore sensitivity landscapes, which provide a global picture of parameter sensitivities while identifying how intrinsic properties of individual neurons influence their firing patterns.
This method can be adapted easily to compare sensitivities of parameters in any computational model.
Our method is defined by three basic steps: parameterizing morphology to permit its systematic variation; evaluating the sensitivity to small perturbations of models across parameter space; and exploring sensitivity landscapes constructed from the local sensitivities to identify global trends.
To simplify the presentation of our method, we use a reduced morphologic model comprising a soma and cylindrical dendrite.
We later demonstrate that our sensitivity analysis method can be applied to models that include more realistic morphologies.
Our analysis identified principal directions over which sensitivity magnitude, and even sign, to all parameters varied most.
At the same time, we found domains across the space in which different groups of parameters, even morphologic ones, had the highest sensitivities.
Together, these results reveal mechanisms that maintain homeostasis, both locally and globally across parameter space.
We show how sensitivities can be used to predict compensatory tradeoffs quantitatively, between morphologic and conductance parameters that can maintain target activity levels in Area II cells.
Such compensatory mechanisms exist in many systems where function is conserved despite variability in dendritic structure, synaptic input, or membrane excitability CITATION, CITATION, and may offset some of the morphologic changes associated with aging and neurodegenerative disorders.
Sensitivity landscapes provide new insight into mechanisms of functional homeostasis throughout the brain, and can facilitate analysis of homeostasis in any biological system.
Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks.
In recent years, natural images have become popular in the study of vision and have been used to show apparently impressive progress in building such models.
Here, we challenge the use of uncontrolled natural images in guiding that progress.
In particular, we show that a simple V1-like model a neuroscientist's null model, which should perform poorly at real-world visual object recognition tasks outperforms state-of-the-art object recognition systems on a standard, ostensibly natural image recognition test.
As a counterpoint, we designed a simpler recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model.
Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction.
Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition real-world image variation.
Visual object recognition is an extremely difficult computational problem.
The core problem is that each object in the world can cast an infinite number of different 2-D images onto the retina as the object's position, pose, lighting, and background vary relative to the viewer.
Yet the brain solves this problem effortlessly.
Progress in understanding the brain's solution to object recognition requires the construction of artificial recognition systems that ultimately aim to emulate our own visual abilities, often with biological inspiration.
Such computational approaches are critically important because they can provide experimentally testable hypotheses, and because instantiation of a working recognition system represents a particularly effective measure of success in understanding object recognition.
However, a major challenge is assessing the recognition performance of such models.
Ideally, artificial systems should be able to do what our own visual systems can, but it is unclear how to evaluate progress toward this goal.
In practice, this amounts to choosing an image set against which to test performance.
Although controversial, a popular recent approach in the study of vision is the use of natural images CITATION, CITATION CITATION, in part because they ostensibly capture the essence of problems encountered in the real world.
For example, in computational vision, the Caltech101 image set has emerged as a gold standard for testing natural object recognition performance CITATION.
The set consists of a large number of images divided into 101 object categories plus an additional background category.
While a number of specific concerns have been raised with this set, its images are still currently widely used by neuroscientists, both in theoretical and experimental contexts.
The logic of Caltech101 is that the sheer number of categories and the diversity of those images place a high bar for object recognition systems and require them to solve the computational crux of object recognition.
Because there are 102 object categories, chance performance is less than 1 percent correct.
In recent years, several object recognition models have shown what appears to be impressively high performance on this test better than 60 percent correct CITATION, CITATION CITATION, suggesting that these approaches, while still well below human performance, are at least heading in the right direction.
However, we argue here for caution, as it is not clear to what extent such natural image tests actually engage the core problem of object recognition.
Specifically, while the Caltech101 set certainly contains a large number of images, variations in object view, position, size, etc., between and within object category are poorly defined and are not varied systematically.
Furthermore, image backgrounds strongly covary with object category.
The majority of images are also composed photographs, in that a human decided how the shot should be framed, and thus the placement of objects within the image is not random and the set may not properly reflect the variation found in the real world.
Furthermore, if the Caltech101 object recognition task is hard, it is not easy to know what makes it hard different kinds of variation are all inextricably mixed together.
Such problems are not unique to the Caltech101 set, but also apply to other uncontrolled natural image sets .
Understanding of the intracellular molecular machinery that is responsible for the complex collective behavior of multicellular populations is an exigent problem of modern biology.
Quorum sensing, which allows bacteria to activate genetic programs cooperatively, provides an instructive and tractable example illuminating the causal relationships between the molecular organization of gene networks and the complex phenotypes they control.
In this work we to our knowledge for the first time present a detailed model of the population-wide transition to quorum sensing using the example of Agrobacterium tumefaciens.
We construct a model describing the Ti plasmid quorum-sensing gene network and demonstrate that it behaves as an on off gene expression switch that is robust to molecular noise and that activates the plasmid conjugation program in response to the increase in autoinducer concentration.
This intracellular model is then incorporated into an agent-based stochastic population model that also describes bacterial motion, cell division, and chemical communication.
Simulating the transition to quorum sensing in a liquid medium and biofilm, we explain the experimentally observed gradual manifestation of the quorum-sensing phenotype by showing that the transition of individual model cells into the on state is spread stochastically over a broad range of autoinducer concentrations.
At the same time, the population-averaged values of critical autoinducer concentration and the threshold population density are shown to be robust to variability between individual cells, predictable and specific to particular growth conditions.
Our modeling approach connects intracellular and population scales of the quorum-sensing phenomenon and provides plausible answers to the long-standing questions regarding the ecological and evolutionary significance of the phenomenon.
Thus, we demonstrate that the transition to quorum sensing requires a much higher threshold cell density in liquid medium than in biofilm, and on this basis we hypothesize that in Agrobacterium quorum sensing serves as the detector of biofilm formation.
Molecular networks, which integrate signal transduction and gene expression into the unified decision circuitry, are ultimately responsible for the realization of all life activities of biological cells including internal developmental programs and responses to environmental factors.
One of the main challenges of systems biology is to uncover and understand the relationships between the properties of these molecular circuits and the macroscopic cellular phenotypes that are controlled by them CITATION.
Particularly important are the phenotypes involving interaction and cooperative action of multiple cells.
The mapping of networks onto phenotypes is still difficult to accomplish in multicellular eukaryotic organisms owing to their staggering complexity.
Less complex and more experimentally accessible prokaryotic organisms became the systems of choice for dissecting social behavior at the genetic level CITATION.
The phenomenon of bacterial quorum sensing gives us a particularly unique opportunity to follow the causal relationships from molecular circuitry to cooperative population dynamics.
QS refers to the ability of bacterial populations to collectively activate certain gene expression programs, e.g., toxin release or antibiotic production, once some critical population density has been reached.
QS is found in a vast variety of bacterial species and has been extensively studied experimentally CITATION CITATION.
In Gram-negative bacteria, the QS phenomenon is usually controlled by a small gene expression network that functions as an environmentally activated on off gene expression switch CITATION, CITATION whose operation is analogous to that of radar.
At the low cell density that normally corresponds to the off switch state, a key transcription factor required for the expression of proteins responsible for the phenotype is suppressed.
At the same time, the cell steadily produces a small amount of the QS signaling molecule, termed the autoinducer, that can freely diffuse in and out of the cell.
While the population density is low, most of the autoinducer molecules are washed out and dispersed in the environment by diffusion.
As the cell density grows, more molecules of autoinducer enter the bacterium from outside.
Once certain cell quorum is reached, the inbound autoinducer signal triggers the transition of the QS network to the on state, resulting in the production of the transcription factor and the expression of the target genes.
This transition on both intracellular and population-wide scales is the focus of our study.
We investigate the phenomenon of QS in the soil-dwelling plant pathogen Agrobacterium tumefaciens, the causative agent of crown gall disease CITATION.
Bacteria of this species often harbor Ti plasmids that endow their hosts with the unique ability to genetically modify susceptible plants through a cross-kingdom DNA transfer.
Like many other soil bacteria, Agrobacterium is chemotactic to exudates released by plant wounds and is capable of catabolizing various nutrients that leave injured plant roots.
Once bacteria form physical contact with the surface of the wound, Ti plasmids offer their hosts an extraordinary advantage over their plasmidless competitors.
A fragment of the plasmid, termed the vir region, is injected into the plant cell in the form of a virion-like complex and is stably incorporated into the plant genome CITATION.
One of the imported genes is responsible for the synthesis of opines, a class of low-molecular-weight nitrogen-rich metabolites, that can be utilized as a nutrient only by the bacteria that harbor the Ti plasmid.
Other transferred genes cause a vigorous proliferation of infected plant cells that eventually results in the formation of a characteristic gall tumor.
Once productive infection is established, Ti plasmids attempt to propagate themselves into the plasmidless bacteria of the same or related species by means of genetic conjugation.
It has been shown that the conjugal transfer of Ti plasmids requires the QS phenomenon CITATION .
Functional significance of QS for the control of Ti plasmid conjugation remains an ecological and evolutionary puzzle.
It is widely believed CITATION, CITATION that QS controls processes, such as production of toxins and antibiotics, that are either inefficient or devoid of adaptive value if not performed on a population scale.
Thus, the fact that the establishment of QS is upstream of the initiation of conjugation seems to imply that plasmids await the critical density of donors to collectively begin transfer to recipients.
Since multiple donors cannot cooperate in DNA transfer, the necessity for collective action does not seem to be relevant in our case.
Instead, to increase the probability of successful conjugation it would appear beneficial to exceed a certain number of recipients per donor.
However the density of plasmidless recipients cannot be estimated using QS since they do not produce the autoinducer.
This seemingly paradoxical situation may imply that our understanding of the biological function of QS is not yet complete.
Indeed, an alternative function for QS as a sensor of the volume enclosing the bacteria has also been proposed CITATION.
To answer what bacteria really measure using QS in each particular situation, it is necessary to consider the ecologically relevant conditions of bacterial growth CITATION .
An experimental approach to this problem is often complicated by the technical difficulty of work in real ecosystems.
On the other hand, mathematical modeling can significantly aid and complement experimental methods in answering biological questions that involve spatial and temporal scales of the QS phenomenon.
Some aspects of either intracellular CITATION CITATION or population CITATION CITATION dynamics have been mathematically modeled to gain insight into the QS phenomenon in Pseudomonas aeruginosa and Vibrio fischeri.
However, because of the lack of detailed molecular information, experimentally testable conclusions on the connections between intracellular and population dynamics have rarely been made.
Here we develop a multi-level modeling approach that describes both the intracellular and the population-wide dynamics and allows us to follow the connections between them explicitly.
Although much has been learned about the molecular details of the Agrobacterium QS network, it is not always clear what functions they perform.
Here we construct a detailed model of the QS network in Agrobacterium and analyze it both quantitatively and qualitatively.
We demonstrate that the network possesses properties of the on off gene expression switch robust to molecular noise.
We further develop a population-scale model that incorporates bacterial motion, cell division, and chemical communication while explicitly considering the individual intracellular dynamics of each cell.
This allows us to describe the transition to QS on both cellular and population scales and quantitatively predict the values of critical autoinducer concentration and threshold cell density as functions of various intracellular and environmental parameters.
Finally, comparing feasibility of the transition to QS in homogeneous medium and biofilm, we present a hypothesis explaining the ecological and evolutionary roles of QS in regulation of Ti plasmid conjugal transfer.
All genes that are thought to constitute the QS network are located on the Ti plasmid itself CITATION.
The entire QS network is controlled upstream by the availability of the plant-produced opines to ensure that energetically expensive conjugation machinery is activated only after the establishment of a successful plant wound infection.
Based on the chemical nature of the encoded opines, Ti plasmids are divided into two major types CITATION, of which we consider only the octopine type.
We reconstructed the layout of the QS network for the octopine-type Ti plasmids from the published experimental data.
In this plasmid class, octopine molecules that are imported through the cell wall eventually cause activation of transcription from the operon occ CITATION.
In the model, we assume that octopine is constitutively available at the saturating concentration that results in the maximal rate of occ transcription.
The last open reading frame of this operon codes for the QS transcription activator TraR.
Binding of TraR to its cognate autoinducer is thought to occur only within a narrow window of time during traR mRNA translation when the newly formed protein chain tightly winds around a single molecule of Agrobacterium autoinducer CITATION CITATION.
This total engulfment of AAI molecule makes formation of the TraR AAI complex practically irreversible.
Furthermore, TraR protein translated in the absence of AAI is misfolded, insoluble, and unable to bind AAI CITATION, CITATION.
This has an important consequence in that the rate of production of TraR depends on the concentrations of traR mRNA and AAI and does not depend on the accumulation of misfolded TraR protein, as explicitly shown in Figure 1.
Once formed, TraR quickly dimerizes to form a stable transcriptionally active TraR dimer with a relatively short half-life of 35 min CITATION.
TraRd is capable of activating a number of operons that encode proteins necessary for conjugation.
The first open reading frame of the trb operon codes for the acyl-homoserine lactone synthetase TraI, which utilizes two metabolites abundant in the bacterial cell to create AAI CITATION.
Since our model considers transition to QS in the mostly nutrient-rich, stress-free conditions of an optimized growth medium, we assume that the substrates of TraI are present in excess and their concentrations do not limit the rate of AAI production.
Both traR and traI were shown to be expressed at some low constitutive rate even in the absence of octopine CITATION.
The TraR TraI couple constitutes the classic QS positive feedback loop found in many Gram-negative bacteria.
Additional feedback loops that also involve other components of the QS network are specific for Agrobacterium.
Thus, negative control of QS is provided by the antiactivator traM, whose transcription is directly activated by TraRd CITATION.
TraM effectively sequesters TraRd through the formation of a very stable complex in which TraRd is unable to bind DNA CITATION, CITATION.
Recently, a number of authors reported that, like TraR, TraM also forms a dimer CITATION CITATION.
The stoichiometry of the reaction between TraR and TraM, however, remains controversial CITATION CITATION.
In our model we follow the original hypothesis of Swiderska et al. CITATION, which assumes that the complex consists of one TraRd and one monomer of TraM.
This hypothesis is partially supported by Chen et al. CITATION, who showed that the TraM dimer must dissociate to form a complex with TraR.
Under these assumptions we disregard dimerization of TraM as not affecting the network behavior.
An additional positive feedback loop arises because TraRd activates transcription of the msh operon, which is a suboperon of occ that contains traR itself.
Several lines of evidence suggest that active transporters facilitate traffic of the QS signaling molecules through the cell wall in a number of bacterial species including Agrobacterium CITATION CITATION.
In our model, we explore the hypothesis that AAI is imported from the environment by an active pump that is also under the transcriptional control of TraRd.
Indeed, the msh operon contains five open reading frames that encode a putative ABC-type importer whose function is not completely understood but that has been hypothesized to be an active transporter of AAI into the cell CITATION.
Taking into consideration this uncertainty, the putative AAI importer in the model is denoted simply as Imp.
Protein interactions play a vital part in the function of a cell.
As experimental techniques for detection and validation of protein interactions are time consuming, there is a need for computational methods for this task.
Protein interactions appear to form a network with a relatively high degree of local clustering.
In this paper we exploit this clustering by suggesting a score based on triplets of observed protein interactions.
The score utilises both protein characteristics and network properties.
Our score based on triplets is shown to complement existing techniques for predicting protein interactions, outperforming them on data sets which display a high degree of clustering.
The predicted interactions score highly against test measures for accuracy.
Compared to a similar score derived from pairwise interactions only, the triplet score displays higher sensitivity and specificity.
By looking at specific examples, we show how an experimental set of interactions can be enriched and validated.
As part of this work we also examine the effect of different prior databases upon the accuracy of prediction and find that the interactions from the same kingdom give better results than from across kingdoms, suggesting that there may be fundamental differences between the networks.
These results all emphasize that network structure is important and helps in the accurate prediction of protein interactions.
The protein interaction data set and the program used in our analysis, and a list of predictions and validations, are available at LINK.
For understanding the complex activities within an organism, a complete and error-free network of protein interactions which occur in the organism would be a significant step forward.
Experimentally, protein interactions can be detected by a number of techniques, and the data is publicly available from several databases such as DIP, Database of Interacting Proteins CITATION, and MIPS, Munich Information Center for Protein Sequences CITATION.
Unfortunately, these experimentally detected interactions show high false negative CITATION and high false positive rates CITATION, CITATION.
In this paper we develop a new computational approach to predict interactions and validate experimental data.
Computational methods have already been developed for these purposes.
For interaction validation, these have mainly centered on the use of expression data CITATION, CITATION or the co-functionality or co-localisation of the proteins involved CITATION, CITATION .
For prediction of protein interactions in contrast, many methods have been suggested.
The majority of these generate lists of proteins with a functional relationship rather than physical interactions CITATION, CITATION .
In terms of physical interaction prediction the available methods can be typified by the two approaches of Deng et al. CITATION and Jonsson et al. CITATION .
In Deng et al.'s method, a domain interaction based approach, a protein interaction is inferred on the basis of domain contacts.
If a domain pair is frequently found in observed protein interactions, it is likely that other protein pairs containing this domain pair might also interact.
From the observed protein interaction network, the probabilities of domain-domain interactions are estimated.
The expectation-maximum algorithm is employed to compute maximum likelihood estimates, assuming that protein interactions occur independently of each other.
This likelihood is then used to construct a probability score for a protein pair to interact, it is inferred based on the estimated probabilities of domain interactions within the protein pair.
Deng et al.'s prediction is based on a total of 5,719 interactions from S.cerevisiae.
However, the limited number of known domains may well not be enough to describe the variety of protein interactions.
This approach has had further extensions, such as an improved scoring for domain interactions CITATION and the inclusion of other biological information CITATION.
Liu et al.'s model CITATION is an extension of Deng et al.'s method which integrates multiple organisms.
In addition to S.cerevisiae, two other organisms, C.elegans, D.melanogaster, are included.
The second type of approach, as used by Jonsson et al. CITATION, is homology-based.
It searches for interlogs among protein interactions from other organisms.
If an interlog of a protein interaction exists in many other organisms, this protein interaction will score highly.
In addition to searching for orthologous interlogs, Mika and Saeed CITATION, CITATION suggest that paralogous interlogs may provide even more information for inferring interacting protein pairs.
In principle, statistical clustering algorithms such as CITATION and CITATION which identify cliques in the network could be viewed as a prediction method, predicting that all proteins within a clique interact with each other.
This interpretation is biologically questionable, and as the focus in the statistical clustering approach is on locating cliques and overlapping modules rather than on predicting individual interactions, we exclude it from our comparisons.
Neither Deng et al.'s method nor Jonsson et al.'s method make use of network structure beyond pairwise interactions; interactions are considered as isolated pairs.
However these pairs could and should be considered as a network, where the proteins are nodes and their interactions are links CITATION, CITATION.
Topological examination of these networks has revealed many interesting properties, including a clustering tendency CITATION, CITATION, see also Supporting Information.
In our method we exploit the network structure by developing a score which considers triadic patterns of interactions rather than pairs.
In this paper we thus take the established idea that the characteristics of a protein will affect its interactions alongside the not yet fully explored idea that its network position will also affect its interactions, in order to develop a novel predictive tool.
Our goal is to predict protein interactions of the type x with y, where both x and y interact with a third protein z. Therefore in our approach we particularly focus on two simple three node network structures, triangles and lines.
A triangle is a subnet formed by an interacting protein pair with a common neighbour.
A line, by contrast, is a subnet formed by an non-interacting protein pair with a common neighbour.
We will show that these network structures and the protein characteristics within them help to predict protein interactions.
We apply our method to the S.cerevisiae interaction network from the DIP database.
During the validation we assume that function and structure are known for all proteins and that the protein interaction network is known for all but one interaction.
With triadic interacting patterns, we predict the interaction status of those protein pairs with at least one common neighbour and compare our results with those from three other published scores.
We go on to demonstrate that the requirement to have fully annotated proteins can be relaxed to include partially annotated proteins, with a slight drop in the accuracy.
The prediction is also compared with simulated networks where all proteins are shuffled while the network structure is maintained, in order to examine whether the specific network structure, triangles and lines, keep useful information in forming protein interaction networks.
To measure the true positive rate in a set of protein pairs, Deane et al CITATION proposed the expression profile index, a measure of the true positive rate in a set of protein pairs based on biological relevance.
We compare the EPR index to our score, showing that, with a suitable cut-off, our predictions achieve a high true positive rate.
We also give examples of validated experimental data and predict new interactions.
Our predictive model uses a prior interaction database and for this we use three prior databases, pooling protein interactions collected from prokaryotes, eukaryotes and all interactions.
The results from using different prior databases show that the use of interactions from within the same kingdom rather than across kingdoms significantly improves the results, indicating as in CITATION that interaction networks may be significantly different between the kingdoms.
Comparing our method to three other standard approaches, namely the domain-based approach by Deng et al. and an extension by Liu et al., and a homology-based approach by Jonsson et al., we find that our method outperforms the above approaches on the subset of interactions in the DIP Yeast data set which contains enough annotation and connectivity to be included in our analysis.
Our method complements the methods by Deng et al. and Liu et al., as their approaches apply to a rather different subset of potential interactions yielded from the DIP Yeast data set.
Massive amounts of data are being generated in an effort to represent for the brain the expression of all genes at cellular resolution.
Critical to exploiting this effort is the ability to place these data into a common frame of reference.
Here we have developed a computational method for annotating gene expression patterns in the context of a digital atlas to facilitate custom user queries and comparisons of this type of data.
This procedure has been applied to 200 genes in the postnatal mouse brain.
As an illustration of utility, we identify candidate genes that may be related to Parkinson disease by using the expression of a dopamine transporter in the substantia nigra as a search query pattern.
In addition, we discover that transcription factor Rorb is down-regulated in the barrelless mutant relative to control mice by quantitative comparison of expression patterns in layer IV somatosensory cortex.
The semi-automated annotation method developed here is applicable to a broad spectrum of complex tissues and data modalities.
High-resolution maps of gene expression provide important information about how genes regulate biological processes at cellular and molecular levels.
Therefore, a multitude of efforts are in progress to depict gene expression at single cell resolution in specimens ranging from organs to embryos.
Common to these genome-scale projects is that they generate vast numbers of images of expression patterns that reveal the presence of transcripts or proteins in a particular cell or group of cells within a natural context.
However, large collections of images are of limited usefulness per se without efficient means to mine these images and to characterize and compare gene or protein expression patterns.
In analogy to the requirements for mining genomic sequence information, meaningful retrieval of expression patterns requires suitable annotation.
By annotation, we mean associating sites and strengths of expression with a digital representation of the anatomy of a specimen.
The annotation approach taken by the Gene Expression Database CITATION is to hand-curate published gene expression patterns using an extensive dictionary of anatomical terms.
This annotation is facilitated by the Edinburgh Mouse Atlas Project, which provides anatomical ontology relationships using a hierarchical tree CITATION.
Visualization is achieved by associating these terms with locations in a volumetric model CITATION.
The Edinburgh Mouse Atlas Project also provides tools to map in situ hybridization images directly into a three-dimensional atlas CITATION.
Although hand curation is an effective method for annotation, it is not an efficient means for handling the large-scale datasets systematically collected by robotic ISH CITATION.
In addition, if future changes are made to anatomical designations, updating the annotation may require a laborious review of previously annotated data.
Here we present a completely novel approach that uses a geometric modeling technique to create a digital atlas of the postnatal day 7 mouse brain.
This deformable atlas can then be adjusted to match the major anatomical structures present in P7 mouse brain tissue sections, accurately define the boundaries between structures, and provide a smooth multi-resolution coordinate representation of small structures.
When combining this technique with a method for detecting strength of gene expression, one can efficiently and automatically annotate a large number of gene expression patterns in a way that subsequently allows queries and comparisons of expression patterns in user-defined regions of interest.
P7 mouse brain was selected as the specimen because at this developmental stage, many complex brain functions begin to be established yet the existing information on underlying molecular mechanisms is still relatively limited.
We describe here the creation of a prototype 200-gene dataset generated using robotic ISH, and the application of our deformable atlas-based annotation method to this dataset.
We then demonstrate the utility of the approach with two examples: searching for genes expressed in the substantia nigra, and identifying genes potentially involved in functional regionalization of the cortex.
Functional brain networks detected in task-free functional magnetic resonance imaging have a small-world architecture that reflects a robust functional organization of the brain.
Here, we examined whether this functional organization is disrupted in Alzheimer's disease.
Task-free fMRI data from 21 AD subjects and 18 age-matched controls were obtained.
Wavelet analysis was applied to the fMRI data to compute frequency-dependent correlation matrices.
Correlation matrices were thresholded to create 90-node undirected-graphs of functional brain networks.
Small-world metrics were computed using graph analytical methods.
In the low frequency interval 0.01 to 0.05 Hz, functional brain networks in controls showed small-world organization of brain activity, characterized by a high clustering coefficient and a low characteristic path length.
In contrast, functional brain networks in AD showed loss of small-world properties, characterized by a significantly lower clustering coefficient, indicative of disrupted local connectivity.
Clustering coefficients for the left and right hippocampus were significantly lower in the AD group compared to the control group.
Furthermore, the clustering coefficient distinguished AD participants from the controls with a sensitivity of 72 percent and specificity of 78 percent.
Our study provides new evidence that there is disrupted organization of functional brain networks in AD.
Small-world metrics can characterize the functional organization of the brain in AD, and our findings further suggest that these network measures may be useful as an imaging-based biomarker to distinguish AD from healthy aging.
Alzheimer's disease is a neurodegenerative disorder characterized by progressive impairment of episodic memory and other cognitive domains resulting in dementia and, ultimately, death.
Imaging studies in AD have begun a shift from studies of brain structure CITATION, CITATION to more recent studies highlighting focal regions of abnormal brain function CITATION CITATION.
Most recently, fMRI studies have moved beyond focal activation abnormalities to dysfunctional brain connectivity.
Functional connectivity is defined as temporal correlations between spatially distinct brain regions CITATION.
PET studies, restricted to across-subject connectivity measures, have shown that AD patients have decreased hippocampus connectivity with prefrontal cortex CITATION and posterior cingulate cortex CITATION during memory tasks.
Using fMRI, we demonstrated that AD patients performing a simple motor task had reduced intra-subject functional connectivity within a network of brain regions termed the default-mode network that includes posterior cingulate cortex, temporoparietal junction, and hippocampus CITATION.
Bokde et al. reported abnormalities in fusiform gyrus connectivity during a face-matching task in subjects with mild cognitive impairment frequently a precursor to AD CITATION.
Three recent studies have reported reduced default-mode network deactivation in MCI and/or AD patients during encoding tasks CITATION, CITATION and during a semantic classification task CITATION.
Celone et al also reported increased default-mode network deactivation in a subset of less impaired MCI patients.
In addition to analyzing functional connectivity during task performance, functional connectivity has also been investigated during task-free conditions.
Task-free functional connectivity MRI detects interregional correlations in spontaneous blood oxygen level-dependent signal fluctuations CITATION.
Using this approach, Wang et al. found disrupted functional connectivity between hippocampus and several neocortical regions in AD CITATION.
Similarly, Li et al. reported reduced intrahippocampal connectivity during task-free conditions CITATION.
Most recently Sorg et al. CITATION reported reduced resting-state functional connectivity in the default-mode network of MCI patients.
Although evidence is accumulating that AD disrupts functional connections between brain regions CITATION, it is not clear whether AD disrupts global functional brain organization.
Graph metrics the clustering coefficient and the characteristic path length are useful measures of global organization of large-scale networks CITATION.
Graphs are data structures which have nodes and edges between the nodes.
The clustering coefficient is a measure of local network connectivity.
A network with a high average clustering coefficient is characterized by densely connected local clusters.
The characteristic path length is a measure of how well connected a network is. A network with a low characteristic path length is characterized by short distances between any two nodes.
Small-world network is characterized by a high clustering coefficient and a low characteristic path length CITATION, CITATION.
In a graphical representation of a brain network, a node corresponds to a brain region while an edge corresponds to the functional interaction between two brain regions.
Functional connectivity networks of the human brain derived from electroencephalograms, magnetoencephalograms and task-free fMRI data exhibit small-world characteristics CITATION CITATION.
In a recent EEG study, Stam et al. reported that small-world architecture in functional networks in the brain is disrupted in AD CITATION .
Here we examined the global functional organization of the brain in AD by creating whole-brain functional connectivity networks from task-free fMRI data, characterizing the organization of these networks using small-world metrics, and comparing these characteristics between AD patients and age-matched controls.
We hypothesized that global functional brain organization would be abnormal in AD.
Further, given the need for a reliable, non-invasive clinical test for AD CITATION, we sought to determine whether a small-world metric obtained from task-free fMRI data might provide a sensitive and specific biomarker in AD.
Most mammalian genes are able to express several splice variants in a phenomenon known as alternative splicing.
Serious alterations of alternative splicing occur in cancer tissues, leading to expression of multiple aberrant splice forms.
Most studies of alternative splicing defects have focused on the identification of cancer-specific splice variants as potential therapeutic targets.
Here, we examine instead the bulk of non-specific transcript isoforms and analyze their level of disorder using a measure of uncertainty called Shannon's entropy.
We compare isoform expression entropy in normal and cancer tissues from the same anatomical site for different classes of transcript variations: alternative splicing, polyadenylation, and transcription initiation.
Whereas alternative initiation and polyadenylation show no significant gain or loss of entropy between normal and cancer tissues, alternative splicing shows highly significant entropy gains for 13 of the 27 cancers studied.
This entropy gain is characterized by a flattening in the expression profile of normal isoforms and is correlated to the level of estimated cellular proliferation in the cancer tissue.
Interestingly, the genes that present the highest entropy gain are enriched in splicing factors.
We provide here the first quantitative estimate of splicing disruption in cancer.
The expression of normal splice variants is widely and significantly disrupted in at least half of the cancers studied.
We postulate that such splicing disorders may develop in part from splicing alteration in key splice factors, which in turn significantly impact multiple target genes.
The majority of mammalian genes produce alternative transcripts as part of their normal expression program CITATION CITATION.
Alternative transcripts include splicing, polyadenylation and transcription initiation variants which can be expressed differentially in different tissues CITATION CITATION providing the fine tuning of gene expression required for cell differentiation and tissue-specific functions.
Disruptions in the balance of alternative transcripts, especially at the splicing level, are known to affect angiogenesis CITATION, cell differentiation CITATION and invasion CITATION.
A large body of evidence has established connections between alternative splicing defects and cancer, so that the identification of transcript isoforms is now considered an important avenue in cancer diagnosis and therapy CITATION, CITATION .
The disruption of splicing isoform expression in cancer may result from very different underlying genetic events.
On one hand, mutations in cis-regulatory sequences lead to the abnormal expression of specific isoforms, as observed for example in the BRCA1 gene in breast and ovarian cancer CITATION.
Another class of event includes alterations of the mRNA processing machinery or its signalling pathway.
These may affect the splicing of specific genes such as CD44 CITATION CITATION, but may also cause wider perturbations of isoform expression as the processing of multiple genes can be simultaneously affected CITATION CITATION.
Evidence for wider changes in alternative transcription linked with cancer are present for instance in EST databases, where a large fraction of splice variant are actually tumor-specific CITATION.
However, while most studies of splicing and cancer attempt to isolate signature splice variants with significant over-expression in disease cells, no published work to date has focused on the bulk of splicing disruption that potentially arises when the splicing machinery is impaired.
The aim of the present study is to evaluate the extent and modalities of non-specific alternative transcript disruptions in cancer.
Instead of seeking interesting signature isoforms, we analyzed the distribution of all isoforms from a single gene in a given tissue.
We postulated that, in a tissue where the splicing machinery is impaired, the distribution of isoforms may be more disordered than in a control tissue.
To measure the level of disorder in cDNA and cDNA tag libraries, we borrowed the notion of entropy from information theory.
We applied this measure to all three types of alternative transcription, comparing isoform distributions in pairs of disease and normal tissues.
Our results show that neither alternative polyadenylation nor alternative transcription initiation are associated with a disordered isoform expression.
However, in half of the cancers studied, alternative splicing showed a highly significant entropy gain relative to the corresponding normal tissues.
We analyze this entropy gain and discuss its possible causes.
Macrophages are versatile immune cells that can detect a variety of pathogen-associated molecular patterns through their Toll-like receptors.
In response to microbial challenge, the TLR-stimulated macrophage undergoes an activation program controlled by a dynamically inducible transcriptional regulatory network.
Mapping a complex mammalian transcriptional network poses significant challenges and requires the integration of multiple experimental data types.
In this work, we inferred a transcriptional network underlying TLR-stimulated murine macrophage activation.
Microarray-based expression profiling and transcription factor binding site motif scanning were used to infer a network of associations between transcription factor genes and clusters of co-expressed target genes.
The time-lagged correlation was used to analyze temporal expression data in order to identify potential causal influences in the network.
A novel statistical test was developed to assess the significance of the time-lagged correlation.
Several associations in the resulting inferred network were validated using targeted ChIP-on-chip experiments.
The network incorporates known regulators and gives insight into the transcriptional control of macrophage activation.
Our analysis identified a novel regulator that may have a role in macrophage activation.
Dynamic cellular processes, such as the response to a signaling event, are governed by complex transcriptional regulatory networks.
These networks typically involve a large number of transcription factors that are activated in different combinations in order to produce a particular cellular response.
The macrophage, a vital cell type of the mammalian immune system, marshals a variety of phenotypic responses to pathogenic challenge, such as secretion of pro-inflammatory mediators, phagocytosis and antigen presentation, stimulation of mucus production, and adherence.
In the innate immune system, the first line of defense against infection, the macrophage's Toll-like receptors play a crucial role by recognizing distinct pathogen-associated molecular patterns, such as flagellin, lipopeptides, or double-stranded RNA CITATION, CITATION.
TLR signals are first channeled through adapter molecules and then through parallel cross-talking signal pathways.
These activated pathways initiate a transcriptional program in which over 1,000 genes CITATION and hundreds of TF genes CITATION can be differentially expressed, and which is tailored to the type of infection CITATION, CITATION.
The transcriptional network underlying macrophage activation can exhibit many distinct steady-states which are associated with tissue- and infection-specific macrophage functions CITATION.
The transcriptional response is also dynamic and characterized by temporal waves of gene activation CITATION, CITATION, CITATION, each enriched for distinct sets of gene functions CITATION, CITATION and likely to be controlled by different combinations of transcriptional regulators CITATION, CITATION.
Long-term, elucidating the transcriptional network underlying TLR-stimulated macrophage activation, and identifying key regulators and their functions, would greatly enhance our understanding of the innate immune response to infection and potentially yield new ideas for vaccine development.
Computational analysis of high-throughput experimental data is proving increasingly useful in the inference of transcriptional regulatory interaction networks CITATION CITATION and in the identification and prioritization of potential regulators for targeted experimental validation CITATION, CITATION.
Time-course microarray expression measurements have been used to infer dynamic transcriptional networks in yeast CITATION, CITATION and static influence networks in mammalian cell lines CITATION.
In the context of primary macrophages, expression-based computational reconstruction of the transcriptional control logic underlying the activation program is not straightforward and progress is difficult to measure, for several reasons.
First, transcriptional control within mammalian networks in general CITATION, and for key TLR-responsive genes in particular CITATION, is combinatorial.
Second, many induced TFs are subject to post-translational activation CITATION and dynamic control of nuclear localization CITATION.
Third, targeted genetic perturbations are presently infeasible to perform on a large scale in a mammalian animal model, and expression knockdown is difficult in macrophages due to the tendency of the vector to stimulate TLRs.
Finally, the few transcriptional regulatory interactions that have been validated through targeted experiments in TLR-stimulated primary macrophages are not available in a single gold standard dataset.
Therefore, in the context of transcriptional regulation in the mammalian macrophage, with presently accessible expression data sets, large-scale computational inference is primarily useful for statistically identifying potential regulatory interactions, rather than as an inference tool for predicting the transcriptional control logic for specific target genes.
For the reasons described above, in order to computationally infer transcriptional regulatory interactions in a mammalian system, it is necessary to include additional sources of evidence to constrain or inform the transcriptional network model selection.
Computational scanning of the promoter sequences of clusters of co-expressed genes for known transcription factor binding site motifs has proved particularly valuable when combined with global expression data CITATION, CITATION, CITATION.
Recently, Nilsson et al. CITATION used a combination of expression clustering and promoter sequence scanning for TFBS motifs to construct an initial transcriptional network of the macrophage stimulated with the TLR4 stimulus lipopolysaccharide.
Their work identified two novel regulators, but the clustering was based on an expression dataset with a single stimulus, limited biological replicates, and few time points.
Moreover, TFBS motif scanning of co-expressed clusters, without utilizing expression dynamics, provides only a limited and static picture of the underlying transcriptional network.
Many TFBS motifs are often recognized by multiple TFs, making difficult the unambiguous identification of the regulating TF from TFBS enrichment alone.
Furthermore, because of the tendency of TFBS motifs to co-occur CITATION, it is difficult to determine from among a set of co-occurring motifs which associated TF is the most relevant to the condition-specific regulation of the target cluster.
In the TLR-stimulated macrophage, core transcription factors already expressed in the cell are rapidly activated and initiate transcriptional regulation of second wave TF genes CITATION.
Such transcriptionally regulated TF genes are key candidates for an integrated analysis combining TF-specific dynamic expression data and sequence-based motif scanning data.
This work is concerned with using computational data integration to identify a set of core differentially expressed transcriptional regulators in the TLR-stimulated macrophage and, in the form of statistical associations, the clusters of co-expressed genes that they may regulate.
The clusters are differentiated based on temporal and stimulus-specific activation, and in this sense, the inferred associations constitute a preliminary dynamic transcriptional network for the TLR-stimulated macrophage.
To achieve this, we used a novel computational approach incorporating TFBS motif scanning and statistical inference based on time-course expression data across a diverse array of stimuli.
Our approach involved four steps.
A set of genes was identified that were differentially expressed by wild-type macrophages under at least one TLR stimulation experiment.
These genes were clustered based on their expression profiles across a wide range of conditions and strains, grouping genes based on the similarity of the timing and stimulus-dependence of their induction.
Gene Ontology annotations were used to identify functional categories enriched within the gene clusters.
Promoter sequences upstream of the genes within each cluster were scanned for a library of TFBS motifs, each recognized by at least one differentially expressed TF, to identify possible associations between TFs and gene clusters.
Across eleven different time-course studies, dynamic expression profiles of TF genes and target genes were compared in order to identify possible causal influences between differentially expressed TF genes and clusters.
Several techniques have been developed specifically for model inference from time-course expression data, notably dynamic Bayesian networks CITATION and ODE-based model selection CITATION.
However, the parametric complexity of these model classes makes it difficult to apply them to infer a network underlying a specific cellular perturbation with a limited expression dataset.
Here, potential transcriptional regulatory influence is inferred from time-course expression data using the time-lagged correlation statistic, which has been used to infer biochemical interaction networks CITATION as well as transcriptional networks CITATION CITATION.
The TLC has the advantage that it accounts for the time delay between differential expression of an induced TF and differential expression of a target gene.
In contrast to standard correlation-based methods that identify co-expressed genes, the TLC method uses temporal ordering of expression to determine whether the time lag between two correlated genes is consistent with a causal interaction.
We developed a novel method to identify the optimal time lag for each gene pair, and used a prior probability distribution of transcriptional time delays to score possible interactions.
By combining the promoter scanning-based evidence with the evidence obtained by the time-lagged correlation analysis of the expression data, we were able to identify a network of statistically significant associations between 36 TF genes and 27 co-expressed clusters.
Overall, 63 percent of differentially expressed genes are included in the network.
The network provided insights into the temporal organization of the transcriptional response and into combinations of TFs that may act as key regulators of macrophage activation.
Finally, the analysis identified a potential transcriptional regulator, TGIF1, which was not previously known to play a role in macrophage activation.
As a targeted experimental validation of the inferred network, two transcriptional regulators, p50 and IRF1, were assayed for binding to cis-regulatory elements in LPS-stimulated macrophages using ChIP-on-chip, and were confirmed to bind the promoters of genes in four out of five predicted target clusters at significantly higher proportions than expected for a random set of TLR-responsive genes.
Fluorescent proteins have been widely used as genetically encodable fusion tags for biological imaging.
Recently, a new class of fluorescent proteins was discovered that can be reversibly light-switched between a fluorescent and a non-fluorescent state.
Such proteins can not only provide nanoscale resolution in far-field fluorescence optical microscopy much below the diffraction limit, but also hold promise for other nanotechnological applications, such as optical data storage.
To systematically exploit the potential of such photoswitchable proteins and to enable rational improvements to their properties requires a detailed understanding of the molecular switching mechanism, which is currently unknown.
Here, we have studied the photoswitching mechanism of the reversibly switchable fluoroprotein asFP595 at the atomic level by multiconfigurational ab initio calculations and QM/MM excited state molecular dynamics simulations with explicit surface hopping.
Our simulations explain measured quantum yields and excited state lifetimes, and also predict the structures of the hitherto unknown intermediates and of the irreversibly fluorescent state.
Further, we find that the proton distribution in the active site of the asFP595 controls the photochemical conversion pathways of the chromophore in the protein matrix.
Accordingly, changes in the protonation state of the chromophore and some proximal amino acids lead to different photochemical states, which all turn out to be essential for the photoswitching mechanism.
These photochemical states are a neutral chromophore, which can trans-cis photoisomerize, an anionic chromophore, which rapidly undergoes radiationless decay after excitation, and a putative fluorescent zwitterionic chromophore.
The overall stability of the different protonation states is controlled by the isomeric state of the chromophore.
We finally propose that radiation-induced decarboxylation of the glutamic acid Glu215 blocks the proton transfer pathways that enable the deactivation of the zwitterionic chromophore and thus leads to irreversible fluorescence.
We have identified the tight coupling of trans-cis isomerization and proton transfers in photoswitchable proteins to be essential for their function and propose a detailed underlying mechanism, which provides a comprehensive picture that explains the available experimental data.
The structural similarity between asFP595 and other fluoroproteins of interest for imaging suggests that this coupling is a quite general mechanism for photoswitchable proteins.
These insights can guide the rational design and optimization of photoswitchable proteins.
Fluorescent proteins have been widely used as genetically encodable fusion tags to monitor protein localizations and dynamics in live cells CITATION CITATION.
Recently, a new class of green fluorescent protein -like proteins has been discovered, which can be reversibly photoswitched between a fluorescent and a non-fluorescent state CITATION CITATION.
As the reversible photoswitching of photochromic organic molecules such as fulgides or diarylethenes is usually not accompanied by fluorescence CITATION, this switching reversibility is a very remarkable and unique feature that may allow fundamentally new applications.
For example, the reversible photoswitching, also known as kindling, may provide nanoscale resolution in far field fluorescence optical microscopy much below the diffraction limit CITATION CITATION.
Likewise, reversibly switchable fluorescent proteins will enable the repeated tracking of protein location and movement in single cells CITATION.
Since fluorescence can be sensitively read out from a bulky crystal, the prospect of erasable three-dimensional data storage is equally intriguing CITATION .
The GFP-like protein asFP595, isolated from the sea anemone Anemonia sulcata, is a prototype for a reversibly switchable fluorescent protein.
The protein can be switched from its non-fluorescent off state to the fluorescent on state by green light of 568 nm wavelength CITATION, CITATION, CITATION, CITATION.
From this so-called kindled on state, the same green light elicits a red fluorescence emission at 595 nm.
Upon kindling, the intensity of the absorption maximum at 568 nm diminishes, and an absorption peak at 445 nm appears.
The kindled on state can be promptly switched back to the initial off state by this blue light of 445 nm.
Alternatively, the off state is repopulated through thermal relaxation within seconds.
In addition, if irradiated with intense green light over a long period of time, asFP595 can also be irreversibly converted into a fluorescent state that cannot be quenched by light any more CITATION.
The nature of this state is hitherto unknown.
The switching cycle of asFP595 is reversible and can be repeated many times without significant photobleaching.
These properties render asFP595 a promising fluorescence marker for high-resolution optical far-field microscopy, as recently demonstrated by Hofmann and coworkers CITATION.
Currently, however, with its low fluorescence quantum yield and rather slow switching kinetics, the photochromic properties of asFP595 need to be improved.
To systematically exploit the potential of such switchable proteins and to enable rational improvements to the properties of asFP595, a detailed molecular understanding of the photoswitching mechanism is mandatory.
The aim of this study is to obtain a detailed mechanistic picture of the photoswitching mechanism of asFP595 at the atomic level, i.e., to understand the dynamics of both the activation process and the de-activation process .
High-resolution crystal structures of the wild-type asFP595 in its off state CITATION, CITATION, CITATION, of the Ser158Val mutant in its on state CITATION, and of the Ala143Ser mutant in its on and off states CITATION were recently determined.
Similar to GFP, asFP595 adopts a -barrel fold enclosing the chromophore, a 2-acetyl-5-imidazolinone.
The chromophore is post-translationally formed in an autocatalytic cyclization-oxidation reaction of the Met63-Tyr64-Gly65 triad.
As compared to the GFP chromophore, the -system of MYG is elongated by an additional carbonyl group CITATION .
Reversible photoswitching of asFP595 was possible even within protein crystals, and x-ray analysis showed that the off-on switching of the fluorescence is accompanied by a conformational trans-cis isomerization of the chromophore CITATION.
In a recent study CITATION, we have shown that the isomerization induces changes of the protonation pattern of the chromophore and some of the surrounding amino acids, and that these changes account for the observed shifts in the absorption spectrum upon kindling.
Based on the comparison between measured and calculated absorption spectra, the major protonation states in the ground state have been assigned to the zwitterion and the anion for the trans conformer, whereas the neutral chromophore is dominant for the cis conformation .
Here, we study the photochemical behavior of each of the previously identified protonation states.
We have addressed the following questions: How does light absorption induce the isomerization of the chromophore within the protein matrix, and how do the different protonation states affect the internal conversion mechanism?
Which is the fluorescent species, and how can the fluorescence quantum yield be increased?
To address these questions, we have carried out nonadiabatic molecular dynamics simulations using a hybrid quantum-classical QM/MM approach.
This approach includes diabatic surface hopping between the excited state and the ground state.
The forces acting on the chromophore were calculated using the CASSCF CITATION, CITATION multi-reference method, which, although not always yielding highly accurate excitation and fluorescence energies, has shown to be a reliable method for mechanistic studies of photochemical reactions involving conical intersections CITATION .
A number of approaches for modeling nonadiabatic dynamics have been described in the literature, such as Tully's fewest switches surface hopping CITATION, and multiple spawning CITATION.
For recent reviews, see CITATION, CITATION.
In the context of QM/MM simulations, the surface hopping approach to photobiological problems has been pioneered by Warshel and coworkers CITATION, CITATION.
The diabatic surface hopping approach used in this work differs from the other approaches in two main respects.
First, in our approach a binary decision is made at each integration time step of the trajectory, based only on the current wavefunctions of the ground and excited states.
Second, hopping is only allowed at the conical intersection seam, where hopping probability approaches unity.
This could in principle underestimate the crossing probabilty, because we do not allow for transitions in regions of strong coupling but no real crossing.
However, for ultra-fast photochemical reactions in large polyatomic systems, decay predominantly takes place at the CI seam, as also shown by others CITATION.
Thus, most surface hops are essentially diabatic, justifying our approach.
In addition, both energy and momentum are conserved upon a transition, as the trajectory never leaves the diabatic energy surface.
The theoretical background and algorithmic implementation of the diabatic surface hopping are detailed in the Supporting Information .
Several theoretical studies on the photochemistry of the GFP chromophore have been conducted, applying both static ab initio CITATION CITATION and DFT calculations CITATION, and dynamics simulations based on a semi-empirical Hamiltonian CITATION.
In addition, vertical excitation energies of asFP595 model chromophores in the gas phase and in a continuum dielectric were calculated by DFT and ab initio methods CITATION, CITATION, as well as in a minimal protein environment by means of DFT and CASSCF calculations within a QM/MM approach CITATION .
By identifying key residues in the cavity of the asFP595 chromophore, our nonadiabatic QM/MM molecular dynamics simulations elucidate how the protein surrounding governs the photoreactivity of this photoswitchable protein.
Based on the simulations, we provide a new mechanism that qualitatively explains measured decay times and quantum yields, and that predicts the structures and protonation states of the photochemical intermediates and of the irreversibly fluorescent state.
We also suggest excited state proton transfer to play an important mechanistic role.
However, the detailed study of such ESPT processes is beyond the scope of this paper.
Our predictions can be probed by, e.g., time-resolved Fourier transform infrared spectroscopy and x-ray crystallography.
Even in the post-genomic era, the identification of candidate genes within loci associated with human genetic diseases is a very demanding task, because the critical region may typically contain hundreds of positional candidates.
Since genes implicated in similar phenotypes tend to share very similar expression profiles, high throughput gene expression data may represent a very important resource to identify the best candidates for sequencing.
However, so far, gene coexpression has not been used very successfully to prioritize positional candidates.
We show that it is possible to reliably identify disease-relevant relationships among genes from massive microarray datasets by concentrating only on genes sharing similar expression profiles in both human and mouse.
Moreover, we show systematically that the integration of human-mouse conserved coexpression with a phenotype similarity map allows the efficient identification of disease genes in large genomic regions.
Finally, using this approach on 850 OMIM loci characterized by an unknown molecular basis, we propose high-probability candidates for 81 genetic diseases.
Our results demonstrate that conserved coexpression, even at the human-mouse phylogenetic distance, represents a very strong criterion to predict disease-relevant relationships among human genes.
In the last two decades, positional cloning has been remarkably successful in the identification of genes involved in human disorders.
More recently, our ability to map genetic disease loci has strikingly increased due to the availability of the entire genome sequence.
Nevertheless, once a disease locus has been mapped, the identification of the mutation responsible for the phenotype still represents a very demanding task, because the mapped region may typically contain hundreds of candidates CITATION.
Accordingly, many phenotypes mapped on the genome by linkage analysis are not yet associated to any validated disease gene.
Therefore, the definition of strategies that can pinpoint the most likely targets to be sequenced in patients is of critical importance CITATION.
Many different strategies have been proposed to prioritize genes located in critical map intervals.
Some of the methods so far developed rely on the observation that disease genes tend to share common global properties, which can be deduced directly by absolute and comparative sequence analysis CITATION.
However, most of the available prioritization strategies are based on the widely accepted idea that genes and proteins of living organisms deploy their functions as part of sophisticated functional modules, based on a complex series of physical, metabolic and regulatory interactions CITATION, CITATION.
Although this principle has been extensively used even in the pre-genome era to identify the critical players of many different biological phenomena, the present availability of genome-scale information on gene function, protein-protein interactions and gene expression in different experimental models allows unprecedented opportunities for approaching the prioritization problem with greater efficiency.
In theory, the use of functional gene annotations would represent the most straightforward approach for candidate prioritization.
However, although this strategy may be very useful in selected cases CITATION, CITATION, at the present stage it has clear limitations, either because it overlooks non-annotated genes CITATION, CITATION or because it is not evident how the annotated functions of the candidates relate to the disease phenotype.
Therefore, computational methods less biased toward already consolidated knowledge, may have strong advantages CITATION .
In particular, protein-protein interaction maps and gene coexpression data from microarray experiments represent extremely rich sources of potentially relevant information.
Recently, the direct integration of a very heterogeneous human interactome with a text mining-based map of phenotype similarity has allowed the prediction of high confidence candidates within large disease-associated loci CITATION .
Although this approach is highly efficient, it is clearly not exhaustive because very close functional relationships between genes and proteins are possible in the absence of direct molecular binding.
In addition, the protein-protein interaction space is currently under-sampled and many genuine biological interactions have not yet been identified in experiments.
Conversely, high-throughput experiments are known to result in a large fraction of false positives.
The consistently low overlap of protein-protein interactions between large-scale experiments, even when the same proteins are considered, is testament to these problems CITATION.
Finally, many of the known protein-protein interactions have been ascertained through low-throughput experiments and are thus strongly biased towards better-studied proteins CITATION .
Since genes involved in the same functions tend to show very similar expression profiles, coexpression analysis could be a very powerful approach for inferring functional relationships, which may correlate with similar disease phenotypes.
Accordingly, global analyses have shown that genes highly coexpressed across microarray experiments display very similar functional annotation CITATION.
However, with notable exceptions CITATION, so far the coexpression criterion has not been employed very successfully for the prediction of genetic disease candidates and has been used to this purpose only in combination with other independent evidence CITATION, CITATION .
The noisy nature of high-throughput gene expression datasets may represent one of the possible explanations for this shortcoming.
Moreover, even when the coexpression of two genes is reproducibly observed under a high number of experimental conditions, this does not necessarily imply that the genes are functionally related.
For instance, extensive meta analysis of microarray data across different species has revealed that neighboring genes are more likely to be coexpressed than genes encoded in distant genomic regions, even if they are not functionally related in any obvious manner CITATION, CITATION .
Phylogenetic conservation has been previously proposed as a very strong criterion to identify functionally relevant coexpression links among genes CITATION, CITATION.
Indeed, significant coexpression of two or more orthologous genes is very likely due to selective advantage, strongly suggesting a functional relation.
Therefore, conserved coexpression could be a much stronger criterion than single species coexpression to relate genes involved in similar disease phenotypes.
In this report we show that conserved coexpression and phenome analysis can be effectively integrated to produce accurate predictions of human disease genes.
Using this approach we were able to select a small number of strong candidates for 81 human diseases, corresponding to a wide spectrum of different phenotypes.
As modeling of changes in backbone conformation still lacks a computationally efficient solution, we developed a discretisation of the conformational states accessible to the protein backbone similar to the successful rotamer approach in side chains.
The BriX fragment database, consisting of fragments from 4 to 14 residues long, was realized through identification of recurrent backbone fragments from a non-redundant set of high-resolution protein structures.
BriX contains an alphabet of more than 1,000 frequently observed conformations per peptide length for 6 different variation levels.
Analysis of the performance of BriX revealed an average structural coverage of protein structures of more than 99 percent within a root mean square distance of 1 Angstrom.
Globally, we are able to reconstruct protein structures with an average accuracy of 0.48 Angstrom RMSD.
As expected, regular structures are well covered, but, interestingly, many loop regions that appear irregular at first glance are also found to form a recurrent structural motif, albeit with lower frequency of occurrence than regular secondary structures.
Larger loop regions could be completely reconstructed from smaller recurrent elements, between 4 and 8 residues long.
Finally, we observed that a significant amount of short sequences tend to display strong structural ambiguity between alpha helix and extended conformations.
When the sequence length increases, this so-called sequence plasticity is no longer observed, illustrating the context dependency of polypeptide structures.
High-resolution structure determination of proteins and protein complexes via experimental methods occurs at a significantly slower pace than the collection of novel protein sequences.
As a result, less than 30 percent of human proteins have a known structure in the Protein Data Bank and the percentage for other species is significantly lower CITATION.
In addition, structures mostly cover one or a small number of protein domains, thus covering only a fraction of the total sequence of the protein.
Homology modeling improves this coverage using related proteins with known structures to build a model CITATION CITATION.
The construction of an adequate homolog can be divided into two related tasks: the placement of the amino acid side chains on a given backbone template and the detection of changes in backbone conformations that are required to accommodate the new sequence.
For proteins that are relatively close in terms of sequence identity, the backbone-modeling problem is usually ignored, but in many cases the best homology template shows less than 50 percent homology with the target, and small compensatory changes to the backbone are likely to be required to obtain an accurate model.
Recent advances in protein backbone modeling are based on the observation that protein structures are built from a finite repertoire of structural folds CITATION.
Structural redundancy allowed the classification of protein folds such as in the SCOP database CITATION, the CATH database CITATION or the FSSP classification CITATION, CITATION.
The unit of fold classification is usually a protein domain, since large proteins are generally composed of multiple domains.
As a consequence, the classification comprises a hierarchical organisation of protein domains that embodies evolutionary and structural relationships.
By creating more categories and thus refining the secondary structure descriptions, it has been proposed that a set of discrete backbone conformational states can be derived CITATION, CITATION.
Different research groups demonstrated the usefulness of such fragment libraries when reconstructing protein structures by generating sets of protein decoys CITATION CITATION.
In the latest editions of CASP, prediction approaches that assemble fragments of known structures into a candidate structure have proven to be successful CITATION CITATION.
In fragment assembly methods, the assumption is made that local interactions create a particular conformational bias, but do not uniquely define local structure CITATION CITATION.
Instead, environmental constraints will determine the overall compact protein conformation.
The construction of a final model is composed of three steps: The first step involves a selection of fragment candidates based on their stability that can be measured by a simplified scoring function CITATION.
In the second step the fragments are assembled combinatorially CITATION, CITATION.
In the final step the obtained structure is optimized through the employment of a force field CITATION, CITATION.
This method works well for small all class proteins, and reasonably well for /, and all class proteins.
The fragment approach has been successfully applied in the structure prediction algorithm Rosetta of Baker and co-workers CITATION CITATION, which also proved to be successful in accurately designing new folds CITATION.
Publicly accessible libraries however are limited; they are typically small and consider lengths between 4 and 7 residues.
For instance, by examining fragments of 5 residues, Kolodny and Levitt CITATION created a library of 20 fragments, while Etchebest found only 16 building blocks of this length CITATION.
The alphabet of Camproux CITATION consists of 27 structural classes and is based on motives of 4 residues.
By employing their hypercosine method on a set of 150,000 length-7 protein fragments, Hunter and Subramaniam CITATION discovered 13 minimal centroids or representative fragment shapes found in proteins at a resolution of 0.80 Angstrom.
As such low resolution approaches, restricted to a single fragment length and thus resulting in a limited set of building blocks, might constitute an advantage in terms of computational efficiency for ab initio structure prediction methods, it will also lead to a significant loss of information.
Wainreb et al made it possible to cluster variable sized fragments, consisting of at least 15 residues, through the implementation of their SSGS algorithm CITATION.
By allowing more variability in the alignment of loop locations, they created a library of 8,933 building blocks.
An alternative approach, as implemented by DePristo et al CITATION, uses an ensemble of artificially generated small polypeptide conformations instead of sampling conformations from known protein structures.
By constraining the chemical properties such as the idealized geometry, phi/psi angles and excluded volume they constructed ensembles of near-native conformations consistent with a surrounding fixed protein structure.
Our strategy focuses on obtaining a comprehensive set of high-resolution structural fragments without using artificial data or restricting fragment lengths.
We decided to partition a non-redundant set of high-resolution protein structures into fragments that consist of 4 to 14 residues, because preliminary tests indicated the lack of high structural similarity for more than 50 percent of all fragments when larger lengths were considered.
Subsequently, clustering techniques were employed to identify structural motifs that are recurrent in different protein structures.
Over 1,000 recurrent fragment structures or classes were found for each considered peptide length when a structural variation proportional to the length of the fragment was allowed.
As suggested in CITATION, CITATION, it is important to determine how well the classes of the fragment library cover fold space in order to estimate its value.
When applied to protein structures not used in the construction of the database, this coverage turned out to be 99 percent on average using a 1 Angstrom RMSD threshold.
The latter implies that in the majority of the cases studied the so-called irregular regions or loops can also be reconstructed from recurrent building blocks.
Through the employment of a global fit reconstruction algorithm, backbone traces were generated having an average accuracy of 0.48 Angstrom RMSD.
Additionally, the ability to use BriX for local secondary structure prediction was examined by looking at the sequence-structure relationship within classes.
According to previous findings CITATION, the sequence conservation within classes was rather low because of the large number of determined building blocks originating from different families.
Nonetheless, this analysis led to a quantitative illustration of the context-dependence of polypeptide structure.
A significant amount of small sequences tend to display strong structural ambiguity: for fragments of length 5, 14 percent of the fragment pairs with identical sequences have structural difference within the range of a helix-to-sheet jump.
These so-called plastic sequences, i.e. sequences that display diverse structural conformations, display a strong preference for the aliphatic residues Alanine, Valine, and Leucine.
For fragments of more than 5 residues sequence plasticity is no longer observed, showing that the need for additional context to determine secondary structure is much reduced for longer fragments.
The diversity of virus populations within single infected hosts presents a major difficulty for the natural immune response as well as for vaccine design and antiviral drug therapy.
Recently developed pyrophosphate-based sequencing technologies can be used for quantifying this diversity by ultra-deep sequencing of virus samples.
We present computational methods for the analysis of such sequence data and apply these techniques to pyrosequencing data obtained from HIV populations within patients harboring drug-resistant virus strains.
Our main result is the estimation of the population structure of the sample from the pyrosequencing reads.
This inference is based on a statistical approach to error correction, followed by a combinatorial algorithm for constructing a minimal set of haplotypes that explain the data.
Using this set of explaining haplotypes, we apply a statistical model to infer the frequencies of the haplotypes in the population via an expectation maximization algorithm.
We demonstrate that pyrosequencing reads allow for effective population reconstruction by extensive simulations and by comparison to 165 sequences obtained directly from clonal sequencing of four independent, diverse HIV populations.
Thus, pyrosequencing can be used for cost-effective estimation of the structure of virus populations, promising new insights into viral evolutionary dynamics and disease control strategies.
Pyrosequencing is a novel experimental technique for determining the sequence of DNA bases in a genome CITATION, CITATION.
The method is faster, less laborious, and cheaper than existing technologies, but pyrosequencing reads are also significantly shorter and more error-prone than those obtained from Sanger sequencing CITATION CITATION .
In this paper we address computational issues that arise in applying this technology to the sequencing of an RNA virus sample.
Within-host RNA virus populations consist of different haplotypes that are evolutionarily related.
The population can exhibit a high degree of genetic diversity and is often referred to as a quasispecies, a concept that originally described a mutation-selection balance CITATION, CITATION.
Viral genetic diversity is a key factor in disease progression CITATION, CITATION, vaccine design CITATION, CITATION, and antiretroviral drug therapy CITATION, CITATION.
Ultra-deep sequencing of mixed virus samples is a promising approach to quantifying this diversity and to resolving the viral population structure CITATION CITATION .
Pyrosequencing of a virus population produces many reads, each of which originates from exactly one but unknown haplotype in the population.
Thus, the central problem is to reconstruct from the read data the set of possible haplotypes that is consistent with the observed reads and to infer the structure of the population, i.e., the relative frequency of each haplotype.
Here we present a computational four-step procedure for making inference about the virus population based on a set of pyrosequencing reads.
First, the reads are aligned to a reference genome.
Second, sequencing errors are corrected locally in windows along the multiple alignment using clustering techniques.
Next, we assemble haplotypes that are consistent with the observed reads.
We formulate this problem as a search for a set of covering paths in a directed acyclic graph and show how the search problem can be solved very efficiently.
Finally, we introduce a statistical model that mimics the sequencing process and we employ the maximum likelihood principle for estimating the frequency of each haplotype in the population.
The alignment step of the proposed procedure is straightforward for the data analyzed here and has been discussed elsewhere CITATION.
Due to the presence of a reference genome, only pair-wise alignment is necessary between each read and the reference genome.
We will therefore focus on the core methods of error correction, haplotype reconstruction, and haplotype frequency estimation.
Two independent approaches are pursued for validating the proposed method.
First, we present extensive simulation results of all the steps in the method.
Second, we validate the procedure by reconstructing four independent HIV populations from pyrosequencing reads and comparing these populations to the results of clonal Sanger sequencing from the same samples.
These datasets consist of approximately 5000 to 8000 reads of average length 105 bp sequenced from a 1 kb region of the pol gene from clinical samples of HIV-1 populations.
Pyrosequencing can produce up to 200,000 usable reads in a single run.
Part of our contribution is an analysis of the interaction between the number of reads, the sequencing error rate and the theoretical resolution of haplotype reconstruction.
The methods developed in this paper scale to these huge datasets under reasonable assumptions.
However, we concentrate mainly on a sample size that produces finer resolution than what is typically obtained using limiting dilution clonal sequencing.
Since many samples can be run simultaneously and independently, this raises the possibility of obtaining data from about 20 populations with one pyrosequencing run.
Estimating the viral population structure from a set of reads is, in general, an extremely hard computational problem because of the huge number of possible haplotypes.
The decoupling of error correction, haplotype reconstruction, and haplotype frequency estimation breaks this problem into three smaller and more manageable tasks, each of which is also of interest in its own right.
The presented methods are not restricted to RNA virus populations, but apply whenever a reference genome is available for aligning the reads, the read coverage is sufficient, and the genetic distance between haplotypes is large enough.
Clonal data indicates that the typical variation in the HIV pol gene is about 3 to 5 percent in a single patient CITATION.
We find that as populations grow more diverse, they become easier to reconstruct.
Even at 3 percent diversity, we find that much of the population can be reconstructed using our methods.
The pol gene has been sequenced extensively and only one specific insertion seems to occur, namely the 69 insertion complex, which occurs under NRTI pressure CITATION.
None of our samples were treated with NRTIs, and the Sanger clones did not display this indel.
Therefore we assume throughout that there are no true indels in the population.
However, the algorithms developed in this paper generalize in a straightforward manner for the case of true indels.
The problem of estimating the population structure from sequence reads is similar to assembly of a highly repetitive genome CITATION.
However, rather than reconstructing one genome, we seek to reconstruct a population of very similar genomes.
As such, the problem is also related to environmental sequencing projects, which try to assess the genomes of all species in a community CITATION.
While the associated computational biology problems are related to those that appear in other metagenomics projects CITATION, novel approaches are required to deal with the short and error-prone pyrosequencing reads and the complex structure of viral populations.
The problem is also similar to the haplotype reconstruction problem CITATION, with the main difference being that the number of haplotypes is unknown in advance, and to estimating the diversity of alternative splicing CITATION .
More generally, the problem of estimating diversity in a population from genome sequence samples has been studied extensively for microbial populations.
For example, the spectrum of contig lengths has been used to estimate diversity from shotgun sequencing data CITATION.
Using pyrosequencing reads, microbial diversity has been assessed by counting BLAST hits in sequence databases CITATION.
Our methods differ from previous work in that we show how to analyze highly directed, ultra-deep sequencing data using a rigorous mathematical and statistical framework.
Neural networks consisting of globally coupled excitatory and inhibitory nonidentical neurons may exhibit a complex dynamic behavior including synchronization, multiclustered solutions in phase space, and oscillator death.
We investigate the conditions under which these behaviors occur in a multidimensional parametric space defined by the connectivity strengths and dispersion of the neuronal membrane excitability.
Using mode decomposition techniques, we further derive analytically a low dimensional description of the neural population dynamics and show that the various dynamic behaviors of the entire network can be well reproduced by this reduced system.
Examples of networks of FitzHugh-Nagumo and Hindmarsh-Rose neurons are discussed in detail.
Information processing associated with higher brain functions is believed to be carried out by large scale neural networks CITATION CITATION.
Significant theoretical and computational efforts have been devoted over the years to understand the dynamical behavior of such networks.
While any modeling attempt aspires to preserve the most relevant physical and dynamical characteristics of these networks, certain simplifying hypothesis are usually employed in order to decrease the overwhelming complexity of the problem.
In particular, computational models of large scale networks make use of the implicit assumption of neurocomputational unit.
Such a unit designates a population of thousands of neurons which exhibit a similar behavior.
A large scale network is then defined by these units and their interconnections.
In order to describe the dynamics of the unit, further assumptions are employed.
For instance, the neurons may be regarded as identical entities, the nature and strength of their connections may be neglected and the temporal details of their spiking activity considered irrelevant for the dynamics of the large network.
Consequently, a small neural network with these properties will show a very well synchronized dynamics which can be easily captured by a conventional neural mass model .
A remarkable amount of scientific work has been devoted to the understanding of the behavior of neural networks when some of these assumptions are dismissed.
Many of these studies consider either the inhomogeneities in the network connectivity, or heterogeneous inputs and give a special attention to the synchronized state of the network.
Among the first attempts, one may consider the studies on coupled oscillators by Kuramoto CITATION who introduced an order parameter capturing the degree of synchronization as a function of the coupling strength or frequency distribution.
More generally, Pecora et al. CITATION have derived the master stability equation, serving as a stability condition for the synchronized state of an arbitrary network.
Recently, Hennig et al. CITATION derive similar conditions considering the connectivity as well as heterogeneous inputs.
Another direction for describing the dynamical behavior of such networks involves the derivation of the equations for the synchronized state along with the equations describing the deviations from synchrony CITATION, CITATION.
These approaches are suitable only when the deviation from the synchronized state is not very strong.
On the other hand, there exists another class of approaches based on mean field theory.
The traditional mean field approaches are incapable of addressing synchronized neural activity, since their basic assumption is that the incoming spike-train to a given neuron in the network is Poissonian and hence uncorrelated.
Other dynamical behaviors far from synchrony, such as multi-clustering in the phase for instance, also require expansions of the current approaches.
First attempts to do so include the consideration of higher orders in the mean field expansion CITATION or mode decompositions of the network dynamics in the phase space CITATION.
The latter approach by Assisi et al. CITATION successfully identified network modes of characteristic behavior, but has been limited to biologically unrealistic situations such as purely excitatory or inhibitory networks and simplistic neuron models.
While it is true that strong reductionist assumptions are common in large-scale network modeling CITATION CITATION, these assumptions on the network node's dynamics are usually made adhoc and limit the network dynamics to a small range.
Evidently a reduced small scale network model is desirable to serve as a node in a large scale network simulation whereby displaying a sufficiently rich dynamic repertoire.
Here it is of less importance to find a quantitatively precise reduced description of a neural population; rather more importantly, we seek a computationally inexpensive population model which is able to display the major qualitative dynamic behaviors for realistic parameter ranges as observed in the total population of neurons.
Here it is also desirable to include biologically more realistic neuron dynamics such as bursting behavior, since novel phenomena on the small scale network level may occur, which need to be captured by the reduced population model.
In this paper we extend the approach by Assisi et al. CITATION towards biologically more realistic network architectures including mixed excitatory and inhibitory networks, as well as more realistic neuron models capable of displaying spiking and bursting behavior.
Our reduced neural population models not only account for a correct reproduction of the mean field amplitude of the original networks but also capture the most important temporal features of its dynamics.
In this way, complex dynamical phenomena such as multi-clustered oscillations, multi-time scale synchronization and oscillation death become available for simulations of large scale neural networks at a low computational cost.
We start by investigating first, the main features of the dynamic behavior of a globally coupled heterogeneous neural population comprising both excitatory and inhibitory connections.
Then, using mode decomposition techniques, we derive analytically a low dimensional representation of the network dynamics and we show that the main features of the neural population's collective behavior can be captured well by the dynamics of a few modes.
Two different neuronal models, a network of FitzHugh-Nagumo neurons and a network of Hindmarsh-Rose neurons are discussed in detail.
Advances in time-lapse fluorescence microscopy have enabled us to directly observe dynamic cellular phenomena.
Although the techniques themselves have promoted the understanding of dynamic cellular functions, the vast number of images acquired has generated a need for automated processing tools to extract statistical information.
A problem underlying the analysis of time-lapse cell images is the lack of rigorous methods to extract morphodynamic properties.
Here, we propose an algorithm called edge evolution tracking to quantify the relationship between local morphological changes and local fluorescence intensities around a cell edge using time-lapse microscopy images.
This algorithm enables us to trace the local edge extension and contraction by defining subdivided edges and their corresponding positions in successive frames.
Thus, this algorithm enables the investigation of cross-correlations between local morphological changes and local intensity of fluorescent signals by considering the time shifts.
By applying EET to fluorescence resonance energy transfer images of the Rho-family GTPases Rac1, Cdc42, and RhoA, we examined the cross-correlation between the local area difference and GTPase activity.
The calculated correlations changed with time-shifts as expected, but surprisingly, the peak of the correlation coefficients appeared with a 6 8 min time shift of morphological changes and preceded the Rac1 or Cdc42 activities.
Our method enables the quantification of the dynamics of local morphological change and local protein activity and statistical investigation of the relationship between them by considering time shifts in the relationship.
Thus, this algorithm extends the value of time-lapse imaging data to better understand dynamics of cellular function.
Cell morphological change is a key process in the development and homeostasis of multicellular organisms CITATION, CITATION.
Various types of morphological change appear during migration and differentiation; essential events occurring as part of these processes usually accompany morphologically different phenotypes.
Therefore, cell morphology has been used as a key indicator of cell state CITATION.
High-throughput analyses of cell morphodynamic properties have been used recently to discover new functions of specific proteins CITATION.
Moreover, the outcomes of morphological change such as the intricate shape of neuronal dendrites, remind us that morphogenesis itself plays a role in the emergence of cellular function CITATION .
Quantitative approaches are helping to unveil cellular morphodynamic systems, and they are generating new technical requirements.
Because cellular morphological change is highly dynamic, time-lapse imaging is necessary to understand the mechanism of cell morphology regulation.
Progress in the development of fluorescent probes has enabled the direct observation of cell morphological changes and/or the localization and activity of specific proteins CITATION CITATION, but time-lapse imaging has highlighted the difficulty of extracting characteristic information from an immense number of images.
Nevertheless, several approaches in the context of quantitative analysis have appeared recently.
A series of studies using quantitative fluorescent speckle microscopy, for instance, revealed the power of computer-assisted high-throughput analysis for time-lapse microscopy images: analysis of the number of moving and blinking speckles suggested distinct regulation of actin reorganization dynamics in different intracellular regions CITATION, CITATION .
Indeed, computational methods have been used to determine the properties of morphological dynamics, protein activity and gene expression CITATION CITATION.
There are two major approaches for the detailed analysis of local morphological changes of cells.
One is the kymograph, which is a widely used method to describe motion with a time-position map of the morphology time course.
The time course of change in intensity could also be monitored by arranging sequential images of a specific region of interest CITATION.
Although there are drawbacks to this approach, such as restriction of the analyzed area to a narrow ROI and the need to manually define the ROI, recent studies have avoided these limitations by using polar coordinates to explore the motility dynamics of the entire peripheral region of round cells.
Indeed, the polar coordinate-based approach showed isotropic and anisotropic cell expansion, and examined stochastic, transient extension periods or periodic contractions CITATION, CITATION.
The second approach is to track cellular edge boundaries by tracing virtually defined markers.
Kass and Terzopoulos introduced an active contour model known as SNAKES CITATION, which is widely used to analyze moving video images in applications including biomedicine.
For example, Dormann et al. used SNAKES to quantify cell motility and analyze the specific translocation of PH domain-containing proteins into the leading edge CITATION.
Marker-based tracking has advantages in quantifying highly motile cell morphology, because it does not require a fixed axis, which is necessary in the kymograph approach.
Recently, Machacek and Danuser developed an elegant framework to trace a moving edge, using marker tracking modified by the level set method to elucidate morphodynamic modes of various motile cells such as fibroblasts, epithelial cells, and keratocytes CITATION .
Although previous methodologies have successfully described the specific aspects of cellular morphodynamics, there remain challenges to quantify the relationship between morphodynamics and signaling events.
One representative problem is the association between regions in different frames.
To scrutinize the dynamic relationship between morphological change and molecular signaling, we need to cross-correlate them in a time-dependent manner.
A polar coordinate system does not ensure the association of time-shifted local domains, and is unsuitable for non-circular cell shapes.
The virtual marker tracking method satisfies this requirement for cells with broadly consistent shapes, but its fixed number of markers causes unequal distribution when a dramatic shape change such as the persistent growth of neurites in neurons, occurs.
Taking these problems into account, we perceive the need for a novel quantification method to better understand the mechanisms of morphodynamic regulation by molecular signaling.
We focused on the Rho-family small GTPases, or Rho GTPases, as signaling molecules associated with cell morphodynamics.
Rho GTPases, which act as binary switches by cycling between inactive and active states, play key roles in linking biochemical signaling with biophysical cellular behaviors CITATION, CITATION mainly through reorganization of the actin and microtubule cytoskeleton CITATION.
It is well known that RhoA, Rac1, and Cdc42 have unique abilities to induce specific filamentous actin structures, i.e., stress fibers, lamellipodia, and filopodia, respectively CITATION.
Considerable evidence, mainly obtained using constitutively-active or dominant-negative mutants, supports a promotional role of Rac1 and Cdc42 and an inhibitory role of RhoA in cell protrusion CITATION, CITATION.
Although some researchers have challenged this widely-accepted notion in a variety of cell contexts CITATION CITATION, our current study has been motivated by this predominant view.
The objective of this study was to uncover the relationship between spatio-temporal activities of Rho GTPases and morphological changes of the cells.
To achieve this, we needed a data analysis tool to assess the link between biochemical signaling and biophysical phenomena.
However, we do not focus on unveiling the orchestration of the complete signaling pathways that regulate cell morphology.
In addition, we elucidated how Rho GTPases regulate two-dimensional morphological changes of cells, rather than three-dimensional changes.
These findings will however be meaningful because the results can be compared with earlier findings CITATION CITATION.
Therefore, we first present an algorithm called edge evolution tracking to quantify local morphological change.
The main features of our method are that identification of a local morphological change is based on an area difference between two consecutive frames; cell edge is not characterized by point markers, but by line segments, which are defined by the area difference; and past history and future evolution of each segment can be evaluated by connecting segments between consecutive frames.
Therefore, this method enables us to trace complex cell edge extension and contraction while maintaining the consistency of the ROI during the analysis.
Second, applying EET to fluorescence resonance energy transfer time-lapse images of three Rho GTPases, we found a significant time-shifted cross-correlation between morphological change and GTPase activity.
Our study reveals the utility of detailed cellular morphodynamic profiling and spatio-temporal signal profiling to measure the time-shifted relationship between morphodynamics and protein activity.
There is evidence that biological synapses have a limited number of discrete weight states.
Memory storage with such synapses behaves quite differently from synapses with unbounded, continuous weights, as old memories are automatically overwritten by new memories.
Consequently, there has been substantial discussion about how this affects learning and storage capacity.
In this paper, we calculate the storage capacity of discrete, bounded synapses in terms of Shannon information.
We use this to optimize the learning rules and investigate how the maximum information capacity depends on the number of synapses, the number of synaptic states, and the coding sparseness.
Below a certain critical number of synapses per neuron, we find that storage is similar to unbounded, continuous synapses.
Hence, discrete synapses do not necessarily have lower storage capacity.
Memory in biological neural systems is believed to be stored in the synaptic weights.
Numerous computational models of such memory systems have been constructed in order to study their properties and to explore potential hardware implementations.
Storage capacity and optimal learning rules have been studied both for single-layer associative networks CITATION, CITATION, studied here, and for auto-associative networks CITATION, CITATION.
Commonly, synaptic weights in such models are represented by unbounded, continuous real numbers.
However, in biology, as well as in potential hardware, synaptic weights should take values between certain bounds.
Furthermore, synapses might be restricted to have a limited number of synaptic states, e.g. the synapse might be binary.
Although binary synapses might have limited storage capacity, they can be made more robust to biochemical noise than continuous synapses CITATION.
Consistent with this, experiments suggest that synaptic weight changes occur in steps.
For example, putative single synapse experiments show that a switch-like increment or reduction to the excitatory post-synaptic current can be induced by pairing brief pre-synaptic stimulation with appropriate post-synaptic depolarization CITATION, CITATION .
Networks with bounded synapses have the palimpsest property, i.e. old memories decay automatically as they are overwritten by new ones CITATION CITATION.
In contrast, in networks with continuous, unbounded synapses, storing additional memories reduces the quality of recent and old memories equally.
Forgetting of old memories must in that case be explicitly incorporated, for instance via a weight decay mechanism CITATION, CITATION.
The automatic forgetting of discrete, bounded synapses allows one to study learning in a realistic equilibrium context, in which there can be continual storage of new information.
It is common to use the signal-to-noise ratio to quantify memory storage in neural networks CITATION, CITATION.
The SNR measures the separation between responses of the network; the higher the SNR, the more the memory stands out and the less likely it will be lost or distorted.
When weights are unbounded, each stored pattern has the same SNR.
Storage capacity can then be defined as the maximum number of patterns for which the SNR is larger than some fixed, minimum value.
However, for discrete, bounded synapses performance must be characterized by two quantities: the initial SNR, and its decay rate.
Ideally, a memory has a high SNR and a slow decay, but altering learning rules typically results in either an increase in memory lifetime but a decrease in initial SNR CITATION, or an increase in initial SNR but a decrease in memory lifetime.
Optimization of the learning rule is ambivalent because an arbitrary trade-off must be made between these two effects.
In this paper we resolve this conflict between learning and forgetting by analyzing the capacity of synapses in terms of Shannon information.
We describe a framework for calculating the information capacity of bounded, discrete synapses, and use this to find optimal learning rules.
We model a single neuron, and investigate how information capacity depends on the number of synapses and the number of synaptic states.
We find that below a critical number of synapses, the total capacity is linear in the number of synapses, while for more synapses the capacity grows only as the square root of the number of synapses per neuron.
This critical number is dependent on the sparseness of the patterns stored, as well as on the number of synaptic states.
Furthermore, when increasing the number of synaptic states, the information initially grows linearly with the number of states, but saturates for many states.
Interestingly, for biologically realistic parameters, capacity is just at this critical point, suggesting that the number of synapses per neuron is limited to prevent sub-optimal learning.
Finally, the capacity measure allows direct comparison of discrete with continuous synapses, showing that under the right conditions their capacities are comparable.
One of the striking features of evolution is the appearance of novel structures in organisms.
Recently, Kirschner and Gerhart have integrated discoveries in evolution, genetics, and developmental biology to form a theory of facilitated variation.
The key observation is that organisms are designed such that random genetic changes are channeled in phenotypic directions that are potentially useful.
An open question is how FV spontaneously emerges during evolution.
Here, we address this by means of computer simulations of two well-studied model systems, logic circuits and RNA secondary structure.
We find that evolution of FV is enhanced in environments that change from time to time in a systematic way: the varying environments are made of the same set of subgoals but in different combinations.
We find that organisms that evolve under such varying goals not only remember their history but also generalize to future environments, exhibiting high adaptability to novel goals.
Rapid adaptation is seen to goals composed of the same subgoals in novel combinations, and to goals where one of the subgoals was never seen in the history of the organism.
The mechanisms for such enhanced generation of novelty are analyzed, as is the way that organisms store information in their genomes about their past environments.
Elements of facilitated variation theory, such as weak regulatory linkage, modularity, and reduced pleiotropy of mutations, evolve spontaneously under these conditions.
Thus, environments that change in a systematic, modular fashion seem to promote facilitated variation and allow evolution to generalize to novel conditions.
The origin of the ability to generate novelty is one of the main mysteries in evolution.
Pioneers of evolutionary theory, including Baldwin CITATION, Simpson CITATION, and Waddington CITATION, CITATION, suggested how useful novelty might be enhanced by physiological adaptations and by the robustness of the developmental process.
These early theories were limited by a lack of knowledge of the molecular mechanisms of development.
Recent decades saw breakthroughs in the depth of understanding of molecular and developmental biology.
Many of these findings were unified in the theory of facilitated variation CITATION, presented by Kirschner and Gerhart, that addresses the following question: how can small, random genetic changes be converted into complex useful innovations?
In order to understand novelty in evolution, Kirschner and Gerhart integrated observations on molecular mechanisms to show how the current design of an organism helps to determine the nature and the degree of future variation.
The key observation is that the organism, by its intrinsic construction, biases both the type and the amount of its phenotypic variation in response to random genetic mutation CITATION, CITATION, CITATION CITATION.
In other words, the organism seems to be built in such a way that small genetic mutations have a high chance of yielding a large phenotypic payoff.
To understand FV, it is important to compare it to the related concept of evolvability.
A biological system is evolvable if it can readily acquire novel functions through genetic changes that help the organism survive and reproduce in future environments CITATION.
Evolvability is composed of two aspects: variability: the capacity to generate new phenotypes fitness: the fitness of the new phenotypes in future environments.
Most studies of evolvability focused on the first aspect, variability.
Such studies measured the range and diversity of the phenotypic variation that can be generated by a given mutation, usually without discerning between potentially useful phenotypes and non-useful ones CITATION CITATION.
FV theory adds to previous considerations by focusing on the nature of the generated variation, and specifically on the organism's ability to generate novel phenotypes which are potentially useful.
Facilitated variation is made possible by certain features of biological design.
One of these is the existence of weak regulatory linkage CITATION, CITATION, CITATION, where general and non-instructive signals can trigger large pre-prepared responses.
For example, changes in growth hormone concentration at a localized position can trigger large useful changes in the shape of the limb, driven by the conserved mechanisms for growth of bones, muscles, blood vessels, and nerves CITATION.
A good example is the ease of changing beak shapes with any of many possible mutations that affect the concentration of a single morphogenic factor CITATION.
In weak regulatory linkage, the information about the output is pre-built into the regulated system without instruction from the regulator, which only selects between states.
Such regulatory organization reduces the constraints for evolving new regulations and for generating complex potentially useful phenotypes.
An additional feature that is important for FV is modular design CITATION CITATION, seen for example, in the highly conserved body-plan of the embryo CITATION, CITATION and in the compartmental organization of gene regulation and signaling networks CITATION.
Modularity helps to relieve the concern that a mutation might interfere with many different parts of the organism.
With properly designed modularity, variation within each module can be generated without harming other modules CITATION CITATION .
Facilitated variation can be in principle studied experimentally, for example by generating mutants and scanning the types of phenotypes generated.
For example, a study on mutants of the lac regulatory region indicated that the shape of the gene input function is channeled in directions of AND-like and OR-like functions, rather than other possibilities CITATION .
An open question is how does FV spontaneously evolve?
It is not clear how selection in a present environment can lead to designs that increase the probability of useful changes in future environments.
How does evolutionary theory account for the emergence of special designs that make it easy to generate novel and useful variation?
The key point in our study is the observation that environments in nature do not vary randomly, but rather seem to have common rules or regularities CITATION CITATION.
Specifically, environmental goals faced by organisms or molecules may be thought of as composed of a combination of subgoals CITATION.
When environments change, the organisms encounter a new goal that is still made of the same or similar subgoals.
For example, on the level of the organism, the same subgoals, such as digesting food, avoiding predation, and reproducing, must be fulfilled in each new environment but with different nuances and combinations.
On the level of cells, the same subgoals such as adhesion and signaling must be fulfilled in each tissue type but with different input and output signals.
On the level of proteins, the same subgoals, such as enzymatic activity, binding to other proteins, regulatory input domains, etc., are shared by many proteins but with different combinations in each case.
One may thus propose that in many cases, the different possible environments share a language of modularity, in the sense that they are all made of certain combinations of a set of subgoals.
We thus test the possibility that under such patterned varying environments, the organism can learn over many generations the language common to the environments encountered in its past.
We ask whether FV arises in such systematically varying environments, by measuring the ability of simple model systems to adapt to new, previously unseen goals, which are in the same language as past goals.
We employ two well-studied model systems: combinatorial logic circuits CITATION, CITATION and RNA secondary structure CITATION.
We find that the standard experiment of setting a goal which remains constant over time leads to highly optimized systems that show little FV.
In contrast, FV is readily generated under modularly varying goals, in which goals change over time but share the same subgoals CITATION.
We find that MVG evolution enhances the ability to generate novel phenotypes as long as novelty is modular: phenotypes with novel modules or novel combinations of modules.
We show that organisms under MVG store information about past goals in their genomes, and evolve weak linkage that allows small genetic changes to unleash large phenotypic responses that do not ruin the modular structure of the organism.
Our study thus suggests that environments that change in a systematic fashion promote the evolution of facilitated variation, and leave an imprint on the evolvability properties of the organisms, allowing them to generalize to new conditions that are in the same language as past conditions.
Hidden Markov models have been successfully applied to the tasks of transmembrane protein topology prediction and signal peptide prediction.
In this paper we expand upon this work by making use of the more powerful class of dynamic Bayesian networks.
Our model, Philius, is inspired by a previously published HMM, Phobius, and combines a signal peptide submodel with a transmembrane submodel.
We introduce a two-stage DBN decoder that combines the power of posterior decoding with the grammar constraints of Viterbi-style decoding.
Philius also provides protein type, segment, and topology confidence metrics to aid in the interpretation of the predictions.
We report a relative improvement of 13 percent over Phobius in full-topology prediction accuracy on transmembrane proteins, and a sensitivity and specificity of 0.96 in detecting signal peptides.
We also show that our confidence metrics correlate well with the observed precision.
In addition, we have made predictions on all 6.3 million proteins in the Yeast Resource Center database.
This large-scale study provides an overall picture of the relative numbers of proteins that include a signal-peptide and/or one or more transmembrane segments as well as a valuable resource for the scientific community.
All DBNs are implemented using the Graphical Models Toolkit.
Source code for the models described here is available at LINK.
A Philius Web server is available at LINK, and the predictions on the YRC database are available at LINK.
The structure of a protein determines its function.
Knowledge of the structure can therefore be used to guide the design of drugs, to improve the interpretation of other information such as the locations of mutations, and to identify remote protein homologs.
Indirect methods such as X-ray crystallography and nuclear magnetic resonance spectroscopy are required to determine the tertiary structure of a protein.
Membrane proteins are essential to a variety of processes including small-molecule transport and signaling, and are of significant biological interest.
However, they are not easily amenable to existing crystallization methods, and even though some of the most difficult problems in this area have been overcome in recent years, the number of known tertiary structures of membrane structures remains very low.
Computational methods that can accurately predict the basic topology of transmembrane proteins from easily available information therefore continue to be of great interest.
To be most valuable, a predicted topology include not only the locations of the membrane-spanning segments, but should also correctly localize the N- and C-termini relative to the membrane.
Many proteins include a short N-terminal signal peptide that initially directs the post-translational transport of the protein across the membrane and is subsequently cleaved off after transport.
A signal peptide includes a strongly hydrophobic segment which is not a part of the mature protein but is often misclassified as a membrane-spanning portion of a transmembrane protein.
Conversely, a transmembrane protein with a membrane-spanning segment near the N-terminus is often misclassified as having a signal peptide.
Therefore, signal peptide prediction and transmembrane topology prediction should be performed simultaneously, rather than being treated as two separate tasks.
Membrane proteins are classically divided into two structural classes: those which traverse the membrane using an -helical bundle, such as bacteriorhodopsin, and those which use a -barrel, such as porin.
The -barrel motif is found only in a small fraction of all membrane proteins.
Lately, some attention has been given to some irregular structures such as re-entrant loops and random coil regions.
In this work, however, we focus on the -helical class, both because most membrane proteins fall into this class, and because they constitute most of the known 3D structures.
The two most common machine learning approaches applied to the prediction of both signal peptides and the topology of transmembrane proteins are hidden Markov models and artificial neural networks, while some predictors use a combination of these two approaches.
HMMs are particularly well suited to sequence labeling tasks, and task-specific prior knowledge can be encoded into the structure of the HMM, while ANNs can learn to make classification decisions based on hundreds of inputs.
The first HMM-based transmembrane protein topology predictors were introduced ten years ago: TMHMM CITATION and HMMTOP CITATION.
Both of these predictors define a set of structural classes which capture the variation in amino acid composition of different portions of the membrane protein.
For example, the membrane-spanning helix is known to be highly hydrophobic, and cytoplasmic loops generally contain more positively charged amino acids than non-cytoplasmic loops.
During training the HMM learns a set of emission distributions, one for each of the structural classes.
TMHMM is trained using a two-pass discriminative training approach followed by decoding using the one-best algorithm CITATION.
HMMTOP introduced the hypothesis that the difference between the amino acid distributions in the various structural classes is the main driving force in determining the final protein topology, and that therefore the most likely topology is the one that maximizes this difference for a given protein.
HMMTOP CITATION was also the first to allow constrained decoding to incorporate additional evidence regarding the localization of one or more positions within the protein sequence.
The presence of a signal peptide within a given protein has also been successfully predicted using both HMMs CITATION and ANNs CITATION .
As mentioned above, the confusion between signal peptides and transmembrane segments is one of the largest sources of error both for conventional transmembrane topology predictors and signal peptide predictors CITATION, CITATION.
Motivated by this difficulty, the HMM Phobius CITATION was designed to combine the signal peptide model of SignalP-HMM CITATION with the transmembrane topology model of TMHMM CITATION.
The authors showed that including a signal peptide sub-model improves overall accuracy in detecting and differentiating proteins with signal peptides and proteins with transmembrane segments.
In this work, we introduce Philius, a combined transmembrane topology and signal peptide predictor that extends Phobius by exploiting the power of dynamic Bayesian networks.
The application of DBNs to this task provides several advantages, specifically: a new two-stage decoding procedure, a new way of expressing non-geometric duration distributions, and a new approach to expressing label uncertainty during training.
Philius is inspired by Phobius and tackles the problem of discriminating among four basic types of proteins: globular, globular with a signal peptide, transmembrane, and transmembrane with a signal peptide.
Philius also predicts the location of the signal peptide cleavage site and the complete topology for membrane proteins.
We report state-of-the-art results on the discrimination task and improvements over Phobius on the topology prediction task.
We also introduce a set of confidence measures at three different levels: at the level of protein type, at the level of the individual topology segment, and at the level of the full topology.
Confidence measures for topology predictions were introduced by Mel n et al. CITATION, and we expand upon this work with these three types of scores that correlate well with the observed precision.
Finally, based on the Philius predictions on the entire Yeast Resource Center CITATION protein database, we provide an overview of the relative percentages of different types of proteins in different organisms as well as the composition of the class of membrane proteins.
Transmembrane protein topology prediction can be stated as a supervised learning problem over amino acid sequences.
The training set consists of pairs of sequences of the form where o o 1, ,o n is the sequence of amino acids for a protein of known topology, and s s 1, ,s n is the corresponding sequence of labels.
The o i are drawn from the alphabet of 20 amino acids FORMULA, and the s i are drawn from the alphabet of topology labels, FORMULA, corresponding respectively to cytoplasmic loops, membrane-spanning segments, non-cytoplasmic loops, and signal peptides.
After training, a learned model with parameters takes as input a single amino acid test sequence o and seeks to predict the best corresponding label sequence s .
We solve this problem using a DBN, which we call Philius.
Before describing the details of our model, we first review HMMs and explain how they are a simple form of DBN.
The generality of the DBN framework provides significantly expanded flexibility relative to HMMs, as described in CITATION.
A recently published primer CITATION provides an introduction to probabilistic inference using Bayesian networks for a variety of applications in computational biology.
Circadian rhythm is fundamental in regulating a wide range of cellular, metabolic, physiological, and behavioral activities in mammals.
Although a small number of key circadian genes have been identified through extensive molecular and genetic studies in the past, the existence of other key circadian genes and how they drive the genomewide circadian oscillation of gene expression in different tissues still remains unknown.
Here we try to address these questions by integrating all available circadian microarray data in mammals.
We identified 41 common circadian genes that showed circadian oscillation in a wide range of mouse tissues with a remarkable consistency of circadian phases across tissues.
Comparisons across mouse, rat, rhesus macaque, and human showed that the circadian phases of known key circadian genes were delayed for 4 5 hours in rat compared to mouse and 8 12 hours in macaque and human compared to mouse.
A systematic gene regulatory network for the mouse circadian rhythm was constructed after incorporating promoter analysis and transcription factor knockout or mutant microarray data.
We observed the significant association of cis-regulatory elements: EBOX, DBOX, RRE, and HSE with the different phases of circadian oscillating genes.
The analysis of the network structure revealed the paths through which light, food, and heat can entrain the circadian clock and identified that NR3C1 and FKBP/HSP90 complexes are central to the control of circadian genes through diverse environmental signals.
Our study improves our understanding of the structure, design principle, and evolution of gene regulatory networks involved in the mammalian circadian rhythm.
Circadian rhythm is a daily time-keeping mechanism fundamental to a wide range of species.
The basic molecular mechanism of circadian rhythm has been studied extensively.
It has been shown that the negative transcriptional translational feedback loops formed by a set of key circadian genes are responsible for giving rise to the circadian physiology.
In mammals, the master clock resides in the suprachiasmatic nucleus and the SCN orchestrates the circadian clocks in peripheral tissues by directing the secretion of hormones such as glucocorticoids.
Through many years of molecular and genetic studies, at least 19 key circadian genes Per family, Cry family, Bmal1, Clock, Npas2, Dec1/Dec2, Rev-erb /, Rora/Rorb/Rorc, Dbp/Tef/Hlf, and E4bp4 have been identified in mammals CITATION.
As is now commonly accepted, Arntl and Clock proteins form a complex that positively regulates the transcription of Per and Cry family genes through activating the cis-regulatory element E-box in their promoters.
Per and Cry family proteins form a complex that inhibits Arntl/Clock transcriptional activity, thus completing the negative feedback loop.
Other key circadian genes such as Dbp and Nfil3 controlling the D-box element and Rora/Rorb/Rorc and Nr1d1/Nr1d2 controlling the RRE have also been shown to be important to the mammalian circadian rhythm.
Since 2002, there have been a series of microarray experiments aimed at identifying circadian oscillating genes at the genome-wide level in various tissues of mammalian species, including mouse, rat, rhesus macaque, and human.
These experiments usually identified hundreds of circadian oscillating genes, suggesting that the circadian rhythm drives a genomewide circadian oscillation of gene expression.
However, microarray data are intrinsically noisy, and further, these microarray experiments differed in the animals that they used, experimental conditions, and sampling times, etc. Indeed, these microarray experiments have so far not been compared or integrated.
In a few cases where two tissues were studied in a single experiment, the overlap of circadian oscillating genes between tissues was very limited CITATION, CITATION.
Assuming that a set of common circadian genes exists in most tissues and cell types, integration of different circadian microarray datasets in multiple tissues could potentially identify such a common set of circadian genes CITATION.
Comparison of circadian oscillating genes and their oscillating patterns across different tissues can help us understand the tissue-specific functions of circadian rhythm.
Comparison across different mammalian species can also shed light on the molecular mechanisms that lead to their different physiologies and behaviors.
Because many known key circadian genes such as Arntl/Clock, Nr1d1/Nr1d2, and Dbp/Nfil3 are transcription factors, transcriptional regulation must have played an important role in the genome-wide circadian oscillation of gene expression.
Ueda et al. constructed a small-scale gene regulatory network consisting of 16 genes and 3 cis-regulatory elements based on in vitro luciferase reporter assays CITATION.
However, the construction of a circadian gene regulatory network at the system level based on promoter analysis alone has been almost impossible due to the difficulties in transcription factor binding site prediction CITATION.
The existence of other cis-regulatory elements associated with circadian oscillation has remained elusive.
On the other hand, there are a large body of microarray experiments from transcription factor knockout or mutant animals currently available at public databases.
Incorporating the knockout or mutant microarray experiment results with the promoter sequence analysis can greatly facilitate the identification of functional transcription factor binding sites.
In general, construction and analysis of gene regulatory networks involved in the mammalian circadian rhythm will improve our understanding on how key circadian genes are driving circadian-controlled genes, and will pave the way for more detailed quantitative modeling of the mammalian circadian rhythm.
The cellular response elicited by an environmental cue typically varies with the strength of the stimulus.
For example, in the yeast Saccharomyces cerevisiae, the concentration of mating pheromone determines whether cells undergo vegetative growth, chemotropic growth, or mating.
This implies that the signaling pathways responsible for detecting the stimulus and initiating a response must transmit quantitative information about the intensity of the signal.
Our previous experimental results suggest that yeast encode pheromone concentration as the duration of the transmitted signal.
Here we use mathematical modeling to analyze possible biochemical mechanisms for performing this dose-to-duration conversion.
We demonstrate that modulation of signal duration increases the range of stimulus concentrations for which dose-dependent responses are possible; this increased dynamic range produces the counterintuitive result of signaling beyond saturation in which dose-dependent responses are still possible after apparent saturation of the receptors.
We propose a mechanism for dose-to-duration encoding in the yeast pheromone pathway that is consistent with current experimental observations.
Most previous investigations of information processing by signaling pathways have focused on amplitude encoding without considering temporal aspects of signal transduction.
Here we demonstrate that dose-to-duration encoding provides cells with an alternative mechanism for processing and transmitting quantitative information about their surrounding environment.
The ability of signaling pathways to convert stimulus strength into signal duration results directly from the nonlinear nature of these systems and emphasizes the importance of considering the dynamic properties of signaling pathways when characterizing their behavior.
Understanding how signaling pathways encode and transmit quantitative information about the external environment will not only deepen our understanding of these systems but also provide insight into how to reestablish proper function of pathways that have become dysregulated by disease.
Many substances, such as hormones, neurotransmitters and a variety of pharmaceuticals, affect cellular behavior by binding to membrane receptors and activating intracellular signaling pathways.
These pathways transmit information from the plasma membrane to selected cellular components to generate an appropriate response to the environmental cue.
However, signaling networks are not simply passive relay systems, but actively modulate the transmitted signals.
For example, cross inhibition is used to avoid spurious crosstalk between pathways.
Similarly, negative feedback allows pathways to adapt or desensitize to persistent stimuli CITATION, CITATION.
In many cases, the nature of the response depends on the dose of the stimulus.
Thus, in addition to relaying qualitative information, signaling pathways must also transmit quantitative information about the intensity of the stimulus.
Many signaling pathways consist of a cell surface receptor, G protein transducer, and a series of protein kinases, including a mitogen activated protein kinase.
This architecture is widely employed in mammalian cells, but is also found in single-cell eukaryotes such as yeast CITATION.
The pheromone response pathway of the yeast Saccharomyces cerevisiae provides an instructive example in which the elicited cellular response depends on the concentration of the stimulus.
At low pheromone levels, cells continue vegetative growth.
At intermediate concentrations, cells develop an elongated morphology and in the presence of a pheromone gradient the growth is directed to the source of the stimulus, a process known as chemotropic growth CITATION CITATION.
Finally, at high pheromone concentrations cells initiate a mating program that eventually leads to growth arrest and the development of mating projections.
Therefore, for yeast to make the correct developmental decision, quantitative information about the pheromone concentration must be reliably transmitted to the appropriate cellular components.
Here we use mathematical modeling to investigate the different ways this information can be transferred.
The results of this analysis taken together with our recently published data demonstrate that the pheromone pathway uses a strategy in which the agonist dose is encoded as the duration of the signal.
Because the yeast pheromone response pathway consists of a G-protein coupled receptor and MAP kinase cascade, the results of our investigations should have direct implications for signal transduction in mammalian cells.
A cornerstone of biotechnology is the use of microorganisms for the efficient production of chemicals and the elimination of harmful waste.
Pseudomonas putida is an archetype of such microbes due to its metabolic versatility, stress resistance, amenability to genetic modifications, and vast potential for environmental and industrial applications.
To address both the elucidation of the metabolic wiring in P. putida and its uses in biocatalysis, in particular for the production of non-growth-related biochemicals, we developed and present here a genome-scale constraint-based model of the metabolism of P. putida KT2440.
Network reconstruction and flux balance analysis enabled definition of the structure of the metabolic network, identification of knowledge gaps, and pin-pointing of essential metabolic functions, facilitating thereby the refinement of gene annotations.
FBA and flux variability analysis were used to analyze the properties, potential, and limits of the model.
These analyses allowed identification, under various conditions, of key features of metabolism such as growth yield, resource distribution, network robustness, and gene essentiality.
The model was validated with data from continuous cell cultures, high-throughput phenotyping data, 13C-measurement of internal flux distributions, and specifically generated knock-out mutants.
Auxotrophy was correctly predicted in 75 percent of the cases.
These systematic analyses revealed that the metabolic network structure is the main factor determining the accuracy of predictions, whereas biomass composition has negligible influence.
Finally, we drew on the model to devise metabolic engineering strategies to improve production of polyhydroxyalkanoates, a class of biotechnologically useful compounds whose synthesis is not coupled to cell survival.
The solidly validated model yields valuable insights into genotype phenotype relationships and provides a sound framework to explore this versatile bacterium and to capitalize on its vast biotechnological potential.
Pseudomonas putida is one of the best studied species of the metabolically versatile and ubiquitous genus of the Pseudomonads CITATION CITATION.
As a species, it exhibits a wide biotechnological potential, with numerous strains able to efficiently produce a range of bulk and fine chemicals.
These features, along with their renowned stress resistance, amenability for genetic manipulation and suitability as a host for heterologous expression, make Pseudomonas putida particularly attractive for biocatalysis.
To date, strains of P. putida have been employed to produce phenol, cinnamic acid, cis-cis-muconate, p-hydroxybenzoate, p-cuomarate, and myxochromide CITATION CITATION.
Furthermore, enzymes from P. putida have been employed in a variety of other biocatalytic processes, including the resolution of d/l-phenylglycinamide into d-phenylglycinamide and l-phenylglycine, production of non-proteinogenic l-amino acids, and biochemical oxidation of methylated heteroaromatic compounds for formation of heteroaromatic monocarboxylic acids CITATION.
However, most Pseudomonas-based applications are still in infancy largely due to a lack of knowledge of the genotype-phenotype relationships in these bacteria under conditions relevant for industrial and environmental endeavors.
In an effort towards the generation of critical knowledge, the genomes of several members of the Pseudomonads have been or are currently being sequenced, and a series of studies are underway to elucidate specific aspects of their genomic programs, physiology and behavior under various stresses .
The sequencing of P. putida strain KT2440, a workhorse of P. putida research worldwide and a microorganism Generally Recognized as Safe CITATION, CITATION, provided means to investigate the metabolic potential of the P. putida species, and opened avenues for the development of new biotechnological applications CITATION, CITATION CITATION.
Whole genome analysis revealed, among other features, a wealth of genetic determinants that play a role in biocatalysis, such as those for the hyper-production of polymers and industrially relevant enzymes, the production of epoxides, substituted catechols, enantiopure alcohols, and heterocyclic compounds CITATION, CITATION.
However, despite the clear breakthrough in our understanding of P. putida through this sequencing effort, the relationship between the genotype and the phenotype cannot be predicted simply from cataloguing and assigning gene functions to the genes found in the genome, and considerable work is still needed before the genome can be translated into a fully functioning metabolic model of value for predicting cell phenotypes CITATION, CITATION .
Constraint-based modeling is currently the only approach that enables the modeling of an organism's metabolic and transport network at genome-scale CITATION.
A genome-wide constraint-based model consists of a stoichiometric reconstruction of all reactions known to act in the metabolism of the organism, along with an accompanying set of constraints on the fluxes of each reaction in the system CITATION, CITATION.
A major advantage of this approach is that the model does not require knowledge on the kinetics of the reactions.
These models define the organism's global metabolic space, network structural properties, and flux distribution potential, and provide a framework with which to navigate through the metabolic wiring of the cell CITATION CITATION .
Through various analysis techniques, constraint-based models can help predict cellular phenotypes given particular environmental conditions.
Flux balance analysis is one such technique, which relies on the optimization for an objective flux while enforcing mass balance in all modeled reactions to achieve a set of fluxes consistent with a maximal output of the objective function.
When a biomass sink is chosen as the objective in FBA, the output can be correlated with growth, and the model fluxes become predictive of growth phenotypes CITATION, CITATION.
Constraint-based analysis techniques, including FBA, have been instrumental in elucidating metabolic features in a variety of organisms CITATION, CITATION, CITATION and, in a few cases thus far, they have been used for concrete biotechnology endeavors CITATION CITATION .
However, in all previous applications in which a constraint-based approach was used to design the production of a biochemical, the studies addressed only the production of compounds that can be directly coupled to the objective function used in the underlying FBA problem.
The major reason for this is that FBA-based methods predict a zero-valued flux for any reaction not directly contributing to the chosen objective.
Since the production pathways of most high-added value and bulk compounds operate in parallel to growth-related metabolism, straightforward application of FBA to these biocatalytic processes fails to be a useful predictor of output.
Other constraint-based analysis methods, such as Extreme Pathways and Elementary Modes analysis, are capable of analyzing non-growth related pathways in metabolism, but, due to combinatorial explosion inherent to numerical resolution of these methods, they could not be used so far to predict fluxes or phenotypes at genome-scale for guiding biocatalysis efforts CITATION .
To address both the elucidation of the metabolic wiring in P. putida and the use of P. putida for the production of non-growth-related biochemicals, we developed and present here a genome-scale reconstruction of the metabolic network of Pseudomonas putida KT2440, the subsequent analysis of its network properties through constraint-based modeling and a thorough assessment of the potential and limits of the model.
The reconstruction is based on up-to-date genomic, biochemical and physiological knowledge of the bacterium.
The model accounts for the function of 877 reactions that connect 886 metabolites and builds upon a constraint-based modeling framework CITATION, CITATION.
Only 6 percent of the reactions in the network are non gene-associated.
The reconstruction process guided the refinement of the annotation of several genes.
The model was validated with continuous culture experiments, substrate utilization assays CITATION, 13C-measurement of internal fluxes CITATION, and a specifically generated set of mutant strains.
We evaluated the influence of biomass composition and maintenance values on the outcome of flux balance analysis simulations, and utilized the metabolic reconstruction to predict internal reaction fluxes, to identify different mass-routing possibilities, and to determine necessary gene and reaction sets for growth on minimal medium.
Finally, by means of a modified OptKnock approach, we utilized the model to generate hypotheses for possible improvements of the production by P. putida of polyhydroxyalkanoates, a class of compounds whose production consumes resources that would be otherwise used for growth.
This reconstruction thus provides a modeling framework for the exploration of the metabolic capabilities of P. putida, which will aid in deciphering the complex genotype-phenotype relationships governing its metabolism and will help to broaden the applicability of P. putida strains for bioremediation and biotechnology.
Metabolic rate, heart rate, lifespan, and many other physiological properties vary with body mass in systematic and interrelated ways.
Present empirical data suggest that these scaling relationships take the form of power laws with exponents that are simple multiples of one quarter.
A compelling explanation of this observation was put forward a decade ago by West, Brown, and Enquist.
Their framework elucidates the link between metabolic rate and body mass by focusing on the dynamics and structure of resource distribution networks the cardiovascular system in the case of mammals.
Within this framework the WBE model is based on eight assumptions from which it derives the well-known observed scaling exponent of 3/4.
In this paper we clarify that this result only holds in the limit of infinite network size and that the actual exponent predicted by the model depends on the sizes of the organisms being studied.
Failure to clarify and to explore the nature of this approximation has led to debates about the WBE model that were at cross purposes.
We compute analytical expressions for the finite-size corrections to the 3/4 exponent, resulting in a spectrum of scaling exponents as a function of absolute network size.
When accounting for these corrections over a size range spanning the eight orders of magnitude observed in mammals, the WBE model predicts a scaling exponent of 0.81, seemingly at odds with data.
We then proceed to study the sensitivity of the scaling exponent with respect to variations in several assumptions that underlie the WBE model, always in the context of finite-size corrections.
Here too, the trends we derive from the model seem at odds with trends detectable in empirical data.
Our work illustrates the utility of the WBE framework in reasoning about allometric scaling, while at the same time suggesting that the current canonical model may need amendments to bring its predictions fully in line with available datasets.
Whole-organism metabolic rate, B, scales with body mass, M, across species as CITATION FORMULAwhere B 0 is a normalization constant and is the allometric scaling exponent, typically measured to be very close to 3/4 CITATION.
The empirical regularity expressed in Equation 1 with 3/4 is known as Kleiber's Law CITATION, CITATION .
Many other biological rates and times scale with simple multiples of 1/4.
For example, cellular or mass-specific metabolic rates, heart and respiratory rates, and ontogenetic growth rates scale as M 1/4, whereas blood circulation time, development time, and lifespan scale close to M 1/4 CITATION CITATION.
Quarter-power scaling is also observed in ecology and evolution CITATION, CITATION, CITATION.
The occurrence of quarter-power scaling at such diverse levels of biological organization suggests that all these rates are closely linked.
Metabolic rate seems to be the most fundamental because it is the rate at which energy and materials are taken up from the environment, transformed in biochemical reactions, and allocated to maintenance, growth, and reproduction.
In a series of papers starting in 1997, West, Brown, and Enquist published a model to account for the 3/4-power scaling of metabolic rate with body mass across species CITATION, CITATION CITATION.
The broad theory of biological allometry developed by WBE and collaborators attributes such quarter-power scaling to near-optimal fractal-like designs of resource distribution networks and exchange surfaces.
There is some evidence that such designs are realized at molecular, organelle, cellular, and organismal levels for a wide variety of plants and animals CITATION, CITATION .
Intensifying controversy has surrounded the WBE model since its original publication, even extending to a debate about the quality and analysis of the data CITATION CITATION.
One of the most frequently raised objections is that the WBE model cannot predict scaling exponents for metabolic rate that deviate from 3/4 CITATION, CITATION, even though the potential for such deviations was appreciated by WBE themselves CITATION.
If this criticism were true, WBE could not in principle explain data for taxa whose scaling exponents have been reported to be above or below 3/4 CITATION CITATION, or deviations from 3/4 that have been observed for small mammals CITATION.
Likewise, the WBE model would be unable to account for the scaling of maximal metabolic rate with body mass, which appears to have an exponent of 0.88 CITATION.
It is important to note that the actual nature of maximal metabolic rate scaling is, however, not without its own controversy; see CITATION for an argument that maximal metabolic rate scales closer to 3/4 when body temperature is taken into consideration.
Much of the work aimed at answering these criticisms has relied on alteration of the WBE model itself.
Enquist and collaborators account for different scaling exponents among taxonomic groups by emphasizing differences in the normalization constant B 0 of Equation 1 and deviations from the WBE assumptions regarding network geometry CITATION, CITATION CITATION.
While these results are suggestive, it remains unclear whether or not WBE can predict exponents significantly different from 3/4 and measurable deviations from a pure power law even in the absence of any variation in B 0 and with networks following exactly the geometry required by the theory.
Although WBE has been frequently tested and applied CITATION CITATION, it is remarkable that no theoretical work has been published that provides more detailed predictions from the original theory.
Also, work aimed at extending WBE by relaxing or modifying some of its assumptions has hardly been complete; many variations in network structure might have important and far-reaching consequences once properly analyzed.
This is what we set out to do in the present contribution.
We show that a misunderstanding of the original model has led to the claim that WBE can only predict a 3/4 exponent.
This is because many of the predictions and tests of the original model are derived from leading-order approximations.
In this paper we derive more precise predictions and tests.
For the purpose of stating our conclusions succinctly, we refer to the WBE framework as an approach to explaining allometric scaling phenomena in terms of resource distribution networks and to the WBE model as an instance of the WBE framework that employs particular parameters specifying geometry and dynamics of these networks CITATION, CITATION.
Our main findings are: 1.
The 3/4 exponent only holds exactly in the limit of organisms of infinite size.
2.
For finite-sized organisms we show that the WBE model does not predict a pure power-law but rather a curvilinear relationship between the logarithm of metabolic rate and the logarithm of body mass. 3.
Although WBE recognized that finite size effects would produce deviations from pure 3/4 power scaling for small mammals and that the infinite size limit constitutes an idealization CITATION, the magnitude and importance of finite-size effects were unclear.
We show that, when emulating current practice by calculating the scaling exponent of a straight line regressed on this curvilinear relationship over the entire range of body masses, the exponent predicted by the WBE model can differ significantly from 3/4 without any modifications to its assumptions or framework.
4.
When realistic parameter values are employed to construct the network, we find that the exponent resulting from finite-size corrections comes in at 0.81, significantly higher than the 3/4 figure based on current data analysis.
5.
Our data analysis indeed detects a curvilinearity in the relationship between the logarithm of metabolic rate and the logarithm of body mass. However, that curvilinearity is opposite to what we observe in the WBE model.
This implies that the WBE model needs amendment and/or the data analysis needs reassessment.
Beyond finite-size corrections we examine the original assumptions of WBE in two ways.
First, we vary the predicted switch-over point above which the vascular network architecture preserves the total cross-sectional area of vessels at branchings and below which it increases the total cross-sectional area at branchings.
These two regimes translate into different ratios of daughter to parent radii at vessel branch points.
Second, we allow network branching ratios to differ for large and small vessels.
We analyze the sensitivity of the scaling exponent with respect to each of these changes in the context of networks of finite size.
This approach is similar in spirit to Price et al. CITATION, who relaxed network geometry and other assumptions of WBE in the context of plants.
In the supplementary online material Text S1, we also argue that data analysis should account for the log-normal distribution of body mass abundance, thus correcting for the fact that there are more small mammals than large ones.
Despite differences in the structure and hydrodynamics of the vascular systems of plants and animals CITATION, CITATION, detailed models of each yield a scaling exponent of 3/4 to leading-order.
In the present paper, we focus on the WBE model of the cardiovascular system of mammals.
All of our assumptions, derivations, and calculations should be interpreted within that context.
Finite-size corrections and departures from the basic WBE assumptions are important in the context of plants as well, as shown in recent studies by Enquist and collaborators CITATION, CITATION CITATION .
In final analysis, we are led to the seemingly incongruent conclusions that many of the critiques of the WBE framework are misguided and the exact predictions of the WBE model are not fully supported by empirical data.
The former means that the WBE framework remains, once properly understood, a powerful perspective for elucidating allometric scaling principles.
The latter means that the WBE model must become more respectful of biological detail whereupon it may yield predictions that more closely match empirical data.
Our work explores how such details can be added to the model and what effects they can have.
The paper is organized as follows.
For the sake of a self-contained presentation, we start with a systematic overview of the assumptions, both explicit and implicit, underlying the WBE theory.
In Text S1, we provide a detailed exposition of the hydrodynamic derivations that the model rests upon.
These calculations are not original, but they have not appeared to a full extent before in the literature.
While nothing in section Assumptions of the WBE model is novel, there seems to be no single go to place in the WBE literature that lays out all components of the WBE theory.
Our paper then proceeds with a brief derivation of the exact, rather than approximate, relationship between metabolic rate and body mass. We then calculate the exact predictions for scaling exponents for networks of finite size and revisit certain assumptions of the theory.
In section Comparison to empirical data we compare our results to trends detectable in empirical data.
We put forward our conclusions in the Discussion section.
The exact lengths of linker DNAs connecting adjacent nucleosomes specify the intrinsic three-dimensional structures of eukaryotic chromatin fibers.
Some studies suggest that linker DNA lengths preferentially occur at certain quantized values, differing one from another by integral multiples of the DNA helical repeat, 10 bp; however, studies in the literature are inconsistent.
Here, we investigate linker DNA length distributions in the yeast Saccharomyces cerevisiae genome, using two novel methods: a Fourier analysis of genomic dinucleotide periodicities adjacent to experimentally mapped nucleosomes and a duration hidden Markov model applied to experimentally defined dinucleosomes.
Both methods reveal that linker DNA lengths in yeast are preferentially periodic at the DNA helical repeat, obeying the forms 10n 5 bp.
This 10 bp periodicity implies an ordered superhelical intrinsic structure for the average chromatin fiber in yeast.
Eukaryotic genomic DNA exists in vivo as a hierarchically compacted protein-DNA complex called chromatin CITATION.
In the first level of compaction, 147 bp lengths of DNA are wrapped in 1 3/4 superhelical turns around protein spools, forming nucleosomes CITATION.
Consecutive nucleosomes are separated by short stretches of unwrapped linker DNA.
Most chromatin in vivo is further folded into shorter, wider fibers, 30 nm in diameter.
Despite much effort, the structure of the 30 nm fiber remains unresolved CITATION, CITATION .
Here we report that an analysis of the relative locations of nucleosomes along the DNA sheds new light on chromatin fiber structure.
The connection arises from the helical symmetry of DNA itself CITATION CITATION.
Each base pair increase in separation between two consecutive nucleosomes moves them apart by 0.34 nm along the DNA - a potentially minor change relative to the 30 nm fiber's width.
However, because of the 10.2 10.5 bp per turn helical symmetry of DNA, this 0.34 nm translation is coupled to a 35 rotation about the DNA helix axis, rotating the second nucleosome to an entirely different location in space, creating an entirely different intrinsic chromatin structure.
In vivo, attractive nucleosome-nucleosome interactions CITATION, CITATION might overwhelm this intrinsic structure for the chromatin fiber, and impose a particular folded structure that is independent of exact linker DNA lengths.
In that case, changes in the fiber's intrinsic structure would be manifested instead as changes in the folded fiber's stability CITATION.
Because of the high torsional stiffness of DNA and the short lengths of linker DNAs, such changes in stability would be of great energetic significance.
While steps of one or several bp profoundly alter the intrinsic fiber structure, steps of 10 11 n bp do not: instead, the next nucleosome rotates n complete turns around the DNA helix axis, ending up rotationally near where it began, but translated along the DNA by 3.4 3.7 n nm.
If linker DNA lengths varied randomly about an average value, the resulting intrinsic chromatin structure would be a random flight chain.
But if linker DNA segments instead were equal in length modulo the DNA helical repeat, this would define an intrinsically ordered superhelical structure for the chromatin fiber, with the detailed intrinsic structure highly depending on the phase offset d 0 for linker DNAs of length 10n d 0 bp.
There are many hints in the literature for a 10 bp-periodicity in lengths of linker DNAs CITATION CITATION, CITATION CITATION ; however, the results are inconsistent.
An early analysis of oligonucleosome DNA lengths suggested that linker DNAs in the yeast S. cerevisiae preferentially occur in lengths of 10n 5 bp, while those in human HeLa and chicken erythrocyte cells have no periodicity CITATION.
Analogous studies on rat liver chromatin first did not CITATION, but later did CITATION, reveal periodic linker DNA lengths, again of the form 10n 5 bp.
Later genome-wide correlation analyses of AA and TT dinucleotides similarly yielded variable results, suggesting preferences of the form 10n 5 CITATION or 10.6n 8 bp for yeast CITATION, 10.6n 8 for Caenorhabditis elegans and Drosophila melanogaster, CITATION, and 10n 8 for human k562 cells CITATION .
These conflicting conclusions of existing studies motivated us to develop two new independent computational methods and new experimental data, to define the probability distribution of linker DNA lengths in yeast.
Our results from both approaches show that linker DNA lengths in yeast are indeed preferentially periodic, implying that the yeast genome encodes an intrinsically ordered three-dimensional structure for its average chromatin fiber.
Protein function is mediated by different amino acid residues, both their positions and types, in a protein sequence.
Some amino acids are responsible for the stability or overall shape of the protein, playing an indirect role in protein function.
Others play a functionally important role as part of active or binding sites of the protein.
For a given protein sequence, the residues and their degree of functional importance can be thought of as a signature representing the function of the protein.
We have developed a combination of knowledge- and biophysics-based function prediction approaches to elucidate the relationships between the structural and the functional roles of individual residues and positions.
Such a meta-functional signature, which is a collection of continuous values representing the functional significance of each residue in a protein, may be used to study proteins of known function in greater detail and to aid in experimental characterization of proteins of unknown function.
We demonstrate the superior performance of MFS in predicting protein functional sites and also present four real-world examples to apply MFS in a wide range of settings to elucidate protein sequence structure function relationships.
Our results indicate that the MFS approach, which can combine multiple sources of information and also give biological interpretation to each component, greatly facilitates the understanding and characterization of protein function.
Vast amounts of sequence and structural data are being generated by high-throughput technologies.
Functional annotations of the uncharacterized sequences and structures are significantly lagging.
The time and cost of experimental techniques required to probe the function of all uncharacterized proteins are prohibitive.
Therefore, computational means have been increasingly useful and popular in predicting and annotating functions for the huge amount of sequence and structure data CITATION, CITATION .
However, protein function prediction is itself a difficult problem to formulate, since it is difficult to define function CITATION, CITATION.
Various functional definition schemes have been developed over the years and have addressed various aspects of protein function.
Instead of adopting an existing functional definition scheme, we proposed to probe the role of individual amino acid residues in protein function, regardless of the functional definition schemes that are used.
In such cases, the protein function can be represented simply as a series of quantitative values, each of which indicates the functional importance of the corresponding amino acid residue in the protein sequence or structure.
To calculate the quantitative values for each residue, we used a combined approach, the meta-functional signature, which takes into account the individual scores from various function prediction algorithms and generates a composite score for each amino acid residue in a given protein.
Currently our signature generation protocol consists of the following four types of scores for four different types of information: sequence conservation, evolutionary conservation, structural stability, and amino acid type.
All these scores are generated via conceptually simple and easily implementable algorithms, and their combined use outperforms sophisticated algorithms that use only one source of information.
Sequence conservation is one of the most utilized methods for measuring the functional importance of individual amino acids.
Amino acid residues with more conservative variation patterns are usually more important for the preservation of protein function.
This concept is often used to identify the functional regions of proteins by building multiple alignments between the target sequence and all its sequence homologues, and then analyzing the degree of sequence conservation among each alignment site.
Various measures of sequence conservation have been proposed over the years, with differing complexity and sophistication CITATION.
The simplest measures of sequence conservation are the entropy score and its variants CITATION CITATION.
More complicated measures CITATION CITATION incorporate other information, such as amino acid pairwise similarity, physicochemical properties, and theoretical sequence profiles, into the scoring schemes.
The AL2CO program package incorporates nine different scoring schemes, but these scores tend to correlate with each other CITATION.
Recently it was also shown that a Jensen-Shannon divergence measure improves predicting functionally important residues, and that considering conservation in sequentially neighboring sites further improves accuracy CITATION.
We previously demonstrated that a relative entropy measure which incorporates amino acid background frequencies, can better predict functional sites than simple entropy measures CITATION.
Furthermore, we found that incorporating the amino acid frequencies as estimated by the hidden Markov Models further improves the performance of the relative entropy measure CITATION.
In the current study, we use a sequence conservation measure derived from HMMs as one component of our meta-functional signature generation protocol.
In addition to sequence conservation, we also incorporate evolutionary conservation information in the meta-functional signature.
Many studies have shown that the use of phylogenetic relationships among a group of evolutionarily related sequences help accurate prediction of functional sites.
The Evolutionary Trace method, one of the first and the most successful of such methods, analyzes residue variation patterns within and between protein subfamilies from multiple alignments, maps important residues to protein structure, and quantitatively ranks residue importance CITATION, CITATION.
A further development of the Evolutionary Trace method allows quantitative ranking of residue importance, by combining the use of evolutionary information and the entropy measures CITATION, CITATION.
Similarly, the ConSurf method constructs phylogenetic relationships from a group of similar sequences, calculates the conservation score by a Bayesian or a maximum likelihood method, and maps the conservation information to the protein surface CITATION, CITATION.
Further, a study by Soyer et al. used site-specific evolutionary models that assumed a different substitution matrix for each site, for detecting protein functional sites CITATION.
La et al. used evolutionary relationships among sequence fragments to infer protein functional sites CITATION.
del Sol Mesa et al. presented several automated methods that divide a given protein family into subfamilies and search for residues that determine specificity CITATION.
The commonality among all these methods is that sequence relationships are analyzed based on the topology of an evolutionary tree, thus providing an additional level of information instead of relying on multiple sequence alignments alone.
Here, we propose a novel method, called the state to step ratio score, for measuring evolutionary conservation.
Based on given multiple alignments, we construct a maximum parsimony tree, and analyze the variation patterns from the root of the tree to the leaf of the tree to create a score for each amino acid residue.
The SSR score is a simple yet effective way of measuring evolutionary conservation.
Functional signature scores can also be derived from biophysics-based methods, using experimentally determined or computationally predicted protein structures.
For example, a recent study demonstrated that destabilizing regions in protein structures can often be used to provide valuable information for functional inference and functional site identification CITATION.
For a given structure and a given position, we propose that we can mutate the wild-type residue to 19 other amino acids and calculate their structural stability scores, which can in turn be used to assign a score to each residue in a protein.
Hence, these scores can also serve as a component of protein function prediction.
We previously developed a residue-specific all-atom probability discriminatory function CITATION that compiles statistics from a database of experimental structures to score and pick decoy structures that are more likely to be similar to experimentally derived structures.
The RAPDF has been optimized and enhanced in recent years for protein structure prediction CITATION CITATION.
Here, we further expanded the RAPDF to score residue mutations on a per-residue basis.
Each residue in a given protein was mutated to one of the 19 alternative amino acids, producing new structures that were further optimized for topology and maximized for stability.
In our current MFS generation protocol, we used two RAPDF based scoring functions, to measure how all mutated structures deviate from each other and how the experimentally determined structure differs from mutated structures, which represent the potential impact on stability for the position and for the naturally occurring residue, respectively.
These scores separate residues conserved for structure versus function.
An additional component of the meta-functional signature is information on the type of amino acids, such as histidine and cysteine, which are more likely to be located in functional sites than other amino acids.
However, such prior probability for a functional site is not explicitly modeled and incorporated by most current functional site prediction algorithms.
In our MFS generation protocol, we used 19 binary variables to represent the amino acid identity for each position in a given protein.
We also examined whether the explicit use of amino acid information, as opposed to the implicit use, could provide additional information and better performance.
Given the complexity of defining and identifying protein functional sites, clearly no single method will always work to capture all protein functional site information.
Therefore, several groups have begun to incorporate information from various sources, especially structure-derived information, to give more accurate predictions.
Work by Chelliah et al. has shown that distinguishing the structural and functional constraints for amino acid residues leads to better prediction of protein interaction sites CITATION.
We have shown that by considering both structural and functional constraints on protein evolution, we can better identify functional sites and signatures CITATION, CITATION.
Recently, Petrova et al. showed that integration of seven selected sequence and structure features into a support vector machine framework can improve identification of catalytic sites CITATION.
Furthermore, Fischer et al. integrated sequence conservation, amino acid distribution, predicted secondary structure and relative solvent accessibility into a probability density framework, and showed that at 20 percent sensitivity the integrated method leads to a 10 percent increase in precision over non-integrated methods for predicting catalytic residues from the Catalytic Site Atlas and PDB SITE records CITATION.
Youn et al. investigated the various features for discriminating catalytic from noncatalytic residues in novel structural folds, and showed that a measure of sequence conservation, a measure of structural conservation, a degree of uniqueness of a residue's structural environment, solvent accessibility, and residue hydrophobicity are the best predictors of catalytic sites CITATION.
Other similar studies also incorporated dozens to hundreds of features into a machine-learning framework for catalytic site identification CITATION, CITATION.
Altogether, the previous work suggests great value in using several complementary sequence and structure components for scoring catalytic sites.
Unlike these approaches that were largely based on machine-learning algorithms, in the current study, we aim to combine several sources of information regarding the sequence, structure, evolution, and type of amino acids together via a simple logistic regression model for function prediction, including both catalytic sites and binding sites.
The major advantage of the regression model is that each component can be associated with a biologically meaningful interpretation, and that individual scores for a protein can be manually studied to gain additional insights into different aspects of protein function, which are not available when many components are thrown into a sophisticated machine-learning framework.
We compare the MFS approach with several other functional site prediction algorithms, propose enhancements to our approach, exemplify the wide definition of function assessed by MFS, and discuss how different components of MFS can be used to understand biological function via four real-world examples.
During embryonic development, the positional information provided by concentration gradients of maternal factors directs pattern formation by providing spatially dependent cues for gene expression.
In the fruit fly, Drosophila melanogaster, a classic example of this is the sharp on off activation of the hunchback gene at midembryo, in response to local concentrations of the smooth anterior posterior Bicoid gradient.
The regulatory region for hb contains multiple binding sites for the Bcd protein as well as multiple binding sites for the Hb protein.
Some previous studies have suggested that Bcd is sufficient for properly sharpened Hb expression, yet other evidence suggests a need for additional regulation.
We experimentally quantified the dynamics of hb gene expression in flies that were wild-type, were mutant for hb self-regulation or Bcd binding, or contained an artificial promoter construct consisting of six Bcd and two Hb sites.
In addition to these experiments, we developed a reaction diffusion model of hb transcription, with Bcd cooperative binding and hb self-regulation, and used Zero Eigenvalue Analysis to look for multiple stationary states in the reaction network.
Our model reproduces the hb developmental dynamics and correctly predicts the mutant patterns.
Analysis of our model indicates that the Hb sharpness can be produced by spatial bistability, in which hb self-regulation produces two stable levels of expression.
In the absence of self-regulation, the bistable behavior vanishes and Hb sharpness is disrupted.
Bcd cooperative binding affects the position where bistability occurs but is not itself sufficient for a sharp Hb pattern.
Our results show that the control of Hb sharpness and positioning, by hb self-regulation and Bcd cooperativity, respectively, are separate processes that can be altered independently.
Our model, which matches the changes in Hb position and sharpness observed in different experiments, provides a theoretical framework for understanding the data and in particular indicates that spatial bistability can play a central role in threshold-dependent reading mechanisms of positional information.
How an embryo achieves pattern and form from an initially undifferentiated state has fascinated people at least since the time of Aristotle.
Scientific advances on this began over a century ago, with, for example, the experiments of Hans Driesch on sea urchin embryos CITATION, from which he proposed that the embryo has a coordinate system specifying cellular position; and from the experiments of Ethel Browne CITATION, who showed that a piece of hydra mount induced a secondary axis when grafted into the body of another hydra.
These and other subsequent results were synthesized by Lewis Wolpert in 1969 CITATION into a definition of positional information.
According to this concept, the spatial asymmetries of concentration gradients of chemical signals provide positional information during cellular differentiation; each cell reads its position from the local morphogen concentration and differentiates accordingly.
Wolpert's concept of morphogen gradients has become a central tenet of developmental biology CITATION CITATION.
Modern molecular techniques have demonstrated numerous cases of protein concentration patterns in embryogenesis, and many have been shown to act as morphogens.
In the late 1980's, the Bicoid protein gradient was characterized and its concentration-dependent effect on downstream target genes in Drosophila was demonstrated CITATION CITATION.
This has since become one of the most studied examples of morphogen gradient signaling in developmental biology CITATION, CITATION .
Reaction-network models have been successfully applied to describe a great variety of systems in physics, chemistry, and biology CITATION CITATION.
Along with this, many mathematical tools have been developed to support such applications.
With these tools, one can show that certain reaction networks may exhibit multiple stationary states, for particular ranges of their rate constants.
Bistability is a special case, in which the system can evolve to either of two asymptotically stable steady states.
Under certain conditions, spatial patterning or oscillations can arise CITATION CITATION.
In biology, bistability has long been established in control of the cell cycle and other oscillations CITATION, CITATION, and also recently reported in an artificial gene regulation network CITATION.
In Drosophila, spatial bistability has been proposed for dorso-ventral patterning CITATION, CITATION .
In early embryogenesis, the diffusion of Bcd protein, translated from mRNA localized at the anterior end of the egg, forms an exponential concentration gradient, establishing the anterior posterior axis CITATION, CITATION, CITATION.
Bcd is a transcriptional regulator, and through its asymmetric distribution acts as a morphogen, governing the positions at which the downstream gap genes will be activated.
In combination with cross-regulation between these genes, the initial Bcd asymmetry is propagated and refined, establishing the first stage of embryo segmentation CITATION, CITATION CITATION.
It is still not well characterized, however, what mechanisms interpret the smooth Bcd positional information into sharp and precisely positioned downstream target gene expression.
hunchback is one of the first gap genes activated by Bcd, with strong anterior expression and a sharp on off boundary at mid-embryo CITATION, CITATION CITATION.
Anterior hb activation depends on Bcd, as shown by Struhl et al CITATION and Driever et al CITATION, and on its own self-regulation, as already reported by Treisman et al CITATION and Margolis et al CITATION ; many Bcd and Hb binding sites have been identified in the hb promoter region, as reported by Treisman et al., among others CITATION CITATION.
Hb has maternal and zygotic contributions, and provides positional information for other gap genes, such as Kr ppel, knirps, and giant, and for the homeotic gene Ultrabithorax CITATION CITATION.
Removal of both maternal and zygotic hb expression results in severe deletions and polarity reversals of the most anterior segments CITATION.
In normal development, Hb expression drops from highest to lowest over about 10 percent egg length ; Considerable attention has been focused on what molecular mechanism generates this Hb sharpness.
Early on, it was shown that a hb enhancer element of 300 base pairs, containing 6 Bcd binding sites, is sufficient to reproduce the regulatory activity of Bcd on hb CITATION, CITATION.
It was shown that Bcd binds to these sites cooperatively and it was hypothesized that, due to this cooperativity, a small change in Bcd concentration across some threshold could produce a large change in hb promoter occupancy, generating the on off expression pattern CITATION, CITATION, CITATION CITATION.
However, these studies did not establish that cooperativity is sufficient to generate Hb border sharpness.
To quantify the degree of Bcd's cooperativity, Ma et al. CITATION used a six-Bcd site fragment of the hb promoter in a DNase I footprint assay, and found a Hill coefficient of about 3.6; Burz et al. CITATION, using a gel-shift assay with a 230 bp hb enhancer, found a Hill coefficient of 3.0.
From quantified in vivo patterns of Bcd and Hb proteins, Gregor et al. CITATION, estimated a higher value for this coefficient, of around 5 ; and suggested that it could support the proposal of Crauk and Dostatni CITATION that Hb expression is entirely determined by Bcd cooperative binding.
However, systems with such high Hill coefficients would be expected to show temperature sensitivity.
Houchmandzadeh et al. CITATION showed that the Bcd gradient is strongly affected by temperature changes of 20 C, but that the Hb pattern is largely unaffected.
Dependence on Bcd with Hill coefficients between 3 and 5 would be expected to show far greater effects on Hb than are observed, indicating that Bcd cannot be the only factor regulating the Hb border.
The insufficiency of Bcd cooperativity to produce Hb sharpness is also supported by the findings of Simpson-Brose et al. CITATION, who showed that synergy between Hb and Bcd is necessary to establish the expression patterns of the gap genes, including hb itself.
To address these issues, we have taken a combined experimental and theoretical approach to understand how the hb gene converts the positional information of the smooth Bcd gradient into a sharp expression pattern.
We used wild-type embryos to experimentally determine how Hb position and sharpness change in time; and we measured how these quantities are affected in embryos mutant for Bcd cooperative binding and for hb self-regulation, and by use of an artificial promoter with 6 Bcd and 2 Hb binding sites.
We also developed a predictive reaction diffusion model of hb transcription, incorporating both Bcd cooperative binding and hb self-regulation.
By fitting this model to wild-type Bcd and Hb patterns, we determined kinetic parameters of the model, such as binding constants.
With these parameters, our model successfully reproduces the dynamic development of the Hb pattern.
By reducing Bcd binding constants or the number of Bcd binding sites, our model reproduces the same mutant phenotypes as our experiments, and predicts a loss of sharpness for a hb self-regulation mutant, which we experimentally verified.
By applying dynamical systems theory to the model, we show that Hb sharpness is due to spatial bistability stemming from hb self-regulation.
This means that Hb does not have a single steady-state concentration continuously dependent on Bcd, but that at a threshold Bcd concentration, two stable steady states become available to Hb, and a small change in Bcd concentration can create a dramatic shift in Hb concentration.
This shift between steady states is responsible for the sharpness of the Hb boundary.
The position of the Bcd threshold is controlled by Bcd cooperative binding, but this mechanism itself is not sufficient to generate hb's expression sharpness.
Recent improvements in technology have made DNA sequencing dramatically faster and more efficient than ever before.
The new technologies produce highly accurate sequences, but one drawback is that the most efficient technology produces the shortest read lengths.
Short-read sequencing has been applied successfully to resequence the human genome and those of other species but not to whole-genome sequencing of novel organisms.
Here we describe the sequencing and assembly of a novel clinical isolate of Pseudomonas aeruginosa, strain PAb1, using very short read technology.
From 8,627,900 reads, each 33 nucleotides in length, we assembled the genome into one scaffold of 76 ordered contiguous sequences containing 6,290,005 nucleotides, including one contig spanning 512,638 nucleotides, plus an additional 436 unordered contigs containing 416,897 nucleotides.
Our method includes a novel gene-boosting algorithm that uses amino acid sequences from predicted proteins to build a better assembly.
This study demonstrates the feasibility of very short read sequencing for the sequencing of bacterial genomes, particularly those for which a related species has been sequenced previously, and expands the potential application of this new technology to most known prokaryotic species.
Genome sequencing technology has moved into a new era with the introduction of extremely fast sequencing technologies that can produce over one billion base pairs of DNA in a single run.
Some of the fastest methods today, based on strategies such as cyclic reversible termination CITATION and ligation-based sequencing CITATION, produce the shortest read lengths, ranging from 15 50 bp.
These lengths are sufficient for resequencing projects, including efforts to sample the human population, but they have yet to prove as useful for sequencing of novel species.
The difficulty is that no existing assembly algorithms can accurately reconstruct a genome from such short reads CITATION .
The first published report of a bacterial genome sequence from short reads used pyrosequencing technology, which was able to generate reads averaging 110 bp.
That study CITATION demonstrated the feasibility of assembling the small bacterial genome of Mycoplasma genitalium from reads that covered the genome 40-fold.
This combination of coverage and read length allowed Margulies et al. to generate contiguous stretchs of DNA averaging 22.4 kilobases.
Results using pyrosequencing have improved steadily as read lengths have increased to 250 bp and longer, but the difficulty of de novo assembly has raised questions about the utility of alternative sequencing technologies those that produce reads shorter than 50 bp for genome sequencing projects.
Assembly of novel strains and species where the genome has not previously been sequenced from very short reads has proven more difficult, although simulation studies have indicated that it should be possible CITATION.
A recent study showed that a combination of pyrosequencing reads and paired-end sequencing could be used to assemble a 4 million base pair genome into just 139 contigs, linked together in 22 scaffolds CITATION.
Another recent effort used a hybrid strategy that mixed pyrosequencing and traditional Sanger sequencing to produce draft assemblies of marine microbes CITATION.
In contrast, the very short reads generated by the Solexa Sequence Analyzer have thus far been useful primarily for polymorphism discovery in the human genome, for resequencing and polymorphism discovery in Caernohabditis elegans CITATION, and for other applications such as ChIP-seq CITATION, which identifies genomic regions bound by transcription factors.
The very short reads currently 30 35 bp produced by CRT technologies such as Solexa present a far more difficult assembly problem.
Standard assembly algorithms such as Arachne CITATION, CITATION and Celera Assembler CITATION cannot process such short reads at all, spurring the development of several new algorithms designed for short reads, including SSAKE CITATION, Velvet CITATION, Edena CITATION, and ALLPATHS CITATION.
These latter methods can handle Solexa data, but they produce highly fragmented assemblies when provided with whole-genome data from a bacterial genome.
The inherent problem with very short reads is that every repetitive sequence longer than the read length causes breaks in the assembly.
To demonstrate the feasibility of assembling a bacterial genome from 33 bp reads, using related genomes to assist the process, we chose Pseudomonas aeruginosa strain PAb1, a highly virulent strain isolated from a frostbite patient.
P. aeruginosa is a ubiquitous environmental bacteria of clinical importance as the leading cause of gram-negative nosocomial infections CITATION, CITATION.
Several P. aeruginosa genomes have been sequenced previously, including two laboratory strains: PAO1, originally isolated from a wound, and PA14 isolated from a burn CITATION, CITATION.
PA14 and PAO1 are 99 percent identical across the 6.05 Mbp shared by both genomes, and their similarity to PAb1 allowed us to improve the assembly and provided a means to check its accuracy.
One of our goals in sequencing PAb1 was to identify genomic differences that contribute to its altered pathogenicity.
Here we report the assembly of P. aeruginosa PAb1 entirely from 33 bp reads, using a novel assembly strategy that takes advantage of related genomes and homologous protein sequences.
The assembly is of very high quality, comparable to or better than draft assemblies produced using earlier sequencing technologies.
This study shows that a novel bacterial genome can be sequenced entirely with very short read technology, without the use of paired-end sequences, and assembled into a high-quality genome.
Even at 40-fold coverage, the amount of sequence represents just one-quarter of a single sequencing run on a Solexa instrument, which brings the sequencing cost easily within the reach of most scientists.
By making all of our assembly software free and open source, we hope to further bring down the barriers to desktop whole-genome sequencing.
We generated 8,627,900 random shotgun reads from P. aeruginosa PAb1 using Solexa technology.
All reads were exactly 33 bp in length.
We used four distinct computational steps to assemble the genome of PAb1.
For the initial step, we used the comparative assembly algorithm AMOScmp CITATION, which aligns all reads to a reference genome, and then builds contigs based on these alignments.
The algorithm gains efficiency by avoiding the costly all-versus-all overlapping step, which is particularly difficult with very short reads due to the high incidence of false overlaps CITATION.
We modified AMOSCmp by tuning the MUMmer software CITATION, which is run within AMOScmp, to look for exact matches to the reference genome of at least 17 bp, allowing at most two mismatches in each read.
We found that careful trimming of the reads based on their matches to the reference produced better assemblies than un-trimmed reads.
The initial assembly used 7,500,501 reads, leaving 1,127,399 as singletons.
The PAb1 genome is closer to PA14 than to PAO1, and we therefore used PA14 as the primary reference for orienting the contigs.
Our second step was a novel enhancement to the comparative assembly strategy, in which we used multiple reference genomes.
We used the complete genomes of both PAO1 CITATION and PA14 CITATION separately to build multiple comparative assemblies, and found that PA14 produced the better assembly, comprising 2,053 contigs containing 6,206,284 bp.
The bulk of the sequence was contained in 157 contigs longer than 10 Kbp, which collectively covered 5,568,616 bp.
There were 331,364 bp in the PA14 genome that were not covered by the initial assembly, due to divergence between the two strains.
However, the gaps in the comparative assembly based on PAO1 occurred in different locations due to differences between the strains.
The best assembly based on PAO1 comprised 2797 contigs covering 6,043,652 bp.
We aligned the two assemblies to one another to identify locations where a contig in the PAO1-based assembly might span two or more contigs in the PA14-based assembly.
For each such case, we filled the gap with the sequence from the PAO1 assembly using the Minimus assembler CITATION to stitch together the contigs.
This algorithm closed 203 gaps, reducing the number of contigs to 1850, of which all but 305 were 200 bp.
The bulk of the genome, 5,949,162 bp, was contained in just 113 contigs of 10,000 bp or longer.
Note that the overlapping contigs between the two assemblies did not agree perfectly.
In order to produce a clean merged assembly, we re-mapped the reads to the contigs using AMOScmp to create consistent multi-alignments.
The third step used a novel algorithm, gene-boosted assembly.
For this step, we took the contigs from the previous step and identified protein-coding genes using our annotation pipeline, which is based on Glimmer CITATION and Blast CITATION.
Because amino acid sequences are much more conserved than nucleotide sequences, we were able to use the predicted protein sequences to fill gaps even where the DNA sequences diverged.
The annotation pipeline identified 5,769 proteins in the 305 longest contigs.
From the initial annotation, we identified those genes that extended beyond the ends of contigs or that spanned the gaps between contigs.
We extracted the amino acid sequences corresponding to these gap positions, with a small buffer sequence included on each side of each gap.
Next we used tblastn CITATION to align each protein sequence to all the unused reads translated in all 6 frames.
This step identified, for each gap, a small set of reads that would fill in the missing protein sequence, and the tblastn results provided initial locations for a multiple alignment.
We then used a new program, ABBA, to assemble the reads together with the flanking contigs and close the gaps.
This gene-boosted assembly protocol extended many contigs and closed 185 gaps, ranging in length from 14 1095 bp, reducing the number of long contigs to 120.
As a separate test, we conducted a gene-boosted assembly of PAb1 using only the annotated proteins from PA14 without any reference genomic sequence.
For this experiment, we aligned all the translated reads to each protein and used ABBA to assemble each one.
For 4,572 of the proteins, ABBA produced a single contig that covered the entire reference protein, and another 831 proteins assembled into a few contigs.
Thus 5,403 out of 5,602 of the PAb1 proteins can be assembled using a pure gene-boosting approach, and additional proteins would likely be assembled if we used a large set of proteins for boosting.
This demonstrates that in the absence of a closely related genome sequence, gene-boosted assembly can use protein sequences which diverge much more slowly than genomic DNA to assemble most of the genes of a new bacterial strain, although the results will lack global genome structure information.
The fourth step of our method identified any remaining DNA sequences that were unique to PAb1 and outside predicted gene regions.
We separately constructed pure de novo assemblies of the 8.6 million Solexa reads using SSAKE, Edena, and Velvet.
The Velvet assembly was the best of the three, creating 10,684 contigs, the longest being 16,239 bp.
We used MUMmer to align these contigs to the 120 long contigs in our scaffold from the previous step, and identified cases where de novo contigs spanned gaps or extended contigs.
This step allowed us to close 46 gaps, reducing the number of contigs in our main scaffold to 74.
After removing Velvet contigs that were already contained in our scaffold, we had 436 unplaced de novo contigs spanning 416,897 bp.
The longest unplaced contig was 10,493 bp.
When coordinating movements, the nervous system often has to decide how to distribute work across a number of redundant effectors.
Here, we show that humans solve this problem by trying to minimize both the variability of motor output and the effort involved.
In previous studies that investigated the temporal shape of movements, these two selective pressures, despite having very different theoretical implications, could not be distinguished; because noise in the motor system increases with the motor commands, minimization of effort or variability leads to very similar predictions.
When multiple effectors with different noise and effort characteristics have to be combined, however, these two cost terms can be dissociated.
Here, we measure the importance of variability and effort in coordination by studying how humans share force production between two fingers.
To capture variability, we identified the coefficient of variation of the index and little fingers.
For effort, we used the sum of squared forces and the sum of squared forces normalized by the maximum strength of each effector.
These terms were then used to predict the optimal force distribution for a task in which participants had to produce a target total force of 4 16 N, by pressing onto two isometric transducers using different combinations of fingers.
By comparing the predicted distribution across fingers to the actual distribution chosen by participants, we were able to estimate the relative importance of variability and effort of 1 7, with the unnormalized effort being most important.
Our results indicate that the nervous system uses multi-effector redundancy to minimize both the variability of the produced output and effort, although effort costs clearly outweighed variability costs.
The motor system is highly redundant: the same task can always be accomplished by many different sequences of motor commands CITATION.
Part of this redundancy is caused by the fact that there are often multiple muscles or effectors that can produce the same desired effect.
Thus, in the case of multi-effector redundancy, the brain has to choose how to distribute a given task across the set of muscles.
Despite the infinite number of possibilities, the motor system appears to prefer particular solutions.
For example, when moving the wrist, we combine the action of different forearm muscles in a predictable, cosine-tuning-like fashion CITATION.
To explain these regularities, we can ask why the brain is coordinating movements this way CITATION, i.e. we can propose a hypothetical cost function that the biological system minimized over the course of learning.
By determining the form of this cost function, and by assuming that the nervous system had sufficient exploration of the task dynamics to find an optimal solution, we can make testable predictions about how biological movements should be produced under a given task constraint.
A number of different cost functions for biological movements have been proposed CITATION CITATION.
Most of these studies have addressed movements for which the redundancy is temporal: here there may be only one muscle with the desired effect, but there are still many different ways of distributing the motor commands over the movement period.
For example, of all the possible shapes of arm or eye movement, the motor system consistently chooses a bell-shaped velocity profile CITATION .
Different components of cost functions can generally be divided into two classes: effort and variability costs.
Effort costs usually take a form of the sum of the squared muscle activations or motor commands CITATION, CITATION.
Alternatively, both Harris and Wolpert CITATION and Burdet and Milner CITATION proposed that the nervous system chooses the sequence of motor commands that minimizes the variability at the endpoint of a movement.
Under the assumption of signal-dependent noise, i.e. noise that increases monotonically with the motor command, this model can predict important characteristics of the control of both arm and eye.
While effort and variability costs have different theoretical implications for the learning mechanism that is involved in the optimization of motor behaviours, they make very similar predictions concerning the temporal shape of the optimal movement.
Indeed, it can be shown that the requirement to reduce variability under signal dependent noise leads to a term in the cost function that penalizes the sum of the squared motor commands over the movement, identically to the term commonly associated with effort CITATION.
Thus, for motor behaviours with mainly temporal redundancy, variability and effort costs are hard to dissociate.
For motor behaviours with multi-effector redundancy, however, the minimization of variability costs and the minimization of effort costs can lead to substantially different predictions concerning the distribution of work across effectors, because the noise and effort characteristics of different effectors can be partly independent.
Here we study how humans distribute work across different fingers when they have to produce a given target force.
By measuring the independent noise characteristics and the maximal force of the finger, we can dissociate the influence of variability and effort costs on coordination.
Numerous studies are currently underway to characterize the microbial communities inhabiting our world.
These studies aim to dramatically expand our understanding of the microbial biosphere and, more importantly, hope to reveal the secrets of the complex symbiotic relationship between us and our commensal bacterial microflora.
An important prerequisite for such discoveries are computational tools that are able to rapidly and accurately compare large datasets generated from complex bacterial communities to identify features that distinguish them.
We present a statistical method for comparing clinical metagenomic samples from two treatment populations on the basis of count data to detect differentially abundant features.
Our method, Metastats, employs the false discovery rate to improve specificity in high-complexity environments, and separately handles sparsely-sampled features using Fisher's exact test.
Under a variety of simulations, we show that Metastats performs well compared to previously used methods, and significantly outperforms other methods for features with sparse counts.
We demonstrate the utility of our method on several datasets including a 16S rRNA survey of obese and lean human gut microbiomes, COG functional profiles of infant and mature gut microbiomes, and bacterial and viral metabolic subsystem data inferred from random sequencing of 85 metagenomes.
The application of our method to the obesity dataset reveals differences between obese and lean subjects not reported in the original study.
For the COG and subsystem datasets, we provide the first statistically rigorous assessment of the differences between these populations.
The methods described in this paper are the first to address clinical metagenomic datasets comprising samples from multiple subjects.
Our methods are robust across datasets of varied complexity and sampling level.
While designed for metagenomic applications, our software can also be applied to digital gene expression studies.
A web server implementation of our methods and freely available source code can be found at LINK.
The increasing availability of high-throughput, inexpensive sequencing technologies has led to the birth of a new scientific field, metagenomics, encompassing large-scale analyses of microbial communities.
Broad sequencing of bacterial populations allows us a first glimpse at the many microbes that cannot be analyzed through traditional means.
Studies of environmental samples initially focused on targeted sequencing of individual genes, in particular the 16S subunit of ribosomal RNA CITATION CITATION, though more recent studies take advantage of high-throughput shotgun sequencing methods to assess not only the taxonomic composition, but also the functional capacity of a microbial community CITATION CITATION .
Several software tools have been developed in recent years for comparing different environments on the basis of sequence data.
DOTUR CITATION, Libshuff CITATION, -libshuff CITATION, SONs CITATION, MEGAN CITATION, UniFrac CITATION, and TreeClimber CITATION all focus on different aspects of such an analysis.
DOTUR clusters sequences into operational taxonomic units and provides estimates of the diversity of a microbial population thereby providing a coarse measure for comparing different communities.
SONs extends DOTUR with a statistic for estimating the similarity between two environments, specifically, the fraction of OTUs shared between two communities.
Libshuff and -libshuff provide a hypothesis test for deciding whether two communities are different, and TreeClimber and UniFrac frame this question in a phylogenetic context.
Note that these methods aim to assess whether, rather than how two communities differ.
The latter question is particularly important as we begin to analyze the contribution of the microbiome to human health.
Metagenomic analysis in clinical trials will require information at individual taxonomic levels to guide future experiments and treatments.
For example, we would like to identify bacteria whose presence or absence contributes to human disease and develop antibiotic or probiotic treatments.
This question was first addressed by Rodriguez-Brito et al. CITATION, who use bootstrapping to estimate the p-value associated with differences between the abundance of biological subsytems.
More recently, the software MEGAN of Huson et al. CITATION provides a graphical interface that allows users to compare the taxonomic composition of different environments.
Note that MEGAN is the only one among the programs mentioned above that can be applied to data other than that obtained from 16S rRNA surveys.
These tools share one common limitation they are all designed for comparing exactly two samples therefore have limited applicability in a clinical setting where the goal is to compare two treatment populations each comprising multiple samples.
In this paper, we describe a rigorous statistical approach for detecting differentially abundant features between clinical metagenomic datasets.
This method is applicable to both high-throughput metagenomic data and to 16S rRNA surveys.
Our approach extends statistical methods originally developed for microarray analysis.
Specifically, we adapt these methods to discrete count data and correct for sparse counts.
Our research was motivated by the increasing focus of metagenomic projects on clinical applications .
Note that a similar problem has been addressed in the context of digital gene expression studies.
Lu et al. CITATION employ an overdispersed log-linear model and Robinson and Smyth CITATION use a negative binomial distribution in the analysis of multiple SAGE libraries.
Both approaches can be applied to metagenomic datasets.
We compare our tool to these prior methodologies through comprehensive simulations, and demonstrate the performance of our approach by analyzing publicly available datasets, including 16S surveys of human gut microbiota and random sequencing-based functional surveys of infant and mature gut microbiomes and microbial and viral metagenomes.
The methods described in this paper have been implemented as a web server and are also available as free source-code from LINK.
One selection pressure shaping sequence evolution is the requirement that a protein fold with sufficient stability to perform its biological functions.
We present a conceptual framework that explains how this requirement causes the probability that a particular amino acid mutation is fixed during evolution to depend on its effect on protein stability.
We mathematically formalize this framework to develop a Bayesian approach for inferring the stability effects of individual mutations from homologous protein sequences of known phylogeny.
This approach is able to predict published experimentally measured mutational stability effects with an accuracy that exceeds both a state-of-the-art physicochemical modeling program and the sequence-based consensus approach.
As a further test, we use our phylogenetic inference approach to predict stabilizing mutations to influenza hemagglutinin.
We introduce these mutations into a temperature-sensitive influenza virus with a defect in its hemagglutinin gene and experimentally demonstrate that some of the mutations allow the virus to grow at higher temperatures.
Our work therefore describes a powerful new approach for predicting stabilizing mutations that can be successfully applied even to large, complex proteins such as hemagglutinin.
This approach also makes a mathematical link between phylogenetics and experimentally measurable protein properties, potentially paving the way for more accurate analyses of molecular evolution.
Knowledge of the impact of individual amino acid mutations on a protein's stability is valuable both for understanding the protein's natural evolution and for altering its properties for engineering purposes.
Experimentally measuring the effects of mutations on protein stability is a laborious process, so a variety of methods have been devised to predict these effects computationally.
Most existing methods rely on some type of physicochemical modeling of the mutation in the context of the protein's three-dimensional structure, often augmented by information gleaned from statistical analyses of protein sequences and structures.
These types of methods are moderately accurate at predicting the effects of mutations on the stabilities of small soluble proteins CITATION CITATION.
There is little or no published data evaluating their performance on the larger and more complex proteins that are frequently of greatest biological interest, although it might be expected to be worse given the greater difficulty of modeling larger structures.
An alternative approach to predicting the effects of mutations on protein stability utilizes the information contained in alignments of evolutionarily related sequences.
This approach, which was originally introduced by Steipe and coworkers CITATION, envisions an alignment of related sequences as representing a random sample of all possible sequences that fold into a given protein structure.
Based on a loose analogy with statistical physics, the frequency of a given residue in the sequence alignment is assumed to be an exponential function of its contribution to the protein's stability.
This is often called the consensus approach, since it always predicts that the most stabilizing mutation will be to the most commonly occurring residue.
The consensus approach has proven to be surprisingly successful, with a wide range of studies supporting the basic notion that stabilizing residues tend to appear more frequently in sequence alignments of homologous proteins CITATION CITATION .
But although it is often effective, the consensus approach suffers from an obvious conceptual flaw: alignments of natural proteins do not represent random samples of all possible sequences encoding a given structure, but instead are highly biased by evolutionary relationships.
A particular residue might occur frequently because it has arisen repeatedly through independent amino acid substitutions, or it might occur frequently simply because it occurred in the common ancestor of many related sequences in the alignment.
The sequence evolution of even distantly related protein homologs is non-ergodic, and so this problem will plague all natural sequence alignments.
Therefore, it would clearly be desirable to extract information about protein stability from sequence alignments using a method that accounts for evolutionary relationships.
In fact, there are already highly developed mathematical descriptions of the divergence of evolving protein sequences.
The widely used likelihood-based methods for inferring protein phylogenies employ explicit models of amino acid substitution to assess the likelihood of phylogenetic trees CITATION.
However, these methods make no effort to determine how selection for protein stability might manifest itself in the ultimate frequencies of amino acids in an alignment of evolved sequences.
Instead, in their simplest form, these phylogenetic methods simply assume that there is a universal average tendency for one particular amino acid to be substituted with another.
More advanced phylogenetic methods sometimes allow for different average substitution tendencies for different classes of protein residues CITATION CITATION.
Still other methods use simulations or other structure-based methods to derive site-specific substitution matrices for different positions in a protein CITATION CITATION.
However, none of these methods relate the substitution probabilities to the effects of mutations on experimentally measurable properties such as protein stability, nor do they provide a method for predicting the effects of the mutations from the protein phylogenies.
Here we present an approach for using protein phylogenies to infer the effects of amino acid mutations on protein stability.
We begin by describing a conceptual framework that quantitatively links a mutation's effect on protein stability to the probability that it will be fixed by evolution.
We then show how this framework can be used to calculate the likelihood of specific phylogenetic relationships given the stability effects of all possible amino acid mutations to a protein.
Our actual goal is to do the reverse, and infer the stability effects given a known protein phylogeny.
To robustly accomplish this, we use Bayesian inference with informative priors derived from an established physicochemical modeling program.
We compare the inferred stability effects to published experimental values for several proteins, and show that our method outperforms both the physicochemical modeling program and the consensus approach.
Finally, we use our method to predict mutations that increase the temperature-stability of influenza hemagglutinin, a complex multimeric membrane-bound glycoprotein for which stabilizing mutations have never previously been successfully predicted by any approach.
We introduce the predicted stabilizing mutations into hemagglutinin, and experimentally demonstrate that several of them increase the temperature-stability of the protein in the context of live influenza virus.
Overall, our work presents a unified framework for incorporating protein stability into phylogenetic analyses, as well as demonstrating a powerful new approach for predicting stabilizing mutations.
Identification of pathways involved in the structural transitions of biomolecular systems is often complicated by the transient nature of the conformations visited across energy barriers and the multiplicity of paths accessible in the multidimensional energy landscape.
This task becomes even more challenging in exploring molecular systems on the order of megadaltons.
Coarse-grained models that lend themselves to analytical solutions appear to be the only possible means of approaching such cases.
Motivated by the utility of elastic network models for describing the collective dynamics of biomolecular systems and by the growing theoretical and experimental evidence in support of the intrinsic accessibility of functional substates, we introduce a new method, adaptive anisotropic network model, for exploring functional transitions.
Application to bacterial chaperonin GroEL and comparisons with experimental data, results from action minimization algorithm, and previous simulations support the utility of aANM as a computationally efficient, yet physically plausible, tool for unraveling potential transition pathways sampled by large complexes/assemblies.
An important outcome is the assessment of the critical inter-residue interactions formed/broken near the transition state, most of which involve conserved residues.
Many proteins assume more than one functional conformation, stabilized by ligand binding or changes in environmental conditions.
A typical example is the bacterial chaperonin GroEL CITATION, a widely studied ATP-regulated molecular machine and member of heat shock protein Hsp60 family.
GroEL assists in unfolding and refolding misfolded or partially folded proteins CITATION CITATION.
It is composed of two back-to-back stacked rings, each containing seven subunits of 60 kDa.
Each subunit is, in turn, composed of three domains, equatorial, intermediate and apical.
GroEL works together with the co-chaperonin, GroES.
The activity of GroEL-GroES complex entails a series of allosteric transitions in structure, triggered by ATP binding and hydrolysis, described in Figure 1: In the absence of nucleotide binding, both rings assume the closed state, designated as T/T state for the two rings.
Cooperative binding of seven ATP molecules to the subunits in one of the rings, hereafter referred to as the cis ring, drives the conformational change of these subunits to the open state, thus leading to the R/T form of the cis/trans rings.
The R/T form exposes a number of hydrophobic residues at the apical domains of the cis ring subunits.
These groups attract the unfolded or partially folded peptide to be encapsulated in the cylindrical chamber following the attachment of GroES.
ATP hydrolysis provides the energy needed to process the substrate and leads to the state R /T. This process is terminated upon binding of seven ATP molecules to the adjoining ring, hence the term negative cooperative effect induced by ATP binding.
The structure with ADP and ATP molecules bound to the respective cis and trans rings favors the opening of the GroES cap and release of the peptide and ADP molecules to start a new cycle, this time, with the roles of the former cis and trans rings being inverted.
Of interest is to understand the molecular basis of the negative cooperative effect triggered upon binding of seven ATP molecules to the trans ring.
ATP binding induces in this case a structural change on the cap-binding region at a distance of 65.
Understanding the mechanism of this allosteric signaling is of fundamental importance because of the critical role chaperonins play in preventing aggregation and regulating folding vs. degradation events.
Several studies have been published on the allosteric pathways and dynamics of GroEL CITATION CITATION since the original determination of the ADP-bound complex CITATION.
These studies provide valuable insights into the successive states involved in the chaperonin cycle.
The elucidation of allosteric transition mechanisms has been a challenge, however, both experimentally and computationally, due to the transient nature of the high energy conformers between the states, the multiplicity of pathways, and the large size of this biomolecular system.
Several computational methods have been developed in the last two decades for exploring the structural transition pathways of biomolecules.
These include methods based on minimizing path-dependent functionals CITATION, CITATION, stochastic path integration CITATION, MaxFlux method for identifying the path of maximum flux CITATION, CITATION, nudged elastic band method CITATION, and the determination of temperature-independent reaction coordinates by action minimization CITATION CITATION.
Other groups resorted to targeted molecular dynamics simulations in the presence of holonomic constraints CITATION, CITATION CITATION, Monte Carlo- and MD-based methods for sampling ensembles of stochastic transition paths.
Yet, the identification of the transition state and accompanying conformational rearrangements are by and large inaccessible for systems of the order of megadaltons like GroEL.
Coarse-grained models and methods appear as the only tractable tools in such cases.
Perhaps the most comprehensive computational study of GroEL allosteric dynamics is that of Hyeon, Lorimer and Thirumalai, where the T R R transition has been examined by Brownian dynamics simulations using a state-dependent self-organized polymer model CITATION.
These simulations, performed for subunit dimers and heptamers, provided valuable insights on the heterogeneity of the transition pathways and the kinetics of salt bridges' formation/rupture at the successive transitions T R and R R .
Ryanodine receptors are ion channels that regulate muscle contraction by releasing calcium ions from intracellular stores into the cytoplasm.
Mutations in skeletal muscle RyR give rise to congenital diseases such as central core disease.
The absence of high-resolution structures of RyR1 has limited our understanding of channel function and disease mechanisms at the molecular level.
Here, we report a structural model of the pore-forming region of RyR1.
Molecular dynamics simulations show high ion binding to putative pore residues D4899, E4900, D4938, and D4945, which are experimentally known to be critical for channel conductance and selectivity.
We also observe preferential localization of Ca 2 over K in the selectivity filter of RyR1.
Simulations of RyR1-D4899Q mutant show a loss of preference to Ca 2 in the selectivity filter as seen experimentally.
Electrophysiological experiments on a central core disease mutant, RyR1-G4898R, show constitutively open channels that conduct K but not Ca 2.
Our simulations with G4898R likewise show a decrease in the preference of Ca 2 over K in the selectivity filter.
Together, the computational and experimental results shed light on ion conductance and selectivity of RyR1 at an atomistic level.
Muscle contraction upon excitation by nerve impulse is initiated by a rapid rise in cytoplasmic Ca 2.
In skeletal muscle, the rise in cytoplasmic Ca 2 is brought about by the opening of the ryanodine receptor, which releases Ca 2 from intracellular stores CITATION, CITATION.
RyRs are large tetrameric ion channels present in the membranes of endoplasmic/sarcoplasmic reticulum.
They have high conductance for monovalent and divalent cations, while being selective for divalent cations CITATION.
RyRs are important mediators of excitation-contraction coupling and congenital mutations of RyRs result in neuromuscular diseases such as malignant hypothermia and central core disease CITATION .
Although RyRs are physiologically important, the molecular basis of their function is poorly understood.
RyRs have unique properties such as their modes of selectivity and permeation not seen in other ion channels with known structures.
Next to the putative selectivity filter, there are two negatively charged residues in RyR1 that are essential for normal selectivity and conductance CITATION.
K channels have an analogous selectivity filter, but in contrast to RyR1, have only one adjacent negative residue that is not even conserved while other Ca 2 channels have only one conserved negative residue in the equivalent position CITATION.
In the selectivity filter, mutations result in non-functional channels CITATION leading to CCD.
A structural model of the pore region that would reveal the location and function of these residues will be useful in understanding the role of these residues in channel function.
An early model of RyR ion permeation postulated potential barriers within the pore corresponding to three putative binding sites CITATION.
Without any knowledge of the structure of the pore, the model was able to quantitatively reproduce conductance data of various ions.
A PNP-DFT model CITATION accurately modeled the role of residues D4899 and E4900 in RyR1 in generating the high ion conductances of RyRs established by mutagenesis CITATION, CITATION.
Selectivity was attributed to charge-space competition, as Ca 2 could accommodate the most charge in least space compared to K. However, since the channel model used in these simulations relied on a fixed structure, it could not predict changes due to mutations that potentially alter the structure of the channel.
Additionally, there are two homology models of the RyR pore region CITATION, CITATION based on KcsA, a bacterial K channel whose solution structure is known CITATION.
Shah et al. CITATION used bioinformatics approaches to construct models for RyR and the related inositol triphosphate channel.
The luminal loop in their RyR model begins at 4890G resulting in the selectivity filter being 4890GVRAGG.
However, mutagenesis has shown that residues I4897, G4898, D4899 and E4900 are important for channel conductance and selectivity, which suggests that they are part of the conduction pathway of RyR1 resulting in the predicted selectivity filter being 4894GGGIGDE.
Welch et al. also constructed a homology model for the cardiac ryanodine receptor using the structure of the KcsA channel CITATION and performed simulations to identify residues important for channel function.
Their simulations failed to identify D4899 as an important residue for ion permeation contrary to what has been shown experimentally CITATION.
Furthermore, cryo-electron microscopy of RyR1 revealed significant differences between the pore region of KcsA and RyR1 CITATION .
Experimental structure determinations of the RyRs have been mainly performed by cryo-EM CITATION CITATION.
These studies revealed conformational changes that accompany channel opening CITATION and the binding sites of various effectors of RyRs CITATION CITATION.
Cryo-EM has a resolution of 10 25 and thus is able to provide only limited structural information regarding the pore structure.
Samso et al. CITATION manually docked the KcsA pore structure into the transmembrane region of their cryoEM map of the intact closed RyR1.
Furthermore, they predicted the presence of at least 6 transmembrane helices from simple volumetric constraints.
Ludtke et al. CITATION identified several secondary structure elements in their 10 resolution cryo-EM map of the closed RyR1.
The pore-forming region as visualized by Ludtke et al. consists of a long inner helix made up of 31 residues and a pore helix made up of 15 residues that are presumably connected by a long luminal loop made up of 24 residues.
Since the structure is derived from cryo-EM, the positions of pore residues' side chains and the structure of loops connecting the helices are unknown.
We build a molecular model of the pore region of RyR1 based on their cryo-EM study by adding the luminal loop and the missing side chains of residues forming the helices of the pore.
Furthermore, in our molecular dynamics simulations we examine the interactions of the pore region with mono- and divalent cations known to permeate the channel .
We have developed a multi-scale biophysical electromechanics model of the rat left ventricle at room temperature.
This model has been applied to investigate the relative roles of cellular scale length dependent regulators of tension generation on the transduction of work from the cell to whole organ pump function.
Specifically, the role of the length dependent Ca 2 sensitivity of tension, filament overlap tension dependence, velocity dependence of tension, and tension dependent binding of Ca 2 to Troponin C on metrics of efficient transduction of work and stress and strain homogeneity were predicted by performing simulations in the absence of each of these feedback mechanisms.
The length dependent Ca 50 and the filament overlap, which make up the Frank-Starling Law, were found to be the two dominant regulators of the efficient transduction of work.
Analyzing the fiber velocity field in the absence of the Frank-Starling mechanisms showed that the decreased efficiency in the transduction of work in the absence of filament overlap effects was caused by increased post systolic shortening, whereas the decreased efficiency in the absence of length dependent Ca 50 was caused by an inversion in the regional distribution of strain.
Contraction of the heart is a fundamental whole organ phenomenon driven by cellular mechanisms.
With each beat the myocytes in the heart generate tension and relax.
This local cellular scale tension is transduced into a coordinated global whole heart deformation resulting in an effective, organized and efficient system level pump function.
Fundamental to achieving this efficient transudation of work is the integration of organ, tissue and cellular scale mechanisms.
However, while efficiency is important in the heart, the role and relative importance of the underlying mechanisms responsible for achieving the efficient transduction of work from the cell to the organ remains unclear.
In the healthy heart, structural heterogeneities in the morphology, electrophysiology, metabolic and neural mechanisms provide a stable physiological framework that facilitates a coordinated contraction CITATION resulting in the ETW.
However, over shorter time scales, sub cellular mechanisms are the most likely candidates for regulating the ETW in the face of dynamic variation in cardiac demand.
Specifically, the sarcomeres themselves contain tension and deformation feedback mechanisms that regulate the development of active tension based on the local tension, strain and strain rate.
These provide a regulatory process to modulate deformation and tension signals experienced by the cell into a coordinated global response CITATION CITATION .
The four major TDF mechanisms are length dependent changes in Ca 2 sensitivity CITATION, filament overlap CITATION, tension dependent binding of Ca 2 to troponin C CITATION and velocity dependent cross bridge kinetics CITATION.
TDF mechanisms 1 and 2 are characterised by the length dependent changes in the steady state force Ca 2 relationship, which is routinely described by a Hill curve CITATION, CITATION.
Length dependent changes in Ca 50 are measured by the decreased concentration of Ca 2 required to produce half maximal activation as the muscle increases in length.
Length dependent changes in the filament overlap result in active tension increasing as the muscle increases in length.
Ca 2 binding to TnC acts as a switch activating tension generation.
As crossbridges bind to generate tension they increase the affinity of Ca 2 to TnC causing more Ca 2 to bind, which results in the generation of more tension.
The velocity dependence of tension can be described by a transient and stable component.
The transient component is characterised by the multiphase tension response to step changes in length and the stable component is characterised by the tension during contraction at a constant velocity.
In general as the velocity of contraction increases the active tension decreases.
These four mechanisms provide both positive and negative feedback for tension development and are fundamental to the functioning of the heart, yet their relative roles, if any, in the ETW have not been investigated.
This is in part due to the experimental challenges in studying subcellular function in whole heart preparations CITATION and the modelling challenges in performing biophysical whole organ coupled electromechanics simulations CITATION, CITATION.
Recent advances in computer power and coupling methods CITATION now allow the simulation of strongly coupled multi-scale electromechanical models of the left ventricle.
These models contain explicit biophysical representations of cellular electrophysiology, Ca 2 dynamics, tension generation, deformation and the multiple feedback loops that operate between each of these systems.
In this study we analyse the transduction of local cellular scale work into whole organ pressure-volume work in the heart using computational modelling.
Using the definitions of Hill CITATION for positive and negative work, we propose a new metric to quantify the ETW during each phase of the contraction cycle as the ratio of positive work to total work.
To isolate and quantify the role of TDF in the transduction of cellular work into whole organ pump function over a heart beat we have developed a model of the rat left ventricle, at room temperature, that incorporates the TDF mechanisms.
The model contains a biophysical electromechanical rat myocyte model CITATION, transversely isotropic constitutive law CITATION and heterogeneous fiber orientation CITATION.
By comparing the ETW over each phase of the heart beat in the absence of each of the TDF mechanisms we aim to quantify the effect of each of the TDF mechanisms.
Halobacterium salinarum is a bioenergetically flexible, halophilic microorganism that can generate energy by respiration, photosynthesis, and the fermentation of arginine.
In a previous study, using a genome-scale metabolic model, we have shown that the archaeon unexpectedly degrades essential amino acids under aerobic conditions, a behavior that can lead to the termination of growth earlier than necessary.
Here, we further integratively investigate energy generation, nutrient utilization, and biomass production using an extended methodology that accounts for dynamically changing transport patterns, including those that arise from interactions among the supplied metabolites.
Moreover, we widen the scope of our analysis to include phototrophic conditions to explore the interplay between different bioenergetic modes.
Surprisingly, we found that cells also degrade essential amino acids even during phototropy, when energy should already be abundant.
We also found that under both conditions considerable amounts of nutrients that were taken up were neither incorporated into the biomass nor used as respiratory substrates, implying the considerable production and accumulation of several metabolites in the medium.
Some of these are likely the products of forms of overflow metabolism.
In addition, our results also show that arginine fermentation, contrary to what is typically assumed, occurs simultaneously with respiration and photosynthesis and can contribute energy in levels that are comparable to the primary bioenergetic modes, if not more.
These findings portray a picture that the organism takes an approach toward growth that favors the here and now, even at the cost of longer-term concerns.
We believe that the seemingly greedy behavior exhibited actually consists of adaptations by the organism to its natural environments, where nutrients are not only irregularly available but may altogether be absent for extended periods that may span several years.
Such a setting probably predisposed the cells to grow as much as possible when the conditions become favorable.
Halobacterium salinarum is a halophilic archaeon that thrives in extremely saline environments with salt concentrations reaching 4 M or higher.
The organism is perhaps most well known for its retinal-protein bacteriorhodopsin, which is a light-driven proton pump.
BR is the only known nonchlorophyll structure that allows photosynthesis CITATION.
It is currently being developed for applications in optical security CITATION, optical data storage CITATION, and holography CITATION.
Accordingly, H. salinarum's photosynthetic capabilities are its most well-studied aspects.
For example, the 3D structure of BR has been resolved, and its complete catalytic cycle elucidated at the molecular level.
However, despite the focus on BR, photosynthesis is not the only means by which H. salinarum can generate energy.
Respiration CITATION, CITATION as well as the fermentation of arginine CITATION, CITATION are other mechanisms utilized by the organism.
This bioenergetic flexibility makes the archaeon a good model system for investigating the interplay between different energy production modes.
H. salinarum is also one of the few reported organisms that can use potassium gradients for long term energy storage in a battery-like manner CITATION .
The metabolic network of an organism can be reconstructed from genomic, biochemical, and physiological data CITATION, CITATION, CITATION.
This network consists of the known and hypothesized reactions that take place within the organism, and is considered to be on a genome-scale when most or all of the genes with known metabolic function are included CITATION.
We, in a previous study, have reconstructed and proposed such a network for Halobacterium salinarum CITATION.
In addition to the immediate information gained from metabolic reconstructions, these networks can be analyzed to gain insights on emergent system properties through the use of appropriate computational methods.
In this respect, the constraints-based framework has emerged as an important and convenient tool for modeling such systems because it does not require the detailed information typically required by full kinetic models.
Rather, constraints-based models require only generally available physicochemical information such as stoichiometry, reversibility, energy balance, and, when available, reaction velocities CITATION, CITATION, CITATION .
One of the methods available under the constraints-based framework is Flux Balance Analysis.
Essentially, FBA uses linear optimization to find a flux distribution that maximizes a particular objective function, e.g., growth rate or ATP production CITATION, CITATION.
It has been shown that such optimality principles, within limits and under defined conditions, can describe the operation of metabolic networks, including the prediction of internal fluxes CITATION, CITATION.
Extensions to FBA include hybrid models that introduce some degree of dynamics through the integration of time-variant input rates to the static model CITATION, CITATION, CITATION .
Our aim in this study is two-fold.
First, we set out to investigate the interplay between energy generation, nutrient utilization and biomass production under different bioenergetic modes.
Second, we also analyzed the relationships between the different energy producing mechanisms of respiration, photosynthesis and fermentation themselves, which are typically examined individually.
To achieve these, we used a genome-scale metabolic network that connects the different aspects.
Our results include several findings that are contrary to assumptions which are typically made; particularly with respect to the utilization of nutrients, and how the bioenergetic modes operate.
From a more methodological perspective, we also sought out to extend the existing framework for hybrid genome-scale metabolic models to handle biological systems where nutrient utilization and growth rates vary with time.
Such changes in nutrient consumption, for example, can be the result of the differences between growth phases, or can arise from the interactions between the supplied metabolites.
We demonstrate that the extended methodology not only accounts for such dynamics, but in several instances actually led to the identification of the underlying causes.
An increasing number of cis-regulatory RNA elements have been found to regulate gene expression post-transcriptionally in various biological processes in bacterial systems.
Effective computational tools for large-scale identification of novel regulatory RNAs are strongly desired to facilitate our exploration of gene regulation mechanisms and regulatory networks.
We present a new computational program named RSSVM, which employs Support Vector Machines for efficient identification of functional RNA motifs from random RNA secondary structures.
RSSVM uses a set of distinctive features to represent the common RNA secondary structure and structural alignment predicted by RNA Sampler, a tool for accurate common RNA secondary structure prediction, and is trained with functional RNAs from a variety of bacterial RNA motif/gene families covering a wide range of sequence identities.
When tested on a large number of known and random RNA motifs, RSSVM shows a significantly higher sensitivity than other leading RNA identification programs while maintaining the same false positive rate.
RSSVM performs particularly well on sets with low sequence identities.
The combination of RNA Sampler and RSSVM provides a new, fast, and efficient pipeline for large-scale discovery of regulatory RNA motifs.
We applied RSSVM to multiple Shewanella genomes and identified putative regulatory RNA motifs in the 5 untranslated regions in S. oneidensis, an important bacterial organism with extraordinary respiratory and metal reducing abilities and great potential for bioremediation and alternative energy generation.
From 1002 sets of 5 -UTRs of orthologous operons, we identified 166 putative regulatory RNA motifs, including 17 of the 19 known RNA motifs from Rfam, an additional 21 RNA motifs that are supported by literature evidence, 72 RNA motifs overlapping predicted transcription terminators or attenuators, and other candidate regulatory RNA motifs.
Our study provides a list of promising novel regulatory RNA motifs potentially involved in post-transcriptional gene regulation.
Combined with the previous cis-regulatory DNA motif study in S. oneidensis, this genome-wide discovery of cis-regulatory RNA motifs may offer more comprehensive views of gene regulation at a different level in this organism.
The RSSVM software, predictions, and analysis results on Shewanella genomes are available at LINK.
RNA is remarkably versatile CITATION, CITATION, acting not only as messengers to transfer genetic information from DNA to protein, but also as critical structural components CITATION and catalytic enzymes CITATION, CITATION in the cell.
More intriguingly, non-coding RNAs have been found to play important regulatory roles.
They can mediate gene expression post-transcriptionally in two ways: one is to serve as trans-acting antisense RNAs, such as microRNAs, which hybridize with target mRNAs to silence their expression CITATION, CITATION ; the other is to form structural cis-elements in the mRNAs, such as riboswitches, which regulate gene expression by mediating transcription termination or translation initiation CITATION, CITATION.
The regulatory roles of ncRNAs make them promising drug targets CITATION and efficient tools for drug development and gene therapy CITATION, CITATION .
In the past a few years, many cis-regulatory RNA structural motifs have been identified in prokaryotes CITATION CITATION.
They are often located in the 5 untranslated regions of the mRNAs and can sense or interact with cognate factors, including proteins, RNAs, small metabolites, or even temperature changes, to mediate transcription attenuation CITATION, translation initiation CITATION, or mRNA stability CITATION.
The functions of the regulatory RNAs are intrinsically tied to their secondary structures, mostly recognizable as stem-loops or pseudoknots.
Moreover, regulatory RNAs are often conserved during evolution: similar regulatory RNA elements can be shared by multiple co-regulated genes in the same metabolic pathway, or conserved in orthologous genes across closely related species CITATION .
Experimental screenings CITATION for cis-regulatory RNAs are highly labor and time consuming.
As demonstrated by previous studies CITATION, CITATION, a parallel way is to find good candidates computationally followed by targeted experimental validation.
Because functional regulatory RNAs are often evolutionarily conserved in their secondary structures, we can identify them by finding significantly conserved RNA secondary structures in orthologous genes across closely related species.
To accomplish this, we need two tools: one is to accurately predict common RNA secondary structures in multiple related sequences, and the other is to distinguish functional RNA secondary structures from random foldings of RNA sequences.
A number of algorithms have been developed for common RNA secondary structure prediction, such as RNAalifold CITATION, Dynalign CITATION, comRNA CITATION, CMFinder CITATION and FoldAlign CITATION, CITATION.
We recently published a new algorithm, called RNA Sampler CITATION, for predicting common RNA secondary structures and structural alignments in multiple sequences.
Both our study CITATION and independent studies from other researchers CITATION, CITATION have demonstrated that RNA Sampler provides more accurate structure predictions and generates better structural alignments on sequences of a wide range of identities than other leading software for similar purposes.
Moreover, RNA Sampler runs fast and is feasible for common RNA secondary structure prediction on the genome scale.
Studies have shown that for a single sequence RNA secondary structure alone is not sufficient to distinguish functional RNA from random sequence CITATION, CITATION.
However, with the availability of multiple RNA sequences from related species, comparative genomics approaches provide additional power to identify functional RNA structures.
One strategy is to design a scoring function for the predicted RNA secondary structures and examine the difference between the score distributions of real structures and randomly permutated structures, as employed by the RNA identification pipeline based on CMfinder CITATION or comRNA CITATION.
But one limitation of such an approach is that the user needs to generate a large number of random sequence sets for each set of real sequences and doing structure predictions on these permutated sequence sets is usually time consuming.
Besides, it can be difficult to find a score cutoff to make the call between functional and random RNAs.
Another strategy is to train a classification model based on features that can distinguish common structures of known functional RNAs from those of random RNAs and then apply the classification model on the newly predicted common RNA structures to determine whether they are of functional or random RNAs.
RNA classification algorithms employing this strategy include QRNA, RNAz and Dynalign LIBSVM.
QRNA CITATION classifies a pairwise sequence alignment by the posterior probabilities of three probabilistic models, RNA, Coding and Null.
RNAz CITATION and Dynalign LIBSVM CITATION both employ support vector machines to build the classification models.
To train a classification model, the developer still needs to generate a large number of random sequence sets as the negative training sets and make structure predictions on them, but once the classification model is trained, the user can directly utilize the model to identify functional RNAs without the need to generate, and perform folding of, random sequences.
The type of sequences used to train the classification models is essential to their classification performance on new sequences.
QRNA and Dynalign LIBSVM only use tRNAs and rRNAs in their training on RNA structures, and RNAz is trained on multiple RNA gene/motif families from the Rfam database but only uses sequence sets with high identities.
To avoid overfitting the classification model to specific classes of RNAs, using training sets that cover a wide range of sequence identities and a variety of RNA families is more desirable.
In addition, training the classification model using more accurately predicted RNA common structures and alignments is advantageous for more sensitive classification of functional RNAs from random ones.
RNAz uses RNAalifold CITATION for common RNA structure prediction.
When using sequence alignments as its input, RNAalifold performs poorly in predicting RNA structures on sequence sets of low identities CITATION.
The structure prediction accuracy of RNAalifold may be improved by using structural alignments, but RNAz might need to be re-trained to use structural alignments.
In this paper, we present a new SVM based functional RNA identifier named RSSVM.
RSSVM applies a set of features to represent common RNA secondary structures and structural alignments generated by RNA Sampler, which predicts RNA structures more accurately than other approaches CITATION CITATION.
RSSVM is trained with RNA sets with a wide range of sequence identities from all bacterial RNA motif/gene families in the Rfam database CITATION.
RSSVM is more sensitive in identifying real functional RNAs than other leading RNA classification programs, including RNAz, Dynalign LIBSVM and QRNA, at the same false positive rate.
We applied RSSVM on multiple Shewanella genomes to identify putative cis-regulatory RNA motifs in the 5 -UTRs of orthologous genes.
Shewanella oneidensis is a facultative, gram-negative -proteobacterium.
It has extraordinary abilities to use a wide variety of metals and organic molecules as electron acceptors in respiration CITATION CITATION, which gives it great potential to be applied in bioremediation of both metal and organic pollutants.
The complete genomic sequences of Shewanella oneidensis and multiple other Shewanella species provide good resources for discovering cis-regulatory RNAs using comparative genomics approaches.
Combining with the recent predictions of putative DNA cis-regulatory motifs in S. oneidensis CITATION, we will have a more complete view of gene regulation in S. oneidensis at different regulation levels.
It is well established that various cortical regions can implement a wide array of neural processes, yet the mechanisms which integrate these processes into behavior-producing, brain-scale activity remain elusive.
We propose that an important role in this respect might be played by executive structures controlling the traffic of information between the cortical regions involved.
To illustrate this hypothesis, we present a neural network model comprising a set of interconnected structures harboring stimulus-related activity, and a group of executive units with task-related activity patterns that manage the information flowing between them.
The resulting dynamics allows the network to perform the dual task of either retaining an image during a delay, or recalling from this image another one that has been associated with it during training.
The model reproduces behavioral and electrophysiological data gathered on the inferior temporal and prefrontal cortices of primates performing these same tasks.
It also makes predictions on how neural activity coding for the recall of the image associated with the sample emerges and becomes prospective during the training phase.
The network dynamics proves to be very stable against perturbations, and it exhibits signs of scale-invariant organization and cooperativity.
The present network represents a possible neural implementation for active, top-down, prospective memory retrieval in primates.
The model suggests that brain activity leading to performance of cognitive tasks might be organized in modular fashion, simple neural functions becoming integrated into more complex behavior by executive structures harbored in prefrontal cortex and/or basal ganglia.
An important unanswered question in neurobiology is how neural activity organizes itself to produce coherent behavior.
Lesion, electrophysiological, and imaging studies targeting specific cognitive functions have provided very detailed insights into how different regions of the brain contribute to behavior.
More specifically, they have shown the role of various regions of cortex in implementing functions such as visual representation of stimuli CITATION, CITATION, sustainment of the memory of a stimulus CITATION, representation of tasks CITATION or abstract rules CITATION, CITATION, selection of a response among a set of possibilities CITATION, CITATION, shielding of memory from distractions CITATION, and planning of movements CITATION.
However, it remains unclear how the different regions of cortex interact together to build even the simplest behavior.
Indeed, even the elementary action of looking at an object and preparing to reach for it requires a cascade of neural processes that have to take place in the right order and with the proper timing to be successful.
Here, we propose that adequate behavior can be generated from the set of functions mentioned above if the information these cortical regions contain and exchange with each other is managed by executive or control structures in a manner suiting the task at hand.
Brain-scale activity coding for integrated behavior might then be constructed by these executive units, from a repertoire of simple neurocognitive functions, which would be selected, recruited, ordered, and synchronized to implement the necessary neural computations.
To illustrate this hypothesis, we present a neural network model able to pass the mixed-delayed response task, which was introduced to study memory retrieval in the monkey using visual associations CITATION, CITATION, CITATION.
This task consists of randomly mixed delayed-matching to sample and delayed-pair association trials CITATION, CITATION, which require that the subject either maintain the memory of an image during a delay, or remember an image associated with it, respectively.
Which type of trial is to be performed is signaled to the subject during the delay.
The network contains structures harboring image-related activities, each of them implementing one of the elementary functions crucial for task execution: visual representation, working memory, and planning memory.
These areas are complemented by executive units, which control the activity held in these layers and regulate the information flow between them.
The neural computations necessary to perform the tasks are encoded in the firing patterns of these control units, which are coordinated and depend strongly both on the trial type and the phase of the trial.
The resulting dynamics allows the network to be trained for the DMS and then for the DPA task while reproducing electrophysiological data gathered on the inferior temporal CITATION, CITATION and prefrontal CITATION cortices of monkeys performing similar tasks.
In 50 percent of progressing HIV-1 patients, CXCR4-tropic virus emerges late in infection, often overtaking CCR5-tropic virus as the dominant viral strain.
This phenotypic switch is strongly associated with rapidly declining CD4 T cell counts and AIDS onset, yet its causes remain unknown.
Here, we analyze a mathematical model for the mechanism of X4 emergence in late-stage HIV infection and use this analysis to evaluate the utility of a promising new class of antiretroviral drugs CCR5 inhibitors in dual R5, X4 infection.
The model shows that the R5-to-X4 switch occurs as CD4 T cell activation levels increase above a threshold and as CD4 T cell counts decrease below a threshold during late-stage HIV infection.
Importantly, the model also shows that highly active antiretroviral therapy can inhibit X4 emergence but that monotherapy with CCR5 blockers can accelerate X4 onset and immunodeficiency if X4 infection of memory CD4 T cells occurs at a high rate.
Fortunately, when CXCR4 blockers or HAART are used in conjunction with CCR5 blockers, this risk of accelerated immunodeficiency is eliminated.
The results suggest that CCR5 blockers will be more effective when used in combination with CXCR4 blockers and caution against CCR5 blockers in the absence of an effective HAART regimen or during HAART failure.
The extent to which self-adopted or intervention-related changes in behaviors affect the course of epidemics remains a key issue for outbreak control.
This study attempted to quantify the effect of such changes on the risk of infection in different settings, i.e., the community and hospitals.
The 2002 2003 severe acute respiratory syndrome outbreak in Hong Kong, where 27 percent of cases were healthcare workers, was used as an example.
A stochastic compartmental SEIR model was used: the population was split into healthcare workers, hospitalized people and general population.
Super spreading events were taken into account in the model.
The temporal evolutions of the daily effective contact rates in the community and hospitals were modeled with smooth functions.
Data augmentation techniques and Markov chain Monte Carlo methods were applied to estimate SARS epidemiological parameters.
In particular, estimates of daily reproduction numbers were provided for each subpopulation.
The average duration of the SARS infectious period was estimated to be 9.3 days.
The model was able to disentangle the impact of the two SSEs from background transmission rates.
The effective contact rates, which were estimated on a daily basis, decreased with time, reaching zero inside hospitals.
This observation suggests that public health measures and possible changes in individual behaviors effectively reduced transmission, especially in hospitals.
The temporal patterns of reproduction numbers were similar for healthcare workers and the general population, indicating that on average, an infectious healthcare worker did not infect more people than any other infectious person.
We provide a general method to estimate time dependence of parameters in structured epidemic models, which enables investigation of the impact of control measures and behavioral changes in different settings.
Emerging infectious diseases have been defined as, infections that have newly appeared in a population or have existed previously but are rapidly increasing in incidence or geographic range.
CITATION Several features may make them particularly threatening.
First, recognizing the disease can be difficult when the first cases appear, especially when the symptoms are non-specific.
Second, no vaccine or specific treatment may be known initially.
Moreover, heterogeneities in disease transmission may create high-risk groups, such as healthcare workers CITATION CITATION and high-risk geographical areas, thereby dramatically enhancing the impact of the outbreak CITATION .
The 2003 severe acute respiratory syndrome outbreak in Hong Kong is remarkably illustrative of the above issues: symptoms were similar to pneumonia CITATION ; the incubation period was long enough for local and international transmission to occur CITATION ; no vaccine or treatment was available; as much as 21 percent of cases worldwide were healthcare workers CITATION.
The outbreak also demonstrated the possible existence of super-spreading events CITATION, during which a few infectious individuals contaminated a high number of secondary cases.
Hong Kong had two SSEs: the first occurred in Hospital X around March 3 and led to about 125 cases CITATION ; the second occurred in Housing Estate Y on March 19, and led to over 300 cases CITATION, CITATION.
Despite its particularly threatening features, the outbreak was brought under control.
In this context, once the epidemic is detected, spontaneous changes in behavior will occur, and non-pharmacological measures are usually initiated to control the outbreak.
The resulting effects of these two phenomena on disease transmission is not easily quantified.
The effective contact rate, which reflects the combined influences of social proximity and the probability of infection through each contact, is an essential determinant of disease spread.
Our aim was to estimate the temporal variation of this parameter in the community and hospitals, over the course of the outbreak.
Previously published mathematical models of parameter estimation addressed the issues of temporal variability CITATION, CITATION or social heterogeneity CITATION, CITATION.
Here we present an approach that deals with both issues, together with the occurrence of SSEs.
Then the method is applied to the 2003 SARS epidemic in Hong Kong .
Microarray comparative genomic hybridisation provides an estimate of the relative abundance of genomic DNA taken from comparator and reference organisms by hybridisation to a microarray containing probes that represent sequences from the reference organism.
The experimental method is used in a number of biological applications, including the detection of human chromosomal aberrations, and in comparative genomic analysis of bacterial strains, but optimisation of the analysis is desirable in each problem domain.
We present a method for analysis of bacterial aCGH data that encodes spatial information from the reference genome in a hidden Markov model.
This technique is the first such method to be validated in comparisons of sequenced bacteria that diverge at the strain and at the genus level: Pectobacterium atrosepticum SCRI1043 and Dickeya dadantii 3937 ; and Lactococcus lactis subsp.
lactis IL1403 and L. lactis subsp.
cremoris MG1363.
In all cases our method is found to outperform common and widely used aCGH analysis methods that do not incorporate spatial information.
This analysis is applied to comparisons between commercially important plant pathogenic soft-rotting enterobacteria Pba1043, P. atrosepticum SCRI1039, P. carotovorum 193, and Dda3937.
Our analysis indicates that it should not be assumed that hybridisation strength is a reliable proxy for sequence identity in aCGH experiments, and robustly extends the applicability of aCGH to bacterial comparisons at the genus level.
Our results in the SRE further provide evidence for a dynamic, plastic accessory genome, revealing major genomic islands encoding gene products that provide insight into, and may play a direct role in determining, variation amongst the SRE in terms of their environmental survival, host range and aetiology, such as phytotoxin synthesis, multidrug resistance, and nitrogen fixation.
Microarray comparative genomic hybridisation provides an estimate of the relative abundance of genomic DNA taken from comparator and reference organisms by hybridisation to a microarray containing probes that represent sequences from the reference organism.
This method has been used in a number of biological applications, including the detection of human chromosomal aberrations CITATION, CITATION ; comparisons of bacterial human pathogens CITATION CITATION ; bacterial plant pathogens CITATION, CITATION ; industrially-important bacteria CITATION ; and comparative transcriptomics of Xenopus laevis CITATION .
Numerous algorithms and software packages have been applied to the analysis of this aCGH data in prokaryotes.
The majority of these partition reference organism sequences into two mutually exclusive classes: sequences that are present and sequences that are absent or divergent in the comparator organism CITATION, CITATION, CITATION, CITATION.
Observed hybridisation data are, in each case, assumed to be reliable proxies for these classes.
In this manuscript we describe and apply an improved method for analysis of aCGH data from bacterial genome comparisons.
This method incorporates spatial information about CDS location on the reference genome in a hidden Markov model.
This spatial information is expected to capture pertinent biological and evolutionary information, such as operon structure, and regions of lateral gene transfer.
Our approach differs from previously proposed, and widely-used, methods applied to bacterial aCGH, such as GACK and MPP, that consider hybridisation intensities of each reference probe as measurements that are independent of their genomic location CITATION, CITATION, and is thus more similar to methods such as ArrayLeaRNA CITATION, which incorporates predicted operon structure into interpretations of microarray expression data, for a restricted set of organisms.
We compare the relative performance of our method to commonly used bacterial aCGH analysis algorithms and software.
We demonstrate that several assumptions of common bacterial aCGH analysis methods concerning the relationship between observed hybridisation scores and ratios and the presence or absence of a reference CDS in the comparator organism do not always hold strongly, and that this is particularly the case for more distantly-related organisms.
Our data in particular do not support a distinction between present and absent or divergent classes of sequence, but rather between those sequences in the reference organism that do, and those that do not, have putative orthologues in the comparator genome.
We find that the HMM is a better predictor of reference sequences that do not have a putative orthologue in the comparator organism than the other methods tested.
Spatial organisation of sequences on the reference genome has previously been incorporated into methods applied to aCGH analyses of copy number variation in human genomes.
This has been represented using HMM CITATION and segmentation methods CITATION.
Simple smoothing methods have also been used to identify breakpoints in this data CITATION.
However, the problem domain of human copy number aCGH differs from the problem domain of bacterial comparative genomic aCGH.
To the best of our knowledge, this study describes the first application of a method incorporating such spatial information to aCGH for comparative genomics of unsequenced bacteria, and the first demonstration of the applicability of the technique as a whole across bacterial genera.
Pectobacterium atrosepticum, Pectobacterium carotovorum, and Dickeya spp.
are plant pathogenic soft-rotting enterobacteria that share a common ancestor.
Despite their many similarities, these commercially significant pathogens differ in their host range, geographical distribution, aetiology and environmental persistence CITATION.
The molecular origins of these differences are not well understood, but this ecological flexibility is likely indicative of a dynamic, plastic genome with core and accessory components.
There are currently two publicly available annotated genomes for these organisms: Pba strain SCRI1043 CITATION, and Dickeya dadantii strain 3937.
The availability of these sequences has rapidly advanced our understanding of these organisms, but broader comparisons are expected to deliver greater insight into the evolution and function of the SRE.
The major common virulence factors of the SRE are plant cell wall degrading enzymes that degrade the plant cell wall to release nutrients in a so-called brute force attack CITATION CITATION.
Other virulence factors include virulence-associated secretion systems, siderophores, cell-surface polysaccharides and agglutinins CITATION, CITATION .
By contrast, bacterial plant pathogens such as Pseudomonas spp.
are associated with a biotrophic stealth interaction with the host.
These stealth pathogens employ mechanisms such as the type III secretion system to translocate effectors into host cells.
The effectors modulate the host plant's biochemical responses, implementing a wide array of strategies to circumvent host immunity CITATION CITATION.
However, Pba1043, Dda3937, and other SREs also encode a functioning T3SS and other gene products associated with this stealth interaction, indicating a more complex relationship with their hosts than simple brute-force necrotrophy CITATION, CITATION CITATION.
Key factors with a confirmed role in virulence include type IV and type VI secretion systems, and the phytotoxin coronafacic acid, which is synthesised by the cfa gene cluster CITATION, CITATION.
Other factors associated with persistence in, and adaptation to, the wider environment have been identified, such as genes associated with opine uptake, biofilm formation, antibiotic production, and nitrogen fixation CITATION .
In many bacteria, such genes associated with pathogenicity, and other phenotypically-distinguishing characters, are frequently associated with islands of horizontal gene transfer.
This gene complement is often variable between strains and species, and is sometimes termed the accessory genome, in order to distinguish it from the core genome that provides functionality presumed to be essential to all related organisms CITATION, CITATION, CITATION, CITATION CITATION.
We expect that observed differences between the gene complements of SRE will reflect differences in their phenotypes, and adaptations to their distinct environments, and that these differences will be preferentially located in islands of genes in their genomes.
We use aCGH and apply our analysis method to identify genomic islands in Pba1043 that do not have putative orthologues in the unsequenced Pba strain SCRI1039 and Pcc strain SCRI193, and in the sequenced Dda3937.
In this study, coding sequences from Pba1043 that are predicted by aCGH to be absent or divergent in Pba1039, Pcc193 or Dda3937 are of interest because they may potentially contribute to Pba1043-specific phenotypes, including host interactions.
Pairwise comparisons between Pba1043 and these three organisms span a range of evolutionary distances since their most recent common ancestor with Pba1043, and represent variation at strain, species and genus levels.
Our results for the SRE support a hypothesis that the genomes of SRE continue to be modified by the acquisition of genomic islands, and the model of an accessory genome of niche-specific functionality that is composed, at least in part, of horizontally-acquired genomic islands.
We identify major differences in the CDS carried within the accessory genomes of SRE and, while these recapitulate previous observations of major genomic islands made using alternative approaches CITATION, CITATION, we also find a number of unexpected differences that provide insight into, and may play a direct role in determining, variation amongst the SRE in terms of their environmental survival, host range and aetiology.
Molecular chaperones are essential elements of the protein quality control machinery that governs translocation and folding of nascent polypeptides, refolding and degradation of misfolded proteins, and activation of a wide range of client proteins.
The prokaryotic heat-shock protein DnaK is the E. coli representative of the ubiquitous Hsp70 family, which specializes in the binding of exposed hydrophobic regions in unfolded polypeptides.
Accurate prediction of DnaK binding sites in E. coli proteins is an essential prerequisite to understand the precise function of this chaperone and the properties of its substrate proteins.
In order to map DnaK binding sites in protein sequences, we have developed an algorithm that combines sequence information from peptide binding experiments and structural parameters from homology modelling.
We show that this combination significantly outperforms either single approach.
The final predictor had a Matthews correlation coefficient of 0.819 when assessed over the 144 tested peptide sequences to detect true positives and true negatives.
To test the robustness of the learning set, we have conducted a simulated cross-validation, where we omit sequences from the learning sets and calculate the rate of repredicting them.
This resulted in a surprisingly good MCC of 0.703.
The algorithm was also able to perform equally well on a blind test set of binders and non-binders, of which there was no prior knowledge in the learning sets.
The algorithm is freely available at LINK.
Hsp70 molecular chaperones are part of the quality control machinery that functions to assist protein folding.
Members of the Hsp70 family have been implicated in refolding of misfolded proteins, folding of newly synthesized polypeptide chains, disassembly of larger aggregates and translocation of proteins in organelles CITATION.
Hsp70 molecules also enable cell survival during stress or heat-shock conditions that are characterized by an increased concentration of denatured polypeptides.
These chaperones recognize and bind misfolded or aggregation-prone peptide stretches through exposed hydrophobic regions which are normally buried in the protein core.
Such exposed regions are typical for non-native proteins CITATION, CITATION .
Hsp70 molecular chaperones consist of two distinct domains, an N-terminal ATPase domain CITATION and a C-terminal peptide binding domain CITATION.
Hsp70 function is dependent on an ATP-regulated cycle of substrate binding and release CITATION.
With ATP bound, substrate affinity is low and Hsp70 resides in an open state, ready to receive a suitable substrate.
Once the substrate is bound, ATP is hydrolyzed to ADP and Hsp70 undergoes a conformational change to a high affinity state, subsequently trapping the substrate.
The co-chaperone Hsp40 binds Hsp70 and stimulates the ATPase function, causing retention of the substrate.
Hsp40 also recognizes hydrophobic stretches and may serve as a substrate delivery chaperone to Hsp70 CITATION, CITATION.
Upon exchange of ADP for ATP, Hsp70 returns to a low affinity state, enabling binding of another substrate or providing another refolding cycle for the same substrate if necessary.
The crystallisation of the archetypical and well characterized E coli Hsp70 DnaK bound to a peptide reflects a heptameric substrate binding motif requiring a hydrophobic core region and preferably basic flanking residues that complement the overall negatively charged DnaK surface CITATION.
This preference was later confirmed in the seminal work of Bukau and co-workers by binding studies of DnaK to cellulose-based peptide libraries and a DnaK binding profile was derived CITATION .
Contrary to these previous studies on DnaK binding motif profiling which utilised only sequence information, we complement the experimental binding information on a set of peptides with structural data from homology modelling to obtain an accurate predictor.
Similar dual based approaches have already been shown successful to predict other peptide signatures.
Prediction of binding of endogenous antigenic peptides to MHC class I molecules was aided by adding structural information from molecular models to the sequence data CITATION.
Branetti et al used structural data from various SH3/ligand complexes and sequence information from phage libraries to predict preferred ligand binding to different SH3 domains CITATION.
Recently, an algorithm to predict amylogenic regions in protein sequences profited greatly from the combination of sequence based data and structural information from amyloid fibers crystallographic studies .
In this article we introduce such a dual based method for profiling DnaK binding sequences.
We combine sequence based information from experimental binding assays with structural information from molecular modelling via the FoldX force field CITATION.
We present a DnaK binding prediction algorithm that, under cross-validated conditions, performs strikingly accurate.
Protein loops, the flexible short segments connecting two stable secondary structural units in proteins, play a critical role in protein structure and function.
Constructing chemically sensible conformations of protein loops that seamlessly bridge the gap between the anchor points without introducing any steric collisions remains an open challenge.
A variety of algorithms have been developed to tackle the loop closure problem, ranging from inverse kinematics to knowledge-based approaches that utilize pre-existing fragments extracted from known protein structures.
However, many of these approaches focus on the generation of conformations that mainly satisfy the fixed end point condition, leaving the steric constraints to be resolved in subsequent post-processing steps.
In the present work, we describe a simple solution that simultaneously satisfies not only the end point and steric conditions, but also chirality and planarity constraints.
Starting from random initial atomic coordinates, each individual conformation is generated independently by using a simple alternating scheme of pairwise distance adjustments of randomly chosen atoms, followed by fast geometric matching of the conformationally rigid components of the constituent amino acids.
The method is conceptually simple, numerically stable and computationally efficient.
Very importantly, additional constraints, such as those derived from NMR experiments, hydrogen bonds or salt bridges, can be incorporated into the algorithm in a straightforward and inexpensive way, making the method ideal for solving more complex multi-loop problems.
The remarkable performance and robustness of the algorithm are demonstrated on a set of protein loops of length 4, 8, and 12 that have been used in previous studies.
The characterization of protein loop structures and their motions is essential in understanding the function of proteins and the biological processes they mediate CITATION, CITATION.
However, due to their conformational flexibility, it is notoriously difficult to uniquely determine their structure via traditional experimental techniques such as X-ray scattering or nuclear magnetic resonance.
As a result, structures with missing loops are not uncommon in the Protein Data Bank.
The sequence and structure variability of protein loops also presents a major challenge in homology modeling.
With moderate sequence identity and good quality experimental template structures, it is generally feasible to obtain the overall tertiary structure and some acceptable degree of detail for the loop in question.
However, the errors could be significant in the loop regions where the sequences between the target and template protein differ significantly.
In our view, the loop closure problem, namely the construction of a protein fragment that closes the gap between two fixed end points, remains unsolved.
A satisfactory solution to this problem will not only benefit experimental structure determination and comparative modeling, but also be useful in de novo protein structure prediction and phase space sampling, as the importance of local moves without changing the rest of the system has been repeatedly demonstrated for chain molecules CITATION, CITATION .
A complete solution to the protein loop reconstruction problem usually involves two important components, the buildup of the loop structure and the selection of the most promising candidates through an appropriate scoring function.
The current study addresses the former problem.
A variety of algorithms has been developed to tackle the loop closure problem.
Many methods construct protein loops by reusing representative loop blocks from a database of experimentally determined protein structures CITATION CITATION.
Naturally, these methods are highly dependent on the size and quality of the experimental data, and their performance has improved substantially with the rapid growth of PDB CITATION, CITATION.
More importantly, since the number of possible conformations increases exponentially with length, this approach is limited to relatively short loops.
This is not a problem for ab initio methods which construct loops by either distorting existing structures or by relaxing distorted non-physical structures with molecular dynamics, simulated annealing, gradient minimization, random tweaking, discrete dihedral angle sampling, or self-consistent field optimization CITATION CITATION.
These algorithms often include energy calculations using classical force fields and implicit or explicit treatment of solvent effects, and therefore tend to be computationally expensive.
Several groups have combined knowledge-based and sampling approaches, sometimes with considerable success CITATION, CITATION CITATION.
For example, through modeling the crystal environment, careful refinements, and extensive conformational sampling, PLOP CITATION obtained an average prediction accuracy of 0.84 and 1.63 RMSD from the crystal structures for a series of 8- and 11-residue loops.
The performance of PLOP was further improved by Zhu and coauthors through an improved sampling algorithm and a new energy model CITATION, and was successfully applied even to loops in inexact environments CITATION .
An alternative class of methods determine proper loop structures by identifying all possible solutions to a set of algebraic equations derived from distance geometry, as described in the pioneering work of Go and Sheraga CITATION and many other analytical methods adopted from kinematic theory CITATION, CITATION CITATION.
In particular, Canutescu and Dunbrack introduced a very attractive approach known as cyclic coordinate descent, which can close loops of different lengths through iterative adjustment of dihedral angles CITATION.
This method has been incorporated into the well-known de novo protein design package Rosetta and demonstrated its strength in generating conformations for the loop regions CITATION, CITATION.
More recently, Coutsias and coauthors cast the determination of loop conformations of six torsions into a problem of finding the real roots of a 16 th degree single-variable polynomial, and demonstrated the efficiency and applicability to various loops CITATION.
A thorough review of loop closure algorithms is beyond the scope of this paper.
For more information, the reader is referred to several recent articles CITATION CITATION, CITATION .
In computational modeling, a protein loop can be conveniently represented by a set of connected points in three-dimensional Cartesian space.
A chemically sensible conformation must satisfy a set of geometric constraints derived from the loop's covalent structure.
The connectivity and common covalent bond lengths and angles require that the distance d ij between any pair of atoms i and j falls between certain bounds, FORMULA.
Non-bonded interactions introduce additional constraints, as do the planarity of conjugated systems and the chirality of stereocenters.
These can be further supplemented with external constraints derived from experimental techniques such as 2D NMR and fluorescent resonance energy transfer.
Taken together, these constraints greatly reduce the search space that needs to be sampled in order to identify the loop's accessible conformations.
Distance geometry is a class of methods that aim specifically at generating conformations that satisfy such geometric constraints.
DG attempts to minimize an error function that measures the violation of geometric constraints CITATION, CITATION.
DG methods involve four basic steps: generating the interatomic distance bounds, assigning a random value to each distance within the respective bounds, converting the resulting distance matrix into a starting set of Cartesian coordinates, and refining the coordinates by minimizing distance constraint violations.
To ensure that reasonable conformations are generated, the original upper and lower bounds are usually refined using an iterative triangular smoothing procedure.
Although this process improves the initial guess, the randomly chosen distances may still be inconsistent with a valid 3-dimensional geometry, necessitating expensive metrization schemes CITATION CITATION or higher dimensional embeddings CITATION prior to error refinement, or lengthy refinement procedures if random starting coordinates are used.
Although DG methods can generate sensible starting geometries, these geometries are rather crude for most practical applications, and need to be further refined by some form of energy minimization.
Since its first chemical applications in 1978 by Crippen and Havel CITATION, DG has been applied to a wide range of problems, including NMR structure determination, conformational analysis CITATION, CITATION, homology modeling CITATION, CITATION, and ab initio fold prediction CITATION .
Recently, a new self-organizing technique known as stochastic proximity embedding has been developed as an extremely attractive alternative to conventional DG embedding procedures CITATION.
SPE starts from random initial atomic positions, and gradually refines them by repeatedly selecting an individual constraint at random, and updating the respective atomic coordinates towards satisfying that specific constraint.
This procedure is performed repeatedly until a reasonable conformation is obtained.
The method, which was originally developed for dimensionality reduction CITATION and nonlinear manifold learning CITATION, is simple, fast and efficient, and can be applied to molecular topologies of arbitrary complexity.
Because it avoids explicit evaluation of an error function that measures all possible interatomic distance bound violations in every refinement step, the method is extremely fast and scales linearly with the size of the molecule.
SPE is significantly more effective in sampling the full range of conformational space compared to other conformational search methods CITATION, particularly when used in conjunction with conformational boosting CITATION, a heuristic for biasing the search towards more extended or compact geometries.
Furthermore, SPE is insensitive to permuted input, a problem that plagues many systematic search algorithms CITATION .
Zhu and Agrafiotis subsequently proposed an improved variant of SPE referred to as self-organizing superimposition that accelerates convergence by decomposing the molecule into rigid fragments and using pre-computed conformations for those fragments in order to enforce the desired geometry CITATION.
Starting from completely random initial coordinates, the SOS algorithm repeatedly superimposes the templates to adjust the positions of the atoms, thereby gradually refining the conformation of the molecule.
Coupled with pair-wise atomic adjustments to resolve steric clashes, the method is able to generate conformations that satisfy all geometric constraints at a fraction of the time required by SPE.
The approach is conceptually simple, mathematically straightforward, and numerically robust, and allows additional constraints to be readily incorporated.
Since rigid fragments are pre-computed, planarity and chirality constraints are automatically satisfied after the template superimposition process, and local geometry is naturally restored.
Furthermore, because each embedding starts from completely random initial atomic coordinates, each new conformation is independent of those generated in the previous runs, resulting in greater diversity and more effective sampling.
As the algorithm only involves pairwise distance adjustments and superimposition of relatively small fragments, it is impressively efficient.
In this paper, we present the new variant of the SOS algorithm, which has been adapted from conformational sampling of small molecules and tailored to the protein loop closure problem.
In the remaining sections, we provide a detailed description of the modified SOS algorithm and its implementation, and present comparative results for a set of protein loops of residue size 4, 8, and 12, which have been used in previous validation studies.
Predicting protein function from structure remains an active area of interest, particularly for the structural genomics initiatives where a substantial number of structures are initially solved with little or no functional characterisation.
Although global structure comparison methods can be used to transfer functional annotations, the relationship between fold and function is complex, particularly in functionally diverse superfamilies that have evolved through different secondary structure embellishments to a common structural core.
The majority of prediction algorithms employ local templates built on known or predicted functional residues.
Here, we present a novel method that automatically generates structural motifs associated with different functional sub-families within functionally diverse domain superfamilies.
Templates are created purely on the basis of their specificity for a given FSG, and the method makes no prior prediction of functional sites, nor assumes specific physico-chemical properties of residues.
FLORA is able to accurately discriminate between homologous domains with different functions and substantially outperforms popular structure comparison methods and a leading function prediction method.
We benchmark FLORA on a large data set of enzyme superfamilies from all three major protein classes and demonstrate the functional relevance of the motifs it identifies.
We also provide novel predictions of enzymatic activity for a large number of structures solved by the Protein Structure Initiative.
Overall, we show that FLORA is able to effectively detect functionally similar protein domain structures by purely using patterns of structural conservation of all residues.
The prediction of protein function from structure has become of increasing interest as a significant proportion CITATION of structures solved by the structural genomics initiatives lack functional annotation.
Furthermore, structure-based approaches are of particular interest for predicting binding sites and/or catalytic sites for the purposes of protein engineering and pharmaceutical development.
Many current methods focus on encoding a template of functional residues and then aligning this template to whole structures.
The problems with taking this approach are deciding what qualifies as a functional residue and creating biologically-accurate templates for the ever increasing number of available protein structures being deposited in the PDB CITATION.
Resources such as the Catalytic Site Atlas CITATION are carefully curated by hand and restricted to residues directly involved in catalysis, whereas MSDSite CITATION and PDBSite CITATION, CITATION generate templates based on active site residues defined in the PDB file by the authors.
Although these resources are undoubtedly extremely valuable, it is questionable whether sufficient coverage of the PDB can be maintained when manual intervention is required.
To address the problem of generating templates for all protein structures, there are a number of methods that aim to do this automatically.
For example, the reverse template method CITATION decomposes a query structure into tri-peptide fragments, which are then matched against a non-redundant set of PDB structures using the search algorithm JESS CITATION.
Hits are evaluated according to the sequence similarity of the local environment of the template.
The GASP method CITATION uses a genetic algorithm to construct templates based on their ability to discriminate between different protein families against a background of representatives from the SCOP database CITATION.
Similarly, DRESPAT CITATION implements a graph theoretical approach to discover structural patterns associated with a given family of proteins to locate ligand binding motifs.
MultiProt CITATION can provide template of structures through a multiple structure alignment.
A recent extension of the Evolutionary Trace method for binding site prediction was used to create structural templates based on predicted functional residues CITATION.
SiteEngines CITATION produces templates by matching the geometry and physico-chemical properties of residues in binding site clefts.
As well as atom or residue-level templates, other non-template-based approaches seek to compare the electrostatic properties of binding sites or surface accessible clefts which often co-locate with active sites .
One inherent complexity of using PDB structures to transfer annotations between enzymes is the binding state in which the protein is crystallised for example, structures crystallised with non-cognate ligands, substrate analogs, transition states or apo-enzymes CITATION.
As a consequence, precise geometric matching in the active site region can be problematic due to the conformational changes that occur on ligand binding.
To address this issue, the methods mentioned above use a variety of approaches such as graph matching or geometric hashing with various tolerance levels.
The SOIPPA method CITATION, CITATION takes the alternative approach of using a geometric potential to characterise the shape formed by a given set of C atoms, to account for both local and global relationships between residues across the protein structure.
In a recent ligand-binding site comparison analysis, SOIPPA was able to detect distant similarities between very different protein folds binding a range of adenine-containing ligands CITATION .
Despite the many template methods present in the literature, very few are publicly available to the general user.
Hence, the first step in assigning function by structure is often to use global structure comparison methods, which can detect distant evolutionary relationships even where sequence similarity is weak.
These methods have been specifically applied to function prediction to assign confidence values when inheriting GO terms between related structures.
However, detecting very distant relatives remains a challenge as structure comparison methods generally give an absolute measure of structural distance, such as RMSD, and applying a cut-off at which one can deduce that two proteins perform related functions results in many missed relationships.
Analyses of CATH CITATION, CITATION have shown that although function and structure are well conserved in the majority of superfamilies, there are a significant number of highly diverse superfamilies where this is not the case CITATION.
Moreover, the latter superfamilies are disproportionately represented in both the PDB and in the genomes and tend to exhibit a wide range of core biological functions across a large range of species CITATION.
An analysis by Reeves et al. CITATION showed that relatives within these superfamilies tend to share a common evolutionary core, but this core is embellished with different insertions of secondary structure elements that often correlate with changes in function.
However, although structural embellishments might change some facet of function, others have found that relatives can still retain other aspects in common CITATION, CITATION.
Therefore, calculating a global measure of structural similarity or distance between two proteins can be less informative than focussing on the structural motifs relevant to a given aspect of function.
The FLORA algorithm presented here was designed to derive structural templates for functional sub-groups within diverse CATH superfamilies.
FLORA first performs global structure alignment across the superfamily to recognise the distinctive structural patterns associated with each FSG and builds templates based on these patterns.
New functional homologues are then detected by using the global structural alignments to relatives in each FSG again, but only scoring the similarity over positions identified by the FLORA motif.
This approach performs very well in discriminating between different enzymatic functions, compared to global methods and another motif-based approach.
Although we benchmark here on enzyme superfamilies, the method is applicable to superfamilies containing non-enzymatic relatives.
To test FLORA, we have automatically generated a large data set of domains from 29 diverse superfamilies.
Our data set allows us to look at the variation of FLORA results between superfamilies and to stress the importance of using a large test data set for benchmarking methods.
We have benchmarked FLORA against CE CITATION, CATHEDRAL CITATION and Reverse Templates CITATION to give an indication of how it performs in comparison to other standard methods of function prediction.
We also present some examples of structural motifs identified by FLORA and explain their functional relevance.
Finally, we use FLORA to make novel predictions of function for proteins solved by the Protein Structure Initiative .
Structural and functional studies of the ABL and EGFR kinase domains have recently suggested a common mechanism of activation by cancer-causing mutations.
However, dynamics and mechanistic aspects of kinase activation by cancer mutations that stimulate conformational transitions and thermodynamic stabilization of the constitutively active kinase form remain elusive.
We present a large-scale computational investigation of activation mechanisms in the ABL and EGFR kinase domains by a panel of clinically important cancer mutants ABL-T315I, ABL-L387M, EGFR-T790M, and EGFR-L858R.
We have also simulated the activating effect of the gatekeeper mutation on conformational dynamics and allosteric interactions in functional states of the ABL-SH2-SH3 regulatory complexes.
A comprehensive analysis was conducted using a hierarchy of computational approaches that included homology modeling, molecular dynamics simulations, protein stability analysis, targeted molecular dynamics, and molecular docking.
Collectively, the results of this study have revealed thermodynamic and mechanistic catalysts of kinase activation by major cancer-causing mutations in the ABL and EGFR kinase domains.
By using multiple crystallographic states of ABL and EGFR, computer simulations have allowed one to map dynamics of conformational fluctuations and transitions in the normal and oncogenic kinase forms.
A proposed multi-stage mechanistic model of activation involves a series of cooperative transitions between different conformational states, including assembly of the hydrophobic spine, the formation of the Src-like intermediate structure, and a cooperative breakage and formation of characteristic salt bridges, which signify transition to the active kinase form.
We suggest that molecular mechanisms of activation by cancer mutations could mimic the activation process of the normal kinase, yet exploiting conserved structural catalysts to accelerate a conformational transition and the enhanced stabilization of the active kinase form.
The results of this study reconcile current experimental data with insights from theoretical approaches, pointing to general mechanistic aspects of activating transitions in protein kinases.
Protein kinase genes are signaling switches with a conserved catalytic domain that phosphorylate protein substrates and thereby play a critical role in cell signaling CITATION CITATION.
As a result, many protein kinases have emerged as important therapeutic targets for combating diseases caused by abnormalities in signal transduction pathways, especially various forms of cancer.
A large number of protein kinase crystal structures in the free form and complexes with various inhibitors have been determined, resulting in the growing wealth of structural information about the kinase catalytic domain CITATION CITATION.
The crystal structures have revealed considerable structural differences between closely related active and highly specific inactive kinase forms CITATION CITATION.
Conformational plasticity and diversity of crystal structures of the ABL CITATION CITATION and EGFR kinase domains CITATION CITATION have demonstrated the existence of active, inactive, Src-like inactive and intermediate conformational forms.
Conformational transitions and dynamic equilibrium between these distinct conformational states are important characteristics of the kinase regulation and recognition by other molecules CITATION CITATION.
Evolutionary analysis of the functional constraints acting on eukaryotic protein kinases demonstrated that protein kinase mechanisms may have evolved through elaboration of a simple structural component that included the HxD-motif adjoining the catalytic loop, the F-helix, an F-helix aspartate, and the catalytically critical Asp-Phe-Gly motif from the activation loop.
This computational analysis showed how distinctive structural elements of the kinase core may be linked with the conformational changes of the DFG motif in kinase regulation CITATION.
A surface comparison of crystal structures for serine threonine and tyrosine kinases has recently identified the conserved residues that are most sensitive to activation CITATION.
According to the proposed model, critical features of the common activation mechanism may include a dynamic assembly of the hydrophobic spine motif and the formation of specific salt bridges that can collectively provide coordination of the kinase lobes during activation process CITATION, CITATION.
These illuminating studies have demonstrated that protein kinase function may be controlled by a dynamic assembly of spatially distributed conserved residues important in regulation of allosteric signaling pathways.
In a subsequent study, it was proposed that the F-helix of the kinase domain may act as a central scaffold in the assembly of active protein kinase forms by anchoring the hydrophobic regulatory spine and a second functional cluster termed catalytic spine CITATION .
Abnormal activation of protein kinases is among major causes of human diseases, especially various cancers.
Resequencing studies of kinase coding regions in tumors have revealed that a small number of kinase mutations contribute to tumor formation, while the majority are neutral mutational byproducts of somatic cell replication CITATION CITATION.
Mutations in protein kinases are implicated in many cancers CITATION and often exemplify the phenomenon of oncogene addiction CITATION, CITATION, whereby structural effects of oncogenic mutations confer a selective advantage for tumor formation during somatic cell replication.
The dominant oncogenes that confer the oncogene addiction effect include ABL, EGFR, VEGFR, BRAF, RET, and MET kinase genes CITATION.
The dependence of chronic myeloid leukemia on the translocated BCR-ABL kinase is correlated with dramatic responses to small molecule inhibitors.
A large number of diverse point mutations that impair the binding of Imatinib to ABL have been described CITATION CITATION, suggesting that some drug resistant mutations could exist before treatment, and may contribute to tumorigenesis.
The profound selectivity of Imatinib at inhibiting a small group of protein tyrosine kinases is achieved by the high precision with which this inhibitor can recognize the inactive conformation of the activation loop in ABL, KIT and PDGFR kinases CITATION, CITATION.
Structurally conserved gate-keeper mutation ABL-T315I is a dominant cancer-causing alteration, leading to the most severe Imatinib resistance by favoring the active form of the ABL kinase.
Subsequently, a series of rationally designed analogs of Imatinib based on the core scaffold were shown to recognize a broader spectrum of inactive kinase conformations and inhibit with equal potency both ABL and C-Src kinases CITATION.
Inhibitors that bind to the inactive conformation face weaker competition from cellular ATP and may act by shifting equilibrium between conformational states in a way that prevents kinase activation, rather than by inhibiting kinase activity directly.
A spectrum of lung cancer-derived EGFR mutations can induce oncogenic transformation by leading to constitutive kinase activity and confer markedly different degrees of sensitivity to EGFR inhibitors CITATION CITATION.
Similarly, EGFR-T790M mutant could cause resistance to Gefitinib and Erlotinib drugs in the treatment of lung cancer CITATION ,.
Importantly, these mutations can promote oncogenic activation, uncontrolled cell proliferation and tumorigenesis even in the absence of the selective pressure from the kinase inhibitors.
An activating mutation in the activation loop of the EGFR kinase domain, L858R is among most frequent mutations in lung cancer, amounting to more than 40 percent of EGFR mutations in this cancer category CITATION CITATION.
While T790M has only a modest effect on EGFR function, a tandem of T790M and L858R mutations can result in a dramatic enhancement of EGFR activity CITATION.
The crystal structures of EGFR-L858R, EGFR-T790M CITATION CITATION and ABL-T315I mutants CITATION, CITATION have shown that these cancer-causing modifications could stabilize the active kinase form.
Recent structural and mutagenesis investigations have asserted a common activating nature of the gatekeeper mutations in c-ABL, c-Src, and EGFR and PDGFR kinases CITATION.
Moreover, mutations of the gatekeeper residues to smaller amino acids and pharmacological intervention by the inhibitor binding, which interfere with the structural integrity of the hydrophobic spine, could effectively abrogate the kinase activity.
Conversely, substitutions of the gatekeeper residues with bulkier modifications, that strengthen the hydrophobic spine, tend to correlate with the enhanced oncogenic activation of ABL and EGFR kinases CITATION.
These studies have proposed a mechanism of activation in which stabilization of the hydrophobic regulatory spine may promote shift of the kinase equilibrium towards the constitutively active kinase form, and thus have a dramatic effect on the regulation of the enzyme.
Crystallographic analysis may not capture the complete ensemble of protein kinase conformations available in solution under physiological conditions.
NMR spectroscopy techniques can effectively complement X-ray studies by providing a more adequate characterization of conformational ensembles and dynamics of transitions between different kinase states CITATION, CITATION.
The first NMR characterization of ABL kinase in complexes with various inhibitors has been recently reported CITATION.
This study has detected microsecond to millisecond motions of the activation loop seen in both the active and inactive states, suggesting that this mobility may be an intrinsic structural requirement for enabling conformational transitions between alternative kinase conformations.
Hydrogen exchange mass spectrometry has been applied to investigate conformational dynamics of ABL upon T315I mutation CITATION.
The effect of ABL-T315I mutation manifested not only in the local conformational disturbances near site of mutation, but also influenced protein flexibility in remote regions of the SH3 domain.
Hence, allosteric interactions and inter-domain communication of ABL regulatory complexes could be considerably perturbed by activating mutations, thereby playing a major role in the kinase regulation in solution.
Computational studies have begun to investigate a molecular basis of protein kinase function and the structural effects of activating mutations, which may ultimately control the activity signatures of cancer drugs and determine the scope of drug resistance mutations CITATION, CITATION.
A molecular mechanism of long-range, allosteric conformational activation of Src tyrosine kinases has been proposed by using a combination of experimental enzyme kinetics and nonequilibrium molecular dynamics simulations CITATION, CITATION.
Atomistic simulations of large-scale allosteric conformational transitions of adenylate kinase have suggested a population-shift mechanism upon inhibitor binding CITATION.
Coarse-grained and all-toms modeling using structural connectivity mapping have allowed to characterize a collective dynamics of conformational transitions between the inactive and active states of the Src kinase CITATION CITATION.
Atomistic dynamics of the open-to-closed movement of the cyclin-dependent kinase 5 has been recently studied using a metadynamics sampling approach, revealing a two-step molecular mechanism and the formation of functionally important intermediates CITATION.
Molecular dynamics simulations of ABL kinase and Imatinib-binding kinetics assays have proposed that a protonation-dependent switch in the DFG motif from the activation loop may allow the kinase to access multiple conformations facilitating nucleotide binding and release cycles CITATION.
Targeted molecular dynamics simulations have attempted to explore conformational transitions in the activation loop of the c-Kit kinase domain CITATION.
Most recently, conformational dynamics of the EGFR kinase domain studied by TMD simulations has suggested that formation of the hydrophobic spine and salt bridges may be important in the activation process CITATION.
Computational studies of protein kinases have elucidated thermodynamic factors of kinase activation, suggesting that cancer mutations with the higher oncogenic activity may have the greater destabilization effect on the inactive kinase structure CITATION, CITATION .
These studies have suggested that the conserved topology of the protein kinase fold could preserve global dynamics in the normal and oncogenic forms, yet allowing for functionally important local and allosteric conformational changes caused by mutations.
The basic mechanistic features of the protein kinase dynamics and activation mechanisms may be interpreted using a conformational selection model CITATION CITATION and the energy landscape perspective CITATION CITATION of protein folding and binding.
This theoretical framework implies an ensemble of preexisting multiple conformational states on the underlying energy landscape, with the mutations shifting the energy landscape and the relative populations of accessible states towards functionally relevant complexes CITATION CITATION.
An important role of conformational selection mechanisms has recently gained further prominence CITATION, suggesting a broad applicability of this model in explaining dynamic effects for a variety of biological systems CITATION CITATION .
It was recently proposed that evolution may have preserved protein flexibility features that retain the ability of kinases to fluctuate normally between active and inactive states.
In contrary, cancer kinase mutations may result in the increased conformational space to be explored in the inactive state CITATION, CITATION.
Thermodynamic and mechanistic effects of cancer mutations may manifest in a preferential shifting of the landscape equilibrium and altering of the accessible conformational space for deleterious mutants through either local or allosteric-based dynamic changes.
A similar energy landscape-based framework for predicting the effects of mutations on protein dynamics and binding was successfully employed for allostery-based rescue mutant design in a tumor suppressor protein CITATION and studies of molecular evolution of affinity and flexibility in the immune system CITATION, CITATION .
Despite recent progress in computational and experimental studies of protein kinases, a quantitative understanding of thermodynamic and mechanistic catalysts of kinase activation by cancer mutations is still lacking.
In this study, we have embarked on a detailed computational analysis of activation mechanisms in the ABL and EGFR kinase domains using homology modeling, MD simulations, protein stability analysis, TMD simulations and molecular docking.
A comparative analysis has been conducted based on computational modeling of the wild type ABL and EGFR kinase domains as well as a panel of clinically important cancer mutants ABL-T315I, ABL-L387M, EGFR-T790M, and EGFR-L858R.
We have also simulated the effect of the gatekeeper ABL-T315I mutation on conformational dynamics and allosteric interactions in the ABL-SH2-SH3 regulatory complexes.
In support of the experimental hypotheses, our results have suggested potential thermodynamic and mechanistic catalysts of the ABL and EGFR kinase activation that may collectively accelerate conformational transitions and result in the enhanced stabilization of the active kinase form.
We have also proposed a multi-stage mechanistic model of the activation process that includes a series of cooperative transitions resulting in the formation of key intermediate states that are characterized by a rapid assembly of the hydrophobic spine and subsequent stabilization of the Src-like structures.
Broadly, the results of study may reconcile current experimental data with the insights from computational approaches, pointing to general mechanistic aspects of activating transitions in protein kinases.
Trp-cage is a designed 20-residue polypeptide that, in spite of its size, shares several features with larger globular proteins.
Although the system has been intensively investigated experimentally and theoretically, its folding mechanism is not yet fully understood.
Indeed, some experiments suggest a two-state behavior, while others point to the presence of intermediates.
In this work we show that the results of a bias-exchange metadynamics simulation can be used for constructing a detailed thermodynamic and kinetic model of the system.
The model, although constructed from a biased simulation, has a quality similar to those extracted from the analysis of long unbiased molecular dynamics trajectories.
This is demonstrated by a careful benchmark of the approach on a smaller system, the solvated Ace-Ala 3-Nme peptide.
For the Trp-cage folding, the model predicts that the relaxation time of 3100 ns observed experimentally is due to the presence of a compact molten globule-like conformation.
This state has an occupancy of only 3 percent at 300 K, but acts as a kinetic trap.
Instead, non-compact structures relax to the folded state on the sub-microsecond timescale.
The model also predicts the presence of a state at FORMULA of 4.4 from the NMR structure in which the Trp strongly interacts with Pro12.
This state can explain the abnormal temperature dependence of the FORMULA and FORMULA chemical shifts.
The structures of the two most stable misfolded intermediates are in agreement with NMR experiments on the unfolded protein.
Our work shows that, using biased molecular dynamics trajectories, it is possible to construct a model describing in detail the Trp-cage folding kinetics and thermodynamics in agreement with experimental data.
Understanding protein folding thermodynamics and kinetics is a central issue in molecular biology CITATION CITATION and computer-aided modeling is becoming increasingly useful also in this field.
Direct comparison between simulations and experiments requires both an accurate description of the system and the possibility to sample extensively the configuration space.
In order to observe folding with molecular dynamics, it is necessary to use very large computers CITATION, CITATION, worldwide distributed computing CITATION, or an enhanced sampling technique CITATION CITATION .
A system that is almost ideal for theoretical investigation is the Trp-cage CITATION, a designed 20-residue miniprotein that folds rapidly CITATION and spontaneously to a globular structure.
The NMR structure CITATION reveals a compact hydrophobic core, in which the Trp side chain is buried.
The secondary structure elements include a short FORMULA, a 3 10-helix and a polyproline II helix at the C-terminus.
The folding mechanism of this system has been studied with several experimental techniques.
Calorimetry, circular dichroism spectroscopy CITATION and fluorescence CITATION show a cooperative two-state folding behavior with transition midpoint at approximately 314 K and a relaxation time of 3.1 s at 296 K CITATION.
UV-Resonance Raman CITATION reveals a more complex unfolding behavior, with the presence of a compact intermediate that retains an FORMULA character and in which the hydrophobic core is even more compact.
NMR experiments CITATION, CITATION show a substantially cooperative thermal unfolding, but the large negative chemical shift deviations of FORMULA and FORMULA suggest that those residues might pack more tightly as the temperature is raised.
Also fluorescence correlation spectroscopy experiments cannot be interpreted in terms of a simple two-state folding and the formation of a molten-globule-like intermediate has been proposed CITATION .
By atomistic modeling the Trp-cage folding has been studied using several different approaches CITATION CITATION.
In particular, with an all-atom explicit-solvent description, the folding of Trp-cage has been studied by replica exchange molecular dynamics CITATION, CITATION.
Starting from an extended configuration, a structure with a FORMULA root mean square deviation 2 from the NMR reference structure is obtained after 100 ns of simulation on 40 replicas CITATION.
A relatively high melting temperature of 440 K is predicted.
Other studies suggested that, even if Trp-cage is a rather small system, achieving statistical convergence in a REMD simulation may require much longer simulation times CITATION, CITATION.
The kinetics of Trp-cage folding was studied, in explicit solvent, by transition path sampling CITATION and transition interface sampling CITATION.
The folding of Trp-cage was also investigated by two of us using the bias exchange metadynamics approach CITATION, in which metadynamics potentials acting on different collective variables are exchanged among molecular dynamics simulations performed at the same temperature.
Using this method it is possible to explore simultaneously a virtually unlimited number of CVs.
Since all the MD simulations are performed at the same temperature the number of replicas does not grow with the system size like in REMD and in the approach of Ref.
CITATION.
Using BE it was possible to reversibly fold Trp-cage CITATION, villin headpiece, advillin headpiece together with two of their mutants CITATION and Insulin chain B CITATION using an explicit solvent force field, in less than 100 nanoseconds of simulation with only eight replicas.
Recently this method was also used for exploring the mechanism of enzyme reactions CITATION .
In atomistic simulations of biological systems, after an exhaustive exploration is achieved, it is necessary to extract from the trajectory the relevant metastable conformations, to assign their occupation probability, and to compute the rates for transitions among them.
Several methods have been developed for this scope CITATION CITATION.
These methods have the big advantage of reducing a complex dynamics in a high-dimensional configuration space to a Markov process describing transitions among a finite number of metastable states.
They are suitable for analyzing an ergodic molecular dynamics trajectory, but they cannot be straightforwardly applied if the system is evolved under the action of an external bias.
In this paper we present a method that allows exploiting the statistics accumulated in a bias exchange metadynamics run CITATION for constructing a detailed kinetic and thermodynamic model of a complex process such as the Trp-cage folding.
The approach presented here aims at extracting the same information from a BE simulation as one can obtain from the analysis of a long ergodic MD run or of several shorter runs CITATION CITATION.
The method relies on the projection of the BE trajectory on the space defined by a set of variables, which are assumed to describe the relevant physics of the system.
These variables are not necessarily the ones that are used for the BE simulation and can be chosen FORMULA.
Once the CVs are selected, the rate model is constructed following three steps:
A cluster analysis is performed on the BE trajectories in a possibly extended CV space, assigning each configuration explored during the biased dynamics to a reference structure that is close by in CV space.
Next, the equilibrium population of each bin is calculated from the BE simulations using a weighted histogram analysis method CITATION exploiting the metadynamics bias potentials.
Finally, a kinetic model is constructed by assigning rates to transitions among bins.
The transition rates are assumed to be of the form introduced in Ref.
CITATION, namely to depend exponentially on the free energy difference between the bins with a prefactor that is determined by a diffusion matrix FORMULA and by the bins relative position.
The only free parameter in the model is FORMULA, as the free energies are already assigned.
Following Ref.
CITATION FORMULA is estimated maximizing the likelihood of an unbiased MD trajectory .
The model constructed in this manner is designed to optimally reproduce the long time scale dynamics of the system.
It can be used, for example, for characterizing the metastable misfolded intermediates of the folding process.
The advantage of using biased trajectories, besides the acceleration of slow transitions, is a greatly enhanced accuracy of the estimated free energy at transition state regions.
This approach is first illustrated on the Ace-Ala 3-Nme peptide.
This system is simple enough to allow benchmarking the results against a long standard MD simulation.
For this system the model is capable of reproducing with excellent accuracy the kinetics and thermodynamics observed in the unbiased run.
The same approach is then applied to the Trp-cage miniprotein.
A model is built that allows describing the folding process, computing the folding rates and the NMR spectra, simulating a T-jump experiment, etc. The scenario that emerges is in good agreement with the available experimental data.
By kinetic Monte Carlo CITATION, CITATION and Markov cluster analysis CITATION, CITATION several metastable sets are identified.
These states, except for the folded cluster, can be considered misfolded intermediates of the folding process.
At 298 K two main clusters are present, with a population of 58 percent and 25 percent, respectively.
The most populated is the folded state and its structural properties are very close to the NMR ensemble.
The second most populated cluster retains a significant amount of secondary structure, but has a FORMULA from the native state of approximately 4.4.
In this cluster, the Trp is trapped in a hydrophobic pocket and its distance from Pro12 and Gly11 is reduced.
The presence of this cluster in the thermal ensemble of the system can explain some anomalies in the temperature behavior observed in NMR CITATION and UV-Raman CITATION experiments.
The structures of the most populated misfolded intermediates are in good agreement with the unfolded states distances reported in Ref.
CITATION.
Using the kinetic model a fluorescence T-jump experiment is also simulated.
In agreement with the experimental results CITATION, a relaxation time of 2.3 0.7 s is found.
This time is primarily determined by the relaxation towards the folded state of a compact molten globule-like structure, which acts as a kinetic trap.
Relaxation times among all the other clusters, including transitions between fully unstructured states and the folded state, are all in the sub-microsecond time domain.
Thus, surprisingly, the relaxation time measured by fluorescence may not be directly related to the folding transition, if one calls folding the transition from a random coil to the native state.
Understanding complex networks of protein-protein interactions is one of the foremost challenges of the post-genomic era.
Due to the recent advances in experimental bio-technology, including yeast-2-hybrid, tandem affinity purification and other high-throughput methods for protein-protein interaction detection, huge amounts of PPI network data are becoming available.
Of major concern, however, are the levels of noise and incompleteness.
For example, for Y2H screens, it is thought that the false positive rate could be as high as 64 percent, and the false negative rate may range from 43 percent to 71 percent.
TAP experiments are believed to have comparable levels of noise.
We present a novel technique to assess the confidence levels of interactions in PPI networks obtained from experimental studies.
We use it for predicting new interactions and thus for guiding future biological experiments.
This technique is the first to utilize currently the best fitting network model for PPI networks, geometric graphs.
Our approach achieves specificity of 85 percent and sensitivity of 90 percent.
We use it to assign confidence scores to physical protein-protein interactions in the human PPI network downloaded from BioGRID.
Using our approach, we predict 251 interactions in the human PPI network, a statistically significant fraction of which correspond to protein pairs sharing common GO terms.
Moreover, we validate a statistically significant portion of our predicted interactions in the HPRD database and the newer release of BioGRID.
The data and Matlab code implementing the methods are freely available from the web site: LINK.
Networks are used to model natural phenomena studied in computational and systems biology.
Nodes in networks represent biomolecules such as genes or proteins, and edges between the nodes indicate interactions between the corresponding biomolecules.
These interactions could be of many different types, including functional, genetic, and physical interactions.
Understanding these complex networks is a fundamental issue in systems biology.
Of particular importance are protein-protein interaction networks.
In PPI networks, nodes correspond to proteins and two nodes are linked by an edge if the corresponding proteins can interact.
The topology of PPI networks can give new insight into the function of individual proteins, protein complexes and cellular machinery as a complex system CITATION, CITATION .
Advances in high-throughput techniques such as yeast-2-hybrid, tandem affinity purification, and mass spectrometric protein complex identification are producing a growing amount of experimental PPI data for many organisms CITATION CITATION.
However, the data produced by these techniques have very high levels of false positives and false negatives.
Y2H screens have false negative rates in the range from 43 percent to 71 percent and TAP has false negative rates of 15 percent 50 percent CITATION.
False positive rates for Y2H could be as high as 64 percent and for TAP experiments they could be as high as 77 percent CITATION.
Thus, reducing the level of noise in PPI networks and assessing the confidence of each interaction is an essential task.
Two recent studies provided two high quality PPI data sets for Saccharomyces cerevisiae CITATION, CITATION.
Gavin et al. CITATION defined socio-affinity scores measuring the log-odds of the number of times two proteins are observed together, relative to their frequency in the data set.
They use not only direct bait-prey connections but also indirect prey-prey relationships.
In this, two proteins are each identified as preys in a purification in which a third protein is used as bait.
Krogan et al. CITATION used machine learning methods, including Bayesian networks and boosted stump decision trees, to define confidence scores for potential interactions.
These scores are based on direct bait-prey observations.
They used a Markov clustering algorithm to define protein complexes.
Data sets produced by these two groups are very different and thought to contain many false positives.
In CITATION these two data sets were merged into one set of experimentally based PPIs by analyzing the primary affinity purification data using the purification enrichment scoring system.
Using the set of manually curated PPIs, they showed that this new data set is more accurate than the original individual sets and is comparable to PPIs defined using small scale experimental methods.
From the original 12,122 interactions from these two studies in the General Repository of Interaction Data CITATION they discarded 7,504 as being of low confidence.
Applying their metric they discovered 4456 new interactions, that were not among the original 12,122 interactions, and produced a set of 9,074 interactions with accuracy comparable to the accuracy of the small scale experiments.
In this paper we use this high confidence data set to test our approach.
In recent years several random graph models have been proposed to model PPI networks: Erd s-R nyi random graphs with the same degree distribution as in data CITATION, scale-free graphs CITATION, geometric random graphs CITATION CITATION, and stickiness-index-based models CITATION.
The technique presented in this paper is one of the first to use a network model of PPI networks for purposes other than just generating synthetic data.
We demonstrate that a geometric graph model can be used for assessing the confidence levels of known interactions in PPI networks and predicting novel ones.
We apply our technique to de-noise PPI data sets by detecting false positives and false negative interactions.
This new approach is compared with existing PPI network post-processing techniques in the final section.
The evolution of cooperation described in terms of simple two-person interactions has received considerable attention in recent years, where several key results were obtained.
Among those, it is now well established that the web of social interaction networks promotes the emergence of cooperation when modeled in terms of symmetric two-person games.
Up until now, however, the impacts of the heterogeneity of social interactions into the emergence of cooperation have not been fully explored, as other aspects remain to be investigated.
Here we carry out a study employing the simplest example of a prisoner's dilemma game in which the benefits collected by the participants may be proportional to the costs expended.
We show that the heterogeneous nature of the social network naturally induces a symmetry breaking of the game, as contributions made by cooperators may become contingent on the social context in which the individual is embedded.
A new, numerical, mean-field analysis reveals that prisoner's dilemmas on networks no longer constitute a defector dominance dilemma instead, individuals engage effectively in a general coordination game.
We find that the symmetry breaking induced by population structure profoundly affects the evolutionary dynamics of cooperation, dramatically enhancing the feasibility of cooperators: cooperation blooms when each cooperator contributes the same cost, equally shared among the plethora of games in which she participates.
This work provides clear evidence that, while individual rational reasoning may hinder cooperative actions, the intricate nature of social interactions may effectively transform a local dilemma of cooperation into a global coordination problem.
Portuguese is no exception: Like any other language, it has many proverbs and popular sayings.
One of them states something like: I have already contributed to that charity CITATION, concerning originally situations in which individuals are faced with the decision of offering a contribution to a common venture, the expression above meaning no.
Interestingly, the amount given is never stated.
It turns out that, quite often, we are confronted with situations in which the act of giving is more important than the amount given.
Let us keep with a charity event, in which some celebrities are invited to participate.
Typically their appearance is given maximal audience, and they are shown contributing a seemingly large amount of money to the charity's cause.
This offer is aimed at stimulating the contribution of many to the same charity, and indeed this mechanism of celebrity participation in charities is common, and presumably effective.
But what is the relevance of the amount contributed by the celebrity?
It is certainly impressive to many, despite being, most likely, a small contribution, both in face of the celebrity's wealth and also in what concerns the overall amount accumulated.
But it does induce, hopefully, a large number of contributions from anonymous charity participants, who feel compelled to contribute given the fact that their role model contributed.
In other words, the majority copies the act of giving, but certainly not the amount given.
Nowadays, web-signed petitions are also examples of collective decisions which, often, benefit from the fact that some well-known people adhere to the petition's cause.
Besides those who are fully aware and agree with the cause, there are also those who sign the petition simply because they admire someone who has signed the petition, again copying the attitude.
Many other examples from real life could be provided along similar lines, from trivia, to fads, to stock markets, to Humanitarian causes up to the salvation of planet Earth CITATION CITATION.
From a theoretical perspective, many of these situations provide beautiful examples of public goods games CITATION, CITATION which are often hard to dissociate from reputation building, social norms and moral principles CITATION CITATION.
This intricate interplay reflects the many-body nature and multi-level complexity of the interactions among the social atoms CITATION .
The simplest PGG involves two persons.
Both have the opportunity to contribute a cost c to a common pool.
A Cooperator is one who contributes; otherwise she is a Defector.
The total amount is multiplied by an enhancement factor F and equally shared between the two participants.
Hence, player i using strategy s i gets a payoff FORMULA from this game, leading to the following payoff matrixFORMULA
For FORMULA Ds dominate unconditionally.
For F 2 no strategy is favored in well mixed populations ; yet, for FORMULA, it is better to play C despite the fact that, in a mixed pair, a D collects a higher payoff than a C. For FORMULA the game is an example of the famous symmetric one-shot two-person prisoner's dilemma CITATION, on which many central results have been obtained over the years, in particular in the context of evolutionary game theory CITATION, CITATION : In 1992 CITATION it has been explicitly shown that population structure matters, despite its importance being recognized already by Darwin, albeit in the form of Group selection CITATION, CITATION.
It clearly makes a difference whether everybody is equally likely to interact with anybody else in the population or not.
In 2004 we learnt that evolutionary game theory in finite populations may behave very differently from that on infinite populations CITATION, even in the absence of any population structure, Evolutionarily Stable Strategies becoming population size dependent.
In 2005 we learnt that heterogeneous population structures play an important role in the evolution of cooperation under the prisoner's and other social dilemmas CITATION, CITATION, a result which spawned a number of investigations CITATION CITATION.
In 2006 a mathematical condition was obtained for Cs to become advantageous on populations structured along the links of homogeneous networks CITATION, subsequently confirmed making use of inclusive fitness methods CITATION for a limited subset of game payoff matrices.
This result, valid in the limit of weak selection, has also unraveled an important feature of evolutionary game theoretical studies: The outcome of cooperation depends on the evolutionary dynamics adopted, dictating how individual strategy evolves from generation to generation.
Furthermore, evolutionary game dynamics on populations structured along multiple networks has been explored CITATION, CITATION, as well as the mechanisms which favor cooperation under adaptive population structures have been identified, both for non-repeated CITATION CITATION and repeated games CITATION, CITATION.
These results consubstantiate and keep stimulating an enormous amount of research work.
Common to all these studies are the settings underlying the social dilemma: in the conventional view, every C pays a fixed cost c per game, providing the same benefit b to the partner.
However, if what matters is the act of giving and not the amount given, then there is no reason to assume that everybody contributes the same cost c to each game.
Depending on the amount of each individual contribution, the overall result of the evolutionary dynamics may change.
The two person game introduced above provides not only the ideal ground to introduce such a diversity of contributions, but also an intuitive coupling between game dynamics and social embedding: The first individual contributes a cost c 1 if playing C and nothing otherwise.
Hence, player i now gets the following payoff from this game:FORMULAreflecting the symmetry breaking induced by possibly different contributions from different cooperating individuals.
This poses a natural question: Who will acquire an evolutionary edge under these conditions?
Often the amount that each individual contributes is correlated with the social context she is actually embedded in CITATION, CITATION, CITATION.
Modern communities are grounded in complex social networks of investment and cooperation, in which some individuals play radically different roles and interact more and more often than others.
Empirical studies have demonstrated that social networks share both small-world properties and heterogeneous distribution of connectivities CITATION CITATION.
In such heterogeneous communities, where different individuals may be embedded in very different social environments, it is indeed hard to imagine that every C will always provide the same amount in every game interaction, hence reducing the problem to the standard two-person prisoner's dilemma studied so far.
In the context of N-person games played in prototypical social networks, it has been found that the diversity of contributions greatly favors cooperation CITATION.
However, and similar to the relation between two-body and many-body interactions in the Physical Sciences, N-person public goods games have an intrinsic complexity which cannot be anticipated from two-person games: In the words of late William Hamilton, The theory of many person games may seem to stand to that of two-person games in the relation of sea-sickness to a headache CITATION .
Here, and besides the conventional scenario in which every C contributes the same cost c to each game she participates, we shall also explore the limit in which every C contributes the same overall amount c. However, this amount is shared between all games she participates, which are defined by the social network in which the players are embedded.
For instance, c may be interpreted as the availability or the amount of resources each individual has to dedicate to all her commitments.
Hence, the contribution to each game will depend now on the social context of each C, and heterogeneity will foster a symmetry breaking of pair-wise interactions, as two individuals may contribute different amounts to the same game.
In this sense, cooperation will be identified with the act of giving and no longer with the amount given.
Transitive inference, class inclusion and a variety of other inferential abilities have strikingly similar developmental profiles all are acquired around the age of five.
Yet, little is known about the reasons for this correspondence.
Category theory was invented as a formal means of establishing commonalities between various mathematical structures.
We use category theory to show that transitive inference and class inclusion involve dual mathematical structures, called product and coproduct.
Other inferential tasks with similar developmental profiles, including matrix completion, cardinality, dimensional changed card sorting, balance-scale, and Theory of Mind also involve these structures.
By contrast, products are not involved in the behaviours exhibited by younger children on these tasks, or simplified versions that are within their ability.
These results point to a fundamental cognitive principle under development during childhood that is the capacity to compute products in the categorical sense.
Children acquire various reasoning skills over remarkably similar periods of development.
Transitive Inference and Class Inclusion are two behaviours among a suite of inferential abilities that have strikingly similar developmental profiles all are acquired around the age of five years CITATION.
For example, older children can infer that if John is taller than Mary, and Mary is taller than Sue, then John is taller than Sue.
This form of reasoning is called Transitive Inference.
Older children also understand that a grocery store will contain more fruit than apples.
That is, the number of items belonging to the superclass is greater than the number of items in any one of its subclasses.
This form of reasoning is called Class Inclusion.
These two types of inference appear to have little in common.
Transitive Inference typically involves physical relationships between objects, while Class Inclusion involves abstract relative sizes of object classes.
Nonetheless, explicit tests of these and other inferences for a range of age groups revealed that success was attained from about the median age of five years CITATION .
Since Piaget, decades of research have revealed important clues regarding the development of inference, yet little is known about the reasons underlying these correspondences.
A common theme in two recent proposals is the computing of relational information CITATION, CITATION.
In regard to Relational Complexity theory CITATION, the correspondence between commonly acquired cognitive behaviours is based on the maximum arity of relations that must be processed.
In regard to Cognitive Complexity and Control theory CITATION, the correspondence is based on the common depth of relation hierarchies.
Although a relational approach to cognitive behaviour has a formal basis in relational algebra CITATION, certain assumptions must be made about the units of analysis.
For tasks as diverse in procedure and content as Transitive Inference and Class Inclusion, it is difficult to see how the analysis of one task leads naturally to the other.
For Relational Complexity theory, Transitive Inference is considered to involve the integration of two binary relations between task elements into an ordered triple, or ternary relation; whereas Class Inclusion is regarded as the integration of three binary relations between three sets of elements into a ternary relation CITATION, CITATION.
For Cognitive Complexity and Control theory, Transitive Inference involves relations over items; whereas Class Inclusion involves relations over sets of items.
This theoretical difficulty is symptomatic of the general problem in cognitive science where the basic components of cognition are unknown.
In the absence of such detailed knowledge, cognitive modelers have been forced to assume a particular representational format.
This approach, however, does not lend itself to the current problem, because the elements of Transitive Inference and Class Inclusion tasks do not share a common basis.
Understandably, then, these sorts of behaviours have tended to be studied in detailed isolation, narrowing the scope for identifying general principles.
Category theory was born out of a desire to establish formal commonalities between various mathematical structures CITATION, CITATION, and has since been applied to the analysis of computational structures in computer science.
The seminal insight was a shift from objects as the primary focus of analysis to their transformations.
Contrast, for instance, sets defined in terms of the objects they contain Set Theory against sets defined in terms of the morphisms that map to or from them Category Theory CITATION.
This insight motivates our categorical approach to the analysis of inference, and our way around the current impasse.
In cognitive science, several authors have used category theory for a conceptual analysis of space and time CITATION CITATION, though we know of only one other application that has modeled empirical data CITATION.
Since our application of category theory to cognitive behaviour is novel, we first introduce the basic category theory constructs needed for our subsequent analysis of Transitive Inference, Class Inclusion, and other paradigms.
The analysis begins with a brief introduction of the sort of data our approach is intended to explain, which primarily concerns contrasts between younger and older children relative to age five, and correlations across paradigms.
Finally, we extend our categorical approach to more complex levels of inference.
Our main point is that, despite the apparent lack of resemblance, all these tasks are formally connected via the categorical product, to be defined below.
The significance of this result is that it opens the door to an entirely new approach to identifying general principles, particularly in regard to the development of inferential abilities, that are less likely to be revealed by standard modeling methods.
This genome-scale study analysed the various parameters influencing protein levels in cells.
To achieve this goal, the model bacterium Lactococcus lactis was grown at steady state in continuous cultures at different growth rates, and proteomic and transcriptomic data were thoroughly compared.
Ratios of mRNA to protein were highly variable among proteins but also, for a given gene, between the different growth conditions.
The modeling of cellular processes combined with a data fitting modeling approach allowed both translation efficiencies and degradation rates to be estimated for each protein in each growth condition.
Estimated translational efficiencies and degradation rates strongly differed between proteins and were tested for their biological significance through statistical correlations with relevant parameters such as codon or amino acid bias.
These efficiencies and degradation rates were not constant in all growth conditions and were inversely proportional to the growth rate, indicating a more efficient translation at low growth rate but an antagonistic higher rate of protein degradation.
Estimated protein median half-lives ranged from 23 to 224 min, underlying the importance of protein degradation notably at low growth rates.
The regulation of intracellular protein level was analysed through regulatory coefficient calculations, revealing a complex control depending on protein and growth conditions.
The modeling approach enabled translational efficiencies and protein degradation rates to be estimated, two biological parameters extremely difficult to determine experimentally and generally lacking in bacteria.
This method is generic and can now be extended to other environments and/or other micro-organisms.
In the era of omics, systems biology has emerged with the availability of genome-wide data from different levels, i.e. genome, transcriptome, proteome, metabolome CITATION, CITATION.
This approach aims at integrating omics data, mainly through computational and mathematical models CITATION, CITATION so as to decipher biological systems as a whole CITATION.
The integration of transcriptomic and proteomic results is a huge challenge by itself.
The literature usually exploits these two approaches as complementary tools and does not often provide a correct confrontation of the two datasets.
Until now, only a few researchers, mainly interested in yeast physiology CITATION, CITATION, have been working on this aspect and the results typically revealed modest correlations between those two datasets CITATION CITATION.
These weak correlations between transcript and protein levels can be the consequence of the involvement of post-transcriptional regulations CITATION, such as translation control and protein degradation as evidenced by Brockmann et al. CITATION.
Translation regulations are believed to be involved in protein level control but are generally studied at the level of controlling specific molecular mechanisms and not at the genome scale CITATION CITATION.
Although polysome profile analysis allows translation efficiencies to be experimentally determined for the various transcripts simultaneously, this technique has been only rarely used and almost exclusively for S. cerevisiae CITATION.
Protein stability can also influence intracellular protein level and the correlation between transcript and protein CITATION, CITATION, CITATION.
However protein stability is rarely studied at the genome scale and data are only available for S. cerevisiae CITATION, CITATION.
Finally, the rate of protein disappearance due to protein dilution by cellular growth is also potentially involved in protein level modifications but this physical phenomenon is generally neglected.
More generally, even if translation efficiency, protein degradation and dilution rate can all influence protein levels, these parameters are not usually studied simultaneously.
The role of each parameter in a whole cellular adaptation process has not been elucidated and it is not clearly known today which parameter is preponderant and if the control is constant or not when environmental conditions are modified.
The aim of this study was to analyse the control of intracellular protein level taking into account all the parameters of this control, in a prokaryotic organism, the model of lactic acid bacteria, Lactococcus lactis.
To achieve this purpose, transcriptomic and proteomic analyses were performed with cells from the same culture.
Transcriptomic data were already available CITATION and the corresponding proteome measurement was performed.
The whole protein related processes including translation, dilution rate and protein degradation were modelled, and, since biological data were obtained at steady state, equations describing the protein levels equilibrium were solved.
This modeling approach allowed translation efficiency and protein degradation to be estimated and the relative involvement of all the various parameters of protein control to be analysed.
The differentiation of embryonic stem cells is initiated by a gradual loss of pluripotency-associated transcripts and induction of differentiation genes.
Accordingly, the detection of differentially expressed genes at the early stages of differentiation could assist the identification of the causal genes that either promote or inhibit differentiation.
The previous methods of identifying differentially expressed genes by comparing different cell types would inevitably include a large portion of genes that respond to, rather than regulate, the differentiation process.
We demonstrate through the use of biological replicates and a novel statistical approach that the gene expression data obtained without prior separation of cell types are informative for detecting differentially expressed genes at the early stages of differentiation.
Applying the proposed method to analyze the differentiation of murine embryonic stem cells, we identified and then experimentally verified Smarcad1 as a novel regulator of pluripotency and self-renewal.
We formalized this statistical approach as a statistical test that is generally applicable to analyze other differentiation processes.
Cellular differentiation is the process by which a less specialized cell becomes a more specialized cell type, characterized by the expression pattern of a subset of genes during the differentiation process.
The search for marker genes is widely pursued in almost every differentiation process, although a principled approach is still missing.
The current practice is to separate distinguishable cell types, measure gene expression from each cell type, and then identify differentially expressed genes.
Such methods require the expression data for both cell types to be available.
A limitation of these methods is that by the time the cell types are distinguishable, for example by morphology, many genes have already shown differential expression.
This set of differentially expressed genes may include the class of early marker genes that are enriched for markers of early differentiating cell lineages as well as genes whose down-regulation triggers differentiation.
However, the set of differentially expressed genes will also include a second, larger class of genes in which gene expression is not important to the regulation of the differentiation process but in which genes are simply characteristic of the fully differentiated cell types.
Traditional sample comparison procedures are not designed to separate the two classes differentially expressed genes and as a result, the large lists of differentially expressed genes usually do not provide direct guidance for dissecting underlining mechanisms of differentiation.
Recognizing early marker genes enables separation of cell types at an early stage of differentiation; in turn, separating cell types at an early stage of differentiation enables identification of early marker genes.
However, neither piece of the puzzle is currently available to a study of a new differentiation process.
We demonstrate that, contrary to common belief, early marker genes can be detected by measuring the average expression of a mixture of cell types, provided that enough biological replicates have been measured and statistical test based on variance ratio has been used.
We provide the theoretical reasoning, a statistical method, and two validation experiments.
Activity in neural circuits is spatiotemporally organized.
Its spatial organization consists of multiple, localized coherent patterns, or patchy clusters.
These patterns propagate across the circuits over time.
This type of collective behavior has ubiquitously been observed, both in spontaneous activity and evoked responses; its function, however, has remained unclear.
We construct a spatially extended, spiking neural circuit that generates emergent spatiotemporal activity patterns, thereby capturing some of the complexities of the patterns observed empirically.
We elucidate what kind of fundamental function these patterns can serve by showing how they process information.
As self-sustained objects, localized coherent patterns can signal information by propagating across the neural circuit.
Computational operations occur when these emergent patterns interact, or collide with each other.
The ongoing behaviors of these patterns naturally embody both distributed, parallel computation and cascaded logical operations.
Such distributed computations enable the system to work in an inherently flexible and efficient way.
Our work leads us to propose that propagating coherent activity patterns are the underlying primitives with which neural circuits carry out distributed dynamical computation.
To understand brain function, it is essential to study the collective electrical activity of neural circuits CITATION.
This activity typically exhibits intriguing spatiotemporally organized patterns: they are commonly observed in multi-unit electrophysiological recording, EEG local field potential recording, MEG, optical imaging and fMRI imaging, both in spontaneous activity CITATION CITATION and evoked responses CITATION CITATION.
In space, these patterns often take the form of localized patches or clusters of activity CITATION CITATION.
Recordings over large populations of neurons have shown that several of such localized patterns can occur simultaneously across cortical regions CITATION CITATION.
Over time, these patterns often do not remain at specific locations.
As self-sustained entities, they propagate or move about in space CITATION CITATION, CITATION CITATION.
In doing so, they interact with each other, resulting in dynamical collective behavior.
Here we will consider what kind of functional role this behavior may have.
Propagating coherent patterns have been registered in the experimental literature as spreading or drifting activity CITATION CITATION or as traveling waves CITATION CITATION.
The simultaneous presence of several of these patterns has been observed in the spontaneous activity of cat visual cortex CITATION, CITATION, in evoked response patterns in turtle olfactory bulb CITATION, and visual cortex of various species CITATION, CITATION, as well as in sensorimotor cortex of behaving mice CITATION.
When several localized, moving patterns occur together, they are likely to interact.
Indeed, interactions have been shown to occur in rat somatosensory cortex CITATION.
To describe the collective activity in olfactory, visual, auditory and somatosensory cortices of behaving rabbits, the term interacting wave packets was explicitly used CITATION, CITATION, which nicely captures the relevance of propagations and interactions of these patterns.
Despite the ubiquity of these patterns and their interactions, their fundamental functional role has remained unknown.
Although some authors have speculated on the role of propagating waves CITATION, the functional implications of other aspects such as the simultaneous presence of multiple propagating patterns or their interactions have remained completely unclear.
Current theoretical frameworks describe neural activity either in computational or dynamical systems perspectives.
Conventional computational theory is based on the manipulation and representation of static symbols CITATION.
This perspective contradicts the temporal variability of brain activity, which calls for a dynamical systems approach.
When dynamical systems theories are applied to neuroscience, the prevailing concept is that of stable low-dimensional attractors CITATION.
This notion, although it has provided many important insights, is less suitable to capture the functional role of brain activity in its actual spatiotemporal complexity.
We need to resolve the restrictions of conventional computation and standard dynamical systems theories, in order to describe neural activity and understand its fundamental function.
This study is based on the consideration that neural circuits are spatially-extended, pattern-forming systems, containing large numbers of simple neurons with spatially restricted connectivity CITATION, CITATION, CITATION.
In spatially extended physical systems composed of large numbers of simple interacting elements, such as reaction-diffusion systems and fluidic systems, localized propagating coherent patterns are a common feature known under different names, including wave packets, spots, breathers and soliton waves, amongst others CITATION, CITATION, CITATION.
They are an emergent, collective property of these systems.
Using these systems as analogy, we construct a simple, spatially extended neural circuit model to represent the gross architecture within the cerebral cortex.
As an emergent, collective property of the system, the circuit exhibits dynamical activity patterns, reproducing some of the complexities observed in empirical studies.
In particular, the circuit provides simultaneous propagation of multiple locally coherent patterns and their interactions.
By revealing how their ongoing collective behavior can naturally embody computation, we demonstrate what fundamental function these patterns can serve.
Propagating coherent spiking patterns can support several essential aspects of a computational processing.
As self-sustained objects, these patterns can signal information by propagating across neural circuits.
Information processing, or computation, occurs when they interact or, specifically, collide with each other.
Collectively, these patterns perform distributed, parallel and cascaded computational operations, thereby enabling neural systems to work in an efficient and flexible way.
We shall call this distributed dynamical computation, which is proposed as a framework for understanding spatiotemporal propagating activity patterns in neural circuits.
This understanding links their dynamics with a form of non-conventional, abstract computation.
The AraC family transcription factor MarA activates 40 genes of the Escherichia coli chromosome resulting in different levels of resistance to a wide array of antibiotics and to superoxides.
Activation of marA/soxS/rob regulon promoters occurs in a well-defined order with respect to the level of MarA; however, the order of activation does not parallel the strength of MarA binding to promoter sequences.
To understand this lack of correspondence, we developed a computational model of transcriptional activation in which a transcription factor either increases or decreases RNA polymerase binding, and either accelerates or retards post-binding events associated with transcription initiation.
We used the model to analyze data characterizing MarA regulation of promoter activity.
The model clearly explains the lack of correspondence between the order of activation and the MarA-DNA affinity and indicates that the order of activation can only be predicted using information about the strength of the full MarA-polymerase-DNA interaction.
The analysis further suggests that MarA can activate without increasing polymerase binding and that activation can even involve a decrease in polymerase binding, which is opposite to the textbook model of activation by recruitment.
These findings are consistent with published chromatin immunoprecipitation assays of interactions between polymerase and the E. coli chromosome.
We find that activation involving decreased polymerase binding yields lower latency in gene regulation and therefore might confer a competitive advantage to cells.
Our model yields insights into requirements for predicting the order of activation of a regulon and enables us to suggest that activation might involve a decrease in polymerase binding which we expect to be an important theme of gene regulation in E. coli and beyond.
Transcription factors control cellular protein production by binding to DNA and changing the frequency with which mRNA transcripts are produced.
There are hundreds of transcription factors in Escherichia coli and while most of these target only a small number of genes, there are several that regulate expression of ten or more genes.
Taken together, such global transcription factors directly regulate more-than half of the 4,300 genes in E. coli and their regulatory interactions yield important insights into the organization of the genetic regulatory network CITATION, CITATION, CITATION.
Because they regulate so many genes, global transcription factors also play a large role in controlling cellular behavior; however, insights into behavior are currently limited by a lack of quantitative information about how transcription factors differentially regulate target genes.
One important global transcription factor is MarA, an AraC family protein that activates 40 genes of the Escherichia coli chromosome resulting in different levels of resistance to a wide array of antibiotics and superoxides.
The effect of MarA at different promoters can vary due to changes in the detailed sequence of the DNA-binding site and its distance from and orientation with respect to the promoter CITATION, CITATION.
These variations can influence the order in which the promoters respond to increasing concentrations of MarA and presumably have important functional consequences for E. coli.
To characterize quantitative variations in MarA regulation at different promoters, we recently placed the expression of MarA under the control of the LacI repressor, determined the relationship between isopropyl -D-1-thiogalactopyranoside concentration and the intracellular concentration of MarA, and examined the expression of 10 promoters of the regulon as a function of activator concentration CITATION.
We found that activation of marA/soxS/rob regulon promoters occurs in a well-defined order with respect to the level of MarA, enabling cells to mount a response that is commensurate to the level of threat detected in the environment.
We also found that only the marRAB, sodA, and micF promoters were saturated at the highest level of MarA.
In contrast with a commonly held assumption, we found that the order of activation does not parallel the strength of MarA binding to promoter sequences.
This finding suggested that interactions between MarA and the RNA polymerase transcriptional machinery play an important role in determining the order of activation, but the data did not immediately reveal what the nature of these interactions might be at the various promoters.
Here, we have developed a computational model of promoter activity to understand how interactions between MarA and polymerase activate transcription at the marRAB, sodA, and micF promoters of the 10 we examined previously, these three promoters are the only ones that exhibited saturation, which provides an important constraint for the modeling.
The model was specifically designed to compare a strict recruitment model in which MarA increases polymerase binding but does not increase the rate of post-binding events CITATION, CITATION, with a more general model in which activator can either increase or decrease polymerase binding, and can either increase or decrease the rate of post-binding events.
For each promoter, we evaluated the agreement of both the strict recruitment model and the general model with the data at many points within a physically reasonable region of parameter space.
The model successfully explains why the order of promoter activation does not parallel the strength of MarA-DNA binding.
For all promoters, the best fit of the general model was better than that of the strict recruitment model.
Comparison to the strict recruitment model and full analysis of the goodness-of-fit landscape suggest that MarA does not increase polymerase binding but does increase the rate of post-binding events at these promoters.
Moreover, the analysis for the micF promoter suggests that MarA activation can involve a decrease in polymerase binding that is associated with low latency in gene regulation.
We discuss the broader significance of these findings.
Evolution is shaping the world around us.
At the core of every evolutionary process is a population of reproducing individuals.
The outcome of an evolutionary process depends on population structure.
Here we provide a general formula for calculating evolutionary dynamics in a wide class of structured populations.
This class includes the recently introduced games in phenotype space and evolutionary set theory.
There can be local interactions for determining the relative fitness of individuals, but we require global updating, which means all individuals compete uniformly for reproduction.
We study the competition of two strategies in the context of an evolutionary game and determine which strategy is favored in the limit of weak selection.
We derive an intuitive formula for the structure coefficient,, and provide a method for efficient numerical calculation.
Constant selection implies that the fitness of individuals does not depend on the composition of the population.
In general, however, the success of individuals is affected by what others are doing.
Then we are in the realm of game theory CITATION CITATION or evolutionary game theory CITATION CITATION.
The latter is the study of frequency dependent selection; the fitness of individuals is typically assumed to be a linear function of the frequencies of strategies in the population.
The population is trying to adapt on a dynamic fitness landscape; the changes in the fitness landscape are caused by the population that moves over it CITATION.
There is also a close relationship between evolutionary game theory and ecology CITATION : the success of a species in an ecosystem depends on its own abundance and the abundance of other species.
The classical approach to evolutionary game dynamics is based on deterministic differential equations describing infinitely large, well-mixed populations CITATION, CITATION.
In a well-mixed population any two individuals interact equally likely.
Some recent approaches consider stochastic evolutionary dynamics in populations of finite size CITATION, CITATION.
Evolutionary game dynamics are also affected by population structure CITATION CITATION.
For example, a well-mixed population typically opposes evolution of cooperation, while a structured population can promote it.
There is also a long standing tradition of studying spatial models in ecology CITATION CITATION, population genetics CITATION, CITATION and inclusive fitness theory CITATION CITATION .
Evolutionary graph theory is an extension of spatial games, which are normally studied on regular lattices, to general graphs CITATION CITATION.
The graph determines who meets whom and reflects physical structure or social networks.
The payoff of individuals is derived from local interactions with their neighbors on the graph.
Moreover, individuals compete locally with their neighbors for reproduction.
These two processes can also be described by separate graphs CITATION .
Games in phenotype space CITATION represent another type of spatial model for evolutionary dynamics, which is motivated by the idea of tag based cooperation CITATION CITATION.
In addition to behavioral strategies, individuals express other phenotypic features which serve as markers of identification.
In one version of the model, individuals interact only with those who carry the same phenotypic marker.
This approach can lead to a clustering in phenotype space, which can promote evolution of cooperation CITATION .
Evolutionary set theory represents another type of spatial model CITATION.
Each individual can belong to several sets.
At a particular time, some sets have many members, while others are empty.
Individuals interact with others in the same set and thereby derive a payoff.
Individuals update their set memberships and strategies by global comparison with others.
Successful strategies spawn imitators, and successful sets attract more members.
Therefore, the population structure is described by an ever changing, dynamical graph.
Evolutionary dynamics in set structured populations can favor cooperators over defectors.
In all three frameworks evolutionary graph theory, games in phenotype space and evolutionary set theory the fitness of individuals is a consequence of local interactions.
In evolutionary graph theory there is also a local update rule: individuals learn from their neighbors on the graph or compete with nearby individuals for placing offspring.
For evolutionary set theory, however, CITATION assumes global updating: individuals can learn from all others in the population and adopt their strategies and set memberships.
Global updating is also a feature of the model for games in phenotype space CITATION.
The approach that is presented in this paper requires global updating.
Therefore, our result holds for evolutionary set theory and for games in phenotype space, but does not apply to evolutionary graph theory.
Co-expression networks are routinely used to study human diseases like obesity and diabetes.
Systematic comparison of these networks between species has the potential to elucidate common mechanisms that are conserved between human and rodent species, as well as those that are species-specific characterizing evolutionary plasticity.
We developed a semi-parametric meta-analysis approach for combining gene-gene co-expression relationships across expression profile datasets from multiple species.
The simulation results showed that the semi-parametric method is robust against noise.
When applied to human, mouse, and rat liver co-expression networks, our method out-performed existing methods in identifying gene pairs with coherent biological functions.
We identified a network conserved across species that highlighted cell-cell signaling, cell-adhesion and sterol biosynthesis as main biological processes represented in genome-wide association study candidate gene sets for blood lipid levels.
We further developed a heterogeneity statistic to test for network differences among multiple datasets, and demonstrated that genes with species-specific interactions tend to be under positive selection throughout evolution.
Finally, we identified a human-specific sub-network regulated by RXRG, which has been validated to play a different role in hyperlipidemia and Type 2 diabetes between human and mouse.
Taken together, our approach represents a novel step forward in integrating gene co-expression networks from multiple large scale datasets to leverage not only common information but also differences that are dataset-specific.
The advent of expression profiling and other high throughput technologies has enabled us to systematically study complex human diseases by simultaneously measuring tens of thousands of molecular species in any given cell-based system CITATION.
It is now routine to organize such large-scale gene expression data into co-expression networks to shed light on the functional relationships among genes, and between genes and disease traits CITATION, CITATION, CITATION, CITATION.
Analysis of co-expression networks can be used to study any tissue or organ, as long as the samples from such organs are collected in a population setting.
Given that mouse and rat populations are commonly used to study human diseases in this manner, it is important to understand the mechanisms that are conserved between human and the rodent species, especially as we seek better predictions of the efficacy of drug targets identified from mouse or rat in human populations.
In addition, identifying mechanisms that differ between humans and rodents can help to improve the design and interpretation of toxicity studies that involve rodent models.
Meta-analysis is the statistical synthesis of data by aggregating results from a set of comparable studies CITATION.
It can be used to systematically examine similarities and differences between molecular profiling studies carried out in populations from different species CITATION.
In a gene co-expression network, relationship between gene pairs is usually measured by correlation coefficients of different forms, such as Pearson correlation, Spearman correlation, or Mutual Information.
Therefore, the problem of combining or comparing co-expression relationships across multiple datasets can be framed in the context of a meta-analysis of correlation coefficients, for which various methods have already been introduced.
One method is Fisher's Inverse FORMULA test, which computes a combined statistic from the p-values of the correlation coefficients obtained from individual datasets as, FORMULA.
Under fairly general conditions this statistic follows a FORMULA distribution with FORMULA degrees of freedom under the joint null hypothesis of no correlation, making it possible to compute p-values of the combined statistic.
Another widely used meta-analysis method involves computing a weighted average of a common metric derived from correlation coefficients in the individual datasets.
Such statistic can then be used to test for homogeneity over the individual measures and for statistical significance.
Datasets in this type of meta-analysis are typically weighted by the accuracy of the effect size they provide, which is a function of the individual sample sizes.
Once the mean effect size is calculated, its statistical significance can be assessed by estimating the pooled variance of the mean effect size.
In defining the effect size, Hedges and Olkin CITATION and Rosenthal and Rubin CITATION both advocated converting the correlation coefficient into a standard normal metric using Fisher's Z-transformation and then calculating a weighted average of these transformed scores.
Depending on whether the effect sizes are assumed to be equal or not in the multiple datasets, fixed effect as well as random effect models can be employed.
In the fixed effect models, the effect size in the population is a fixed but unknown constant and therefore is assumed to be the same for all datasets included in the meta-analysis.
For random effect models, effect sizes may vary from dataset to dataset, and are assumed to be a random sample of all population effect sizes.
Hunter and Schmidt CITATION introduced a single random-effects method based on untransformed correlation coefficients.
One important feature of this type of method is that heterogeneity of the effect sizes can be estimated, which provides a way to assess the difference in correlation coefficients across multiple datasets.
Schulze CITATION provided a thorough review of these meta-analysis methods and their applications.
For a meta-analysis of co-expression networks from diverse datasets, such as those constructed from different species, one central issue is that it is often unreasonable to assume that every gene pair has a unique, true effect size across evolutionarily diverse species.
Although random effect models provide a more realistic way to accommodate cross species variation, it still assumes a parametric distribution on the population effect sizes.
To circumvent this problem, a non-parametric meta-analysis method was introduced for the identification of conserved co-expression modules from human, fly, worm and yeast CITATION.
In this method, Pearson correlation coefficients of expression profiles between every gene pair were computed in each organism and then rank-transformed according to their correlations with all other genes.
A probabilistic test based on order statistics was then applied to evaluate the probability of observing a particular configuration of ranks across the different organisms by chance.
The advantage of this method is two-fold: because the method is based on non-parametric statistics, it makes no assumption on the underlying distribution of correlation coefficients across multiple datasets; and the effect size is defined in a gene-centric fashion such that for any given gene, correlations with all other genes are considered.
However, the method also has several limitations including the loss of power in general given the non-parametric formulization CITATION, CITATION, and the meta-analysis results cannot be represented in the same format as the individual datasets given there is no concept of a mean effect size.
The details of individual methods are presented in the Methods section.
Their pros and cons are summarized in Supplementary Table S1.
In this paper, we develop a method for the meta-analysis of diverse datasets generated across multiple species.
Our method is semi-parametric in nature, requiring fewer assumptions on the distribution of the effect size than a purely parametric approach while retaining better statistical power than a fully non-parametric method.
It also defines an effect size that is gene centric, allows for the computation of a mean effect size, and leads to a heterogeneity statistic to test for differences in correlation structures among distinct datasets.
Unlike most network alignment algorithms CITATION, CITATION, CITATION, CITATION, CITATION or connectivity-based approaches CITATION, our method does not rely on the networks inferred a-priori from individual datasets, but instead focuses on the development of rigorous statistics to test directly the relationship between every gene pair.
The simulation results showed that our method is robust against noises.
When applied to a human, mouse and rat cross species meta-analysis of liver co-expression networks, we demonstrate that our method out-performs existing methods in identifying functionally coherent gene pairs that are conserved among the three species.
Our method also leads to the identification of modules of co-expressed genes that represent core functions of the liver that have been conserved throughout evolution.
Both highly replicated and less confident genome-wide association study candidate genes for blood lipid levels are found to be enriched in the conserved modules, providing a systematic way to elucidate the mechanisms affecting blood lipid levels.
Application of our test for homogeneity leads to the identification of a single sub-network driven by ApoE that distinguishes two nearly identical experimental cross populations whose genetic backgrounds only vary with respect to the gene ApoE.
We further demonstrate that genes involved in human- or rodent- specific liver interactions tend to be under positive selection throughout evolution.
Finally, we identified a human-specific sub-network regulated by RXRG, which has been validated to play a different role in hyperlipidemia and Type 2 diabetes between human and mouse.
Taken together, our approach represents a novel step forward in integrating gene co-expression networks from multiple large scale datasets to leverage not only conserved information but also differences that are dataset-specific.
Representing and analyzing complex networks remains a roadblock to creating dynamic network models of biological processes and pathways.
The study of cell fate transitions can reveal much about the transcriptional regulatory programs that underlie these phenotypic changes and give rise to the coordinated patterns in expression changes that we observe.
The application of gene expression state space trajectories to capture cell fate transitions at the genome-wide level is one approach currently used in the literature.
In this paper, we analyze the gene expression dataset of Huang et al. which follows the differentiation of promyelocytes into neutrophil-like cells in the presence of inducers dimethyl sulfoxide and all-trans retinoic acid.
Huang et al. build on the work of Kauffman who raised the attractor hypothesis, stating that cells exist in an expression landscape and their expression trajectories converge towards attractive sites in this landscape.
We propose an alternative interpretation that explains this convergent behavior by recognizing that there are two types of processes participating in these cell fate transitions core processes that include the specific differentiation pathways of promyelocytes to neutrophils, and transient processes that capture those pathways and responses specific to the inducer.
Using functional enrichment analyses, specific biological examples and an analysis of the trajectories and their core and transient components we provide a validation of our hypothesis using the Huang et al. dataset.
Our understanding of the molecular basis of a wide range of biological processes, including development, differentiation, and disease, has evolved significantly in recent years.
Increasingly, we are coming to recognize that it is not single genes, but rather complex networks of genes, gene products, and other cellular elements that drive cellular metabolism and cell fate, and when perturbed, can lead to development of disease phenotypes.
Representing and analyzing such complex networks, encompassing thousands or tens of thousands of elements, presents significant challenges.
One approach that has begun to be applied is the representation of transcriptional changes as transitions that occur with the state space defined by the expression states of all genes within the cell CITATION, CITATION.
This approach has a number of advantages, including providing a framework for predictive modeling and the incorporation of stochastic components in the biological process.
The underlying assumption in such an analysis is that each cellular phenotype can invariably be traced back to a particular class of genome-wide gene expression signatures representing a specific region of the gene expression state space.
As described in Huang et al. CITATION, this signature for a particular cellular state at a particular instant in time is represented by a multidimensional gene expression vector in a high dimensional space where each coordinate represents the expression level of a particular gene.
By considering all possible configurations that this signature can take, we create a multidimensional landscape that is referred to as the expression state space CITATION.
Each observed phenotype can be represented as a single point in the state space.
When cells transition through successive phenotypes, for example, during the different stages of hematopoietic differentiation, specific sets of genes alter their expression levels as dictated by an underlying transcriptional program and these changes can be represented by a continuous trajectory in expression state space; ultimately these represent the transcriptional program being played out by the cell's collection of gene networks and complex pathways.
Kauffman CITATION first proposed the idea that stable cell fates, the cellular phenotypes we observe, correspond to attractors in the expression state space, stable points to which the system would return to if subjected to a small perturbation.
He points out that in principle cells could adopt any permutation of gene expression states however this is not what we observe in nature.
According to Kauffman, since there are about 250 different cell types, there must be approximately that number of attractors in state space, either valleys or peaks in the landscape, that represent the stable cell fates or cell types that cells will ultimately converge to in the presence of an inducer or perturbation.
While this is an interesting model, direct experimental evidence supporting it and its overall utility in explaining cellular mechanism remain to be seen.
Huang et al. CITATION reported evidence they claim demonstrated the existence of an attractor.
They conducted a gene expression time-course experiment on the differentiation of human HL-60 promyelocytic cells into neutrophils using two different inducers, dimethyl sulfoxide and all-trans retinoic acid.
Time-course data was collected using Affymetrix U95Av2 GeneChips and analyzed to provide gene expression level measures necessary to create a state-space model.
Using principal components analysis, they develop a two-dimensional state space representation in which DMSO and ATRA induce initially divergent trajectories that, over time, converge on a common trajectory leading to a final expression state representing the neutrophils.
They argue that instead of observing trajectories that explore the state space, the trajectories display convergence to a single point and that this therefore provides empirical proof that attractive states exist in nature.
Here, we propose an alternative interpretation of this convergent behavior that does not appeal to the attractor hypothesis but rather explores this observation in the context of a superposition of components that reflect the pathways activated by the applied perturbations.
To this end, we extend the work of Huang et al. CITATION by decomposing the state space trajectories into components comprising two sets of genes, a core group and transient group that capture the stimulus-independent and stimulus-dependent effects, respectively.
The superposition of these components reflect the observation that both sources of effects independently influence the overall shape of the trajectory taken during the cell fate transition.
We show how this division allows us to look at functional behavior of genes and their contribution to the cell fate transitions in a more enlightening way.
Using regression models, we isolate core genes that are common to both stimuli and represent those critical to the differentiation process.
The genes outside the core represent the transient component of the trajectory corresponding to the perturbation effects.
To illustrate our ideas, we apply our method to the same published dataset generated by Huang et al. CITATION .
The HL-60 cell line has long been used as a model to understand the molecular mechanisms driving the progression and pathogenesis of acute promyelocytic leukemia CITATION.
In normal promyelocytes, proliferation and differentiation are tightly coupled processes.
However this balance comes unstuck in APL cells and as a result cells proliferate in a disregulated fashion.
The discovery that inducers like RA and DMSO could reprogram APL cells to overcome this block and resume differentiation, led to the emergence of a class of therapeutics known as differentiation therapy CITATION .
DMSO is an organic solvent but also functions as a cryoprotective agent for tissue cell culture CITATION.
Although it is widely used in veterinary medicine in the treatment of pain and inflammation, it is not generally used in humans because it is known to be hepatotoxic.
The hormone ATRA is a derivative of vitamin A and belongs to a class of molecules called retinoids CITATION.
ATRA is currently used in differentiation therapies that treat human patients with APL.
Current complete remission rates for APL patients on ATRA-based differentiation therapy in combination with chemotherapy have been reported to be as high as 90 95 percent CITATION.
At the molecular level, both DMSO and ATRA arrest the cell cycle at the G1-S phase transition point, and induce terminal differentiation of HL-60 cells, resulting in neutrophil-like cells.
ATRA and DMSO are biochemically distinct molecules that activate slightly different sets of pathways in HL-60 cells.
Huang et al. CITATION explain that this is the reason why the trajectories initially diverge and explore different parts of the expression state space.
They argue that it is the presence of an attractor that then causes the trajectories to converge from different directions to eventually arrive at a common endpoint, and discount the possibility of a specific, unique differentiation pathway that may be triggered by both inducers.
While this argument may seem conceptually appealing, upon further inspection the attractor hypothesis greatly limits our ability to develop mechanistic interpretations or to build predictive models of cell fate transitions.
We believe that there exists an alternative, more plausible interpretation that Huang et al. CITATION and Kauffman CITATION have not considered.
Our interpretation is based on the recognition that there are two types of processes that contribute to cell fate transitions: one, a core biological process inherent to the transition-specific event and two, a transient process related to the direct effects that the particular inducing agent exerts on the cell.
The early divergence seen in the state space trajectories described by Huang et al. CITATION is reflective of the cells' response to specific perturbation and the compound-specific response that follows.
We expect these transient processes to dominate only at the initial period of the time-course since most drugs are metabolized quickly by the cell.
Once this disorder has subsided, the targeted effects of each inducer are expected to have begun triggering the core processes and as this occurs, the directions that both trajectories adopt become more and more convergent because the overlap in activated pathways in DMSO-induced cells and ATRA-induced cells is growing larger as the cells transition towards their common endpoint.
The source of this convergence therefore is not necessarily due to the existence of an attractor but instead can be explained by the combination of these two types of processes exerting their temporal effects on cells.
Indeed, if such an attractor existed, then there should be a whole class of perturbations that would cause transitions from the initial to the final state, rather than a small number that activate a single core pathway.
If one adopts the attractor hypothesis as the basis for cell-fate transitions, then our interpretation is much closer to that of Conrad Waddington, in which he argued for the canalization of state space through the existence of defined paths, or canals, between attractor states CITATION CITATION .
Natural proteins often partake in several highly specific protein-protein interactions.
They are thus subject to multiple opposing forces during evolutionary selection.
To be functional, such multispecific proteins need to be stable in complex with each interaction partner, and, at the same time, to maintain affinity toward all partners.
How is this multispecificity acquired through natural evolution?
To answer this compelling question, we study a prototypical multispecific protein, calmodulin, which has evolved to interact with hundreds of target proteins.
Starting from high-resolution structures of sixteen CaM-target complexes, we employ state-of-the-art computational methods to predict a hundred CaM sequences best suited for interaction with each individual CaM target.
Then, we design CaM sequences most compatible with each possible combination of two, three, and all sixteen targets simultaneously, producing almost 70,000 low energy CaM sequences.
By comparing these sequences and their energies, we gain insight into how nature has managed to find the compromise between the need for favorable interaction energies and the need for multispecificity.
We observe that designing for more partners simultaneously yields CaM sequences that better match natural sequence profiles, thus emphasizing the importance of such strategies in nature.
Furthermore, we show that the CaM binding interface can be nicely partitioned into positions that are critical for the affinity of all CaM-target complexes and those that are molded to provide interaction specificity.
We reveal several basic categories of sequence-level tradeoffs that enable the compromise necessary for the promiscuity of this protein.
We also thoroughly quantify the tradeoff between interaction energetics and multispecificity and find that facilitating seemingly competing interactions requires only a small deviation from optimal energies.
We conclude that multispecific proteins have been subjected to a rigorous optimization process that has fine-tuned their sequences for interactions with a precise set of targets, thus conferring their multiple cellular functions.
Proteins engage in numerous protein-protein interactions, which together regulate the outcome of all biological processes in the cell.
By some estimates, over a third of all mammalian proteins participate in two or more highly specific protein-protein interactions CITATION.
Proteins that can interact with a large number of partners play a central role in the modular organization of protein interaction networks CITATION.
Such proteins, usually referred to as protein hubs, tend to be more essential than others for cell survival CITATION and usually exhibit slower rates of evolution CITATION.
Moreover, the comprehensive biological activity of these proteins typically requires them to recognize a precise set of targets in a specific way.
For example, each subfamily of G protein regulators interacts with only a specific subset of G proteins CITATION.
Proteins with diverse binding capacity have also been termed multispecific proteins CITATION, CITATION .
The central function of multispecific proteins within interaction networks imposes constraints on their amino acid sequences, especially in their protein-protein interfaces, i.e., the regions that are used to mediate intermolecular interactions with various targets.
There exist only a few studies that have characterized in great detail the molecular and structural features of multispecific protein interfaces CITATION ; this is mostly due to sparse representation of such protein-protein complexes in the Protein Data Bank.
A thorough understanding of atomic-level principles governing multispecific interactions is extremely important not only for the advancement of basic science but also for the design of new pharmaceuticals that modify protein-protein interactions.
Furthermore, such molecular insights will provide critical feedback for systems biology research, which views protein-protein interactions from a high-level network approach CITATION .
Calmodulin is a paradigm of a multispecific protein, with more than three hundred CaM targets identified to date CITATION.
CaM is the central player in the FORMULA signaling pathways that control gene transcription, protein phosphorylation, nucleotide metabolism, and ion transport.
This FORMULA sensor protein translates the changes in FORMULA concentration into activity of many downstream targets, including kinases, phosphatases, enzymes, and ion channels CITATION.
Remarkably, CaM targets display considerable variability in sequence and structure.
CaM-binding regions within target proteins are generally rich in hydrophobic and positively charged residues.
Nevertheless, no consensus CaM-binding sequence exists for all CaM target proteins.
Recent structural studies have revealed that there are several binding modes accessible to CaM, allowing this protein to interact with its targets in a FORMULA-saturated state CITATION, CITATION, in a partially-saturated FORMULA state CITATION, and in a FORMULA-free state CITATION, CITATION.
In the FORMULA-saturated form, CaM usually binds to a stretch of FORMULA amino acids that is unfolded in the absence of CaM and becomes helical upon interaction with the protein CITATION.
In this conventional binding mode, CaM undergoes a conformational change and embraces the target helix with its two globular domains, burying a substantial hydrophobic surface area and providing favorable hydrogen bond and salt bridge interactions with the target.
FORMULA-saturated CaM binds to its targets with high affinity, displaying FORMULA values in the FORMULA to FORMULA M range CITATION.
This affinity is reduced at least 1000-fold in the absence of FORMULA, allowing for quick dissociation of CaM from its targets when FORMULA is depleted.
The multitude of binding constraints placed on CaM during evolution is likely to have produced a sequence that may not be optimal for binding to any particular CaM target, but rather presents a compromise essential for interaction with a large number of partners.
In this study, we employ a computational design approach CITATION to understand how the compromises required for functional promiscuity CITATION are achieved both on the level of amino acid sequences and on the level of binding energetics.
First, we computationally evolve CaM to interact with single targets; second, we evolve this protein to bind to multiple partners simultaneously.
Recently, a similar analysis was performed on twenty multispecific proteins, whose interactions with two to seven targets were considered CITATION, CITATION.
In contrast to those works, we report a much more comprehensive investigation of a single multispecific protein, CaM.
We examine interactions in sixteen different CaM-target complexes that exhibit the conventional binding mode.
Using the structures of these complexes, we perform 697 separate CaM design calculations to obtain FORMULA low energy CaM sequences optimal for either a single target or some combination of the targets.
Rigorous quantitative and statistical comparisons of the designed CaM sequences and their energies allows us to draw conclusions regarding CaM evolution and to suggest strategies for the design of binders that are both promiscuous yet highly specific.
In particular, we characterize the CaM binding interface by partitioning its residues into those that are critical for binding affinity and those that are important for multispecificity.
Furthermore, we analyze the sorts of sequence compromises required to yield proteins with promiscuous interactions and show how this fits with past explanations for the ability of CaM to accommodate many targets.
Finally, we examine the energetic compromises inherently crucial for multispecificity CITATION, and we find that our results also shed light on the unexpected findings of previous experimental protein design research.
Many proteins, especially in eukaryotes, contain tandem repeats of several domains from the same family.
These repeats have a variety of binding properties and are involved in protein protein interactions as well as binding to other ligands such as DNA and RNA.
The rapid expansion of protein domain repeats is assumed to have evolved through internal tandem duplications.
However, the exact mechanisms behind these tandem duplications are not well-understood.
Here, we have studied the evolution, function, protein structure, gene structure, and phylogenetic distribution of domain repeats.
For this purpose we have assigned Pfam-A domain families to 24 proteomes with more sensitive domain assignments in the repeat regions.
These assignments confirmed previous findings that eukaryotes, and in particular vertebrates, contain a much higher fraction of proteins with repeats compared with prokaryotes.
The internal sequence similarity in each protein revealed that the domain repeats are often expanded through duplications of several domains at a time, while the duplication of one domain is less common.
Many of the repeats appear to have been duplicated in the middle of the repeat region.
This is in strong contrast to the evolution of other proteins that mainly works through additions of single domains at either terminus.
Further, we found that some domain families show distinct duplication patterns, e.g., nebulin domains have mainly been expanded with a unit of seven domains at a time, while duplications of other domain families involve varying numbers of domains.
Finally, no common mechanism for the expansion of all repeats could be detected.
We found that the duplication patterns show no dependence on the size of the domains.
Further, repeat expansion in some families can possibly be explained by shuffling of exons.
However, exon shuffling could not have created all repeats.
Proteins are composed of domains, recurrent protein fragments with distinct structure, function, and evolutionary history.
Protein domains may occur alone, but are more frequently found in combination with other domains in multidomain proteins.
While the creation of new multidomain architectures through shuffling of protein domains has been studied extensively during the last few years CITATION CITATION, one type of domain recombination has often been ignored: the creation of domain repeats.
Domain repeats contain two or more domains from the same domain family in tandem.
Large repeats with more then ten domains in tandem are common in eukaryotes.
Repeating domains are often short, such as the leucine rich repeat family with a repeating unit of 30 residues.
Some repeated domain families are mainly found in repeats, e.g., LRR and C2H2 zinc fingers, while other families are also frequently found as a single unit.
The repeats may form regular structures, such as antiparallel -sheets or solenoids, while others form filaments or are only structured upon binding to their ligands CITATION.
Some examples of repeats in protein structures can be found in the Propeat database.
Single amino acids or short peptide motifs may be repeated in proteins, too.
However, in this study we have focused on larger repeating units, domains.
Therefore, when repeats are mentioned in this text, it refers to repeats of protein domains.
Domain repeats are often involved in interactions with proteins or other ligands such as DNA or RNA.
Even if the repeated domains have a well-defined and conserved structure, the sequence conservation is often low, with only a few conserved residues required for the correct fold.
Their variable sequences and the variation in number of domains provide flexible binding to multiple binding partners.
Hence, repeats are found in proteins with highly diverse functions such as the tetratrico peptide repeats that are involved in cell-cycle regulation, transcriptional regulation, protein transport, and assisting protein folding CITATION.
In addition, the flexible binding properties and sequence variability of repeats have been exploited to create high affinity binders as an alternative to antibodies CITATION .
The domain repeats are found in all kingdoms of life, and long repeats, containing several domains in tandem, have been observed to be particularly common in multicellular species CITATION, CITATION.
Repeats have been proposed to provide the eukaryotes with an extra source of variability to compensate for low generation rates CITATION.
One such example is the LRRs in plant defense systems that enable plants to adapt to new pathogens CITATION .
Domain repeats are thought to arise via tandem duplications within a gene CITATION, where a segment is duplicated and the copy is inserted next to its origin.
However, the exact mechanism behind this phenomenon is not fully understood.
Nonhomologous recombination in intron regions, i.e., exon shuffling, may be responsible for internal duplications in repeats, and this issue has been addressed in this study.
Another possible explanation is DNA slippage, due to the formation of DNA hairpins, which is common in the creation of nucleotide repeats and short protein repeats CITATION.
However, Marcotte and coworkers have shown that protein repeats are more likely created from recombination than by DNA slippage since the repeat expansion shows weak dependence on repeat length CITATION .
In addition to internal duplications, frequent duplications of repeat-containing genes have occurred in the mammalian genomes CITATION.
This can, in part, explain their abundance in higher eukaryotes.
In addition, variation in number of repeats between orthologous genes indicates that the loss/gain of domains in repeats is frequent in evolution CITATION.
Interestingly, the rapid expansion of repeats in eukaryotes could partly be explained by tandem duplication of units containing several repeated domains CITATION CITATION.
In this study, we aim to investigate how frequent duplications of multiple domains are.
Further, the number of domains that is duplicated is compared among the different domain families.
Domains as defined by the Pfam-A database CITATION were detected using HMM-alignments.
The coverage was increased with relaxed detection criteria for domains in repeated regions of the proteins.
In addition to investigation of duplication sizes, the domain assignments have been used to study the distribution of repeats and repeated domain families in the three kingdoms of life, the position of repeat expansion, and the location of exon boundaries in repeats.
Cells in the wing blade of Drosophila melanogaster exhibit an in-plane polarization causing distal orientation of hairs.
Establishment of the Planar Cell Polarity involves intercellular interactions as well as a global orienting signal.
Many of the genetic and molecular components underlying this process have been experimentally identified and a recently advanced system-level model has suggested that the observed mutant phenotypes can be understood in terms of intercellular interactions involving asymmetric localization of membrane bound proteins.
Among key open questions in understanding the emergence of ordered polarization is the effect of stochasticity and the role of the global orienting signal.
These issues relate closely to our understanding of ferromagnetism in physical systems.
Here we pursue this analogy to understand the emergence of PCP order.
To this end we develop a semi-phenomenological representation of the underlying molecular processes and define a phase diagram of the model which provides a global view of the dependence of the phenotype on parameters.
We show that the dynamics of PCP has two regimes: rapid growth in the amplitude of local polarization followed by a slower process of alignment which progresses from small to large scales.
We discuss the response of the tissue to various types of orienting signals and show that global PCP order can be achieved with a weak orienting signal provided that it acts during the early phase of the process.
Finally we define and discuss some of the experimental predictions of the model.
Epithelia in diverse tissues, in addition to their apico-basal polarization, acquire a polarization within the two-dimensional layer of cells a phenomenon called planar cell polarity CITATION CITATION.
In the developing wing of Drosophila, PCP determines the growth direction of small hairs that extend radially from cell boundaries.
In a wild-type wing, where cells are approximately hexagonal and form a regular honeycomb lattice, all of these hairs point to the distal direction.
A series of recent experiments show that several key proteins CITATION, including the transmembrane proteins Frizzled and Van-Gogh and the cytosolic proteins Dishevelled and Prickled, localize asymmetrically on cell boundaries CITATION CITATION - defining a direction in the plane within each cell and forming a characteristic zig-zag pattern of protein localization on the lattice .
Other experiments show that local PCP orientation depends on inter-cellular signaling.
First, mutant clones in which fz or Vang activity is suppressed or amplified, cause characteristic and reproducible inversion of polarity in large patches of cells that are proximal or distal to the clone CITATION.
These observations are summarized in Figs.
1 C,D. Second, in fat mutant clones CITATION, CITATION hairs do not all point correctly in the distal direction, yet, their orientation is strongly correlated between nearby cells and varies gradually across the tissue creating a characteristic swirling pattern.
Thus the experimental evidence suggests that an interaction between neighboring cells tends to locally align their polarity CITATION, CITATION, CITATION.
This local polarity need not point distally unless, in addition, there is a global orienting signal that picks out the distal direction throughout the wing.
Yet, aside from a clear involvement of protocadherin fat CITATION, CITATION the molecular details of this pathway remains for now unknown.
The swirling patterns in fat mutants CITATION and recent evidence CITATION, CITATION, suggest that the orienting field is related to the presence of a gradient in the fat, four-jointed, and dachs pathway.
These observations evoke an analogy between PCP and the behavior of ferromagnets, extensively studied in physics and well understood in terms of statistical mechanics of relatively simple models CITATION.
In these models each atomic site is assigned a magnetic dipole spin which can assume a different orientation.
The salient properties of ferromagnets arise from the opposing influence of an interaction between neighboring spins, which tends to co-align their orientation, and the influence of thermal fluctuations, which tend to randomize the spin direction.
Ferromagnets typically exhibit two phases of behavior: a high temperature phase, where spins are disordered and a low temperature ferromagnetic phase, where the interactions dominate over thermal fluctuations leading to a spontaneous polarization in an arbitrary direction.
In this state even a small external magnetic field has a big effect on magnetic polarization as the spontaneous polarization aligns itself with the external field, yet the dynamics leading to global alignment can be quite slow.
An essential lesson from statistical mechanics is that the ordered and disordered states exist in a broad class of models and can be discussed in a general context, focusing on a classification of the different regimes as a function of a few parameters.
We follow this lesson by focusing the study on the competition between the intercellular interaction and the disordering influence of the fluctuations introduced by the noisy molecular interactions.
As in statistical mechanics we define a phase diagram which identifies different regimes of behavior in the space of the most relevant parameters.
We then address the role of the global directional signal in the dynamics of global alignment.
A molecular model for PCP formation was recently proposed in Ref.
CITATION, and was shown to reproduce a number of experimental findings.
This model involves 38 parameters that were adjusted to successfully reproduce a set of wild-type and mutant phenotypes.
Here we pursue an alternative approach and instead of moving on to more and more complex models develop a model with a smaller number of degrees of freedom and a smaller number of parameters.
Instead of fixing a particular set of parameters by fitting the data we explore the generic behavior of the model as a function of parameters defining quantitative features characteristic of the different phases.
In formulating the model we identify several essential ingredients, required to obtain the characteristic zig-zag pattern and the non-autonomy of fz and Vang mutant clones.
We expect our simplified model to capture important properties of PCP, although it does not incorporate all the molecular details.
After discussing the essential ingredients of the model, we obtain a phase diagram describing its steady state properties.
We then consider the dynamics of local polarization strength and orientation in the absence and in the presence of a global orienting signal.
We show that global alignment can be achieved with a weak global orienting signal provided it is present throughout the tissue at the earliest stage of PCP dynamics.
Finally we discuss the experimental predictions coming out of the model and the tools required to test these predictions.
Calmodulin kinase II mediates critical signaling pathways responsible for divergent functions in the heart including calcium cycling, hypertrophy and apoptosis.
Dysfunction in the CaMKII signaling pathway occurs in heart disease and is associated with increased susceptibility to life-threatening arrhythmia.
Furthermore, CaMKII inhibition prevents cardiac arrhythmia and improves heart function following myocardial infarction.
Recently, a novel mechanism for oxidative CaMKII activation was discovered in the heart.
Here, we provide the first report of CaMKII oxidation state in a well-validated, large-animal model of heart disease.
Specifically, we observe increased levels of oxidized CaMKII in the infarct border zone.
These unexpected new data identify an alternative activation pathway for CaMKII in common cardiovascular disease.
To study the role of oxidation-dependent CaMKII activation in creating a pro-arrhythmia substrate following myocardial infarction, we developed a new mathematical model of CaMKII activity including both oxidative and autophosphorylation activation pathways.
Computer simulations using a multicellular mathematical model of the cardiac fiber demonstrate that enhanced CaMKII activity in the infarct BZ, due primarily to increased oxidation, is associated with reduced conduction velocity, increased effective refractory period, and increased susceptibility to formation of conduction block at the BZ margin, a prerequisite for reentry.
Furthermore, our model predicts that CaMKII inhibition improves conduction and reduces refractoriness in the BZ, thereby reducing vulnerability to conduction block and reentry.
These results identify a novel oxidation-dependent pathway for CaMKII activation in the infarct BZ that may be an effective therapeutic target for improving conduction and reducing heterogeneity in the infarcted heart.
Calmodulin kinase II mediates diverse roles in the heart, including excitation-contraction coupling, sinus node automaticity, apoptosis, hypertrophy, and gene transcription CITATION, CITATION.
Mounting experimental evidence demonstrates an important role for CaMKII in heart disease and arrhythmias.
Specifically, CaMKII overexpression occurs in human heart failure CITATION and transgenic mice overexpressing CaMKII develop dilated cardiomyopathy CITATION, CITATION.
Conversely, transgenic inhibition of CaMKII prevents structural remodeling and improves heart function following myocardial infarction CITATION while knockout mice lacking the predominant cardiac CaMKII isoform are resistant to development of pressure overload-induced hypertrophy and/or heart failure CITATION, CITATION.
Finally, CaMKII inhibition prevents arrhythmias in several different mouse models of heart disease CITATION, CITATION .
CaMKII is activated by binding of Ca 2 /calmodulin and may undergo inter-subunit autophosphorylation that allows the kinase to retain activity even upon dissociation of Ca 2 /calmodulin CITATION.
Recently, a novel CaMKII activation pathway was identified where oxidation at specific methionine residues in the CaMKII regulatory subunit results in persistent activity independent of autophosphorylation CITATION.
While oxidative-dependent CaMKII activation has been shown to mediate apoptosis in response to chronic AngII treatment in the mouse CITATION as well as arrhythmogenic afterdepolarizations in isolated cardiomyocytes treated with hydrogen peroxide CITATION, nothing is known about its role in large animal models of heart disease.
Considering that levels of reactive oxygen species such as H 2O 2 and superoxide are elevated following myocardial infarction CITATION, we hypothesized that oxidation of CaMKII represents an important pathway for CaMKII activation in the infarct border zone that may provide a mechanistic link between increased ROS production, Na channel remodeling and conduction slowing following MI.
In this study, we describe a dramatic increase in levels of oxidized CaMKII in a well-validated large animal model of arrhythmias following MI CITATION CITATION.
To investigate a role for oxidized CaMKII in regulating refractoriness and conduction in the infarct BZ, we develop a novel mathematical model of CaMKII activity that includes oxidation and autophosphorylation activation pathways.
Our computer simulations show that enhanced CaMKII activity in the BZ, due primarily to increased oxidation, leads to slowed conduction, prolonged refractory periods and increased vulnerability to conduction block at the BZ margin.
Our results identify oxidation-dependent CaMKII activation as a potential link between oxidative stress and electrical remodeling after myocardial infarction.
Furthermore, our findings support CaMKII inhibition as a potential therapy for reducing susceptibility to ventricular tachycardia by improving conduction and reducing refractory gradients in the infarcted heart.
Finally, it is important to note the oxidative activation of CaMKII allows for independent regulation of the kinase by a host of unique upstream activators and signaling partners with great potential relevance to human disease.
As details emerge regarding regulation of the kinase by this newly identified pathway, they may be incorporated into our model to study electrophysiological consequences of CaMKII activation via this independent signaling pathway.
Understanding the mechanisms of cell function and drug action is a major endeavor in the pharmaceutical industry.
Drug effects are governed by the intrinsic properties of the drug and the specific signaling transduction network of the host.
Here, we describe an unbiased, phosphoproteomic-based approach to identify drug effects by monitoring drug-induced topology alterations.
With our proposed method, drug effects are investigated under diverse stimulations of the signaling network.
Starting with a generic pathway made of logical gates, we build a cell-type specific map by constraining it to fit 13 key phopshoprotein signals under 55 experimental conditions.
Fitting is performed via an Integer Linear Program formulation and solution by standard ILP solvers; a procedure that drastically outperforms previous fitting schemes.
Then, knowing the cell's topology, we monitor the same key phosphoprotein signals under the presence of drug and we re-optimize the specific map to reveal drug-induced topology alterations.
To prove our case, we make a topology for the hepatocytic cell-line HepG2 and we evaluate the effects of 4 drugs: 3 selective inhibitors for the Epidermal Growth Factor Receptor and a non-selective drug.
We confirm effects easily predictable from the drugs' main target but we also uncover unanticipated effects due to either drug promiscuity or the cell's specific topology.
An interesting finding is that the selective EGFR inhibitor Gefitinib inhibits signaling downstream the Interleukin-1alpha pathway; an effect that cannot be extracted from binding affinity-based approaches.
Our method represents an unbiased approach to identify drug effects on small to medium size pathways which is scalable to larger topologies with any type of signaling interventions.
The method can reveal drug effects on pathways, the cornerstone for identifying mechanisms of drug's efficacy.
Target-based drug discovery is a predominant focus of the pharmaceutical industry.
The primary objective is to selectively target protein within diseased cells in order to ameliorate an undesired phenotype, e.g., unrestrained cell proliferation or inflammatory cytokine release.
Ideally, other pathways within the diseased cells, as well as similar phenotypes in other cell types, should remain unaffected by the therapeutic approach.
However, despite the plethora of new potential targets emerged from the sequencing of the human genome, rather few have proven effective in the clinic CITATION.
A major limitation is the inability to understand the mechanisms or drug actions either due to the complex signaling transduction networks of cells or due to the complicated profile of drug potency and selectivity.
Finding drug's targets is traditionally based on high-throughput in vitro assays using recombinant enzymes or protein fragments CITATION.
The main goal is to characterize the drug's biochemical activity and depict them in drug-interaction maps CITATION.
In most cases, once the target is known, the in vivo effect on the signaling pathway is validated by measuring the drug's efficiency to inhibit the activity of the downstream protein.
However, beyond that measurement, little is know on how the rest of the signaling network is affected.
In addition, in vivo drug effects can hardly be calculated from in vitro assays for several reasons: most kinase inhibitors are promiscuous CITATION, there is discrepancy between in vivo and in vitro binding affinities of drugs CITATION, and there is an additional discrepancy between in vivo binding affinities and in vivo inhibitor activity for the phosphorylation of downstream signals.
To address drug effects in more physiological conditions, novel genomic and proteomic tools have recently been developed CITATION.
In the genomic arena, large-scale mRNA analysis enhanced by computational approaches for drug target deconvolution have been developed.
Despite the holistic advantages that genomic approaches have to offer, proteomic-based discovery is a step closer to the function of the cell.
Towards this goal, affinity chromatography offers a viable strategy for in-vivo target identification.
This approach utilizes a solid support linked to a bait to enrich for cellular binding proteins that are identified by mass spectrometry CITATION.
However, such experiments usually require large amounts of starting protein, are biased toward more abundant proteins, and result in several hits due to nonspecific interactions CITATION, CITATION.
In order to circumvent the non-specific interaction problem, another bait-based strategy uses quantitative MS with dirty inhibitors for baits to immobilize the kinome CITATION, CITATION.
While this approach significantly reduces the non-specific interaction problem, it also limits the target-searching space to those kinases with the highest affinity to the bait.
More recently, quantitative MS-based proteomics using SILAC technology CITATION extends the search space to all targets that do not bind covalently to the drug.
However, incorporation of the SILAC's isotopes requires 5 population doublings and thus, excludes the application on primary cells with limited replication capabilities.
Taken together, all techniques listed above can -in the best case scenario- list the affinities of all targets to the drug but no information is provided whether this binding affinity is capable of inhibiting the transmission of the signal to the downstream protein or how those preferential bindings can collectively affect the signaling network of the cell.
Here, we describe a significantly different approach to identify drug effects where drugs are evaluated by the alterations they cause on signaling pathways.
Instead of identifying binding partners, we monitor pathway alterations by following key phosphorylation events under several treatments with cytokines.
The workflow is presented in Figure 1.
On the experimental front, using bead-based multiplexed assays CITATION, we measure 13 key phosphorylation events under more than 50 different conditions generated by the combinatorial treatment of stimuli and selective inhibitors.
Based on the signaling response and an a-priori set of possible reactions, we create a cell-type specific pathway using an efficient optimization formulation known as Integer Linear Programming.
This approach builds upon the Boolean optimization approach proposed in CITATION.
The ILP is solved using standard commercial software packages to guaranteed global optimality.
To evaluate drug effects, we subject the cells with the same stimuli in the presence of drugs and we tract the alterations of the same key phosphorylation events.
Then, we reapply the ILP formulation without a-priori assumption of the drug target, and we monitor the changes in the pathway topology with and without drug presence.
To demonstrate our approach, we construct a generic map and optimize it to fit the phosphoproteomic data of the transformed hepatocytic cell lines HepG2.
Then, we identify the effects of four drugs: the dual EGFR/ErbB-2 inhibitor Lapatinib CITATION, two potent EGFR kinase inhibitors Erlotinib CITATION and Gefitinib CITATION, and the dirty Raf kinase inhibitor Sorafenib CITATION.
When our method is applied on those 4 drugs we find their main target effect and we also uncover several unknown but equally active off-target effects.
In the case of Gefitinib, we find a surprising inhibition of cJUN in the IL1 pathway.
In contrast to previously developed techniques, our method is based on the actual effect on phosphorylation events carefully spread into the signaling network.
Theoretically, it can be applied on any type of intracellular perturbations such as ATP-based and allosteric kinase inhibitors, RNAi, shRNA etc. On the computational front, our ILP-based approach performs faster and more efficient than current algorithms for pathway optimization CITATION and can identify the main drug effects as well as unknown off-target effects in areas of pathways constrained between the activated receptors and the measured phosphorylated proteins.
Our fast and unbiased characterization of modes of drug actions can shed a light into the potential mechanisms drug's efficacy and toxicity.
Recent advances in reconstruction and analytical methods for signaling networks have spurred the development of large-scale models that incorporate fully functional and biologically relevant features.
An extended reconstruction of the human Toll-like receptor signaling network is presented herein.
This reconstruction contains an extensive complement of kinases, phosphatases, and other associated proteins that mediate the signaling cascade along with a delineation of their associated chemical reactions.
A computational framework based on the methods of large-scale convex analysis was developed and applied to this network to characterize input output relationships.
The input output relationships enabled significant modularization of the network into ten pathways.
The analysis identified potential candidates for inhibitory mediation of TLR signaling with respect to their specificity and potency.
Subsequently, we were able to identify eight novel inhibition targets through constraint-based modeling methods.
The results of this study are expected to yield meaningful avenues for further research in the task of mediating the Toll-like receptor signaling network and its effects.
Toll-like receptors are a group of conserved pattern recognition receptors that activate the processes of innate and adaptive immunity CITATION.
Recent activity has focused on the characterization of the TLR network and its involvement in the apoptotic, inflammatory, and innate immune responses CITATION CITATION.
TLR signaling is a primary contributor to inflammatory responses and has been implicated in several diseases including cardiovascular disease CITATION, CITATION.
Indeed, even in cases of desired inflammatory response, excessive activation of signaling pathways can lead to septic shock and other serious conditions CITATION .
As such, there is much interest in the development of methods to attenuate or modulate TLR signaling in a targeted fashion.
For example, one approach involves the inhibition of specific reactions or components within the TLR network that will dampen undesired signaling pathways while not adversely affecting other signaling components CITATION, CITATION.
These reactions or components should ideally be highly specific to the TLR network and also to one transcription target.
Therefore, the available, comprehensive data sets of the TLR network need to be put into a more structured, systematic format that enables better understanding of the associated signaling cascades, pathways, and connections to other cellular networks.
Such a systemic approach is necessary to achieve the ultimate goal of mediating the effects of Toll-like receptor signaling upon the inflammatory, immune, and apoptotic responses.
This need is particularly important given the amount of experimental data about TLR signaling that is already too large to be analyzed by simply viewing the complex web of overlapping interactions.
So far, relatively few attempts have been made to organize the plethora of experimental data into a single unified representation CITATION.
Hence, there is clearly a need to investigate the function and capabilities of this network using a computational model, particularly to yield further insights into the mechanistic action of the TLRs and their immunoadjuvant effects.
Constraint-based reconstruction and analysis methods represent a systems approach for computational modeling of biological networks CITATION.
Briefly, all known biochemical transformations for a particular system are collected from various data sources listing genomic, biochemical, and physiological data CITATION, CITATION.
The reconstruction is built on existing knowledge in bottom-up fashion and can be subsequently converted into a condition-specific model CITATION, CITATION allowing the investigation of its functional properties CITATION, CITATION.
This conversion involves translating the reaction list into a so-called stoichiometric matrix by extracting the stoichiometric coefficients of substrates and products from each network reaction and placing lower and upper bounds on the network reactions.
These constraints can include mass-balancing, thermodynamic considerations, and reaction rates CITATION.
Additionally, environmental constraints can be applied to represent different availabilities of medium components.
Many computational analysis tools have been developed CITATION, including Flux balance analysis.
FBA is a formalism in which a reconstructed network is framed as a linear programming optimization problem and a specific objective function is maximized or minimized CITATION.
COBRA methods are well established for metabolic networks and both reconstruction and analysis tools are widely used CITATION.
Furthermore, these methods have been successfully applied to other important cellular functions such as transcription and translation CITATION, transcriptional regulation CITATION, and signaling, including JAK-STAT CITATION and angiogenesis CITATION .
In this study, we present an extended and reformulated model for the TLR network, reconstructed based on the publicly available TLR map CITATION and the COBRA approach CITATION, CITATION.
Signaling networks have been analyzed using extreme pathway analysis CITATION and FBA CITATION.
However, since ExPa analysis becomes computationally challenging in large-scale, mass-balanced networks CITATION, we could not apply this method to the TLR network.
In contrast, network modularization has been established as a method for reducing large-scale networks into more manageable units CITATION CITATION.
Another approach for reducing network complexity is to focus on input output relationships CITATION, CITATION.
We used FBA to simplify the mesh of network reactions into ten functionally distinct input output pathways, which show different patterns of signal activation control.
Furthermore, we used this modular representation of the complex TLR signaling network to determine control points in the network, which are specific for a DIOS pathway.
These control points allow for the modulation of TLR signaling in a targeted fashion, which will induce a change in undesired signaling while not having an adverse effect on other signaling components.
Taken together, we show in this study how a signaling network reconstruction and FBA can be used to identify potential candidates for drug targeting.
Allosteric proteins bind an effector molecule at one site resulting in a functional change at a second site.
We hypothesize that allosteric communication in proteins relies upon networks of quaternary and tertiary motions.
We argue that cyclic topology of these networks is necessary for allosteric communication.
An automated algorithm identifies rigid bodies from the displacement between the inactive and the active structures and constructs quaternary networks from these rigid bodies and the substrate and effector ligands.
We then integrate quaternary networks with a coarse-grained representation of contact rearrangements to form global communication networks.
The GCN reveals allosteric communication among all substrate and effector sites in 15 of 18 multidomain and multimeric proteins, while tertiary and quaternary networks exhibit such communication in only 4 and 3 of these proteins, respectively.
Furthermore, in 7 of the 15 proteins connected by the GCN, 50 percent or more of the substrate-effector paths via the GCN are interdependent paths that do not exist via either the tertiary or the quaternary network.
Substrate-effector pathways typically are not linear but rather consist of polycyclic networks of rigid bodies and clusters of rearranging residue contacts.
These results argue for broad applicability of allosteric communication based on structural changes and demonstrate the utility of the GCN.
Global communication networks may inform a variety of experiments on allosteric proteins as well as the design of allostery into non-allosteric proteins.
The modern concept of allostery began with the models of Monod et al. CITATION and Koshland et al. CITATION, which sought to account for allostery based upon gross properties of the transition between two well-defined end-states.
More recent thermodynamic models of allostery characterize population shifts in conformational ensembles in more detail CITATION CITATION, and there is experimental evidence that alternate allosteric states are simultaneously populated in solution CITATION, CITATION.
Nonetheless, mechanical and chemical transitions in individual molecules underlie the thermodynamic properties of allosteric proteins.
That is, in individual molecules, energetic pathways of spatially contiguous, physically coupled structural changes and/or dynamic fluctuations must link substrate and effector sites CITATION CITATION .
Crystal structures have revealed that most allosteric proteins are complex systems with both tertiary and quaternary structural changes CITATION.
Previously, we quantified allosteric communication through tertiary structure from graphs of residue-residue contacts that form, break, or rearrange in the transition between inactive and active state structures CITATION.
In such network representations of protein structure, putative paths between residues distant in three-dimensional space can be readily identified.
These tertiary networks or contact rearrangement networks identified substrate-effector paths in 6 of 15 proteins tested, which indicated that tertiary changes play a significant but incomplete role in allosteric communication.
In this work, we broaden the CRN approach toward more completely quantifying allosteric coupling mechanisms from structure.
Specifically, we develop a network representation of quaternary structural changes and integrate this representation with the CRN.
We seek to infer information about the allosteric coupling mechanism from gross properties of the differences between inactive and active structures.
In this, our work resembles the MWC CITATION and KNF CITATION approaches but differs from investigations of the kinetic mechanism, that is, the order of events in the transition between inactive and active structural regimes CITATION CITATION.
Most current computational approaches to large-scale protein dynamics predict motions and/or associated energetics by applying to the structure theoretical models like the elastic network CITATION and potential functions.
While these predictions address important problems, most of these approaches do not predict allosteric pathways.
By contrast to these problems, we will argue that allosteric pathway identification is facilitated by a network representation of a protein structural transition.
Network representations of protein structures have previously been used to illuminate dynamic and/or allosteric properties.
For example, large-scale fluctuations predicted from normal mode analysis of the elastic network correlate with known conformational changes CITATION, CITATION, CITATION.
In addition, rigid and flexible regions of protein structures have been predicted from the network of contact and hydrogen bond constraints in a single protein structure CITATION, CITATION.
Furthermore, residues important for maintaining short paths in a contact network are experimentally known to mediate signaling in proteins CITATION.
However, allosteric communication pathways have not previously been derived from a network representation of the quaternary structural transition.
In this paper, we develop a hypothesis for allosteric coupling via networks of quaternary motions.
We elucidate rigid bodies from the differences between inactive and active crystal structures with an automatic algorithm, and we form a quaternary network from the rigid bodies based on contacts between them.
Toward a broader representation of allosteric communication mechanisms, we assess how communication through these networks relates to that through contact rearrangement networks in tertiary structure.
We then integrate quaternary networks with a coarse-grained representation of CRNs to form global communication networks.
We describe the range of topologies of GCNs in several representative proteins from the allosteric benchmark set CITATION, and then we assess substrate-effector communication via CRNs, the quaternary network, and the GCN in 18 DNA-binding proteins and enzymes and classify each protein based on the respective tertiary and quaternary contributions to connectivity.
GCN analysis provides the opportunity to advance the theory of mechanical allosteric coupling in proteins and may guide drug design and allosteric experiments and simulations.
Proteins are active, flexible machines that perform a range of different functions.
Innovative experimental approaches may now provide limited partial information about conformational changes along motion pathways of proteins.
There is therefore a need for computational approaches that can efficiently incorporate prior information into motion prediction schemes.
In this paper, we present PathRover, a general setup designed for the integration of prior information into the motion planning algorithm of rapidly exploring random trees.
Each suggested motion pathway comprises a sequence of low-energy clash-free conformations that satisfy an arbitrary number of prior information constraints.
These constraints can be derived from experimental data or from expert intuition about the motion.
The incorporation of prior information is very straightforward and significantly narrows down the vast search in the typically high-dimensional conformational space, leading to dramatic reduction in running time.
To allow the use of state-of-the-art energy functions and conformational sampling, we have integrated this framework into Rosetta, an accurate protocol for diverse types of structural modeling.
The suggested framework can serve as an effective complementary tool for molecular dynamics, Normal Mode Analysis, and other prevalent techniques for predicting motion in proteins.
We applied our framework to three different model systems.
We show that a limited set of experimentally motivated constraints may effectively bias the simulations toward diverse predicates in an outright fashion, from distance constraints to enforcement of loop closure.
In particular, our analysis sheds light on mechanisms of protein domain swapping and on the role of different residues in the motion.
Mechanistic understanding of protein motions intrigued structural biologists, bio-informaticians and physicists to explore molecular motions for the last five decades.
In two seminal breakthroughs in 1960 CITATION, CITATION, the structures of Haemoglobin and Myoglobin were solved and consequently, for the first time, mechanistic structural insights into the motion of a protein were deduced from its snap-shot image.
This finding paved the way to a by-now classical model for cooperativity in binding of allosteric proteins CITATION.
Nowadays, hundreds of proteins with known multiple conformations, together with their suggested molecular motion, are recorded in databases such as MolMovDB CITATION.
This number increases with the influx of solved structures from the Protein Data Bank CITATION.
An inherent flexibility is characteristic of fundamental protein functions such as catalysis, signal transduction and allosteric regulation.
Elucidating motion of protein structures is essential for understanding their function, and in particular, for understanding control mechanisms that prevent or allow protein motions.
Understanding the relation between protein sequence and protein motion can allow de-novo design of dynamic proteins, enhance our knowledge about transition states and provide putative conformations for targeting drugs.
Accurate prediction of protein motion can also help address other computational challenges.
For instance, Normal Mode Analysis motion predictions CITATION can be used for efficient introduction of localized flexibility into docking procedures CITATION, CITATION .
Experimental knowledge of macro-molecular motions has been discouragingly limited to this day by the fact that high-resolution structures solved by X-ray crystallography are merely the outmost stable conformations of proteins, in a sense a snap shot of a dynamic entity.
While high resolution experimental data of molecular motion are still beyond reach, innovative breakthroughs in time-resolved optical spectroscopy, single molecule F rster resonance energy transfer, small-angle X-ray scattering CITATION, as well as advances in NMR spectroscopy such as residual dipolar coupling methods and paramagnetic relaxation enhancements CITATION CITATION now provide increasingly detailed experimental data on molecular motion, e.g., distance and angle constraints or measurements of rotational motion CITATION .
Certain theories suggest that it should be difficult or impossible to eradicate a vaccine-preventable disease under voluntary vaccination: Herd immunity implies that the individual incentive to vaccinate disappears at high coverage levels.
Historically, there have been examples of declining coverage for vaccines, such as MMR vaccine and whole-cell pertussis vaccine, that are consistent with this theory.
On the other hand, smallpox was globally eradicated by 1980 despite voluntary vaccination policies in many jurisdictions.
Previous modeling studies of the interplay between disease dynamics and individual vaccinating behavior have assumed that infection is transmitted in a homogeneously mixing population.
By comparison, here we simulate transmission of a vaccine-preventable SEIR infection through a random, static contact network.
Individuals choose whether to vaccinate based on infection risks from neighbors, and based on vaccine risks.
When neighborhood size is small, rational vaccinating behavior results in rapid containment of the infection through voluntary ring vaccination.
As neighborhood size increases, a threshold is reached beyond which the infection can break through partially vaccinated rings, percolating through the whole population and resulting in considerable epidemic final sizes and a large number vaccinated.
The former outcome represents convergence between individually and socially optimal outcomes, whereas the latter represents their divergence, as observed in most models of individual vaccinating behavior that assume homogeneous mixing.
Similar effects are observed in an extended model using smallpox-specific natural history and transmissibility assumptions.
This work illustrates the significant qualitative differences between behavior infection dynamics in discrete contact-structured populations versus continuous unstructured populations.
This work also shows how disease eradicability in populations where voluntary vaccination is the primary control mechanism may depend partly on whether the disease is transmissible only to a few close social contacts or to a larger subset of the population.
Model-based analyses of vaccination programmes have often concluded that it should be difficult or impossible to eradicate a vaccine-preventable disease under a voluntary vaccination policy without other incentives CITATION CITATION.
As vaccination coverage increases, the disease becomes increasingly rare due to herd immunity.
Eventually, the infection risk to susceptible individuals decreases to zero, while the individual risk due to the vaccine remains constant.
Hence, the individual motive to vaccinate is also reduced to zero as vaccine coverage increases.
This should be true, in principle, even for a disease such as smallpox with very high case fatality rates, as long as the infection risk is deemed sufficiently small.
This effect, similar to the Prisoner's Dilemma has also been explored in game theoretical analyses of infectious disease dynamics and vaccination CITATION CITATION, CITATION, CITATION.
In game theoretical treatments, it has been shown that vaccine coverage beyond the eradication threshold is not a Nash equilibrium if vaccine risk is nonzero, because a small group of individuals can achieve a higher payoff by switching to a nonvaccinator strategy CITATION.
Such strategic, self-interested behavior has been suggested as a possible contributor to vaccine scares in countries with a voluntary vaccination policy, such as England Wales, which experienced declines in vaccine uptake for pertussis in the 1970s CITATION, CITATION, CITATION, and in measles mumps rubella vaccine uptake more recently CITATION.
Recent work has explored exceptions to this rule, for instance finding cases of multiple equilibria when virulence varies with age CITATION and when vaccines are sufficiently imperfect CITATION .
To date, smallpox is the only vaccine-preventable disease ever to have been globally eradicated CITATION, although polio is closer to eradication than ever before CITATION.
The last foothold of smallpox was in low-income countries, particularly in Africa and South Asia CITATION.
Jurisdictions in these countries often had widely varying vaccination policies.
For instance, vaccination was compulsory in some Indian states, but voluntary in others CITATION.
Even in the final stages of eradication, when outbreaks were becoming less frequent, individuals often continued to voluntarily opt for vaccination, without the benefit of individual financial incentives to vaccinate.
If the foregoing theories are correct that diseases cannot generally be eradicated under voluntary vaccination, how was smallpox globally eradicated despite voluntary vaccination in some jurisdictions?
Most, if not all, previous mathematical models that analyze discrepancies between individually and socially optimal vaccination strategies under voluntary vaccination have considered populations without spatial or social contact structure, and where populations are large enough for the continuum approximation to apply.
This also appears to be true of behavior-infection models more generally CITATION, including those that study vaccine supply-demand dynamics at the international level, and non game-theoretical treatments of the problem.
In these previous analyses, populations are generally considered to be homogeneously mixing, meaning that individuals are as likely to be infected by a member of their own household as they are by someone from the general public.
However, the inadequacy of homogenous mixing models for certain situations has been widely documented, as have the differences between the predictions of homogeneous-mixing models and models where transmission is constrained to take place on a contact network CITATION CITATION.
In the present context, the homogeneous mixing assumption is arguably a good approximation for highly transmissible diseases spread primarily through aerosol droplets, such as measles.
However, the assumption seems less valid for diseases that are transmitted through close contact, such as sexually-transmitted infections.
Despite a few spectacular and widely reported cases of aerosol transmission CITATION, smallpox is also spread primarily through close contact, and typically requires prolonged face-to-face contact CITATION .
Here, we show that disease dynamics under a voluntary vaccination policy are substantially and qualitatively altered by the introduction of individual-level social contact structure.
We analyze a social contact network model, where each node represents an individual, and each link represents a close contact through which infection may spread.
Individuals decide whether or not to vaccinate based upon their expected payoffs for vaccinating versus not vaccinating.
We assume the vaccine is free to individuals, which describes the situation for many major pediatric vaccines in advanced countries, as well as the situation under the WHO smallpox eradication program in the 1970s.
We first study epidemics on this contact network for a general vaccine-preventable infection with simplified SEIR-type disease history.
At baseline parameter values, for small neighborhood sizes, outbreaks are quickly contained using only voluntary ring vaccination.
As the neighborhood size increases while infection risk is held constant, a threshold neighborhood size is reached.
Above this threshold, voluntary vaccination fails and the population experiences both a considerable final epidemic size and a large number vaccinated.
Hence, the limit of large neighborhood size in this model recovers dynamics similar to those of homogeneous mixing models.
Because the force of infection is held constant as neighborhood size increases, the failure of voluntary vaccination is attributable solely to a decrease in how localized disease transmission is on the network.
We associate smaller neighborhood sizes with close contact infections such as smallpox, and larger neighborhood sizes with diseases that do not require close contact for transmission, such as measles.
We carry out a similar investigation for smallpox-specific disease history and vaccine properties, with similar results.
This analysis illustrates the importance of considering discrete, contact-structured populations for modeling vaccinating behavior for close contact infections, and provides a framework for reconciling previous theoretical predictions concerning the ineradicability of infectious diseases under voluntary vaccination to the empirical fact of the global eradication of smallpox and local elimination of many other infectious diseases through voluntary vaccination.
A new monotonicity-constrained maximum likelihood approach, called Partial Order Optimum Likelihood, is presented and applied to the problem of functional site prediction in protein 3D structures, an important current challenge in genomics.
The input consists of electrostatic and geometric properties derived from the 3D structure of the query protein alone.
Sequence-based conservation information, where available, may also be incorporated.
Electrostatics features from THEMATICS are combined with multidimensional isotonic regression to form maximum likelihood estimates of probabilities that specific residues belong to an active site.
This allows likelihood ranking of all ionizable residues in a given protein based on THEMATICS features.
The corresponding ROC curves and statistical significance tests demonstrate that this method outperforms prior THEMATICS-based methods, which in turn have been shown previously to outperform other 3D-structure-based methods for identifying active site residues.
Then it is shown that the addition of one simple geometric property, the size rank of the cleft in which a given residue is contained, yields improved performance.
Extension of the method to include predictions of non-ionizable residues is achieved through the introduction of environment variables.
This extension results in even better performance than THEMATICS alone and constitutes to date the best functional site predictor based on 3D structure only, achieving nearly the same level of performance as methods that use both 3D structure and sequence alignment data.
Finally, the method also easily incorporates such sequence alignment data, and when this information is included, the resulting method is shown to outperform the best current methods using any combination of sequence alignments and 3D structures.
Included is an analysis demonstrating that when THEMATICS features, cleft size rank, and alignment-based conservation scores are used individually or in combination THEMATICS features represent the single most important component of such classifiers.
Development of function prediction capabilities is a major challenge in genomics.
Structural genomics projects are determining the 3D structures of expressed proteins on a high throughput basis.
However, the determination of function from 3D structure has proved to be a challenging task; the functions of most of these structural genomics proteins remain unknown.
Computationally based predictive methods can help to guide and accelerate functional annotation.
The first step toward the prediction of the function of a protein from its 3D structure is to determine its local site of interaction where catalysis and/or ligand recognition occurs.
Such capabilities have many important practical implications for biology and medicine.
We have reported on THEMATICS CITATION CITATION, for Theoretical Microscopic Titration Curves, a technique for the prediction of local interaction sites in a protein from its three-dimensional structure alone.
In the application of THEMATICS, one begins with the 3D structure of the query protein, solves the Poisson-Boltzmann equations using well-established methods, then performs a hybrid procedure to compute the proton occupations of the ionizable sites as functions of the pH.
Residues involved in catalysis and/or recognition have different chemical properties from ordinary residues.
In particular, these functionally important residues have anomalous theoretical proton occupation curves.
THEMATICS exploits this difference and utilizes information from the shapes of the theoretical titration curves of the ionizable residues, as calculated approximately from the computed electrical potential function.
THEMATICS utilizes only the 3D structure of the query protein as input; neither sequence alignments nor structural comparisons are used.
Recently it was shown CITATION that, among the methods based on the 3D structure of the query protein only, THEMATICS achieves by far the best performance, as measured by sensitivity and precision for annotated catalytic residues.
The purpose of the present paper is five-fold: We present a monotonicity-constrained maximum likelihood approach, called Partial Order Optimum Likelihood, to improve performance and expand the capabilities of active site prediction.
Then it is shown that POOL, with THEMATICS input data alone, outperforms previous statistical CITATION and Support Vector Machine CITATION implementations of THEMATICS when applied to a test set of annotated protein structures.
It is then demonstrated that the inclusion of one additional 3D-structure-based feature, representing the ordinal size of the surface cleft to which each residue belongs, can result in some improved performance, as demonstrated by ROC curves and validated by Wilcoxon signed-rank tests.
With the introduction of environment features, POOL then can use the THEMATICS data to predict both ionizable and non-ionizable residues.
This all-residues extension of THEMATICS, together with a cleft size rank feature, results in a simple 3D-structure-based functional site predictor that performs better than other 3D structure based methods and nearly as well as the very best current methods that utilize both the 3D structure and sequence homology.
Finally, the POOL approach is able to take advantage of sequence alignment-based conservation scores, when available, in addition to these structure-based features.
When this additional information is included, the resulting classifier is shown to outperform all other currently available methods using any combination of structure and sequence information.
In prior implementations of THEMATICS for the identification of active-site residues from the 3D structure of the query protein CITATION CITATION, titration curve shapes were described by the moments of their first derivative functions.
These first derivative functions are essentially probability density functions and give unity when integrated over all space.
In Ko et al. CITATION, the third and fourth central moments 3 and 4 of these probability functions were used.
These moments measure asymmetry and, roughly, the area under the tails relative to the area near the mean, respectively.
In Tong et al. CITATION, the first moment and second central moment were also used.
In each of these approaches, the moments measure deviations from normal curve shape and the analyses identify the outliers, the residues that deviate most from the normal proton occupation behavior.
These prior approaches all use spatial clustering, so that outlier residues are reported as positive by the method if and only if they are in sufficiently close spatial proximity to at least one other outlier.
Thus the previous THEMATICS identifications involve two stages, where the first stage makes a binary decision on each residue and the second stage finds spatial clusters of the outliers.
In the new approach reported here, every residue is assigned a probability that it is an active-site residue.
Here, as an alternative to the clustering approach, we introduce features that describe a residue's neighbors; we call these environment features.
For a given scalar feature x, we define the value of the environment feature x env for a given residue r to be:FORMULAwhere r is an ionizable residue whose distance d to residue r is less than 9, and the weight w is given by 1/d 2.
In this study, we use the same features 3 and 4 used in the Ko CITATION approach, along with the additional features 3 env and 4 env.
Thus every ionizable residue in any protein structure is assigned the 4-dimensional feature vector.
The present approach has a number of advantages.
Specifically, active residues may be selected in one step and they can be rank-ordered according to the probability of involvement in an active site.
Furthermore, while THEMATICS previously has been applied to ionizable residues only, the present approach opens the door to direct prediction of non-ionizable active site residues, because the environment features 3 env and 4 env are well defined for all residues, including the non-ionizable ones.
Finally, additional geometric features that are obtainable from the 3D structure only may be readily combined with the four THEMATICS features in order to enhance performance.
Geometric features, such as the relative sizes of the clefts on the surface of the protein structure, have been shown to correlate with active site location CITATION, CITATION.
For instance, for the majority of single-chain proteins, the catalytic residues are in the largest cleft.
However geometric features alone do not perform comparatively well for active residue prediction, particularly because they are not very selective.
It is shown here that cleft size information combined with THEMATICS electrostatic features yields high performance in purely 3D structure based functional site predictions.
Protegrin peptides are potent antimicrobial agents believed to act against a variety of pathogens by forming nonselective transmembrane pores in the bacterial cell membrane.
We have employed 3D Poisson-Nernst-Planck calculations to determine the steady-state ion conduction characteristics of such pores at applied voltages in the range of 100 to 100 mV in 0.1 M KCl bath solutions.
We have tested a variety of pore structures extracted from molecular dynamics simulations based on an experimentally proposed octomeric pore structure.
The computed single-channel conductance values were in the range of 290 680 pS.
Better agreement with the experimental range of 40 360 pS was obtained using structures from the last 40 ns of the MD simulation, where conductance values range from 280 to 430 pS.
We observed no significant variation of the conductance with applied voltage in any of the structures that we tested, suggesting that the voltage dependence observed experimentally is a result of voltage-dependent channel formation rather than an inherent feature of the open pore structure.
We have found the pore to be highly selective for anions, with anionic to cationic current ratios on the order of 10 3.
This is consistent with the highly cationic nature of the pore but surprisingly in disagreement with the experimental finding of only slight anionic selectivity.
We have additionally tested the sensitivity of our PNP model to several parameters and found the ion diffusion coefficients to have a significant influence on conductance characteristics.
The best agreement with experimental data was obtained using a diffusion coefficient for each ion set to 10 percent of the bulk literature value everywhere inside the channel, a scaling used by several other studies employing PNP calculations.
Overall, this work presents a useful link between previous work focused on the structure of protegrin pores and experimental efforts aimed at investigating their conductance characteristics.
Antimicrobial peptides are small proteins produced by the innate immune system of many plants and animals as a first line of defense against bacterial infections CITATION, CITATION.
Due to their persistence in nature as well as their nonspecific mechanism of action, there has been significant research activity aimed at designing novel antibiotics based on AMPs CITATION ; the expectation is that bacteria will not develop significant resistance to antibiotics designed based on these peptides.
Thus far, such drug design efforts have been largely hampered by a lack of understanding of the fundamental mechanism of action of AMPs.
Although recent evidence suggests that intracellular targets may play an important role in the action of many AMPs CITATION, CITATION CITATION, there is a strong body of evidence suggesting that the ability of peptides to interact with and disrupt the bacterial membrane is essential to their mechanism of action CITATION, CITATION CITATION.
AMPs of various structural classes have been shown to have significant disruptive effects on both living bacterial membranes and model membrane systems, such as lipid bilayers CITATION, CITATION, CITATION and lipid monolayers CITATION, CITATION, CITATION.
For thorough reviews of several proposed mechanisms of membrane disruption, the reader is referred to CITATION.
Most relevant for the present work is the model in which AMPs aggregate to form large, nonselective pores in the bacterial membrane, which result in uncontrolled ion leakage, decay of the transmembrane potential, uncontrolled water transport, loss of cell contents and ultimately cell death.
We have focused our efforts on protegrin-1, a particularly potent antimicrobial peptide, which has recently been shown to form such pores in lipid bilayers of certain compositions.
Protegrins are small peptides isolated from porcine leukocytes that exhibit strong antimicrobial activity against a broad range of both Gram-positive and Gram-negative bacteria CITATION.
Protegrins are characterized by a -hairpin conformation that is held together by two cysteine-cysteine disulfide bonds.
They contain 16 18 amino acids, and are typically highly cationic, with the positive charges arising from arginine residues at the hairpin turn region and the two termini.
In the present work, we focus on the most prevalent natural form of protegrin, designated as PG-1, with the amino acid sequence RGGRLCYCRRRFCVCVGR-NH 2.
Mani and coworkers CITATION have conducted solid-state NMR experiments to investigate the membrane-bound structure of a PG-1 peptide, and have concluded that this peptide likely forms octomeric pores in lipid bilayers composed of a 3 1 mixture of palmitoyloleoyl-phosphatidylethanolamine to palmitoyloleoyl-phosphatidylglycerol CITATION.
Langham and coworkers used the structure suggested from these NMR experiments as the starting configuration of a molecular dynamics simulation in a lipid bilayer of the same composition CITATION.
This simulation showed the pore to be stable over more than 150 ns.
Figure 1 shows a cartoon representation a single protegrin peptide, as well as a side view of the proposed pore structure.
Prior to these studies of the pore structure of protegrin, early evidence of protegrin pores was provided by the experiments of Mangoni and coworkers CITATION and Sokolov and coworkers CITATION, in which the conductance characteristics of protegrin-treated membranes were measured.
Such knowledge of the nonequlibrium ion flow through protegrin pores may be closely related to their mechanism of action, since the unrestricted flow of ions through the membrane could result in potentially lethal membrane depolarization.
Mangoni and coworkers conducted voltage clamp experiments in Xenopus laevis oocyte membranes treated with protegrin-1 and several analogues CITATION.
They found that protegrins form weakly anion selective pores in the presence of several different salts, with KCl solutions exhibiting almost no selectivity.
Furthermore, they found that the conductance of such pores does not exhibit any voltage dependence over a voltage range of 100 to 30 mV.
Sokolov and coworkers CITATION carried out conductance measurements across several different types of planar phospholipid bilayers treated with protegrin-1 as well as protegrin-3.
These authors found that both protegrin analogues form weakly anion selective channels in mixed phospholipid bilayers, and moderately cation-selective channels in bilayers containing negatively charged bacterial lipopolysaccharide.
They reported a voltage-dependent single-channel conductance in the range of 40 360 picoSiemens, depending on the peptide used, the lipid bilayer composition, and the applied voltage.
In the present work, we attempt to explain and quantify the conductance behaviour of protegrin pores in terms of the structural information from NMR experiments CITATION and molecular dynamics simulations CITATION.
In particular, we explore the connection between structural features such as the size of the pore opening and the magnitude of the conductance, as well as the surprising experimental finding of both Mangoni and coworkers CITATION and Sokolov and coworkers CITATION that the protegrin pore is only slightly anion selective, despite having a total charge of 56.
Our investigation is based on the Poisson-Nernst-Planck theory, a continuum method of calculating non-equilibrium ion concentrations and fluxes around a fixed structure in the presence of an applied electrical voltage.
In our model, we simulate a voltage across a protegrin pore embedded in a lipid bilayer patch, and measure the resulting current.
Since the PNP model requires a rigid structure, we perform the calculations using several snapshots from the MD simulations of Langham and coworkers CITATION.
The description of the underlying equations and the numerical scheme used to solve them is deferred to the Methods section below.
There are currently a large number of orphan G-protein-coupled receptors whose endogenous ligands are unknown.
Identification of these peptide hormones is a difficult and important problem.
We describe a computational framework that models spatial structure along the genomic sequence simultaneously with the temporal evolutionary path structure across species and show how such models can be used to discover new functional molecules, in particular peptide hormones, via cross-genomic sequence comparisons.
The computational framework incorporates a priori high-level knowledge of structural and evolutionary constraints into a hierarchical grammar of evolutionary probabilistic models.
This computational method was used for identifying novel prohormones and the processed peptide sites by producing sequence alignments across many species at the functional-element level.
Experimental results with an initial implementation of the algorithm were used to identify potential prohormones by comparing the human and non-human proteins in the Swiss-Prot database of known annotated proteins.
In this proof of concept, we identified 45 out of 54 prohormones with only 44 false positives.
The comparison of known and hypothetical human and mouse proteins resulted in the identification of a novel putative prohormone with at least four potential neuropeptides.
Finally, in order to validate the computational methodology, we present the basic molecular biological characterization of the novel putative peptide hormone, including its identification and regional localization in the brain.
This species comparison, HMM-based computational approach succeeded in identifying a previously undiscovered neuropeptide from whole genome protein sequences.
This novel putative peptide hormone is found in discreet brain regions as well as other organs.
The success of this approach will have a great impact on our understanding of GPCRs and associated pathways and help to identify new targets for drug development.
G protein coupled receptors probably represent the largest gene family, making up 3 percent of the mammalian genome CITATION.
These proteins are made up of several subfamilies, including Class A rhodopsin-like, Class B secretin-like, Class C metabotropic glutamate/pheromone-like, and other nonmammalian receptors.
Within each class, there is a very large number of smaller subclassifications, such as a family of receptors for peptide hormones within rhodopsin-like receptors.
There are approximately 1,000 GPCRs, the vast majority of which are olfactory receptors, with more than 650 GPCRs in the rhodopsin family alone CITATION.
A large number of these receptors have been identified only by computational methods, while others have been cloned and transfected into cells; however, the cognate neurotransmitter and the receptor functions for many GPCRs are currently unknown.
Any receptor for which the native neurotransmitter is unknown is considered an orphan receptor.
Of all the orphan receptors that remain, some percentage represents receptors for peptide hormones.
This large family of proteins is important not only from a basic science perspective, but because of their extracellular sites of action and importance as first messengers for cellular signaling, GPCRs have become a primary target for drug development.
In fact, over 30 percent of all pharmaceuticals act either as agonists or antagonists of GPCRs CITATION.
Many pharmaceutical companies are identifying, cloning, and patenting new orphan GPCRs, with the hope that orphan receptors will ultimately lead to new drug development and new pharmaceutical agents.
Although the identification of putative GPCRs can be accomplished relatively easily, the discovery of the endogenous ligands that activate these receptors is far more difficult.
These ligands can exist as small molecules, lipids, peptides, or proteins CITATION, CITATION.
Many, such as ATP, may have important functions other than activating a GPCR.
Even within a class of hormones, there are seldom obvious clues that identify a new candidate.
This is particularly true within the family of peptide hormones, as they are processed from a larger species known as preprohormones CITATION .
Peptide hormones, or neuropeptides, are a string of amino acids ranging from approximately 3 to 50 residues.
They are found within a larger protein, and the production of the actual hormone usually follows specific rules.
Preprohormones are secreted proteins, and each has a signal sequence that is necessary for the transport of the protein out of the Golgi complex into a secretory vesicle for processing and secretion where the signal sequence is removed, revealing the prohormone CITATION.
In general, hormones are surrounded by a pair of basic residues, i.e. Arg-Arg, Arg-Lys, Lys-Arg, or Lys-Lys, which are found directly adjacent to the putative hormone.
These double basic residues act as recognition sites for processing enzymes, usually serine proteases that cleave the prohormone to liberate the active peptide CITATION, CITATION.
In many cases, there is more than a single active peptide within one precursor protein CITATION .
Even with these common features, the identification of a peptide hormone from a DNA or protein sequence is very difficult.
Even though all of the GPCRs are obviously related based upon DNA or protein sequence, the neuropeptides that bind to the receptors are only obviously related within discrete families of prohormones.
For instance, the family of opioid-like peptides has four members.
These prohormones, proopiomelanocortin, proenkephalin, prodynorphin, and pronociceptin, share similar genomic structures and a very slight similarity of protein sequence, most notably the YGGF of enkephalin, -endorphin, dynorphin, and N/OFQ CITATION, CITATION.
However, if one were to conduct a BLAST search in Genbank for DNA sequences similar to proenkephalin, one would not find any other neuropeptide.
Simple search strategies within Genbank are not adequate for identifying novel neuropeptides, especially those not belonging to known neuropepeptide families.
There is an additional feature of neuropeptides that may more clearly differentiate them from other types of molecules.
Neuropeptides are usually well conserved among various species, while the intervening sequences, presumably because they are simply discarded, are not well conserved CITATION.
Here we describe a novel Hidden Markov Model -based computational framework, the Match Profile HMM method for neuropeptide identification based upon an approach that models spatial structure along the genomic sequence simultaneously with the temporal evolutionary path structure across species, and show how such models can be used to discover new functional molecules via cross-genomic sequence comparisons.
This computational tool was used to identify a novel prohormone, NPQ, containing up to four potential neuropeptides CITATION
Nucleoside analogs used in antiretroviral treatment have been associated with mitochondrial toxicity.
The polymerase- hypothesis states that this toxicity stems from the analogs' inhibition of the mitochondrial DNA polymerase leading to mitochondrial DNA depletion.
We have constructed a computational model of the interaction of polymerase- with activated nucleoside and nucleotide analog drugs, based on experimentally measured reaction rates and base excision rates, together with the mtDNA genome size, the human mtDNA sequence, and mitochondrial dNTP concentrations.
The model predicts an approximately 1000-fold difference in the activated drug concentration required for a 50 percent probability of mtDNA strand termination between the activated di-deoxy analogs d4T, ddC, and ddI and the activated forms of the analogs 3TC, TDF, AZT, FTC, and ABC.
These predictions are supported by experimental and clinical data showing significantly greater mtDNA depletion in cell culture and patient samples caused by the di-deoxy analog drugs.
For zidovudine we calculated a very low mtDNA replication termination probability, in contrast to its reported mitochondrial toxicity in vitro and clinically.
Therefore AZT mitochondrial toxicity is likely due to a mechanism that does not involve strand termination of mtDNA replication.
Current guidelines for highly active anti-retroviral treatment regimens of HIV-positive patients recommend two drugs of the nucleoside reverse transcriptase inhibitor class CITATION.
This class currently consists of: stavudine, lamivudine, zidovudine, zalcitabine, didanosine, abacavir, emtricitabine and tenofovir.
Though zalcitabine at the time of this writing is still technically approved for treatment its distribution in the United States was discontinued by Roche in 2006.
In their activated tri-phosphorylated forms, each NRTI acts as a nucleotide analog interacting with the HIV viral reverse transcriptase as an alternative substrate to the natural nucleotides CITATION, CITATION.
Each of these analogs lacks the 3 OH group necessary for incorporation of the next nucleotide thereby terminating viral DNA strand elongation.
Although NRTIs are effective drugs and have helped usher HIV into the category of a controllable chronic disease, they are also often toxic, inducing side effects such as lactic acidosis, neuropathy, nausea, lypodistrophy, and myopathy in patients.
Intolerance of such side effects is a common reason for treatment discontinuation CITATION.
Any decrease in patient compliance to the treatment regimen is a serious concern that can lead to an increase in viral resistance and ultimately to treatment failure.
The first step in ameliorating these side effects and preventing them in future antiviral treatments is to understand the mechanisms behind the mitochondrial toxicity of the NRTIs that are in use today.
As we discuss below, many mechanisms of the mitochondrial toxicity have been proposed.
In this paper we specifically consider the plausibility of the currently most widely accepted hypothesis for the toxicity mechanism for this class of drugs; interference of mitochondrial DNA replication by the activated drug.
Polymerase- is the only polymerase responsible for mitochondrial DNA replication.
While pol- is not believed to directly regulate mtDNA levels, pathogenic mutations in the gene POLG do affect the stability of mtDNA and cause mtDNA depletion CITATION.
Polymorphisms found in the POLG gene in the human population may cause a natural variability in the activity of this complex enzyme and may conceivably play a role in patient variability in NRTI drug toxicities.
In a study conducted by Martin et al. CITATION the approved NRTIs were shown to inhibit various host DNA polymerases.
After the HIV Reverse Transcriptase, the highest affinity of the NRTIs was for polymerase-.
This, along with the fact that many of the NRTI side-effects resemble symptoms of mitochondrial genetic disorders, implicated interaction with polymerase- and subsequent depletion of mtDNA as a potential cause of NRTI toxicity giving rise to the polymerase- hypothesis CITATION.
Indeed, experiments have demonstrated decreased mtDNA amounts in various tissue types of NRTI-treated HIV positive patients CITATION CITATION.
In addition, mtDNA depletion was observed in parallel with cell death, mitochondrial morphological changes, and increased lactate production in liver, heart, neuron, skeletal muscle, adipose, and blood cell cultures after incubation with different NRTIs CITATION CITATION.
The possible polymerase- dependent toxicity mechanisms that comprise the polymerase- hypothesis are direct inhibition of polymerase- by NRTI-triphosphate without incorporation into the mtDNA, chain termination of mtDNA replication following incorporation of the NRTI triphosphate, and incorporation of the analog triphosphate into mtDNA without chain-termination allowing the NRTI to continue as a point mutation in mtDNA CITATION .
However, there also exists a substantial body of data that are not consistent with toxicity mechanisms resulting in depletion of mtDNA.
Martin et al. CITATION showed no association between inhibition of polymerase- by NRTIs and mtDNA depletion.
Mitochondrial dysfunction has been observed in vitro in mouse muscle, white adipose, brain, liver, and heart tissue CITATION, hepatoma cell lines CITATION as well as CD4 cells CITATION after incubation with NRTIs although no significant decrease in mtDNA amount was observed.
Particularly, incubation of liver and skeletal muscle cells with ddC, ddI, d4T, and AZT show a higher rate of lactate production in the presence of AZT, but the least amount of mtDNA depletion CITATION, CITATION.
In clinical settings mtDNA depletion has been seen in parallel with normal cytochrome c oxidase activity, a sign of correct mitochondrial function CITATION, and was not associated with lipoatrophy CITATION.
Taken together, these findings indicate a weak relationship between mtDNA copy number and nucleoside analog toxicity.
This warrants a deeper look at the data concerning the interaction of different NRTIs with polymerase-.
To this end, we have simulated the DNA replication process of mitochondria.
Using enzyme kinetics data gathered from Johnson et al. CITATION, Feng et al. CITATION, and Hanes et al. CITATION, CITATION we have carried out a series of simulations of mtDNA replication in the presence of various nucleoside analogs that interact with polymerase-.
These simulations bridge the gap between the basic enzyme kinetics data and the probability of failure of the mtDNA replication process.
Individual perception of vaccine safety is an important factor in determining a person's adherence to a vaccination program and its consequences for disease control.
This perception, or belief, about the safety of a given vaccine is not a static parameter but a variable subject to environmental influence.
To complicate matters, perception of risk does not correspond to actual risk.
In this paper we propose a way to include the dynamics of such beliefs into a realistic epidemiological model, yielding a more complete depiction of the mechanisms underlying the unraveling of vaccination campaigns.
The methodology proposed is based on Bayesian inference and can be extended to model more complex belief systems associated with decision models.
We found the method is able to produce behaviors which approximate what has been observed in real vaccine and disease scare situations.
The framework presented comprises a set of useful tools for an adequate quantitative representation of a common yet complex public-health issue.
These tools include representation of beliefs as Bayesian probabilities, usage of logarithmic pooling to combine probability distributions representing opinions, and usage of natural conjugate priors to efficiently compute the Bayesian posterior.
This approach allowed a comprehensive treatment of the uncertainty regarding vaccination behavior in a realistic epidemiological model.
Since early vaccination campaigns against smallpox, vaccination policies have been a matter of debate CITATION : mass vaccination versus blocking strategies; compulsory versus voluntary, are some highly debated issues.
Despite these early controversies - and consequent alternative policies implemented in different countries - high disease scare in the past has led to very high vaccine coverage and consequent successful eradication of smallpox, as well as very low incidence of measles, polio, tetanus, diphtheria, etc, resulting in over 98 percent mortality reduction by vaccine preventable diseases in developed countries CITATION .
In recent years, after complete or almost complete elimination of these diseases, the debate is shifting towards issues of vaccine safety.
Increased perception of vaccine risks and lowered perception of disease risks has challenged previous willingness to vaccinate CITATION.
In this scenario, understanding and predicting individual's willingness to vaccinate is paramount for estimating vaccine coverage and compare strategies to achieve coverage goals.
Willingness to vaccinate is highly dependent on the perceived risk of acquiring a serious disease CITATION.
When disease risk is low, however small risk of adverse events from the vaccine become relatively important and may lead to vaccine coverage lower than required to control transmission CITATION.
When serious disease risk is too high, on the other hand, vaccine coverage may increase above that required to guarantee population protection CITATION.
We illustrate these behaviors with two examples:
In the UK, MMR vaccine uptake started to decline after a controversial study linking MMR vaccine to autism CITATION.
In a decade, vaccine coverage went well below the target herd immunity level of 95 percent.
Despite the confidence of researchers and most health professionals on the vaccine safety, the confidence of the public was deeply affected.
In an attempt to find ways to restore this confidence, several studies were carried out to identify factors associated with parent's unwillingness to vaccinate their children.
They found that Not receiving unbiased and adequate information from health professionals about vaccine safety and media's adverse publicity were the most common reasons influencing uptake CITATION.
Other important factors were: lack of belief in information from the government sources ; fear of general practitioners promoting the vaccine for personal reasons ; and media scare.
Note that during this period the risk of acquiring measles was very low due to previously high vaccination coverage.
The goal of human genome re-sequencing is obtaining an accurate assembly of an individual's genome.
Recently, there has been great excitement in the development of many technologies for this, with even more expected to appear.
The costs and sensitivities of these technologies differ considerably from each other.
As an important goal of personal genomics is to reduce the cost of re-sequencing to an affordable point, it is worthwhile to consider optimally integrating technologies.
Here, we build a simulation toolbox that will help us optimally combine different technologies for genome re-sequencing, especially in reconstructing large structural variants.
SV reconstruction is considered the most challenging step in human genome re-sequencing.
To this end, we formulate canonical problems that are representative of issues in reconstruction and are of small enough scale to be computationally tractable and simulatable.
Using semi-realistic simulations, we show how we can combine different technologies to optimally solve the assembly at low cost.
With mapability maps, our simulations efficiently handle the inhomogeneous repeat-containing structure of the human genome and the computational complexity of practical assembly algorithms.
They quantitatively show how combining different read lengths is more cost-effective than using one length, how an optimal mixed sequencing strategy for reconstructing large novel SVs usually also gives accurate detection of SNPs/indels, how paired-end reads can improve reconstruction efficiency, and how adding in arrays is more efficient than just sequencing for disentangling some complex SVs.
Our strategy should facilitate the sequencing of human genomes at maximum accuracy and low cost.
The human genome is comprised of approximately 6 billion nucleotides on two pairs of 23 chromosomes.
Variations between individuals are comprised of 6 million single nucleotide polymorphisms and 1000 relatively large structural variants of 3 kb or larger and many more smaller SVs are responsible for the phenotypic variation among individuals CITATION, CITATION.
Most of these large SVs are due to genomic rearrangements, and a few others contain novel sequences that are not present in the reference genome CITATION.
The goal of personal genomics is to determine all these genetic differences between individuals and to understand how these contribute to phenotypic differences in individuals.
Making personal genomics almost a reality over the past decade, the development of high throughput sequencing technologies has enabled the sequencing of individual genomes CITATION, CITATION.
In 2007, Levy et al. reported the sequencing of an individual's genome based on Sanger CITATION whole-genome shotgun sequencing, followed by de novo assembly strategies.
Wheeler et al. in 2008 presented another individual's genome sequence constructed from 454 sequencing reads CITATION and comparative genome assembly methods.
In the mean time, other new sequencing technologies such as Solexa/Illumina sequencing CITATION have become available for individual genome sequencing with corresponding, specially-designed sequence assembly algorithm designed CITATION CITATION .
These projects and algorithms, however, mostly relied on a single sequencing technology to perform individual re-sequencing and thus did not take full advantage of all the existing experimental technologies.
Table 1 gives a summary of the characteristics of several technologies in comparative individual genome sequencing.
At one extreme, performing long Sanger sequencing with a very deep coverage will lead to excellent results at high cost.
In another, performing only the inexpensive and short Illumina sequencing may generate good and cost-efficient results in SNP detection, but will not be able to either unambiguously locate some of the SVs in repetitive genomic regions or fully reconstruct many of the large SVs.
Moreover, array technologies such as the SNP array CITATION and the CGH array at different resolutions CITATION CITATION can also be utilized to identify the SVs: the SNP arrays can detect SNPs directly, and the CGH array is able to detect kilobase- to megabase- sized copy number variants CITATION, which can be integrated into the sequencing-based SV analysis.
It is thus advantageous to consider optimally combining all these experimental techniques into the individual genome re-sequencing framework and to design experiment protocols and computational algorithms accordingly.
Due to the existence of reference genome assemblies CITATION, CITATION and the high similarity between an individual's genome and the reference CITATION, the identification of small SVs is relatively straightforward in comparative re-sequencing with the analysis of single split-reads covering small SVs.
Meanwhile, although there exist algorithms to detect large SVs with paired-end reads CITATION, the complete reconstruction of a large SV requires the integration of reads spanning a wide region, often involving misleading reads from other locations of the genome.
If there were no repeats or duplications in the human genome, the reconstruction of such large SVs would be trivially accomplished by the de novo assembly with a high coverage of inexpensive short reads around these regions.
With the existence of repeats and duplications in the human genome, however, a set of longer reads will be required to accurately locate some of these SVs in repetitive regions, and a hybrid re-sequencing strategy with both comparative and de novo approaches will be necessary to identify genomic rearrangement events such as deletions and translocations, and also to reconstruct large novel insertions in individuals.
Such steps are thus much harder than the others, and will be the main focus of this paper.
Here we present a toolbox and some representative case studies on how to optimally combine the different experimental technologies in the individual genome re-sequencing project, especially in reconstructing large SVs, so as to achieve accurate and economical sequencing.
An optimal experimental design should be an intelligent combination of the long, medium, and short sequencing technologies and also some array technologies such as CGH.
Some of the previous genome sequencing projects CITATION, CITATION have already incorporated such hybrid approaches using both long and medium reads, although the general problem of optimal experimental design has not yet been systematically studied.
While it is obvious that combining technologies is advantageous, we want to quantitatively show the potential savings based on different integration strategies.
Also, since the technologies are constantly developing, it will be useful to have a general and flexible approach to predict the outcome of integrating different technologies, including the new ones coming in the future.
In the following sections, we will first briefly describe a schematic comparative genome re-sequencing framework, focusing on the intrinsically most challenging steps of reconstructing large SVs, and then use a set of semi-realistic simulations of these representative steps to optimize the integrated experimental design.
Since full simulations are computationally intractable for such steps in the large parameter space of combinations of different technologies, the simulations are carried out in a framework that can combine the real genomic data with analytical approximations of the sequencing and assembly process.
Also, this simulation framework is capable of incorporating new technologies as well as adjusting the parameters for existing ones, and can provide informative guidelines to optimal re-sequencing strategies as the characteristics and cost-structures of such technologies evolve, when combining them becomes a more important concern.
The simulation framework is downloadable as a general toolbox to guide optimal re-sequencing as technology constantly advances.
The conversion from soluble states into cross- fibrillar aggregates is a property shared by many different proteins and peptides and was hence conjectured to be a generic feature of polypeptide chains.
Increasing evidence is now accumulating that such fibrillar assemblies are generally characterized by a parallel in-register alignment of -strands contributed by distinct protein molecules.
Here we assume a universal mechanism is responsible for -structure formation and deduce sequence-specific interaction energies between pairs of protein fragments from a statistical analysis of the native folds of globular proteins.
The derived fragment fragment interaction was implemented within a novel algorithm, prediction of amyloid structure aggregation, to investigate the role of sequence heterogeneity in driving specific aggregation into ordered self-propagating cross- structures.
The algorithm predicts that the parallel in-register arrangement of sequence portions that participate in the fibril cross- core is favoured in most cases.
However, the antiparallel arrangement is correctly discriminated when present in fibrils formed by short peptides.
The predictions of the most aggregation-prone portions of initially unfolded polypeptide chains are also in excellent agreement with available experimental observations.
These results corroborate the recent hypothesis that the amyloid structure is stabilised by the same physicochemical determinants as those operating in folded proteins.
They also suggest that side chain side chain interaction across neighbouring -strands is a key determinant of amyloid fibril formation and of their self-propagating ability.
An increasing number of human pathologies are associated with the conversion of peptides and proteins from their soluble functional forms into well-defined fibrillar aggregates CITATION, CITATION.
The diseases can be broadly grouped into neurodegenerative conditions, in which fibrillar aggregation occurs in the brain, nonneuropathic localised amyloidoses, in which aggregation occurs in a single type of tissue other than the brain, and nonneuropathic systemic amyloidoses, in which aggregation occurs in multiple tissues CITATION, CITATION.
The fibrillar deposits associated with human pathologies are generally described as amyloid fibrils when they accumulate extracellularly, whereas the term intracellular inclusions has been suggested to be more appropriate when fibrils morphologically and structurally related to extracellular amyloid form inside the cell CITATION .
Amyloid formation is not restricted, however, to those polypeptide chains that have recognised links to protein deposition diseases.
Several other proteins that have no such link have been found to form fibrillar aggregates in vitro with morphological, structural, and tinctorial properties that allow them to be classified as amyloid-like fibrils CITATION, CITATION.
This finding has led to the idea that the ability to form the amyloid structure is an inherent property of polypeptide chains, encoded in main backbone chain interactions.
From a theoretical perspective it was also recently shown that simple considerations of geometry and symmetry are sufficient to explain, within the same sequence-independent framework, the emergence of a limited menu of native-like conformations for a single chain and of -aggregate structures for multiple chains CITATION .
The generic ability to form the amyloid structure has apparently been exploited by living systems for specific purposes, as some organisms have been found to convert, during their normal physiological life cycle, one or more of their endogenous proteins into amyloid-like fibrils that have functional properties rather than deleterious effects CITATION CITATION.
Perhaps the most surprising of these functions is the ability of amyloid-like fibrillar aggregates to serve as a nonchromosomal genetic element.
Proteins such as Ure2p and Sup35p or HET-s can adopt a fibrillar conformation that, in addition to giving rise to specific phenotypes, appears to be self-propagating, transmissible, and infectious CITATION .
In their soluble states, the proteins able to form fibrillar aggregates do not share any obvious sequence identity or structural homology to each other.
In spite of these differences in the precursor proteins, morphological inspection reveals common properties in the resulting fibrils CITATION.
Images obtained with transmission electron microscopy or atomic force microscopy reveal that the fibrils usually consist of 2 6 protofilaments, each about 2 5 nm in diameter CITATION.
These protofilaments generally twist together to form fibrils that are typically 7 13 nm wide CITATION, CITATION, or associate laterally to form long ribbons that are 2 5 nm high and up to 30 nm wide CITATION CITATION.
X-ray fibre diffraction data have shown that the protein or peptide molecules are arranged so that the polypeptide chain forms -strands that run perpendicular to the long axis of the fibril CITATION .
Solid-state nuclear magnetic resonance, X-ray micro- or nano-crystallography, and other techniques such as systematic protein engineering coupled with site-directed spin-labelling or fluorescence-labelling have transformed our ability to gain insight into the structures of fibrillar aggregates with residue-specific detail CITATION CITATION.
These advances have allowed us to go beyond the generic notions of the fibrillar appearance and presence of a cross- structure.
These studies have indeed allowed the identification of regions of the sequence that form and stabilise the cross- core of the fibrils, as opposed to those stretches that are flexible and exposed to the solvent.
In many cases, the arrangement of the various molecules in the fibrils has also been determined, clarifying the nature of the intermolecular contacts and the structural stacking of the molecules along the fibril axis.
One frequent characteristic emerging from these studies, particularly for fibrils formed by long sequences, is the parallel in-register arrangements of -strands in the fibril core CITATION CITATION, CITATION CITATION, CITATION, but antiparallel arrangements are also possible, especially for shorter strands CITATION, CITATION .
At the same time, mutational studies of the amyloid aggregation kinetics revealed simple correlations between physico chemical properties and aggregation propensities CITATION.
This allowed the development of different methods, which successfully predict aggregation-prone regions in the amino-acid sequence of a full-length protein CITATION CITATION.
All such approaches focus on predicting the intrinsic -aggregation propensity of a sequence stretch using only the amino-acid sequence as an input.
In CITATION the possible parallel/antiparallel arrangement of the sequence stretch with itself was also taken into account.
Molecular dynamics simulations of sequence fragments mounted on idealized -strand templates, either parallel or antiparallel, were used to identify the most amyloidogenic fragments in a specific case CITATION.
A template amyloid structure based on PIRA is also employed in a very recent method for identifying fibril-forming segments CITATION.
A yet-unanswered question is why PIRA is found to be the most frequent arrangement of -strands in the fibril core.
Here we introduce a computational approach by editing a pairwise energy function based on the propensities of two residues to be found within a -sheet facing one another on neighbouring strands, as determined from a dataset of globular proteins of known native structures.
We extract two different propensity sets depending on the orientation of the neighbouring strands.
Our method associates energy scores to specific -pairings of two sequence stretches of the same length, and further assumes that distinct protein molecules involved in fibril formation will adopt the minimum-energy -pairings in order to better stabilise the cross- core.
A novel feature of our method is the ability to predict the registry of the intermolecular hydrogen bonds formed between amyloidogenic sequence stretches.
In this way we can rationalise the observed tendency of proteins to assemble into parallel -sheets in which the individual strands are in-register, contributing to form stackings of the same residue type along the fibril axis.
Our algorithm is also able to correctly discriminate the orientation between intermolecular -strands, either parallel or antiparallel.
As a further demonstration of the robustness of the approach we will illustrate the ability of our algorithm to predict the portions of the sequence forming the cross- core of the fibrils for a set of proteins, in excellent agreement with the experimentally determined amyloid structures, similar to previously proposed methods CITATION CITATION .
Our approach is based on the key assumption that a universal mechanism is responsible for -sheet formation both in globular proteins and in fibrillar aggregates.
The successful predictions obtained in this work suggest the validity of the above hypothesis in agreement with the unified framework presented previously CITATION .
Mechanical force plays an important role in the physiology of eukaryotic cells whose dominant structural constituent is the actin cytoskeleton composed mainly of actin and actin crosslinking proteins.
Thus, knowledge of rheological properties of actin networks is crucial for understanding the mechanics and processes of cells.
We used Brownian dynamics simulations to study the viscoelasticity of crosslinked actin networks.
Two methods were employed, bulk rheology and segment-tracking rheology, where the former measures the stress in response to an applied shear strain, and the latter analyzes thermal fluctuations of individual actin segments of the network.
It was demonstrated that the storage shear modulus increases more by the addition of ACPs that form orthogonal crosslinks than by those that form parallel bundles.
In networks with orthogonal crosslinks, as crosslink density increases, the power law exponent of G as a function of the oscillation frequency decreases from 0.75, which reflects the transverse thermal motion of actin filaments, to near zero at low frequency.
Under increasing prestrain, the network becomes more elastic, and three regimes of behavior are observed, each dominated by different mechanisms: bending of actin filaments, bending of ACPs, and at the highest prestrain tested, stretching of actin filaments and ACPs.
In the last case, only a small portion of actin filaments connected via highly stressed ACPs support the strain.
We thus introduce the concept of a supportive framework, as a subset of the full network, which is responsible for high elasticity.
Notably, entropic effects due to thermal fluctuations appear to be important only at relatively low prestrains and when the average crosslinking distance is comparable to or greater than the persistence length of the filament.
Taken together, our results suggest that viscoelasticity of the actin network is attributable to different mechanisms depending on the amount of prestrain.
Actin is the most abundant intracellular protein in eukaryotic cells and plays an important role in a wide range of biological and mechanical phenomena CITATION.
Monomeric actin self-assembles to a filamentous form, F-actin, which is crosslinked into the actin cytoskeleton by various actin crosslinking proteins.
It has been known that mechanical force plays a crucial role in the physiology of eukaryotic cells CITATION, and therefore appropriate functions of living cells are attributable to the rigorous control of their rheological properties CITATION.
Thus, investigating rheological properties of actin networks is indispensable for elucidating the mechanics of cells as well as for understanding a wide variety of cellular processes.
Experiments have been conducted to probe viscoelastic properties of cells and reconstituted actin gels using a variety of techniques such as microbead rheology, magnetic bead cytometry, and bulk rheology CITATION CITATION.
In experiments, discrepancies have been observed among measurements using dissimilar methodologies, and many of the observed features are not well understood.
For example, viscoelastic moduli measured by single-bead passive microbead rheology are much smaller than those determined by 2-point microrheology or bulk rheology CITATION CITATION.
Also, although distinct power law responses of the storage modulus have often been observed in vivo and in vitro CITATION CITATION, their origin is not yet clearly understood.
Concurrently, characteristics of semi-flexible polymer networks have been studied theoretically and computationally CITATION CITATION.
Two- CITATION, CITATION and 3-dimensional computational models CITATION studying affine and nonaffine deformations of semi-flexible networks responding to large shear strain revealed two regimes dominated by bending or stretching of filaments, respectively.
Recently, using a microstructure-based continuum mechanics approach, Palmer and Boyce reproduced many of the rheological properties of actin networks observed in experiments CITATION.
The viscoelastic behavior of semi-flexible networks was also investigated using dissipative particle dynamics and the concept of microbead rheology, where scale-free behavior of the bead displacement was observed CITATION, CITATION.
To date, however, most of these models neither explicitly take into account ACP mechanics nor systematically account for thermal fluctuations, nor have they been used to explore the effects of finite prestress on viscoelasticity, all of which are potentially important factors governing matrix viscoelasticity.
With the objective of extending these previous works and providing new insights into underlying mechanisms, we develop a Brownian dynamics model of the actin network that includes features such as steric interaction among filaments, the usage of explicit crosslinkers, a more realistic morphology, and the consideration of crosslinker stiffness.
By measuring stress in response to applied oscillatory shear strain and thermal fluctuations of individual segments in the polymeric chain, we investigate viscoelastic properties of actin-like networks.
Throughout this paper, for convenience, the term, actin network is used to refer to the network being simulated.
It should be noted, however, that some of the properties employed in our model, especially for ACPs, were estimated since they are not well-known experimentally.
Due to simplifications in the model and parameter uncertainty, the results should therefore be viewed as representative of a generic crosslinked network, but lack a quantitatively precise correspondence to actin networks.
Nevertheless, we found features that semi-quantitatively capture experimentally observed behaviors of actin networks.
The storage and loss moduli, G and G, followed power laws as functions of the oscillation frequency.
As the prestrain increased, the network became increasingly elastic.
Bending and extensional stiffnesses of actin filaments and ACPs played an important role depending on the degree of prestrain.
We found that the mechanical response of the network is dominated by a percolating supportive framework, while other actin filaments contribute little to the viscoelastic moduli.
Surprisingly, in typical physiological conditions where the distance between crosslinking points along F-actin is much shorter than the actin persistence length, we found that thermal fluctuation plays little role in viscoelasticity, so that the network consisting of crosslinked F-actins can be viewed essentially as a deterministic overdamped system in a viscous medium.
In sum, our computational model elucidates how various mechanical responses govern viscoelastic properties of the network under different conditions.
The extracellular matrix plays a critical role in orchestrating the events necessary for wound healing, muscle repair, morphogenesis, new blood vessel growth, and cancer invasion.
In this study, we investigate the influence of extracellular matrix topography on the coordination of multi-cellular interactions in the context of angiogenesis.
To do this, we validate our spatio-temporal mathematical model of angiogenesis against empirical data, and within this framework, we vary the density of the matrix fibers to simulate different tissue environments and to explore the possibility of manipulating the extracellular matrix to achieve pro- and anti-angiogenic effects.
The model predicts specific ranges of matrix fiber densities that maximize sprout extension speed, induce branching, or interrupt normal angiogenesis, which are independently confirmed by experiment.
We then explore matrix fiber alignment as a key factor contributing to peak sprout velocities and in mediating cell shape and orientation.
We also quantify the effects of proteolytic matrix degradation by the tip cell on sprout velocity and demonstrate that degradation promotes sprout growth at high matrix densities, but has an inhibitory effect at lower densities.
Our results are discussed in the context of ECM targeted pro- and anti-angiogenic therapies that can be tested empirically.
The extracellular matrix is a major component of the extravascular tissue region, or stroma, and plays a central role in morphogenesis, including embryogenesis CITATION, tissue repair and wound healing CITATION, new blood vessel growth CITATION, and cancer invasion CITATION.
A large body of research is concentrated on understanding how cell-ECM interactions impact and regulate morphogenic processes.
Results from such investigations illuminate the active role of the ECM in transmitting biochemical signals and mechanical forces that mediate cell survival, phenotype, shape, and orientation.
This area continues to be a target of intense investigation.
Cells are equipped with and can upregulate transmembrane receptors that enable them to receive signals from and interact with their environment.
Integrins are one such receptor and are stimulated by the various proteins of the ECM CITATION, CITATION.
Endothelial cells attach directly to the collagen fibers in the ECM through the FORMULA integrin receptors CITATION.
Biochemical signals originating within the cell can affect integrin-ligand binding affinity and consequently modulate cellular adhesion to the matrix.
Focal adhesion complexes form and bind directly to the cell's cytoskeleton CITATION.
Once assembled, a focal adhesion anchors the cell to the ECM, which is used by the cell for movement.
These focal adhesions are assembled and disassembled dynamically to facilitate cell migration.
Migratory guidance via focal adhesion binding sites in the ECM is a phenomenon referred to as contact guidance and plays a key role in guiding new vessel growth CITATION .
The physical properties of the ECM, such as density, heterogeneity, and stiffness, that affect cell behavior is also an area of current investigation.
Matrigel, a popular gelatinous protein substrate for in vitro experiments of angiogenesis, is largely composed of collagen and laminin and contains growth factors, all of which provide an environment conducive to cell survival.
In experiments of endothelial cells on Matrigel, increasing the stiffness of the gel or disrupting the organization of the cellular cytoskeleton, inhibits the formation of vascular cell networks CITATION, CITATION.
Cells respond to alterations in the mechanical properties of the ECM, for example, by upregulating their focal adhesions on stiffer substrates CITATION.
For anchorage-dependent cells, including endothelial cells, increasing the stiffness of the ECM therefore results in increased cell traction and slower migration speeds CITATION.
Measurements of Matrigel stiffness as a function of density show a positive relationship between these two mechanical properties CITATION.
That is, as density increases, so does matrix stiffness.
In light of these two findings, it is not surprising that this experimental study also shows slower cell migration speeds as matrix density increases CITATION.
Moreover, matrices with higher fiber density transfer less strain to the cell CITATION and experiments of endothelial cells cultured on collagen gels demonstrate that directional sprouting, called branching, is induced by collagen matrix tension CITATION.
Thus, via integrin receptors, the mechanical properties of the ECM influence cell-matrix interactions and modulate cell shape, cell migration speed, and the formation of vascular networks.
Understanding how individual cells interpret biochemical and mechanical signals from the ECM is only a part of the whole picture.
Morphogenic processes also require multicellular coordination.
In addition to the guidance cues cells receive from the ECM, they also receive signals from each other.
During new vessel growth, cells adhere to each other through cell-cell junctions, called cadherins, and in order to migrate, cells must coordinate integrin mediated focal adhesions with these cell-cell bonds.
This process is referred to as collective or cluster migration CITATION.
During collective migration, cell clusters often organize as two-dimensional sheets CITATION .
Cells also have the ability to condition the ECM for invasion by producing proteolytic enzymes that degrade specific ECM proteins CITATION.
In addition, cells can synthesize ECM components, such as collagen and fibronectin CITATION, CITATION, and can further reorganize the ECM by the forces they exert on it during migration CITATION, CITATION, CITATION.
Collagen fibrils align in response to mechanical loading and cells reorient in the direction of the applied load CITATION.
Tractional forces exerted by vascular endothelial cells on Matrigel cause cords or tracks of aligned fibers to form promoting cell elongation and motility CITATION.
As more experimental data are amassed, the ECM is emerging as the vital component to morphogenic processes.
In this work, we extend our cellular model of angiogenesis CITATION and validate it against empirical measurements of sprout extension speeds.
We then use our model to investigate the effect of ECM topography on vascular morphogenesis and focus on mechanisms controlling cell shape and orientation, sprout extension speeds, and sprout morphology.
We show the dependence of sprout extension speed and morphology on matrix density, fiber network connectedness, and fiber orientation.
Notably, we observe that varying matrix fiber density affects the likelihood of capillary sprout branching.
The model predicts an optimal density for capillary network formation and suggests matrix heterogeneity as a mechanism for sprout branching.
We also identify unique ranges of matrix density that promote sprout extension or that interrupt normal angiogenesis, and show that maximal sprout extension speeds are achieved within a density range similar to the density of collagen found in the cornea.
Finally, we quantify the effects of proteolytic matrix degradation by the tip cell on sprout velocity and demonstrate that degradation promotes sprout growth at high densities, but has an inhibitory effect at lower densities.
Based on these findings, we suggest and discuss several ECM targeted pro- and anti-angiogenesis therapies that can be tested empirically.
The rise of multi-drug resistant and extensively drug resistant tuberculosis around the world, including in industrialized nations, poses a great threat to human health and defines a need to develop new, effective and inexpensive anti-tubercular agents.
Previously we developed a chemical systems biology approach to identify off-targets of major pharmaceuticals on a proteome-wide scale.
In this paper we further demonstrate the value of this approach through the discovery that existing commercially available drugs, prescribed for the treatment of Parkinson's disease, have the potential to treat MDR and XDR tuberculosis.
These drugs, entacapone and tolcapone, are predicted to bind to the enzyme InhA and directly inhibit substrate binding.
The prediction is validated by in vitro and InhA kinetic assays using tablets of Comtan, whose active component is entacapone.
The minimal inhibition concentration of entacapone for Mycobacterium tuberculosis is approximately 260.0 M, well below the toxicity concentration determined by an in vitro cytotoxicity model using a human neuroblastoma cell line.
Moreover, kinetic assays indicate that Comtan inhibits InhA activity by 47.0 percent at an entacapone concentration of approximately 80 M. Thus the active component in Comtan represents a promising lead compound for developing a new class of anti-tubercular therapeutics with excellent safety profiles.
More generally, the protocol described in this paper can be included in a drug discovery pipeline in an effort to discover novel drug leads with desired safety profiles, and therefore accelerate the development of new drugs.
Tuberculosis, which is caused by the bacterial pathogen Mycobacterium tuberculosis, is a leading cause of mortality among infectious diseases.
It has been estimated by the World Health Organization that almost one-third of the world's population, around 2 billion people, is infected with the disease CITATION.
Every year, more than 8 million people develop an active form of the disease, which subsequently claims the lives of nearly 2 million.
This translates to over 4,900 deaths per day, and more than 95 percent of these are in developing countries CITATION.
In 2002, the WHO estimated that if the worldwide spread of tuberculosis was left unchecked, then the disease would be responsible for approximately 36 million more deaths by the year 2020.
Despite the current global situation, anti-tubercular drugs have remained largely unchanged over the last four decades CITATION.
The widespread use of these agents, and the time needed to remove infection, has promoted the emergence of resistant M.tuberculosis strains.
Multi-drug resistant tuberculosis is defined as resistance to the first-line drugs isoniazid and rifampin.
The effective treatment of MDR-TB necessitates the long-term use of second-line drug combinations, an unfortunate consequence of which is the emergence of extensively drug resistant tuberculosis M.tuberculosis strains that are resistant to isoniazid plus rifampin, as well as key second-line drugs, such as ciprofloxacin and moxifloxacin.
XDR-TB is extremely difficult to treat because the only remaining drug classes exhibit very low potency and high toxicity.
The rise of XDR-TB around the world, including in industrialized nations, imposes a great threat on human health, therefore emphasizing the need to identify new anti-tubercular agents as an urgent priority CITATION .
Currently, anti-infective therapeutics are discovered and developed by either de novo strategies, or through the extension of available chemical compounds that target protein families with the same or similar structures and functions.
De novo drug discovery involves the use of high throughput screening techniques to identify new compounds, both synthetic and natural, as novel drugs.
Unfortunately, this approach has yielded very few successes in the field of anti-infective drug discovery CITATION.
Indeed, the progression from early-stage biochemical hits to robust lead compounds is commonly an unfruitful process.
The identification of both molecular targets that are essential for the survival of the pathogen, and compounds that are active on intact cells, is a challenging task.
Even more formidable, however, is the requirement for appropriate potency levels and suitable pharmacokinetics, in order to achieve efficacy in small animal disease models CITATION.
These challenges are reflected in the high costs involved in bringing new drugs to market.
In fact, it has been estimated that the successful launch of a single new drug costs more than US 800 million CITATION .
Two alternative drug discovery strategies that circumvent some of the challenges associated with de novo drug discovery are the label extension and piggy-back strategies, both of which are widely employed for the discovery of novel therapeutics to treat tropical diseases.
Label extension is a fast-track approach that involves the extension of the indications of an existing treatment to another disease.
Some of the most important anti-parasitic drugs in use today, such as praziquantel for schistosomiasis, were derived from the label extension process.
The major advantages of label extension are the significant reductions in cost and time to market that can be achieved.
Alternatively, when a molecular target that is present in a pathogen is under investigation for other commercial indications, it is possible to adopt the piggy-back strategy by utilizing the identified chemical starting points.
Examples of this approach include the anti-malarial screening of a lead series of cysteine protease inhibitors for the treatment of osteoporosis, and histone deacetylase inhibitors for use in cancer chemotherapy CITATION .
One of the main aims of drug discovery is to develop safe and effective therapeutic agents through the optimization of binding to a specific protein target.
In this way, undesirable effects resulting from side scatter pharmacology are minimized.
However, the recent and rapid completion of numerous genome sequencing projects has revealed that proteins involved in entirely different biochemical pathways, and even residing in different tissues and organs, may possess functional binding pockets with similar shapes and physiochemical properties CITATION.
Therefore, chemical matter for one target could be considered as the basis for leads for an entirely different target.
Recent work on large scale mapping of polypharmacology interactions by Paolini et al. CITATION revealed the extent of promiscuity of drugs and leads across the proteome.
They discovered that around 35 percent of 276,122 active compounds in their database had observed activity for more than one target.
Whilst the majority of these promiscuous compounds were found to be active against targets within the same gene family, a significant number had recorded activity across different gene families.
The finding that so many drugs interact with more than one target provided the rationale behind the selective optimization of side activities approach recently developed by Wermuth CITATION, CITATION.
The SOSA approach involves the use of old drugs for new pharmacological targets, which is a valuable concept considering the finite number of small molecules that can be safely administered to humans.
The process itself involves screening a limited number of structurally diverse drug molecules, and then optimizing the hits so that they show a stronger affinity for the new target and a weaker affinity for the original target.
In this way, it is possible to derive a whole panel of new active molecules from a single marketed drug.
Since the screened drug molecules already have known safety and bioavailability in humans, the overall time and cost of drug discovery is significantly reduced when compared with de novo strategies.
We have developed a novel computational strategy to identify off-targets of major pharmaceuticals on a proteome-wide scale CITATION CITATION.
Our methodology extends the scope of the SOSA concept effectively and systematically across gene families, and is more likely to be successful in achieving the ultimate goal of providing new drugs from old ones.
Our chemical systems biology approach proceeds as follows:
The binding site of a commercially available drug is extracted or predicted from a 3D structure or model of the target protein CITATION .
Off-targets with similar ligand binding sites are identified across the proteome using an efficient and accurate functional site search algorithm CITATION .
Atomic interactions between the putative off-targets and the drug are evaluated using protein-ligand docking.
Only those off-targets that do not experience serious atomic clashes with the drug are selected for further analysis.
The drug is further optimized to enhance its potency, selectivity and ADME properties by taking into account both the primary target and the off-targets across the genome.
Our approach essentially explores complex protein-ligand interaction networks on a proteome-wide scale.
The lead compound can be discovered from all drug targets across different gene families.
Moreover, lead optimization can focus on compounds with excellent safety profiles and known clinical outcomes.
In this way, our approach has the potential to increase the rate of successful drug discovery and development, whilst reducing the costs involved.
In the present study we demonstrate the efficiency and efficacy of our chemical systems biology approach through the discovery of safe chemical compounds with the potential to treat MDR-TB and XDR-TB.
The identified compounds are entacapone and tolcapone.
These drugs primarily target human catechol-O-methyltransferase, which is involved in the breakdown of catecholamine neurotransmitters such as dopamine.
They are used as adjuncts to treat Parkinson's disease by increasing the bioavailability of the primary drug levodopa, which is a substrate of COMT.
Entacapone and tolcapone are predicted to inhibit M.tuberculosis enoyl-acyl carrier protein reductase, which is essential for type II fatty acid biosynthesis and the subsequent synthesis of the bacterial cell wall CITATION.
InhA is the target of the anti-tubercular drugs isoniazid CITATION and ethionamide CITATION.
Similar to newly developed direct InhA inhibitors CITATION, CITATION CITATION, entacapone and tolcapone require no enzymatic activation to bind InhA.
Thus they may avoid the commonly observed resistance mechanism to isoniazid and ethionamide that is exhibited by many MDR strains.
Our computational predictions have been partially validated by demonstrating that the entacapone drug tablet Comtan inhibits the growth of M.tuberculosis at the minimal inhibition concentration of entacapone of 260.0 M, well below the concentration leading to neuroblastoma cellular toxicity.
The direct inhibition of InhA by entacapone is further confirmed by experimental enzyme kinetic assays, in which Comtan is shown to reduce InhA activity by up to 47 percent at the effective entacapone concentration of 80 M. Since entacapone has an excellent safety profile with few side effects, it shows potential as a drug lead in the development of a new class of anti-tubercular therapeutics with favorable ADME/Tox properties.
Some studies suggest that complex arm movements in humans and monkeys may optimize several objective functions, while others claim that arm movements satisfy geometric constraints and are composed of elementary components.
However, the ability to unify different constraints has remained an open question.
The criterion for a maximally smooth motion is satisfied for parabolic trajectories having constant equi-affine speed, which thus comply with the geometric constraint known as the two-thirds power law.
Here we empirically test the hypothesis that parabolic segments provide a compact representation of spontaneous drawing movements.
Monkey scribblings performed during a period of practice were recorded.
Practiced hand paths could be approximated well by relatively long parabolic segments.
Following practice, the orientations and spatial locations of the fitted parabolic segments could be drawn from only 2 4 clusters, and there was less discrepancy between the fitted parabolic segments and the executed paths.
This enabled us to show that well-practiced spontaneous scribbling movements can be represented as sequences of a small number of elementary parabolic primitives.
A movement primitive can be defined as a movement entity that cannot be intentionally stopped before its completion.
We found that in a well-trained monkey a movement was usually decelerated after receiving a reward, but it stopped only after the completion of a sequence composed of several parabolic segments.
Piece-wise parabolic segments can be generated by applying affine geometric transformations to a single parabolic template.
Thus, complex movements might be constructed by applying sequences of suitable geometric transformations to a few templates.
Our findings therefore suggest that the motor system aims at achieving more parsimonious internal representations through practice, that parabolas serve as geometric primitives and that non-Euclidean variables are employed in internal movement representations .
Despite decades of research on the formation of human hand trajectories, the basic mechanisms of neuromotor control underlying the generation of even the simplest drawing movements remain poorly understood CITATION.
Various studies have proposed that human movement preparation aims at optimizing either kinematic CITATION CITATION or dynamic CITATION criteria, or minimizing movement variance CITATION CITATION.
Studies in vertebrates have suggested that voluntary movements are composed of basic movement elements combined in parallel or sequentially CITATION CITATION.
Such modular organization can account for the versatility of animal and human movements and for their ability to acquire new skills.
Geometrically invariant properties of drawing movements were formalized by the two-thirds power law CITATION.
These kinematic constraints were shown to hold both with respect to movement production CITATION and perception CITATION, CITATION.
Earlier studies also showed that the two-thirds power law is equivalent to moving at a constant equi-affine speed CITATION CITATION and there is psychophysical and neurophysiological evidence for the significant role of the invariance of human motion with respect to equi-affine transformations CITATION CITATION.
We argue that geometric invariance may provide a more compact representation of complex movements composed of geometric primitives.
Straight point-to-point movements show geometric invariance under dynamic perturbations involving the use of either elastic or viscous loads CITATION, CITATION.
Point-to-point movements retain the invariance of their geometric properties even when subjects are required to control the movements of a cursor on a computer screen by moving their fingers in an instrumented data glove CITATION.
Recent studies in monkeys CITATION, CITATION, CITATION and humans CITATION have indicated that repeatable geometric shapes used in the construction of complex trajectories emerge after extensive practice in the generation of drawing and sequential movements.
The ability to unify different kinds of movement constraints in the modeling of human and animal movements could lead to further insights CITATION, CITATION.
Parabolic movement primitives meet the demands of geometric invariance, kinematic optimality of movements and simplicity of movement representation, and may subserve as underlying building blocks in arm trajectory formation CITATION, CITATION.
Here, the hypothesis that parabolic segments are geometric primitives in practiced movements was experimentally tested using spontaneous scribbling movements made by two monkeys.
Our choice of the source of the data was motivated by the feasibility of subsequently analyzing the underlying activity of motor cortical neurons CITATION .
The predictions of both the two-thirds power law CITATION and the constrained minimum-jerk model CITATION are identical for a single parabolic stroke CITATION, CITATION.
The fit of the recorded trajectories to the predictions of these two models was assessed and is described in detail in Text S1.
Preliminary version of our findings was presented at the Tenth Biennial Conference of the International Graphonomics Society in 2001 and at the Computational Motor Control Workshops at Ben-Gurion University in 2005 and 2006.
T cell populations are regulated both by signals specific to the T-cell receptor and by signals and resources, such as cytokines and space, that act independently of TCR specificity.
Although it has been demonstrated that disruption of either of these pathways has a profound effect on T-cell development, we do not yet have an understanding of the dynamical interactions of these pathways in their joint shaping of the T cell repertoire.
Complete DiGeorge Anomaly is a developmental abnormality that results in the failure of the thymus to develop, absence of T cells, and profound immune deficiency.
After receiving thymic tissue grafts, patients suffering from DiGeorge anomaly develop T cells derived from their own precursors but matured in the donor tissue.
We followed three DiGeorge patients after thymus transplantation to utilize the remarkable opportunity these subjects provide to elucidate human T-cell developmental regulation.
Our goal is the determination of the respective roles of TCR-specific vs. TCR-nonspecific regulatory signals in the growth of these emerging T-cell populations.
During the course of the study, we measured peripheral blood T-cell concentrations, TCR V gene-segment usage and CDR3-length spectratypes over two years or more for each of the subjects.
We find, through statistical analysis based on a novel stochastic population-dynamic T-cell model, that the carrying capacity corresponding to TCR-specific resources is approximately 1000-fold larger than that of TCR-nonspecific resources, implying that the size of the peripheral T-cell pool at steady state is determined almost entirely by TCR-nonspecific mechanisms.
Nevertheless, the diversity of the TCR repertoire depends crucially on TCR-specific regulation.
The estimated strength of this TCR-specific regulation is sufficient to ensure rapid establishment of TCR repertoire diversity in the early phase of T cell population growth, and to maintain TCR repertoire diversity in the face of substantial clonal expansion-induced perturbation from the steady state.
An essential characteristic of T lymphocytes is their ability, as a population, to recognize an enormous number of peptide antigens.
This capability is essential to the function of the adaptive immune system and is attributable to the diversity of the T-cell receptors they express.
This diversity in turn comes about through the stochastic assembly of TCR from genomic libraries of gene segments for the TCR alpha and beta chains, the two polypeptides that together make up the most common form of the TCR CITATION CITATION.
This rearrangement process takes place in the thymus and is the ultimate source of TCR repertoire diversity, though many other forces shape the raw materials thus provided CITATION.
It is with these other forces that we are concerned in this study.
The peripheral T-cell population is maintained at a constant size in spite of substantial, continual turnover, ongoing thymic production, and clonal expansion in response to immunological challenges CITATION CITATION.
Although a detailed understanding of T-cell homeostasis is not yet complete, it is clear that the maintenance of T cell population size results from the interplay of several mechanisms acting both to enhance population growth at low population density and to limit population growth at high population density CITATION, CITATION.
In a healthy individual, and in the absence of overt immune activation, the rate of T cell division in the periphery is small and is regulated through competition for resources including growth signals.
In a T cell-deficient host, however, so-called lymphopenia-induced proliferation rapidly restores the system to steady-state numbers CITATION .
Diverse lines of investigation imply that signals delivered, and resources provided, through both TCR-specific and TCR-nonspecific channels are essential for the establishment and maintenance of the size and diversity of T cell populations CITATION, CITATION.
Experiments performed thus far have been performed in non-human animals and represent limiting cases in which one or more of these signals or resources is entirely eliminated.
We are interested in understanding the phenomena in humans and in a more natural setting, in which regulation via both TCR-specific and TCR-non-specific mechanisms are intact and in their interplay jointly determine the size of the T cell population and the diversity of the TCR repertoire.
Our intent in this paper is to elucidate the mechanisms that lead to expansion and diversification of the TCR repertoire in a recovering T-cell-deficient host: in our case, a complete DiGeorge subject that has received an unrelated, unmatched thymus transplant.
After transplantation, host T-cell precursors migrate from the bone marrow to the donor thymus where they develop via thymopoiesis into host T cells that then emigrate into the peripheral blood.
Here, T cells have the capacity to undergo spontaneous clonal expansion, thus leading to restoration of the peripheral T-cell pool.
The expansion continues until a steady state is reached.
Throughout this process, selective pressure for survival emerges among and within the clones through competition for stimulatory signals.
The steady state T-cell population size arises in the balance among several phenomena, including the rapid and extensive expansion of rare clones through activation-induced peripheral division and their subsequent contraction and the relatively slow turnover of a diverse pool of naive cells through continuous thymic emigration and cell death.
The specific memory T cells that arise as the result of clonal expansion appear to be regulated largely independently of the naive cells CITATION.
T cells arising through lymphopenia-induced proliferation acquire markers that ordinarily indicate a memory phenotype and may be regulated as memory cells, though this hypothesis has not been definitively tested.
Both the size and diversity of peripheral T cell populations are controlled through competition for limiting resources.
CITATION, CITATION, and may also be controlled directly by the activities of regulatory T cells CITATION.
It has been shown, for example, that normal T-cell population growth is dependent on stimulation by self-peptide major histocompatibility complex complexes through the TCR CITATION CITATION, requiring a TCR that is specific for the spMHC complex.
But T-cell population growth also depends on cytokines such as IL7 and IL15 that act independently of TCR specificity CITATION, CITATION CITATION.
Furthermore, growth and survival in all cells require adequate space and nutrients, the utilization of which is independent of TCR specificity CITATION, CITATION .
Our current understanding of lymphopenia-induced proliferation is due to studies in mice demonstrating that T cells divide rapidly after transfer into T cell-deficient or irradiated mice, but not after transfer to normal mice CITATION, CITATION.
Moreover, overall T cell numbers in T cell-deficient animals after transfer and clonal expansion are similar to T cell numbers in normal animals, suggesting control mechanisms acting on total T cell numbers, rather than in a clone-specific manner.
The importance of TCR specific signals has been studied at length, showing that competition within T cell clones is important in maintaining TCR repertoire diversity.
It has been shown, for example, that in a T-cell-deficient host, a T cell must interact with antigen-presenting cells bearing the MHC allele responsible for that cell's thymic selection in order to proliferate CITATION.
In a T-cell sufficient host, such TCR-spMHC interaction is necessary for T cell survival CITATION, CITATION.
Furthermore, naive polyclonal T cells divide when transferred to TCR-transgenic hosts, as do monoclonal T cells transferred to TCR-transgenic hosts of differing clonotype.
T cells do not divide, however, in hosts of identical clonotype CITATION.
Mice lacking MHC class II expression do not repopulate the periphery with CD4 T cells at all, suggesting that peripheral MHC class II expression is needed for the survival of CD4 T cells CITATION.
In the present context it is important to note that MHC class II matching in thymic grafts for complete DiGeorge subjects is not necessary for the development of CD4 T cells CITATION .
TCR-nonspecific signals include cytokines such as the cytokine interleukin-7, which is necessary for the survival of nave T cells CITATION CITATION.
Lymphopenia-induced proliferation of memory cells requires IL7 or IL15 CITATION, CITATION.
T cells that have lost the ability to respond to IL7 after leaving the thymus are no longer able to proliferate, produce cytokines, or acquire memory cell phenotype CITATION.
In mice in which IL7 signaling has been completely abrogated, the few mature T cells found in the peripheral blood behave abnormally CITATION, CITATION, CITATION.
Experiments in IL7-receptor FORMULA -deficient mice have shown a reduction in T-cell capacity to proliferate upon stimulation, leading to a six- to seven-fold reduction in the frequency of clonogenic T cells compared with T cells from IL7R-sufficient mice, as well as a 50 percent reduction in the average clone size of single IL7R / T cells compared with the IL7R / T cells CITATION.
In another study, mice lacking the interleukin 2 receptor FORMULA chain and/or the Jak family tyrosine kinase had severe combined immune defects with lack of T lymphocyte maturation and function.
This phenomenon is presumably attributable to the fact that FORMULA is part of the receptor for IL7 and IL15 CITATION ; its loss leads to the abrogation of both of these cytokine signaling pathways CITATION, CITATION, and others as well.
Moreover, in humans, in the absence of of FORMULA or of the Jak-3 family, the periphery lacks T cells completely CITATION, CITATION .
To assess the contributions of thymic emigration rate on the steady-state T-cell population size, Berzins et al CITATION engrafted variable numbers of thymuses into mice and observed that the size of the naive T-cell population increased in proportion to the number of thymic grafts, while the size of the memory population remained unchanged.
It may be of importance to note that the thymus grafts themselves produced IL7.
In humans, transplantation of thymic tissue at varying doses into complete DiGeorge anomaly subjects showed no significant effect on the nave CD4 or CD8 T cell numbers CITATION .
